# Comparing `tmp/flash_attn-1.0.1.tar.gz` & `tmp/flash_attn-1.0.2.tar.gz`

## filetype from file(1)

```diff
@@ -1 +1 @@
-gzip compressed data, was "flash_attn-1.0.1.tar", last modified: Wed Apr 12 17:06:34 2023, max compression
+gzip compressed data, was "flash_attn-1.0.2.tar", last modified: Sun Apr 16 05:00:30 2023, max compression
```

## Comparing `flash_attn-1.0.1.tar` & `flash_attn-1.0.2.tar`

### file list

```diff
@@ -1,1699 +1,1777 @@
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 17:06:34.953451 flash_attn-1.0.1/
--rw-r--r--   0 root         (0) root         (0)       56 2022-11-17 23:40:55.000000 flash_attn-1.0.1/AUTHORS
--rw-r--r--   0 root         (0) root         (0)     1558 2022-09-09 19:08:03.000000 flash_attn-1.0.1/LICENSE
--rw-r--r--   0 root         (0) root         (0)      251 2023-04-12 06:28:28.000000 flash_attn-1.0.1/MANIFEST.in
--rw-rw-r--   0 root         (0) root         (0)    10037 2023-04-12 17:06:34.945114 flash_attn-1.0.1/PKG-INFO
--rw-r--r--   0 root         (0) root         (0)     9529 2023-04-12 06:31:27.000000 flash_attn-1.0.1/README.md
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 17:05:54.501406 flash_attn-1.0.1/csrc/
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 17:05:54.664807 flash_attn-1.0.1/csrc/flash_attn/
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 17:05:54.252994 flash_attn-1.0.1/csrc/flash_attn/cutlass/
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 17:05:54.691747 flash_attn-1.0.1/csrc/flash_attn/cutlass/cmake/
--rw-r--r--   0 root         (0) root         (0)     2023 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/cmake/nop.cu
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 17:05:53.848846 flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 17:05:54.718983 flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/00_basic_gemm/
--rw-r--r--   0 root         (0) root         (0)    14698 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/00_basic_gemm/basic_gemm.cu
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 17:05:54.743425 flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/01_cutlass_utilities/
--rw-r--r--   0 root         (0) root         (0)    13255 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/01_cutlass_utilities/cutlass_utilities.cu
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 17:05:54.767196 flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/02_dump_reg_shmem/
--rw-r--r--   0 root         (0) root         (0)     7157 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/02_dump_reg_shmem/dump_reg_shmem.cu
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 17:05:54.862653 flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/03_visualize_layout/
--rw-r--r--   0 root         (0) root         (0)     4478 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/03_visualize_layout/options.h
--rw-r--r--   0 root         (0) root         (0)     6938 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/03_visualize_layout/register_layout.cu
--rw-r--r--   0 root         (0) root         (0)     2691 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/03_visualize_layout/register_layout.h
--rw-r--r--   0 root         (0) root         (0)     5819 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/03_visualize_layout/visualize_layout.cpp
--rw-r--r--   0 root         (0) root         (0)    11415 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/03_visualize_layout/visualize_layout.h
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 17:05:54.885714 flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/04_tile_iterator/
--rw-r--r--   0 root         (0) root         (0)     8226 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/04_tile_iterator/tile_iterator.cu
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 17:05:54.909559 flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/05_batched_gemm/
--rw-r--r--   0 root         (0) root         (0)    15161 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/05_batched_gemm/batched_gemm.cu
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 17:05:54.933226 flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/06_splitK_gemm/
--rw-r--r--   0 root         (0) root         (0)    17570 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/06_splitK_gemm/splitk_gemm.cu
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 17:05:54.961495 flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/07_volta_tensorop_gemm/
--rw-r--r--   0 root         (0) root         (0)    18280 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/07_volta_tensorop_gemm/volta_tensorop_gemm.cu
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 17:05:55.085657 flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/08_turing_tensorop_gemm/
--rw-r--r--   0 root         (0) root         (0)    18226 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/08_turing_tensorop_gemm/turing_tensorop_gemm.cu
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 17:05:55.112313 flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/09_turing_tensorop_conv2dfprop/
--rw-r--r--   0 root         (0) root         (0)    28124 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/09_turing_tensorop_conv2dfprop/turing_tensorop_conv2dfprop.cu
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 17:05:55.138242 flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/10_planar_complex/
--rw-r--r--   0 root         (0) root         (0)    21947 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/10_planar_complex/planar_complex.cu
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 17:05:55.161586 flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/11_planar_complex_array/
--rw-r--r--   0 root         (0) root         (0)    23244 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/11_planar_complex_array/planar_complex_array.cu
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 17:05:55.184950 flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/12_gemm_bias_relu/
--rw-r--r--   0 root         (0) root         (0)    13151 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/12_gemm_bias_relu/gemm_bias_relu.cu
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 17:05:55.882240 flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/
--rw-r--r--   0 root         (0) root         (0)    26102 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/b2b_conv2d_run.h
--rw-r--r--   0 root         (0) root         (0)    22877 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/b2b_gemm_run.h
--rw-r--r--   0 root         (0) root         (0)    28268 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/b2b_interleaved_conv2d_run.h
--rw-r--r--   0 root         (0) root         (0)    24493 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/b2b_interleaved_gemm_run.h
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 17:05:55.924025 flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/device/
--rw-r--r--   0 root         (0) root         (0)    15552 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/device/b2b_gemm.h
--rw-r--r--   0 root         (0) root         (0)    11520 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/device/b2b_implicit_gemm_convolution.h
--rw-r--r--   0 root         (0) root         (0)     8756 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/fused_two_convs_f16_sm75_rf.cu
--rw-r--r--   0 root         (0) root         (0)     8759 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/fused_two_convs_f16_sm75_shmem.cu
--rw-r--r--   0 root         (0) root         (0)     8712 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/fused_two_convs_f16_sm80_rf.cu
--rw-r--r--   0 root         (0) root         (0)     8762 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/fused_two_convs_f16_sm80_shmem.cu
--rw-r--r--   0 root         (0) root         (0)     8787 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/fused_two_convs_s8_sm75_rf.cu
--rw-r--r--   0 root         (0) root         (0)     8793 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/fused_two_convs_s8_sm75_shmem.cu
--rw-r--r--   0 root         (0) root         (0)     8711 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/fused_two_convs_s8_sm80_rf.cu
--rw-r--r--   0 root         (0) root         (0)     8775 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/fused_two_convs_s8_sm80_shmem.cu
--rw-r--r--   0 root         (0) root         (0)     7269 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/fused_two_gemms_f16_sm75_rf.cu
--rw-r--r--   0 root         (0) root         (0)     7338 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/fused_two_gemms_f16_sm75_shmem.cu
--rw-r--r--   0 root         (0) root         (0)     7294 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/fused_two_gemms_f16_sm80_rf.cu
--rw-r--r--   0 root         (0) root         (0)     7359 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/fused_two_gemms_f16_sm80_shmem.cu
--rw-r--r--   0 root         (0) root         (0)     7362 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/fused_two_gemms_s8_sm75_rf.cu
--rw-r--r--   0 root         (0) root         (0)     7430 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/fused_two_gemms_s8_sm75_shmem.cu
--rw-r--r--   0 root         (0) root         (0)     7627 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/fused_two_gemms_s8_sm80_rf.cu
--rw-r--r--   0 root         (0) root         (0)     7634 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/fused_two_gemms_s8_sm80_shmem.cu
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 17:05:56.298799 flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/kernel/
--rw-r--r--   0 root         (0) root         (0)    16152 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/kernel/b2b_gemm.h
--rw-r--r--   0 root         (0) root         (0)    18151 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/kernel/b2b_implicit_gemm_convolution.h
--rw-r--r--   0 root         (0) root         (0)     3973 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/kernel/default_b2b_conv2d_fprop.h
--rw-r--r--   0 root         (0) root         (0)    26762 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/kernel/default_b2b_conv2d_fprop_sm75.h
--rw-r--r--   0 root         (0) root         (0)    26775 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/kernel/default_b2b_conv2d_fprop_sm80.h
--rw-r--r--   0 root         (0) root         (0)    28422 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/kernel/default_b2b_conv2d_fprop_smem_accumulator_sm75.h
--rw-r--r--   0 root         (0) root         (0)    28073 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/kernel/default_b2b_conv2d_fprop_smem_accumulator_sm80.h
--rw-r--r--   0 root         (0) root         (0)    17111 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/kernel/default_b2b_gemm.h
--rw-r--r--   0 root         (0) root         (0)    15658 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/kernel/default_b2b_gemm_smem_accumulator.h
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 17:05:53.556494 flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/reference/
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 17:05:56.332342 flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/reference/device/
--rw-r--r--   0 root         (0) root         (0)    10368 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/reference/device/tensor_scale_bias.h
--rw-r--r--   0 root         (0) root         (0)     3577 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/test_run.h
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 17:05:56.781968 flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/threadblock/
--rw-r--r--   0 root         (0) root         (0)    31616 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/threadblock/b2b_implicit_gemm_multistage.h
--rw-r--r--   0 root         (0) root         (0)    31443 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/threadblock/b2b_implicit_gemm_multistage_smem_accumulator.h
--rw-r--r--   0 root         (0) root         (0)    21010 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/threadblock/b2b_implicit_gemm_pipelined.h
--rw-r--r--   0 root         (0) root         (0)    20493 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/threadblock/b2b_implicit_gemm_pipelined_smem_accumulator.h
--rw-r--r--   0 root         (0) root         (0)     7983 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/threadblock/b2b_mma_base.h
--rw-r--r--   0 root         (0) root         (0)     6047 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/threadblock/b2b_mma_base_smem_accumulator.h
--rw-r--r--   0 root         (0) root         (0)    33824 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/threadblock/b2b_mma_multistage.h
--rw-r--r--   0 root         (0) root         (0)    33518 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/threadblock/b2b_mma_multistage_smem_accumulator.h
--rw-r--r--   0 root         (0) root         (0)    21451 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/threadblock/b2b_mma_pipelined.h
--rw-r--r--   0 root         (0) root         (0)    21065 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/threadblock/b2b_mma_pipelined_smem_accumulator.h
--rw-r--r--   0 root         (0) root         (0)    27144 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/threadblock/default_b2b_mma.h
--rw-r--r--   0 root         (0) root         (0)    27400 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/threadblock/default_b2b_mma_smem_accumulator.h
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 17:05:56.806412 flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/14_ampere_tf32_tensorop_gemm/
--rw-r--r--   0 root         (0) root         (0)    18020 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/14_ampere_tf32_tensorop_gemm/ampere_tf32_tensorop_gemm.cu
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 17:05:56.830679 flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/15_ampere_sparse_tensorop_gemm/
--rw-r--r--   0 root         (0) root         (0)    15042 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/15_ampere_sparse_tensorop_gemm/ampere_sparse_tensorop_gemm.cu
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 17:05:56.855014 flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/16_ampere_tensorop_conv2dfprop/
--rw-r--r--   0 root         (0) root         (0)    27755 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/16_ampere_tensorop_conv2dfprop/ampere_tensorop_conv2dfprop.cu
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 17:05:56.879154 flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/17_fprop_per_channel_bias/
--rw-r--r--   0 root         (0) root         (0)    12580 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/17_fprop_per_channel_bias/fprop_per_channel_bias.cu
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 17:05:56.904258 flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/18_ampere_fp64_tensorop_affine2_gemm/
--rw-r--r--   0 root         (0) root         (0)    14007 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/18_ampere_fp64_tensorop_affine2_gemm/ampere_fp64_tensorop_affine2_gemm.cu
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 17:05:56.928016 flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/19_tensorop_canonical/
--rw-r--r--   0 root         (0) root         (0)    13401 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/19_tensorop_canonical/tensorop_canonical.cu
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 17:05:56.955358 flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/20_simt_canonical/
--rw-r--r--   0 root         (0) root         (0)    12556 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/20_simt_canonical/simt_canonical.cu
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 17:05:56.980547 flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/21_quaternion_gemm/
--rw-r--r--   0 root         (0) root         (0)    17319 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/21_quaternion_gemm/quaternion_gemm.cu
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 17:05:57.004357 flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/22_quaternion_conv/
--rw-r--r--   0 root         (0) root         (0)    21495 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/22_quaternion_conv/quaternion_conv.cu
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 17:05:57.028337 flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/23_ampere_gemm_operand_reduction_fusion/
--rw-r--r--   0 root         (0) root         (0)    27520 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/23_ampere_gemm_operand_reduction_fusion/ampere_gemm_operand_reduction_fusion.cu
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 17:05:57.053094 flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/24_gemm_grouped/
--rw-r--r--   0 root         (0) root         (0)    50996 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/24_gemm_grouped/gemm_grouped.cu
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 17:05:57.095680 flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/25_ampere_fprop_mainloop_fusion/
--rw-r--r--   0 root         (0) root         (0)    26547 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/25_ampere_fprop_mainloop_fusion/ampere_3d_fprop_mainloop_fusion.cu
--rw-r--r--   0 root         (0) root         (0)    25628 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/25_ampere_fprop_mainloop_fusion/ampere_fprop_mainloop_fusion.cu
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 17:05:57.123245 flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/26_ampere_wgrad_mainloop_fusion/
--rw-r--r--   0 root         (0) root         (0)    25538 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/26_ampere_wgrad_mainloop_fusion/ampere_wgrad_mainloop_fusion.cu
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 17:05:57.148679 flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/27_ampere_3xtf32_fast_accurate_tensorop_gemm/
--rw-r--r--   0 root         (0) root         (0)    30446 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/27_ampere_3xtf32_fast_accurate_tensorop_gemm/27_ampere_3xtf32_fast_accurate_tensorop_gemm.cu
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 17:05:57.271018 flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/28_ampere_3xtf32_fast_accurate_tensorop_fprop/
--rw-r--r--   0 root         (0) root         (0)    28159 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/28_ampere_3xtf32_fast_accurate_tensorop_fprop/ampere_3xtf32_fast_accurate_tensorop_fprop.cu
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 17:05:57.294793 flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/29_ampere_3xtf32_fast_accurate_tensorop_complex_gemm/
--rw-r--r--   0 root         (0) root         (0)    28403 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/29_ampere_3xtf32_fast_accurate_tensorop_complex_gemm/29_ampere_3xtf32_fast_accurate_tensorop_complex_gemm.cu
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 17:05:57.320237 flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/30_wgrad_split_k/
--rw-r--r--   0 root         (0) root         (0)    27335 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/30_wgrad_split_k/30_wgrad_split_k.cu
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 17:05:57.345085 flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/31_basic_syrk/
--rw-r--r--   0 root         (0) root         (0)    15206 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/31_basic_syrk/basic_syrk.cu
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 17:05:57.370315 flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/32_basic_trmm/
--rw-r--r--   0 root         (0) root         (0)    15907 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/32_basic_trmm/basic_trmm.cu
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 17:05:57.393911 flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/33_ampere_3xtf32_tensorop_symm/
--rw-r--r--   0 root         (0) root         (0)    31803 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/33_ampere_3xtf32_tensorop_symm/ampere_3xtf32_tensorop_symm.cu
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 17:05:57.421084 flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/34_transposed_conv2d/
--rw-r--r--   0 root         (0) root         (0)    22378 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/34_transposed_conv2d/34_transposed_conv2d.cu
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 17:05:57.486360 flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/35_gemm_softmax/
--rw-r--r--   0 root         (0) root         (0)    23114 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/35_gemm_softmax/gemm_softmax.cu
--rw-r--r--   0 root         (0) root         (0)    16900 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/35_gemm_softmax/gemm_with_epilogue_visitor.h
--rw-r--r--   0 root         (0) root         (0)    18713 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/35_gemm_softmax/gemm_with_softmax.h
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 17:05:57.510391 flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/36_gather_scatter_fusion/
--rw-r--r--   0 root         (0) root         (0)    20795 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/36_gather_scatter_fusion/gather_scatter_fusion.cu
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 17:05:57.576023 flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/37_gemm_layernorm_gemm_fusion/
--rw-r--r--   0 root         (0) root         (0)    31111 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/37_gemm_layernorm_gemm_fusion/gemm_layernorm.cu
--rw-r--r--   0 root         (0) root         (0)    14159 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/37_gemm_layernorm_gemm_fusion/gemm_with_epilogue_visitor.h
--rw-r--r--   0 root         (0) root         (0)    33916 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/37_gemm_layernorm_gemm_fusion/gemm_with_layernorm.h
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 17:05:57.603054 flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/38_syr2k_grouped/
--rw-r--r--   0 root         (0) root         (0)    47455 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/38_syr2k_grouped/syr2k_grouped.cu
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 17:05:57.627710 flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/39_gemm_permute/
--rw-r--r--   0 root         (0) root         (0)    37879 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/39_gemm_permute/gemm_permute.cu
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 17:05:58.015760 flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/41_fused_multi_head_attention/
--rw-r--r--   0 root         (0) root         (0)    18389 2023-01-27 22:40:09.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/41_fused_multi_head_attention/attention_scaling_coefs_updater.h
--rw-r--r--   0 root         (0) root         (0)    11865 2023-03-13 04:24:25.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/41_fused_multi_head_attention/debug_utils.h
--rw-r--r--   0 root         (0) root         (0)     9885 2023-03-13 04:24:25.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/41_fused_multi_head_attention/default_fmha_grouped.h
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 17:05:58.088245 flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/41_fused_multi_head_attention/epilogue/
--rw-r--r--   0 root         (0) root         (0)    22349 2023-03-13 04:24:25.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/41_fused_multi_head_attention/epilogue/epilogue_pipelined.h
--rw-r--r--   0 root         (0) root         (0)     9162 2023-03-13 04:24:25.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/41_fused_multi_head_attention/epilogue/epilogue_rescale_output.h
--rw-r--r--   0 root         (0) root         (0)     6111 2023-03-13 04:24:25.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/41_fused_multi_head_attention/epilogue/epilogue_thread_apply_logsumexp.h
--rw-r--r--   0 root         (0) root         (0)    22349 2023-01-27 22:40:09.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/41_fused_multi_head_attention/epilogue_pipelined.h
--rw-r--r--   0 root         (0) root         (0)     9162 2023-01-27 22:40:09.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/41_fused_multi_head_attention/epilogue_rescale_output.h
--rw-r--r--   0 root         (0) root         (0)     6111 2023-01-27 22:40:09.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/41_fused_multi_head_attention/epilogue_thread_apply_logsumexp.h
--rw-r--r--   0 root         (0) root         (0)     6768 2023-01-27 22:40:09.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/41_fused_multi_head_attention/find_default_mma.h
--rw-r--r--   0 root         (0) root         (0)    34379 2023-03-13 04:24:25.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/41_fused_multi_head_attention/fmha_grouped.h
--rw-r--r--   0 root         (0) root         (0)     6666 2023-01-27 22:40:09.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/41_fused_multi_head_attention/fmha_grouped_problem_visitor.h
--rw-r--r--   0 root         (0) root         (0)    38173 2023-03-13 04:24:25.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/41_fused_multi_head_attention/fused_multihead_attention_fixed_seqlen.cu
--rw-r--r--   0 root         (0) root         (0)    39997 2023-03-13 04:24:25.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/41_fused_multi_head_attention/fused_multihead_attention_variable_seqlen.cu
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 17:05:58.325726 flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/41_fused_multi_head_attention/gemm/
--rw-r--r--   0 root         (0) root         (0)     3994 2023-01-27 22:40:09.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/41_fused_multi_head_attention/gemm/custom_mma.h
--rw-r--r--   0 root         (0) root         (0)     6241 2023-01-27 22:40:09.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/41_fused_multi_head_attention/gemm/custom_mma_base.h
--rw-r--r--   0 root         (0) root         (0)    27198 2023-01-27 22:40:09.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/41_fused_multi_head_attention/gemm/custom_mma_multistage.h
--rw-r--r--   0 root         (0) root         (0)    14091 2023-03-13 04:24:25.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/41_fused_multi_head_attention/gemm/custom_mma_pipelined.h
--rw-r--r--   0 root         (0) root         (0)     6782 2023-03-13 04:24:25.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/41_fused_multi_head_attention/gemm/find_default_mma.h
--rw-r--r--   0 root         (0) root         (0)    13959 2023-03-13 04:24:25.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/41_fused_multi_head_attention/gemm/mma_accum_lambda_iterator.h
--rw-r--r--   0 root         (0) root         (0)    72984 2023-03-13 04:24:25.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/41_fused_multi_head_attention/gemm/mma_from_smem.h
--rw-r--r--   0 root         (0) root         (0)    10878 2023-03-13 04:24:25.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/41_fused_multi_head_attention/gemm_kernel_utils.h
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 17:05:58.453456 flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/41_fused_multi_head_attention/iterators/
--rw-r--r--   0 root         (0) root         (0)    23855 2023-03-13 04:24:25.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/41_fused_multi_head_attention/iterators/epilogue_predicated_tile_iterator.h
--rw-r--r--   0 root         (0) root         (0)     3142 2023-01-27 22:40:09.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/41_fused_multi_head_attention/iterators/make_residual_last.h
--rw-r--r--   0 root         (0) root         (0)    64480 2023-03-13 04:24:26.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/41_fused_multi_head_attention/iterators/predicated_tile_access_iterator_residual_last.h
--rw-r--r--   0 root         (0) root         (0)    64500 2023-01-27 22:40:09.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/41_fused_multi_head_attention/iterators/predicated_tile_iterator_residual_last.h
--rw-r--r--   0 root         (0) root         (0)     2435 2023-03-13 04:24:26.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/41_fused_multi_head_attention/iterators/transpose_warp_iterator.h
--rw-r--r--   0 root         (0) root         (0)     9497 2023-03-13 04:24:26.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/41_fused_multi_head_attention/iterators/warp_iterator_from_smem.h
--rw-r--r--   0 root         (0) root         (0)    48655 2023-03-13 04:24:26.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/41_fused_multi_head_attention/kernel_forward.h
--rw-r--r--   0 root         (0) root         (0)    61195 2023-01-27 22:40:09.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/41_fused_multi_head_attention/mma_from_smem.h
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 17:05:58.583253 flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/41_fused_multi_head_attention/transform/
--rw-r--r--   0 root         (0) root         (0)     3747 2023-03-13 04:24:26.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/41_fused_multi_head_attention/transform/tile_smem_loader.h
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 17:05:58.751608 flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/41_multi_head_attention/
--rw-r--r--   0 root         (0) root         (0)    37396 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/41_multi_head_attention/fused_multihead_attention.cu
--rw-r--r--   0 root         (0) root         (0)    17839 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/41_multi_head_attention/gemm_attention.h
--rw-r--r--   0 root         (0) root         (0)    16152 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/41_multi_head_attention/gemm_grouped_with_softmax_visitor.h
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 17:05:58.779737 flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/42_ampere_tensorop_group_conv/
--rw-r--r--   0 root         (0) root         (0)    23901 2023-01-27 22:40:09.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/42_ampere_tensorop_group_conv/ampere_tensorop_group_conv.cu
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 17:05:58.806032 flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/43_ell_block_sparse_gemm/
--rw-r--r--   0 root         (0) root         (0)    23867 2023-01-27 22:40:09.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/43_ell_block_sparse_gemm/ell_block_sparse_gemm.cu
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 17:05:58.849107 flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/44_multi_gemm_ir_and_codegen/
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 17:05:53.773047 flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/44_multi_gemm_ir_and_codegen/fixed_impl/
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 17:05:53.762185 flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/44_multi_gemm_ir_and_codegen/fixed_impl/epilogue/
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 17:05:58.936547 flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/44_multi_gemm_ir_and_codegen/fixed_impl/epilogue/threadblock/
--rw-r--r--   0 root         (0) root         (0)     6370 2023-01-27 22:40:09.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/44_multi_gemm_ir_and_codegen/fixed_impl/epilogue/threadblock/default_bias_act_epilogue_tensor_op.h
--rw-r--r--   0 root         (0) root         (0)     4099 2023-01-27 22:40:09.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/44_multi_gemm_ir_and_codegen/fixed_impl/epilogue/threadblock/default_thread_map_tensor_op_for_fused_bias.h
--rw-r--r--   0 root         (0) root         (0)     8285 2023-01-27 22:40:09.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/44_multi_gemm_ir_and_codegen/fixed_impl/epilogue/threadblock/fused_bias_act_epilogue.h
--rw-r--r--   0 root         (0) root         (0)    10439 2023-01-27 22:40:09.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/44_multi_gemm_ir_and_codegen/fixed_impl/epilogue/threadblock/output_tile_thread_map_for_fused_bias.h
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 17:05:58.962402 flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/44_multi_gemm_ir_and_codegen/fixed_impl/epilogue/warp/
--rw-r--r--   0 root         (0) root         (0)     6848 2023-01-27 22:40:09.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/44_multi_gemm_ir_and_codegen/fixed_impl/epilogue/warp/fused_bias_act_fragment_iterator_tensor_op.h
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 17:05:53.780119 flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/44_multi_gemm_ir_and_codegen/fixed_impl/gemm/
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 17:05:58.990713 flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/44_multi_gemm_ir_and_codegen/fixed_impl/gemm/warp/
--rw-r--r--   0 root         (0) root         (0)    14747 2023-01-27 22:40:09.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/44_multi_gemm_ir_and_codegen/fixed_impl/gemm/warp/mma_tensor_op_fragment_iterator_without_output_op.h
--rw-r--r--   0 root         (0) root         (0)    10231 2023-01-27 22:40:09.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/44_multi_gemm_ir_and_codegen/leaky_bias.h
--rw-r--r--   0 root         (0) root         (0)     3745 2023-01-27 22:40:09.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/44_multi_gemm_ir_and_codegen/utils.h
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 17:05:59.070922 flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/45_dual_gemm/
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 17:05:59.097454 flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/45_dual_gemm/device/
--rw-r--r--   0 root         (0) root         (0)    16955 2023-03-13 04:24:26.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/45_dual_gemm/device/dual_gemm.h
--rw-r--r--   0 root         (0) root         (0)    12669 2023-03-13 04:24:26.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/45_dual_gemm/dual_gemm.cu
--rw-r--r--   0 root         (0) root         (0)     2366 2023-03-13 04:24:26.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/45_dual_gemm/dual_gemm_common.h
--rw-r--r--   0 root         (0) root         (0)    31360 2023-03-13 04:24:26.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/45_dual_gemm/dual_gemm_run.h
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 17:05:59.122686 flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/45_dual_gemm/kernel/
--rw-r--r--   0 root         (0) root         (0)    18422 2023-03-13 04:24:26.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/45_dual_gemm/kernel/dual_gemm.h
--rw-r--r--   0 root         (0) root         (0)     3577 2023-01-27 22:40:09.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/45_dual_gemm/test_run.h
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 17:05:59.146645 flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/45_dual_gemm/thread/
--rw-r--r--   0 root         (0) root         (0)     5818 2023-01-27 22:40:09.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/45_dual_gemm/thread/left_silu_and_mul.h
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 17:05:59.217104 flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/45_dual_gemm/threadblock/
--rw-r--r--   0 root         (0) root         (0)    15613 2023-01-27 22:40:09.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/45_dual_gemm/threadblock/dual_epilogue.h
--rw-r--r--   0 root         (0) root         (0)     7920 2023-03-13 04:24:26.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/45_dual_gemm/threadblock/dual_mma_base.h
--rw-r--r--   0 root         (0) root         (0)    29976 2023-03-13 04:24:26.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/45_dual_gemm/threadblock/dual_mma_multistage.h
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 17:05:59.242813 flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/46_depthwise_simt_conv2dfprop/
--rw-r--r--   0 root         (0) root         (0)    24464 2023-01-27 22:40:09.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/46_depthwise_simt_conv2dfprop/depthwise_simt_conv2dfprop.cu
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 17:05:59.268852 flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/47_ampere_gemm_universal_streamk/
--rw-r--r--   0 root         (0) root         (0)    22676 2023-01-27 22:40:09.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/47_ampere_gemm_universal_streamk/ampere_gemm_universal_streamk.cu
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 17:05:59.293385 flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/48_hopper_warp_specialized_gemm/
--rw-r--r--   0 root         (0) root         (0)    16736 2023-01-27 22:40:09.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/48_hopper_warp_specialized_gemm/48_hopper_warp_specialized_gemm.cu
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 17:05:59.317461 flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/49_hopper_gemm_schedules_with_collective_builder/
--rw-r--r--   0 root         (0) root         (0)    22625 2023-03-13 04:24:26.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/49_hopper_gemm_schedules_with_collective_builder/49_hopper_gemm_schedules_with_collective_builder.cu
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 17:05:59.344007 flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/50_hopper_gemm_with_epilogue_swizzle/
--rw-r--r--   0 root         (0) root         (0)    18635 2023-01-27 22:40:09.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/50_hopper_gemm_with_epilogue_swizzle/50_hopper_gemm_with_epilogue_swizzle.cu
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 17:05:59.369725 flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/60_cutlass_import/
--rw-r--r--   0 root         (0) root         (0)     2849 2023-01-27 22:40:09.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/60_cutlass_import/main.cpp
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 17:05:59.393950 flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/common/
--rw-r--r--   0 root         (0) root         (0)     1434 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/common/helper.h
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 17:05:53.854874 flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/cute/
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 17:05:59.420707 flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/cute/tutorial/
--rw-r--r--   0 root         (0) root         (0)    14342 2023-01-27 22:40:09.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/cute/tutorial/sgemm_nt_1.cu
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 17:05:53.869316 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 17:06:00.467959 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/
--rw-r--r--   0 root         (0) root         (0)     3805 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/aligned_buffer.h
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 17:06:01.069393 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/arch/
--rw-r--r--   0 root         (0) root         (0)     3442 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/arch/arch.h
--rw-r--r--   0 root         (0) root         (0)    12127 2023-01-27 22:40:09.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/arch/barrier.h
--rw-r--r--   0 root         (0) root         (0)     2691 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/arch/cache_operation.h
--rw-r--r--   0 root         (0) root         (0)    14313 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/arch/memory.h
--rw-r--r--   0 root         (0) root         (0)    10490 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/arch/memory_sm75.h
--rw-r--r--   0 root         (0) root         (0)    15125 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/arch/memory_sm80.h
--rw-r--r--   0 root         (0) root         (0)     8001 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/arch/mma.h
--rw-r--r--   0 root         (0) root         (0)    11096 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/arch/mma_sm50.h
--rw-r--r--   0 root         (0) root         (0)     7040 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/arch/mma_sm60.h
--rw-r--r--   0 root         (0) root         (0)     4193 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/arch/mma_sm61.h
--rw-r--r--   0 root         (0) root         (0)    16554 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/arch/mma_sm70.h
--rw-r--r--   0 root         (0) root         (0)    31766 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/arch/mma_sm75.h
--rw-r--r--   0 root         (0) root         (0)    55580 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/arch/mma_sm80.h
--rw-r--r--   0 root         (0) root         (0)     8254 2023-01-27 22:40:09.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/arch/mma_sm90.h
--rw-r--r--   0 root         (0) root         (0)    43978 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/arch/mma_sparse_sm80.h
--rw-r--r--   0 root         (0) root         (0)     2622 2023-01-27 22:40:09.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/arch/reg_reconfig.h
--rw-r--r--   0 root         (0) root         (0)     3998 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/arch/simd.h
--rw-r--r--   0 root         (0) root         (0)     3656 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/arch/simd_sm60.h
--rw-r--r--   0 root         (0) root         (0)     5102 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/arch/simd_sm61.h
--rw-r--r--   0 root         (0) root         (0)     8473 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/arch/wmma.h
--rw-r--r--   0 root         (0) root         (0)     5286 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/arch/wmma_sm70.h
--rw-r--r--   0 root         (0) root         (0)     7746 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/arch/wmma_sm72.h
--rw-r--r--   0 root         (0) root         (0)     7616 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/arch/wmma_sm75.h
--rw-r--r--   0 root         (0) root         (0)    13397 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/array.h
--rw-r--r--   0 root         (0) root         (0)     3662 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/array_planar_complex.h
--rw-r--r--   0 root         (0) root         (0)    13154 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/array_subbyte.h
--rw-r--r--   0 root         (0) root         (0)     6371 2023-01-27 22:40:09.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/barrier.h
--rw-r--r--   0 root         (0) root         (0)    13352 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/bfloat16.h
--rw-r--r--   0 root         (0) root         (0)     6309 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/blas3.h
--rw-r--r--   0 root         (0) root         (0)     9372 2023-01-27 22:40:09.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/block_striped.h
--rw-r--r--   0 root         (0) root         (0)    15894 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/complex.h
--rw-r--r--   0 root         (0) root         (0)    47943 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/constants.h
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 17:06:01.126118 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/conv/
--rw-r--r--   0 root         (0) root         (0)    21652 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/conv/conv2d_problem_size.h
--rw-r--r--   0 root         (0) root         (0)    16292 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/conv/conv3d_problem_size.h
--rw-r--r--   0 root         (0) root         (0)     5915 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/conv/convolution.h
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 17:06:01.194122 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/conv/device/
--rw-r--r--   0 root         (0) root         (0)     9744 2023-01-27 22:40:09.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/conv/device/direct_convolution.h
--rw-r--r--   0 root         (0) root         (0)    12443 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/conv/device/implicit_gemm_convolution.h
--rw-r--r--   0 root         (0) root         (0)    10044 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/conv/device/implicit_gemm_convolution_fusion.h
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 17:06:01.776801 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/conv/kernel/
--rw-r--r--   0 root         (0) root         (0)     7671 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/conv/kernel/default_conv2d.h
--rw-r--r--   0 root         (0) root         (0)    53546 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/conv/kernel/default_conv2d_dgrad.h
--rw-r--r--   0 root         (0) root         (0)    56838 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/conv/kernel/default_conv2d_fprop.h
--rw-r--r--   0 root         (0) root         (0)    11953 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/conv/kernel/default_conv2d_fprop_fusion.h
--rw-r--r--   0 root         (0) root         (0)     4658 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/conv/kernel/default_conv2d_fprop_with_broadcast.h
--rw-r--r--   0 root         (0) root         (0)     4660 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/conv/kernel/default_conv2d_fprop_with_reduction.h
--rw-r--r--   0 root         (0) root         (0)     7907 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/conv/kernel/default_conv2d_group_fprop.h
--rw-r--r--   0 root         (0) root         (0)    28745 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/conv/kernel/default_conv2d_wgrad.h
--rw-r--r--   0 root         (0) root         (0)    10459 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/conv/kernel/default_conv2d_wgrad_fusion.h
--rw-r--r--   0 root         (0) root         (0)     9324 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/conv/kernel/default_conv3d_dgrad.h
--rw-r--r--   0 root         (0) root         (0)    14864 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/conv/kernel/default_conv3d_fprop.h
--rw-r--r--   0 root         (0) root         (0)    11980 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/conv/kernel/default_conv3d_fprop_fusion.h
--rw-r--r--   0 root         (0) root         (0)    14883 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/conv/kernel/default_conv3d_wgrad.h
--rw-r--r--   0 root         (0) root         (0)     7317 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/conv/kernel/default_depthwise_fprop.h
--rw-r--r--   0 root         (0) root         (0)    18048 2023-01-27 22:40:09.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/conv/kernel/direct_convolution.h
--rw-r--r--   0 root         (0) root         (0)    15454 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/conv/kernel/implicit_gemm_convolution.h
--rw-r--r--   0 root         (0) root         (0)    15709 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/conv/kernel/implicit_gemm_convolution_fusion.h
--rw-r--r--   0 root         (0) root         (0)    17222 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/conv/kernel/implicit_gemm_convolution_strided_dgrad.h
--rw-r--r--   0 root         (0) root         (0)    16749 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/conv/kernel/implicit_gemm_convolution_with_fused_epilogue.h
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 17:06:01.801498 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/conv/thread/
--rw-r--r--   0 root         (0) root         (0)     9689 2023-01-27 22:40:09.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/conv/thread/depthwise_mma.h
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 17:06:02.803285 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/
--rw-r--r--   0 root         (0) root         (0)    15306 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/conv2d_dgrad_filter_tile_access_iterator_analytic.h
--rw-r--r--   0 root         (0) root         (0)    19735 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/conv2d_dgrad_filter_tile_access_iterator_optimized.h
--rw-r--r--   0 root         (0) root         (0)    18940 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/conv2d_dgrad_output_gradient_tile_access_iterator_analytic.h
--rw-r--r--   0 root         (0) root         (0)    26137 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/conv2d_dgrad_output_gradient_tile_access_iterator_optimized.h
--rw-r--r--   0 root         (0) root         (0)    10953 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/conv2d_fprop_activation_tile_access_iterator_analytic.h
--rw-r--r--   0 root         (0) root         (0)    11529 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/conv2d_fprop_activation_tile_access_iterator_few_channels.h
--rw-r--r--   0 root         (0) root         (0)    11333 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/conv2d_fprop_activation_tile_access_iterator_fixed_channels.h
--rw-r--r--   0 root         (0) root         (0)    13664 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/conv2d_fprop_activation_tile_access_iterator_optimized.h
--rw-r--r--   0 root         (0) root         (0)    10627 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/conv2d_fprop_filter_tile_access_iterator_analytic.h
--rw-r--r--   0 root         (0) root         (0)     9314 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/conv2d_fprop_filter_tile_access_iterator_few_channels.h
--rw-r--r--   0 root         (0) root         (0)     9018 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/conv2d_fprop_filter_tile_access_iterator_fixed_channels.h
--rw-r--r--   0 root         (0) root         (0)    10286 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/conv2d_fprop_filter_tile_access_iterator_optimized.h
--rw-r--r--   0 root         (0) root         (0)    30197 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/conv2d_params.h
--rw-r--r--   0 root         (0) root         (0)    11202 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/conv2d_tile_iterator.h
--rw-r--r--   0 root         (0) root         (0)    10350 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/conv2d_wgrad_activation_tile_access_iterator_analytic.h
--rw-r--r--   0 root         (0) root         (0)    11520 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/conv2d_wgrad_activation_tile_access_iterator_optimized.h
--rw-r--r--   0 root         (0) root         (0)     9043 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/conv2d_wgrad_output_gradient_tile_access_iterator_analytic.h
--rw-r--r--   0 root         (0) root         (0)    10832 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/conv2d_wgrad_output_gradient_tile_access_iterator_optimized.h
--rw-r--r--   0 root         (0) root         (0)     8450 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/conv3d_dgrad_filter_tile_access_iterator_analytic.h
--rw-r--r--   0 root         (0) root         (0)     9569 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/conv3d_dgrad_filter_tile_access_iterator_optimized.h
--rw-r--r--   0 root         (0) root         (0)    11020 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/conv3d_dgrad_output_gradient_tile_access_iterator_analytic.h
--rw-r--r--   0 root         (0) root         (0)    15014 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/conv3d_dgrad_output_gradient_tile_access_iterator_optimized.h
--rw-r--r--   0 root         (0) root         (0)     9634 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/conv3d_fprop_activation_tile_access_iterator_analytic.h
--rw-r--r--   0 root         (0) root         (0)    15132 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/conv3d_fprop_activation_tile_access_iterator_optimized.h
--rw-r--r--   0 root         (0) root         (0)     7945 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/conv3d_fprop_filter_tile_access_iterator_analytic.h
--rw-r--r--   0 root         (0) root         (0)     8891 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/conv3d_fprop_filter_tile_access_iterator_optimized.h
--rw-r--r--   0 root         (0) root         (0)    18249 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/conv3d_params.h
--rw-r--r--   0 root         (0) root         (0)     9971 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/conv3d_wgrad_activation_tile_access_iterator_analytic.h
--rw-r--r--   0 root         (0) root         (0)    12024 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/conv3d_wgrad_activation_tile_access_iterator_optimized.h
--rw-r--r--   0 root         (0) root         (0)     8821 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/conv3d_wgrad_output_gradient_tile_access_iterator_analytic.h
--rw-r--r--   0 root         (0) root         (0)    10744 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/conv3d_wgrad_output_gradient_tile_access_iterator_optimized.h
--rw-r--r--   0 root         (0) root         (0)     8871 2023-01-27 22:40:09.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/depthwise_direct_conv_params.h
--rw-r--r--   0 root         (0) root         (0)    10747 2023-01-27 22:40:09.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/depthwise_fprop_activation_tile_access_iterator_direct_conv_fixed_stride_dilation.h
--rw-r--r--   0 root         (0) root         (0)     9899 2023-01-27 22:40:09.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/depthwise_fprop_activation_tile_access_iterator_direct_conv_optimized.h
--rw-r--r--   0 root         (0) root         (0)    20899 2023-01-27 22:40:09.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/depthwise_fprop_direct_conv_multistage.h
--rw-r--r--   0 root         (0) root         (0)     8921 2023-01-27 22:40:09.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/depthwise_fprop_filter_tile_access_iterator_direct_conv_optimized.h
--rw-r--r--   0 root         (0) root         (0)    12744 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/depthwise_fprop_pipelined.h
--rw-r--r--   0 root         (0) root         (0)     8097 2023-01-27 22:40:09.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/depthwise_mma_base.h
--rw-r--r--   0 root         (0) root         (0)    14329 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/depthwise_mma_core_with_lane_access_size.h
--rw-r--r--   0 root         (0) root         (0)    30106 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/implicit_gemm_fprop_fusion_multistage.h
--rw-r--r--   0 root         (0) root         (0)    20086 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/implicit_gemm_multistage.h
--rw-r--r--   0 root         (0) root         (0)    12174 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/implicit_gemm_pipelined.h
--rw-r--r--   0 root         (0) root         (0)    26320 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/implicit_gemm_wgrad_fusion_multistage.h
--rw-r--r--   0 root         (0) root         (0)    16915 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/predicated_scale_bias_vector_access_iterator.h
--rw-r--r--   0 root         (0) root         (0)    12476 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/predicated_scale_bias_vector_iterator.h
--rw-r--r--   0 root         (0) root         (0)     7071 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/threadblock_swizzle.h
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 17:06:02.876350 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/conv/warp/
--rw-r--r--   0 root         (0) root         (0)     5577 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/conv/warp/mma_depthwise_simt.h
--rw-r--r--   0 root         (0) root         (0)     9342 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/conv/warp/mma_depthwise_simt_tile_iterator.h
--rw-r--r--   0 root         (0) root         (0)     8772 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/conv/warp/scale_bias_relu_transform.h
--rw-r--r--   0 root         (0) root         (0)    11581 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/coord.h
--rw-r--r--   0 root         (0) root         (0)    10900 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/core_io.h
--rw-r--r--   0 root         (0) root         (0)     7549 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/cutlass.h
--rw-r--r--   0 root         (0) root         (0)     2665 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/device_kernel.h
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 17:05:53.939272 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/epilogue/
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 17:06:03.298191 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/epilogue/thread/
--rw-r--r--   0 root         (0) root         (0)    17916 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/epilogue/thread/activation.h
--rw-r--r--   0 root         (0) root         (0)     4691 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/epilogue/thread/conversion_op.h
--rw-r--r--   0 root         (0) root         (0)     9349 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/epilogue/thread/linear_combination.h
--rw-r--r--   0 root         (0) root         (0)     8231 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/epilogue/thread/linear_combination_bias_elementwise.h
--rw-r--r--   0 root         (0) root         (0)    13377 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/epilogue/thread/linear_combination_bias_relu.h
--rw-r--r--   0 root         (0) root         (0)    23649 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/epilogue/thread/linear_combination_clamp.h
--rw-r--r--   0 root         (0) root         (0)     9067 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/epilogue/thread/linear_combination_dgelu.h
--rw-r--r--   0 root         (0) root         (0)    15195 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/epilogue/thread/linear_combination_drelu.h
--rw-r--r--   0 root         (0) root         (0)     3669 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/epilogue/thread/linear_combination_gelu.h
--rw-r--r--   0 root         (0) root         (0)     8065 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/epilogue/thread/linear_combination_generic.h
--rw-r--r--   0 root         (0) root         (0)     3693 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/epilogue/thread/linear_combination_hardswish.h
--rw-r--r--   0 root         (0) root         (0)     8296 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/epilogue/thread/linear_combination_leaky_relu.h
--rw-r--r--   0 root         (0) root         (0)     3058 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/epilogue/thread/linear_combination_params.h
--rw-r--r--   0 root         (0) root         (0)     9351 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/epilogue/thread/linear_combination_planar_complex.h
--rw-r--r--   0 root         (0) root         (0)    20486 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/epilogue/thread/linear_combination_relu.h
--rw-r--r--   0 root         (0) root         (0)    19348 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/epilogue/thread/linear_combination_relu0.h
--rw-r--r--   0 root         (0) root         (0)     6760 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/epilogue/thread/linear_combination_residual_block.h
--rw-r--r--   0 root         (0) root         (0)     3688 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/epilogue/thread/linear_combination_sigmoid.h
--rw-r--r--   0 root         (0) root         (0)     3669 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/epilogue/thread/linear_combination_silu.h
--rw-r--r--   0 root         (0) root         (0)     8662 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/epilogue/thread/linear_combination_with_elementwise.h
--rw-r--r--   0 root         (0) root         (0)     3416 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/epilogue/thread/reduction_op.h
--rw-r--r--   0 root         (0) root         (0)     2656 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/epilogue/thread/scale_type.h
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 17:06:04.354829 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/
--rw-r--r--   0 root         (0) root         (0)     9142 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/default_epilogue_complex_tensor_op.h
--rw-r--r--   0 root         (0) root         (0)     9441 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/default_epilogue_complex_tensor_op_blas3.h
--rw-r--r--   0 root         (0) root         (0)     3234 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/default_epilogue_direct_store.h
--rw-r--r--   0 root         (0) root         (0)     7209 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/default_epilogue_planar_complex.h
--rw-r--r--   0 root         (0) root         (0)    10137 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/default_epilogue_simt.h
--rw-r--r--   0 root         (0) root         (0)    23409 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/default_epilogue_tensor_op.h
--rw-r--r--   0 root         (0) root         (0)     7129 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/default_epilogue_tensor_op_blas3.h
--rw-r--r--   0 root         (0) root         (0)    10846 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/default_epilogue_volta_tensor_op.h
--rw-r--r--   0 root         (0) root         (0)     5817 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/default_epilogue_with_broadcast.h
--rw-r--r--   0 root         (0) root         (0)     5763 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/default_epilogue_with_reduction.h
--rw-r--r--   0 root         (0) root         (0)     5947 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/default_epilogue_wmma_tensor_op.h
--rw-r--r--   0 root         (0) root         (0)     4409 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/default_thread_map_simt.h
--rw-r--r--   0 root         (0) root         (0)     7398 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/default_thread_map_tensor_op.h
--rw-r--r--   0 root         (0) root         (0)     7303 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/default_thread_map_volta_tensor_op.h
--rw-r--r--   0 root         (0) root         (0)     4098 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/default_thread_map_wmma_tensor_op.h
--rw-r--r--   0 root         (0) root         (0)     4678 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/direct_store_epilogue_iterator.h
--rw-r--r--   0 root         (0) root         (0)    18231 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/epilogue.h
--rw-r--r--   0 root         (0) root         (0)     8279 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/epilogue_base.h
--rw-r--r--   0 root         (0) root         (0)     7455 2023-01-27 22:40:09.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/epilogue_base_streamk.h
--rw-r--r--   0 root         (0) root         (0)    13424 2023-01-27 22:40:09.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/epilogue_depthwise.h
--rw-r--r--   0 root         (0) root         (0)    13983 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/epilogue_direct_store.h
--rw-r--r--   0 root         (0) root         (0)     7363 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/epilogue_gemm_k_reduction.h
--rw-r--r--   0 root         (0) root         (0)    14610 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/epilogue_planar_complex.h
--rw-r--r--   0 root         (0) root         (0)     9073 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/epilogue_smem_accumulator.h
--rw-r--r--   0 root         (0) root         (0)    16804 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/epilogue_visitor_with_softmax.h
--rw-r--r--   0 root         (0) root         (0)    28708 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/epilogue_with_broadcast.h
--rw-r--r--   0 root         (0) root         (0)    29151 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/epilogue_with_reduction.h
--rw-r--r--   0 root         (0) root         (0)    13454 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/epilogue_with_visitor.h
--rw-r--r--   0 root         (0) root         (0)     7308 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/epilogue_workspace.h
--rw-r--r--   0 root         (0) root         (0)    11733 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/interleaved_epilogue.h
--rw-rw-r--   0 root         (0) root         (0)     2912 2022-06-02 16:47:41.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/output_iterator_parameter.h
--rw-r--r--   0 root         (0) root         (0)    19750 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/output_tile_thread_map.h
--rw-r--r--   0 root         (0) root         (0)    38356 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/predicated_tile_iterator.h
--rw-r--r--   0 root         (0) root         (0)    18821 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/predicated_tile_iterator_affine.h
--rw-r--r--   0 root         (0) root         (0)     5636 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/predicated_tile_iterator_affine_layout_params.h
--rw-r--r--   0 root         (0) root         (0)    21249 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/predicated_tile_iterator_blas3.h
--rw-r--r--   0 root         (0) root         (0)    13873 2023-03-13 04:24:26.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/predicated_tile_iterator_direct_conv.h
--rw-r--r--   0 root         (0) root         (0)    12047 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/predicated_tile_iterator_params.h
--rw-r--r--   0 root         (0) root         (0)     9146 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/predicated_tile_iterator_predicates.h
--rw-r--r--   0 root         (0) root         (0)    15536 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/predicated_tile_iterator_strided_dgrad.h
--rw-r--r--   0 root         (0) root         (0)     7384 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/shared_load_iterator.h
--rw-r--r--   0 root         (0) root         (0)    17404 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/shared_load_iterator_mixed.h
--rw-r--r--   0 root         (0) root         (0)     7394 2023-01-27 22:40:09.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/shared_load_iterator_pitch_liner.h
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 17:06:04.772609 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/epilogue/warp/
--rw-r--r--   0 root         (0) root         (0)     7055 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/epilogue/warp/fragment_iterator_complex_tensor_op.h
--rw-r--r--   0 root         (0) root         (0)     7736 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/epilogue/warp/fragment_iterator_gaussian_complex_tensor_op.h
--rw-r--r--   0 root         (0) root         (0)     5880 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/epilogue/warp/fragment_iterator_simt.h
--rw-r--r--   0 root         (0) root         (0)     9883 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/epilogue/warp/fragment_iterator_tensor_op.h
--rw-r--r--   0 root         (0) root         (0)     8924 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/epilogue/warp/fragment_iterator_volta_tensor_op.h
--rw-r--r--   0 root         (0) root         (0)     6045 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/epilogue/warp/fragment_iterator_wmma_tensor_op.h
--rw-r--r--   0 root         (0) root         (0)     4864 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/epilogue/warp/simt_policy.h
--rw-r--r--   0 root         (0) root         (0)     5979 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/epilogue/warp/tensor_op_policy.h
--rw-r--r--   0 root         (0) root         (0)    15547 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/epilogue/warp/tile_iterator_simt.h
--rw-r--r--   0 root         (0) root         (0)    19983 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/epilogue/warp/tile_iterator_tensor_op.h
--rw-r--r--   0 root         (0) root         (0)    22552 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/epilogue/warp/tile_iterator_tensor_op_mixed.h
--rw-r--r--   0 root         (0) root         (0)    14052 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/epilogue/warp/tile_iterator_volta_tensor_op.h
--rw-r--r--   0 root         (0) root         (0)     7600 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/epilogue/warp/tile_iterator_wmma_tensor_op.h
--rw-r--r--   0 root         (0) root         (0)     7485 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/epilogue/warp/volta_tensor_op_policy.h
--rw-r--r--   0 root         (0) root         (0)     3916 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/epilogue/warp/wmma_tensor_op_policy.h
--rw-r--r--   0 root         (0) root         (0)    24535 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/fast_math.h
--rw-r--r--   0 root         (0) root         (0)    35369 2023-03-13 04:24:26.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/float8.h
--rw-r--r--   0 root         (0) root         (0)     2645 2023-01-27 22:40:09.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/floating_point_nvrtc.h
--rw-r--r--   0 root         (0) root         (0)    61285 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/functional.h
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 17:06:04.793307 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 17:06:05.354252 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/device/
--rw-r--r--   0 root         (0) root         (0)    17035 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/device/base_grouped.h
--rw-r--r--   0 root         (0) root         (0)    23124 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/device/default_gemm_configuration.h
--rw-r--r--   0 root         (0) root         (0)    27616 2023-03-13 04:24:26.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/device/ell_gemm.h
--rw-r--r--   0 root         (0) root         (0)    25194 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/device/gemm.h
--rw-r--r--   0 root         (0) root         (0)    22367 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/device/gemm_array.h
--rw-r--r--   0 root         (0) root         (0)    22415 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/device/gemm_batched.h
--rw-r--r--   0 root         (0) root         (0)    22725 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/device/gemm_complex.h
--rw-r--r--   0 root         (0) root         (0)     2591 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/device/gemm_grouped.h
--rw-r--r--   0 root         (0) root         (0)    13736 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/device/gemm_layernorm_mainloop_fusion.h
--rw-r--r--   0 root         (0) root         (0)    17329 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/device/gemm_sparse.h
--rw-r--r--   0 root         (0) root         (0)    20450 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/device/gemm_splitk_parallel.h
--rw-r--r--   0 root         (0) root         (0)    14576 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/device/gemm_universal.h
--rw-r--r--   0 root         (0) root         (0)     7427 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/device/gemm_universal_adapter.h
--rw-r--r--   0 root         (0) root         (0)    13153 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/device/gemm_universal_base.h
--rw-r--r--   0 root         (0) root         (0)    13968 2023-03-13 04:24:26.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/device/gemm_universal_with_broadcast.h
--rw-r--r--   0 root         (0) root         (0)    14853 2023-03-13 04:24:26.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/device/gemm_with_k_reduction.h
--rw-r--r--   0 root         (0) root         (0)     5690 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/device/gemv.h
--rw-r--r--   0 root         (0) root         (0)    18127 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/device/rank_2k.h
--rw-r--r--   0 root         (0) root         (0)     2747 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/device/rank_2k_grouped.h
--rw-r--r--   0 root         (0) root         (0)    16719 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/device/rank_k.h
--rwxr-xr-x   0 root         (0) root         (0)    21050 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/device/symm.h
--rw-r--r--   0 root         (0) root         (0)    26464 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/device/trmm.h
--rw-r--r--   0 root         (0) root         (0)    11570 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/gemm.h
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 17:06:06.803061 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/
--rw-r--r--   0 root         (0) root         (0)    29360 2023-01-27 22:40:09.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/default_ell_gemm.h
--rw-r--r--   0 root         (0) root         (0)    34810 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/default_gemm.h
--rw-r--r--   0 root         (0) root         (0)    13645 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/default_gemm_complex.h
--rw-r--r--   0 root         (0) root         (0)    12385 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/default_gemm_grouped.h
--rw-r--r--   0 root         (0) root         (0)     6592 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/default_gemm_grouped_softmax_mainloop_fusion.h
--rw-r--r--   0 root         (0) root         (0)     5848 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/default_gemm_layernorm_mainloop_fusion.h
--rw-r--r--   0 root         (0) root         (0)    11104 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/default_gemm_planar_complex_universal.h
--rw-r--r--   0 root         (0) root         (0)     7983 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/default_gemm_sparse.h
--rw-r--r--   0 root         (0) root         (0)     4932 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/default_gemm_splitk_parallel.h
--rw-r--r--   0 root         (0) root         (0)    10934 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/default_gemm_universal.h
--rw-r--r--   0 root         (0) root         (0)     8063 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/default_gemm_with_broadcast.h
--rw-r--r--   0 root         (0) root         (0)     6421 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/default_gemm_with_k_reduction.h
--rw-r--r--   0 root         (0) root         (0)     8086 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/default_gemm_with_reduction.h
--rwxr-xr-x   0 root         (0) root         (0)     5349 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/default_gemv.h
--rw-r--r--   0 root         (0) root         (0)     8301 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/default_rank_2k.h
--rw-r--r--   0 root         (0) root         (0)    13392 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/default_rank_2k_complex.h
--rw-r--r--   0 root         (0) root         (0)    12470 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/default_rank_2k_grouped.h
--rw-r--r--   0 root         (0) root         (0)    10620 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/default_rank_2k_universal.h
--rw-r--r--   0 root         (0) root         (0)     7350 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/default_rank_k.h
--rw-r--r--   0 root         (0) root         (0)    11406 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/default_rank_k_complex.h
--rw-r--r--   0 root         (0) root         (0)     9444 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/default_rank_k_universal.h
--rwxr-xr-x   0 root         (0) root         (0)     9264 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/default_symm.h
--rwxr-xr-x   0 root         (0) root         (0)    13302 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/default_symm_complex.h
--rwxr-xr-x   0 root         (0) root         (0)    10315 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/default_symm_universal.h
--rw-r--r--   0 root         (0) root         (0)     7999 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/default_trmm.h
--rw-r--r--   0 root         (0) root         (0)     7948 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/default_trmm_complex.h
--rw-r--r--   0 root         (0) root         (0)    10850 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/default_trmm_universal.h
--rw-r--r--   0 root         (0) root         (0)    28916 2023-01-27 22:40:09.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/ell_gemm.h
--rw-r--r--   0 root         (0) root         (0)    13381 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/gemm.h
--rw-r--r--   0 root         (0) root         (0)     8717 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/gemm_array.h
--rw-r--r--   0 root         (0) root         (0)     8785 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/gemm_batched.h
--rw-r--r--   0 root         (0) root         (0)    14856 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/gemm_grouped.h
--rw-r--r--   0 root         (0) root         (0)     4314 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/gemm_grouped_problem_visitor.h
--rw-r--r--   0 root         (0) root         (0)    15768 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/gemm_grouped_softmax_mainloop_fusion.h
--rw-r--r--   0 root         (0) root         (0)    28517 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/gemm_layernorm_mainloop_fusion.h
--rwxr-xr-x   0 root         (0) root         (0)     6144 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/gemm_params.h
--rw-r--r--   0 root         (0) root         (0)     5165 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/gemm_pipelined.h
--rw-r--r--   0 root         (0) root         (0)    23763 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/gemm_planar_complex.h
--rw-r--r--   0 root         (0) root         (0)    19398 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/gemm_planar_complex_array.h
--rw-r--r--   0 root         (0) root         (0)     8142 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/gemm_splitk_parallel.h
--rw-r--r--   0 root         (0) root         (0)     4291 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/gemm_transpose_operands.h
--rw-r--r--   0 root         (0) root         (0)    23859 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/gemm_universal.h
--rw-r--r--   0 root         (0) root         (0)    39288 2023-03-13 04:24:26.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/gemm_universal_streamk.h
--rw-r--r--   0 root         (0) root         (0)    25116 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/gemm_with_fused_epilogue.h
--rw-r--r--   0 root         (0) root         (0)    24186 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/gemm_with_k_reduction.h
--rw-r--r--   0 root         (0) root         (0)     8090 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/gemv.h
--rwxr-xr-x   0 root         (0) root         (0)     8979 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/gemv_batched_strided.h
--rw-r--r--   0 root         (0) root         (0)    16982 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/grouped_problem_visitor.h
--rw-r--r--   0 root         (0) root         (0)     7148 2023-01-27 22:40:09.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/params_universal_base.h
--rw-r--r--   0 root         (0) root         (0)    23107 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/rank_2k_grouped.h
--rw-r--r--   0 root         (0) root         (0)    15792 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/rank_2k_grouped_problem_visitor.h
--rw-r--r--   0 root         (0) root         (0)     4334 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/rank_2k_transpose_operands.h
--rw-r--r--   0 root         (0) root         (0)    24162 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/rank_2k_universal.h
--rw-r--r--   0 root         (0) root         (0)    17567 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/rank_k_universal.h
--rw-r--r--   0 root         (0) root         (0)    13610 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/sparse_gemm.h
--rwxr-xr-x   0 root         (0) root         (0)    23900 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/symm_universal.h
--rw-r--r--   0 root         (0) root         (0)    19537 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/trmm_universal.h
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 17:06:07.001332 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/thread/
--rw-r--r--   0 root         (0) root         (0)     3567 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/thread/mma.h
--rw-r--r--   0 root         (0) root         (0)    10767 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/thread/mma_sm50.h
--rw-r--r--   0 root         (0) root         (0)    29987 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/thread/mma_sm60.h
--rw-r--r--   0 root         (0) root         (0)     8142 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/thread/mma_sm61.h
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 17:06:07.964748 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/
--rw-r--r--   0 root         (0) root         (0)    31930 2023-01-27 22:40:09.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/default_ell_mma.h
--rwxr-xr-x   0 root         (0) root         (0)     6979 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/default_gemv_core.h
--rw-r--r--   0 root         (0) root         (0)    34241 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/default_mma.h
--rw-r--r--   0 root         (0) root         (0)     5123 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/default_mma_core.h
--rw-r--r--   0 root         (0) root         (0)    57426 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/default_mma_core_simt.h
--rw-r--r--   0 root         (0) root         (0)    19257 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/default_mma_core_sm70.h
--rw-r--r--   0 root         (0) root         (0)    42310 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/default_mma_core_sm75.h
--rw-r--r--   0 root         (0) root         (0)   102893 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/default_mma_core_sm80.h
--rw-r--r--   0 root         (0) root         (0)    32106 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/default_mma_core_sparse_sm80.h
--rw-r--r--   0 root         (0) root         (0)    12645 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/default_mma_core_with_access_size.h
--rw-r--r--   0 root         (0) root         (0)     7442 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/default_mma_core_with_reduction.h
--rw-r--r--   0 root         (0) root         (0)    20975 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/default_mma_core_wmma.h
--rw-r--r--   0 root         (0) root         (0)     7998 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/default_mma_layernorm_mainloop_fusion.h
--rw-r--r--   0 root         (0) root         (0)     5110 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/default_mma_planar_complex_multistage.h
--rw-r--r--   0 root         (0) root         (0)     4627 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/default_mma_planar_complex_pipelined.h
--rw-r--r--   0 root         (0) root         (0)     7113 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/default_mma_softmax_mainloop_fusion.h
--rw-r--r--   0 root         (0) root         (0)     6323 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/default_mma_with_reduction.h
--rw-r--r--   0 root         (0) root         (0)     7121 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/default_multistage_mma_complex.h
--rw-r--r--   0 root         (0) root         (0)     4959 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/default_multistage_mma_complex_core.h
--rw-r--r--   0 root         (0) root         (0)    64521 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/default_multistage_mma_complex_core_sm80.h
--rw-r--r--   0 root         (0) root         (0)    25495 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/default_multistage_trmm_complex.h
--rw-r--r--   0 root         (0) root         (0)     8509 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/default_sparse_mma.h
--rw-r--r--   0 root         (0) root         (0)    19515 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/default_trmm.h
--rw-r--r--   0 root         (0) root         (0)    24047 2023-01-27 22:40:09.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/ell_mma_multistage.h
--rw-r--r--   0 root         (0) root         (0)    13837 2023-03-13 04:24:26.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/ell_mma_pipelined.h
--rwxr-xr-x   0 root         (0) root         (0)     4726 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/gemv.h
--rw-r--r--   0 root         (0) root         (0)     3652 2023-01-27 22:40:09.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/index_remat.h
--rw-r--r--   0 root         (0) root         (0)     7791 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/mma_base.h
--rw-r--r--   0 root         (0) root         (0)    27413 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/mma_blas3_multistage.h
--rw-r--r--   0 root         (0) root         (0)    32892 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/mma_layernorm_mainloop_fusion_multistage.h
--rw-r--r--   0 root         (0) root         (0)    23286 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/mma_multistage.h
--rw-r--r--   0 root         (0) root         (0)    12351 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/mma_pipelined.h
--rw-r--r--   0 root         (0) root         (0)     6901 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/mma_planar_complex_base.h
--rw-r--r--   0 root         (0) root         (0)    22805 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/mma_planar_complex_multistage.h
--rw-r--r--   0 root         (0) root         (0)    14746 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/mma_planar_complex_pipelined.h
--rw-r--r--   0 root         (0) root         (0)     9864 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/mma_singlestage.h
--rw-r--r--   0 root         (0) root         (0)    27059 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/mma_softmax_mainloop_fusion_multistage.h
--rw-r--r--   0 root         (0) root         (0)     9210 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/mma_sparse_base.h
--rw-r--r--   0 root         (0) root         (0)    25364 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/mma_sparse_multistage.h
--rw-r--r--   0 root         (0) root         (0)    20471 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/mma_with_reduction_multistage.h
--rw-r--r--   0 root         (0) root         (0)    16220 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/threadblock_swizzle.h
--rw-r--r--   0 root         (0) root         (0)    26485 2023-03-13 04:24:26.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/threadblock_swizzle_streamk.h
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 17:06:08.617951 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/warp/
--rw-r--r--   0 root         (0) root         (0)    15979 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/warp/default_mma_complex_tensor_op.h
--rw-r--r--   0 root         (0) root         (0)     6684 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/warp/default_mma_sparse_tensor_op.h
--rw-r--r--   0 root         (0) root         (0)     5160 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/warp/default_mma_tensor_op.h
--rw-r--r--   0 root         (0) root         (0)     9026 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/warp/default_mma_tensor_op_sm80.h
--rw-r--r--   0 root         (0) root         (0)     3990 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/warp/default_mma_with_reduction_tensor_op.h
--rw-r--r--   0 root         (0) root         (0)     4685 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/warp/default_mma_wmma_tensor_op.h
--rw-r--r--   0 root         (0) root         (0)     5725 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/warp/layernorm_scale_bias_transform.h
--rw-r--r--   0 root         (0) root         (0)     2619 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/warp/mma.h
--rw-r--r--   0 root         (0) root         (0)    27916 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/warp/mma_complex_tensor_op.h
--rw-r--r--   0 root         (0) root         (0)    23104 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/warp/mma_complex_tensor_op_fast_f32.h
--rw-r--r--   0 root         (0) root         (0)    78615 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/warp/mma_complex_tensor_op_tile_iterator_sm80.h
--rw-r--r--   0 root         (0) root         (0)    12317 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/warp/mma_gaussian_complex_tensor_op.h
--rw-r--r--   0 root         (0) root         (0)    14589 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/warp/mma_gaussian_complex_tensor_op_tile_iterator_sm80.h
--rw-r--r--   0 root         (0) root         (0)     6144 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/warp/mma_planar_complex.h
--rw-r--r--   0 root         (0) root         (0)     8446 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/warp/mma_simt.h
--rw-r--r--   0 root         (0) root         (0)     3079 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/warp/mma_simt_policy.h
--rw-r--r--   0 root         (0) root         (0)    59793 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/warp/mma_simt_tile_iterator.h
--rw-r--r--   0 root         (0) root         (0)    11758 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/warp/mma_sparse_tensor_op.h
--rw-r--r--   0 root         (0) root         (0)    14407 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/warp/mma_tensor_op.h
--rw-r--r--   0 root         (0) root         (0)    15721 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/warp/mma_tensor_op_fast_f32.h
--rw-rw-r--   0 root         (0) root         (0)    18643 2022-06-02 16:47:41.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/warp/mma_tensor_op_fragment_iterator.h
--rw-r--r--   0 root         (0) root         (0)     2939 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/warp/mma_tensor_op_policy.h
--rw-r--r--   0 root         (0) root         (0)     8966 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/warp/mma_tensor_op_sm70.h
--rw-r--r--   0 root         (0) root         (0)    11017 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/warp/mma_tensor_op_tile_access_iterator.h
--rw-r--r--   0 root         (0) root         (0)   136033 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/warp/mma_tensor_op_tile_iterator.h
--rw-r--r--   0 root         (0) root         (0)    99649 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/warp/mma_tensor_op_tile_iterator_sm70.h
--rw-r--r--   0 root         (0) root         (0)    75179 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/warp/mma_tensor_op_tile_iterator_sm80.h
--rw-r--r--   0 root         (0) root         (0)    13151 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/warp/mma_tensor_op_tile_iterator_sparse.h
--rw-r--r--   0 root         (0) root         (0)    27101 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/warp/mma_tensor_op_tile_iterator_wmma.h
--rw-r--r--   0 root         (0) root         (0)     7241 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/warp/mma_tensor_op_wmma.h
--rw-r--r--   0 root         (0) root         (0)    17231 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/warp/mma_with_reduction_tensor_op.h
--rw-r--r--   0 root         (0) root         (0)    19125 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/warp/scale_bias_tile_iterator.h
--rw-r--r--   0 root         (0) root         (0)     4610 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/warp/softmax_scale_bias_transform.h
--rw-r--r--   0 root         (0) root         (0)     8728 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/warp/tile_iterator_planar_complex.h
--rw-r--r--   0 root         (0) root         (0)    23690 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/half.h
--rw-r--r--   0 root         (0) root         (0)     6908 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/integer_subbyte.h
--rw-r--r--   0 root         (0) root         (0)     2801 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/kernel_launch.h
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 17:06:08.977399 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/layout/
--rw-r--r--   0 root         (0) root         (0)     3020 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/layout/layout.h
--rw-r--r--   0 root         (0) root         (0)    34712 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/layout/matrix.h
--rw-r--r--   0 root         (0) root         (0)     9286 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/layout/permute.h
--rw-r--r--   0 root         (0) root         (0)     4696 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/layout/pitch_linear.h
--rw-r--r--   0 root         (0) root         (0)    18295 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/layout/tensor.h
--rw-r--r--   0 root         (0) root         (0)    29599 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/layout/tensor_op_multiplicand_sm70.h
--rw-r--r--   0 root         (0) root         (0)    33137 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/layout/tensor_op_multiplicand_sm75.h
--rw-r--r--   0 root         (0) root         (0)    29336 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/layout/tensor_op_multiplicand_sm80.h
--rw-r--r--   0 root         (0) root         (0)     3328 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/layout/vector.h
--rw-r--r--   0 root         (0) root         (0)   364115 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/matrix.h
--rw-r--r--   0 root         (0) root         (0)     4991 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/matrix_coord.h
--rw-r--r--   0 root         (0) root         (0)     2726 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/matrix_shape.h
--rw-r--r--   0 root         (0) root         (0)    46719 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/numeric_conversion.h
--rw-r--r--   0 root         (0) root         (0)     3477 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/numeric_types.h
--rw-r--r--   0 root         (0) root         (0)     5492 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/pitch_linear_coord.h
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 17:06:08.998202 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/platform/
--rw-r--r--   0 root         (0) root         (0)    25348 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/platform/platform.h
--rw-r--r--   0 root         (0) root         (0)    15565 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/predicate_vector.h
--rw-r--r--   0 root         (0) root         (0)    18987 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/quaternion.h
--rw-r--r--   0 root         (0) root         (0)     2369 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/real.h
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 17:06:09.019775 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/reduction/
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 17:06:09.107279 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/reduction/device/
--rw-r--r--   0 root         (0) root         (0)     6823 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/reduction/device/reduce_split_k.h
--rw-r--r--   0 root         (0) root         (0)     8152 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/reduction/device/tensor_reduce.h
--rw-r--r--   0 root         (0) root         (0)    11579 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/reduction/device/tensor_reduce_affine_contiguous.h
--rw-r--r--   0 root         (0) root         (0)    11448 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/reduction/device/tensor_reduce_affine_strided.h
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 17:06:09.191671 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/reduction/kernel/
--rw-r--r--   0 root         (0) root         (0)     8762 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/reduction/kernel/reduce_softmax_final.h
--rw-r--r--   0 root         (0) root         (0)     7897 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/reduction/kernel/reduce_split_k.h
--rw-r--r--   0 root         (0) root         (0)    20685 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/reduction/kernel/tensor_reduce_affine_contiguous.h
--rw-r--r--   0 root         (0) root         (0)    21662 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/reduction/kernel/tensor_reduce_affine_strided.h
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 17:06:09.235108 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/reduction/thread/
--rw-r--r--   0 root         (0) root         (0)     7208 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/reduction/thread/reduce.h
--rw-r--r--   0 root         (0) root         (0)     6790 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/reduction/thread/reduction_operators.h
--rw-r--r--   0 root         (0) root         (0)     2936 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/reduction/threadblock_swizzle.h
--rw-r--r--   0 root         (0) root         (0)     5929 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/relatively_equal.h
--rw-r--r--   0 root         (0) root         (0)     4222 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/semaphore.h
--rw-r--r--   0 root         (0) root         (0)    16587 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/subbyte_reference.h
--rw-r--r--   0 root         (0) root         (0)     8964 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/tensor_coord.h
--rw-r--r--   0 root         (0) root         (0)    12207 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/tensor_ref.h
--rw-r--r--   0 root         (0) root         (0)    11201 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/tensor_ref_planar_complex.h
--rw-r--r--   0 root         (0) root         (0)     9509 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/tensor_view.h
--rw-r--r--   0 root         (0) root         (0)    10250 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/tensor_view_planar_complex.h
--rw-r--r--   0 root         (0) root         (0)    12853 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/tfloat32.h
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 17:06:09.259715 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/thread/
--rw-r--r--   0 root         (0) root         (0)     5931 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/thread/matrix.h
--rw-r--r--   0 root         (0) root         (0)     2581 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/trace.h
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 17:06:09.282161 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/transform/
--rw-r--r--   0 root         (0) root         (0)    33157 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/transform/pitch_linear_thread_map.h
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 17:06:09.323356 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/transform/thread/
--rw-r--r--   0 root         (0) root         (0)     3835 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/transform/thread/transpose.h
--rw-r--r--   0 root         (0) root         (0)     4309 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/transform/thread/unary_op.h
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 17:06:10.021321 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/transform/threadblock/
--rw-r--r--   0 root         (0) root         (0)     6181 2023-01-27 22:40:09.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/transform/threadblock/ell_iterator.h
--rw-r--r--   0 root         (0) root         (0)    44443 2023-01-27 22:40:09.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/transform/threadblock/ell_predicated_tile_access_iterator.h
--rw-r--r--   0 root         (0) root         (0)    44309 2023-01-27 22:40:09.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/transform/threadblock/ell_predicated_tile_iterator.h
--rw-r--r--   0 root         (0) root         (0)    12890 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/transform/threadblock/predicated_scale_bias_vector_access_iterator.h
--rw-r--r--   0 root         (0) root         (0)    11097 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/transform/threadblock/predicated_scale_bias_vector_iterator.h
--rw-r--r--   0 root         (0) root         (0)    70077 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/transform/threadblock/predicated_tile_access_iterator.h
--rw-r--r--   0 root         (0) root         (0)    28232 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/transform/threadblock/predicated_tile_access_iterator_2dthreadtile.h
--rwxr-xr-x   0 root         (0) root         (0)    10243 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/transform/threadblock/predicated_tile_access_iterator_params.h
--rw-r--r--   0 root         (0) root         (0)    31412 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/transform/threadblock/predicated_tile_access_iterator_triangular_matrix.h
--rw-r--r--   0 root         (0) root         (0)    62095 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/transform/threadblock/predicated_tile_iterator.h
--rw-r--r--   0 root         (0) root         (0)    27175 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/transform/threadblock/predicated_tile_iterator_2dthreadtile.h
--rw-r--r--   0 root         (0) root         (0)    28064 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/transform/threadblock/predicated_tile_iterator_triangular_matrix.h
--rw-r--r--   0 root         (0) root         (0)    13088 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/transform/threadblock/predicated_vector_access_iterator.h
--rw-r--r--   0 root         (0) root         (0)     8232 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/transform/threadblock/regular_scale_bias_vector_access_iterator.h
--rw-r--r--   0 root         (0) root         (0)     2638 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/transform/threadblock/regular_tile_access_iterator.h
--rw-r--r--   0 root         (0) root         (0)    13283 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/transform/threadblock/regular_tile_access_iterator_pitch_linear.h
--rw-r--r--   0 root         (0) root         (0)    18623 2023-01-27 22:40:09.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/transform/threadblock/regular_tile_access_iterator_pitch_linear_direct_conv.h
--rw-r--r--   0 root         (0) root         (0)    27922 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/transform/threadblock/regular_tile_access_iterator_tensor_op.h
--rw-r--r--   0 root         (0) root         (0)    47789 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/transform/threadblock/regular_tile_access_iterator_tensor_op_sm80.h
--rw-r--r--   0 root         (0) root         (0)     2616 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/transform/threadblock/regular_tile_iterator.h
--rw-r--r--   0 root         (0) root         (0)    16510 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/transform/threadblock/regular_tile_iterator_pitch_linear.h
--rw-r--r--   0 root         (0) root         (0)    15486 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/transform/threadblock/regular_tile_iterator_pitch_linear_2dthreadtile.h
--rw-r--r--   0 root         (0) root         (0)    36050 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/transform/threadblock/regular_tile_iterator_tensor_op.h
--rw-r--r--   0 root         (0) root         (0)    43663 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/transform/threadblock/regular_tile_iterator_tensor_op_sm70.h
--rw-r--r--   0 root         (0) root         (0)     5226 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/transform/threadblock/vector_iterator.h
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 17:06:10.045950 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/transform/warp/
--rw-r--r--   0 root         (0) root         (0)     8828 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/transform/warp/vector_fragment_iterator.h
--rw-r--r--   0 root         (0) root         (0)     8113 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/uint128.h
--rw-r--r--   0 root         (0) root         (0)     3011 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/wmma_array.h
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 17:05:54.041580 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 17:06:10.071100 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 17:06:10.114281 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/common/
--rw-r--r--   0 root         (0) root         (0)     4273 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/common/cutlass_unit_test.h
--rw-r--r--   0 root         (0) root         (0)     4288 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/common/filter_architecture.cpp
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 17:05:54.059941 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/conv/
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 17:06:11.515294 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/conv/device/
--rw-r--r--   0 root         (0) root         (0)    21797 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/conv/device/cache_testbed_output.h
--rw-r--r--   0 root         (0) root         (0)     5344 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_dgrad_implicit_gemm_cf32nhwc_cf32nhwc_cf32nhwc_simt_f32_sm50.cu
--rw-r--r--   0 root         (0) root         (0)     5443 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_dgrad_implicit_gemm_cf32nhwc_cf32nhwc_cf32nhwc_simt_f32_sm80.cu
--rw-r--r--   0 root         (0) root         (0)    11470 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_dgrad_implicit_gemm_f16nhwc_f16nhwc_f16nhwc_tensor_op_f16_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     5239 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_dgrad_implicit_gemm_f16nhwc_f16nhwc_f32nhwc_tensor_op_f32_sm70.cu
--rw-r--r--   0 root         (0) root         (0)     9110 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_dgrad_implicit_gemm_f16nhwc_f16nhwc_f32nhwc_tensor_op_f32_sm75.cu
--rw-r--r--   0 root         (0) root         (0)     8485 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_dgrad_implicit_gemm_f16nhwc_f16nhwc_f32nhwc_tensor_op_f32_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     5243 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_dgrad_implicit_gemm_f32nhwc_f32nhwc_f32nhwc_simt_f32_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     5378 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_dgrad_implicit_gemm_tf32nhwc_tf32nhwc_f32nhwc_tensor_op_f32_sm80.cu
--rw-r--r--   0 root         (0) root         (0)    12054 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_fprop_few_channels_f16nhwc_f16nhwc_f16nhwc_tensor_op_f32_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     9603 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_fprop_fixed_channels_f16nhwc_f16nhwc_f16nhwc_tensor_op_f32_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     5267 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_fprop_implicit_gemm_cf32nhwc_cf32nhwc_cf32nhwc_simt_f32_sm50.cu
--rw-r--r--   0 root         (0) root         (0)     5357 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_fprop_implicit_gemm_cf32nhwc_cf32nhwc_cf32nhwc_simt_f32_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     5089 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_fprop_implicit_gemm_f16nhwc_f16nhwc_f16nhwc_simt_f16_sm60.cu
--rw-r--r--   0 root         (0) root         (0)    13690 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_fprop_implicit_gemm_f16nhwc_f16nhwc_f16nhwc_tensor_op_f16_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     5390 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_fprop_implicit_gemm_f16nhwc_f16nhwc_f16nhwc_tensor_op_f32_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     5191 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_fprop_implicit_gemm_f16nhwc_f16nhwc_f32nhwc_tensor_op_f32_sm70.cu
--rw-r--r--   0 root         (0) root         (0)    11136 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_fprop_implicit_gemm_f16nhwc_f16nhwc_f32nhwc_tensor_op_f32_sm75.cu
--rw-r--r--   0 root         (0) root         (0)     5291 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_fprop_implicit_gemm_f16nhwc_f16nhwc_f32nhwc_tensor_op_f32_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     3551 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_fprop_implicit_gemm_f32nhwc_f32nhwc_f32nhwc_simt_f32_sm50.cu
--rw-r--r--   0 root         (0) root         (0)     5157 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_fprop_implicit_gemm_f32nhwc_f32nhwc_f32nhwc_simt_f32_sm80.cu
--rwxr-xr-x   0 root         (0) root         (0)     8278 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_fprop_implicit_gemm_qf32nhwc_qf32nhwc_qf32nhwc_simt_f32_sm50.cu
--rw-r--r--   0 root         (0) root         (0)    20555 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_fprop_implicit_gemm_s4ncxhwx_s4cxrskx_s4ncxhwx_tensor_op_s32_sm75.cu
--rw-r--r--   0 root         (0) root         (0)    20647 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_fprop_implicit_gemm_s4ncxhwx_s4cxrskx_s4ncxhwx_tensor_op_s32_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     5155 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_fprop_implicit_gemm_s4nhwc_s4nhwc_s32nhwc_tensor_op_s32_sm75.cu
--rw-r--r--   0 root         (0) root         (0)     5239 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_fprop_implicit_gemm_s4nhwc_s4nhwc_s32nhwc_tensor_op_s32_sm80.cu
--rw-r--r--   0 root         (0) root         (0)    26114 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_fprop_implicit_gemm_s8ncxhwx_s8cxrskx_s8ncxhwx_tensor_op_s32_sm75.cu
--rw-r--r--   0 root         (0) root         (0)    26210 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_fprop_implicit_gemm_s8ncxhwx_s8cxrskx_s8ncxhwx_tensor_op_s32_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     5111 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_fprop_implicit_gemm_s8nhwc_s8nhwc_s32nhwc_tensor_op_s32_sm75.cu
--rw-r--r--   0 root         (0) root         (0)     5194 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_fprop_implicit_gemm_s8nhwc_s8nhwc_s32nhwc_tensor_op_s32_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     5738 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_fprop_implicit_gemm_tf32nhwc_tf32nhwc_f32nhwc_tensor_op_f32_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     5439 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_fprop_with_broadcast_sm70.cu
--rw-r--r--   0 root         (0) root         (0)     7363 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_fprop_with_broadcast_sm75.cu
--rw-r--r--   0 root         (0) root         (0)     3984 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_fprop_with_reduction_sm75.cu
--rw-r--r--   0 root         (0) root         (0)    38060 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_problems.h
--rw-r--r--   0 root         (0) root         (0)    14471 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_strided_dgrad_implicit_gemm_f16nhwc_f16nhwc_f32nhwc_tensor_op_f32_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     4662 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_strided_dgrad_implicit_gemm_tf32nhwc_tf32nhwc_f32nhwc_tensor_op_f32_sm80.cu
--rw-r--r--   0 root         (0) root         (0)    26254 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_testbed.h
--rw-r--r--   0 root         (0) root         (0)    21182 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_testbed_interleaved.h
--rw-r--r--   0 root         (0) root         (0)     5179 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_wgrad_implicit_gemm_cf32nhwc_cf32nhwc_cf32nhwc_simt_f32_sm50.cu
--rw-r--r--   0 root         (0) root         (0)     5358 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_wgrad_implicit_gemm_cf32nhwc_cf32nhwc_cf32nhwc_simt_f32_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     5264 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_wgrad_implicit_gemm_f16nhwc_f16nhwc_f16nhwc_tensor_op_f16_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     3615 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_wgrad_implicit_gemm_f16nhwc_f16nhwc_f32nhwc_tensor_op_f32_sm70.cu
--rw-r--r--   0 root         (0) root         (0)     7591 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_wgrad_implicit_gemm_f16nhwc_f16nhwc_f32nhwc_tensor_op_f32_sm75.cu
--rw-r--r--   0 root         (0) root         (0)    10514 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_wgrad_implicit_gemm_f16nhwc_f16nhwc_f32nhwc_tensor_op_f32_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     5157 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_wgrad_implicit_gemm_f32nhwc_f32nhwc_f32nhwc_simt_f32_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     5772 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_wgrad_implicit_gemm_tf32nhwc_tf32nhwc_f32nhwc_tensor_op_f32_sm80.cu
--rw-r--r--   0 root         (0) root         (0)    23472 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_with_broadcast_testbed.h
--rw-r--r--   0 root         (0) root         (0)    21524 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_with_reduction_testbed.h
--rw-r--r--   0 root         (0) root         (0)     5135 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/conv/device/conv3d_dgrad_implicit_gemm_f16ndhwc_f16ndhwc_f32ndhwc_tensor_op_f32_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     5347 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/conv/device/conv3d_dgrad_implicit_gemm_tf32ndhwc_tf32ndhwc_f32ndhwc_tensor_op_f32_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     3736 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/conv/device/conv3d_fprop_implicit_gemm_f16ndhwc_f16ndhwc_f32ndhwc_tensor_op_f32_sm75.cu
--rw-r--r--   0 root         (0) root         (0)     6560 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/conv/device/conv3d_fprop_implicit_gemm_f16ndhwc_f16ndhwc_f32ndhwc_tensor_op_f32_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     5257 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/conv/device/conv3d_fprop_implicit_gemm_tf32ndhwc_tf32ndhwc_f32ndhwc_tensor_op_f32_sm80.cu
--rw-r--r--   0 root         (0) root         (0)    12276 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/conv/device/conv3d_problems.h
--rw-r--r--   0 root         (0) root         (0)    21659 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/conv/device/conv3d_testbed.h
--rw-r--r--   0 root         (0) root         (0)     3622 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/conv/device/conv3d_wgrad_implicit_gemm_f16ndhwc_f16ndhwc_f32ndhwc_tensor_op_f32_sm75.cu
--rw-r--r--   0 root         (0) root         (0)     6560 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/conv/device/conv3d_wgrad_implicit_gemm_f16ndhwc_f16ndhwc_f32ndhwc_tensor_op_f32_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     5256 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/conv/device/conv3d_wgrad_implicit_gemm_tf32ndhwc_tf32ndhwc_f32ndhwc_tensor_op_f32_sm80.cu
--rw-r--r--   0 root         (0) root         (0)    17700 2023-01-27 22:40:09.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/conv/device/depthwise_conv2d_direct_conv_testbed.h
--rw-r--r--   0 root         (0) root         (0)    18451 2023-01-27 22:40:09.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/conv/device/depthwise_conv2d_fprop_direct_conv_f16nhwc_f16nhwc_f16nhwc_simt_f16_sm60.cu
--rw-r--r--   0 root         (0) root         (0)    22194 2023-01-27 22:40:09.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/conv/device/depthwise_conv2d_fprop_direct_conv_fixed_stride_dilation_f16nhwc_f16nhwc_f16nhwc_simt_f16_sm60.cu
--rw-r--r--   0 root         (0) root         (0)     9383 2023-01-27 22:40:09.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/conv/device/depthwise_conv2d_fprop_implicit_gemm_f16nhwc_f16nhwc_f16nhwc_simt_f16_sm60.cu
--rw-r--r--   0 root         (0) root         (0)     9387 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/conv/device/depthwise_fprop_implicit_gemm_f16nhwc_f16nhwc_f16nhwc_simt_f16_sm60.cu
--rw-r--r--   0 root         (0) root         (0)    10179 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/conv/device/group_conv2d_fprop_implicit_gemm_f16nhwc_f16nhwc_f16nhwc_tensor_op_f32_sm80.cu
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 17:06:11.812096 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/core/
--rw-r--r--   0 root         (0) root         (0)     7365 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/core/array.cu
--rw-r--r--   0 root         (0) root         (0)     7353 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/core/bfloat16.cu
--rw-r--r--   0 root         (0) root         (0)     6981 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/core/complex.cu
--rw-r--r--   0 root         (0) root         (0)     4009 2023-01-27 22:40:09.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/core/float8.cu
--rw-r--r--   0 root         (0) root         (0)    13001 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/core/functional.cu
--rw-r--r--   0 root         (0) root         (0)     3553 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/core/half.cu
--rw-r--r--   0 root         (0) root         (0)     5295 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/core/matrix.cu
--rw-r--r--   0 root         (0) root         (0)     8592 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/core/matrix_coord.cu
--rw-r--r--   0 root         (0) root         (0)     7276 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/core/numeric_conversion.cu
--rw-r--r--   0 root         (0) root         (0)     8148 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/core/predicate_vector.cu
--rw-r--r--   0 root         (0) root         (0)     5777 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/core/quaternion.cu
--rw-r--r--   0 root         (0) root         (0)     6746 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/core/tensor_ref.cu
--rw-r--r--   0 root         (0) root         (0)     8885 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/core/tensor_view.cu
--rw-r--r--   0 root         (0) root         (0)     2050 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/core/test_unit_core.cpp
--rw-r--r--   0 root         (0) root         (0)     7088 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/core/tfloat32.cu
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 17:05:54.086315 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/cute/
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 17:06:11.860527 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/cute/ampere/
--rw-r--r--   0 root         (0) root         (0)     3527 2023-01-27 22:40:09.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/cute/ampere/cp_async.cu
--rw-r--r--   0 root         (0) root         (0)    14320 2023-01-27 22:40:09.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/cute/ampere/ldsm.cu
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 17:06:12.113160 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/cute/core/
--rw-r--r--   0 root         (0) root         (0)     3332 2023-01-27 22:40:09.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/cute/core/bitfield.cpp
--rw-r--r--   0 root         (0) root         (0)     4861 2023-01-27 22:40:09.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/cute/core/coalesce.cpp
--rw-r--r--   0 root         (0) root         (0)     5620 2023-01-27 22:40:09.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/cute/core/compare.cpp
--rw-r--r--   0 root         (0) root         (0)     7178 2023-01-27 22:40:09.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/cute/core/complement.cpp
--rw-r--r--   0 root         (0) root         (0)    12569 2023-01-27 22:40:09.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/cute/core/composition.cpp
--rw-r--r--   0 root         (0) root         (0)     4856 2023-01-27 22:40:09.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/cute/core/inverse_left.cpp
--rw-r--r--   0 root         (0) root         (0)     6702 2023-01-27 22:40:09.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/cute/core/inverse_right.cpp
--rw-r--r--   0 root         (0) root         (0)     6734 2023-01-27 22:40:09.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/cute/core/logical_divide.cpp
--rw-r--r--   0 root         (0) root         (0)     5914 2023-01-27 22:40:09.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/cute/core/logical_product.cpp
--rw-r--r--   0 root         (0) root         (0)     3488 2023-01-27 22:40:09.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/cute/core/mixedbits.cpp
--rw-r--r--   0 root         (0) root         (0)     2342 2023-01-27 22:40:09.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/cute/core/transform.cpp
--rw-r--r--   0 root         (0) root         (0)    13304 2023-01-27 22:40:09.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/cute/core/tuple.cpp
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 17:06:12.178557 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/cute/hopper/
--rw-r--r--   0 root         (0) root         (0)    14365 2023-01-27 22:40:09.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/cute/hopper/stsm.cu
--rw-r--r--   0 root         (0) root         (0)    18990 2023-01-27 22:40:09.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/cute/hopper/tma_load.cu
--rw-r--r--   0 root         (0) root         (0)    13875 2023-01-27 22:40:09.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/cute/hopper/tma_store.cu
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 17:06:12.204699 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/cute/layout/
--rw-r--r--   0 root         (0) root         (0)     4544 2023-01-27 22:40:09.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/cute/layout/layout_operator.cu
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 17:05:54.109314 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/epilogue/
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 17:06:12.275070 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/epilogue/thread/
--rw-r--r--   0 root         (0) root         (0)    15783 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/epilogue/thread/activation.cu
--rw-r--r--   0 root         (0) root         (0)     6534 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/epilogue/thread/linear_combination.cu
--rw-r--r--   0 root         (0) root         (0)     9964 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/epilogue/thread/linear_combination_planar_complex.cu
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 17:06:12.739250 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/epilogue/threadblock/
--rw-r--r--   0 root         (0) root         (0)    13824 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/epilogue/threadblock/epilogue_planar_complex.cu
--rw-r--r--   0 root         (0) root         (0)    27176 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/epilogue/threadblock/epilogue_simt.cu
--rw-r--r--   0 root         (0) root         (0)    12061 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/epilogue/threadblock/epilogue_simt_sm60.cu
--rw-r--r--   0 root         (0) root         (0)    25275 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/epilogue/threadblock/epilogue_simt_sm61.cu
--rw-r--r--   0 root         (0) root         (0)    84612 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/epilogue/threadblock/epilogue_tensor_op.cu
--rw-r--r--   0 root         (0) root         (0)    70486 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/epilogue/threadblock/epilogue_volta_tensor_op.cu
--rw-r--r--   0 root         (0) root         (0)    25293 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/epilogue/threadblock/epilogue_with_reduction_tensor_op.cu
--rw-r--r--   0 root         (0) root         (0)    13012 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/epilogue/threadblock/epilogue_with_reduction_testbed.h
--rw-r--r--   0 root         (0) root         (0)     7743 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/epilogue/threadblock/epilogue_wmma_tensor_op_sm70.cu
--rw-r--r--   0 root         (0) root         (0)    19178 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/epilogue/threadblock/output_tile_threadmap.cu
--rw-r--r--   0 root         (0) root         (0)    28433 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/epilogue/threadblock/predicated_tile_iterator.cu
--rw-r--r--   0 root         (0) root         (0)    11038 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/epilogue/threadblock/testbed.h
--rw-r--r--   0 root         (0) root         (0)    11734 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/epilogue/threadblock/testbed_planar_complex.h
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 17:06:12.800491 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/epilogue/warp/
--rw-r--r--   0 root         (0) root         (0)     6783 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/epilogue/warp/fragment_iterator_tensor_op.cu
--rw-r--r--   0 root         (0) root         (0)     7275 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/epilogue/warp/fragment_iterator_volta_tensor_op.cu
--rw-r--r--   0 root         (0) root         (0)     6616 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/epilogue/warp/fragment_iterator_wmma_tensor_op.cu
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 17:05:54.148835 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 17:06:20.432677 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/
--rw-r--r--   0 root         (0) root         (0)    10149 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_b1t_b1n_s32n_tensor_op_s32_sm75.cu
--rw-r--r--   0 root         (0) root         (0)    17819 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_b1t_b1n_s32n_tensor_op_s32_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     8903 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_b1t_b1n_s32n_wmma_tensor_op_s32_sm75.cu
--rw-r--r--   0 root         (0) root         (0)    10124 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_b1t_b1n_s32t_tensor_op_s32_sm75.cu
--rw-r--r--   0 root         (0) root         (0)    17984 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_b1t_b1n_s32t_tensor_op_s32_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     8885 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_b1t_b1n_s32t_wmma_tensor_op_s32_sm75.cu
--rw-r--r--   0 root         (0) root         (0)    16447 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_bf16n_bf16n_f32t_tensor_op_f32_sm80.cu
--rw-r--r--   0 root         (0) root         (0)    16495 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_bf16t_bf16t_bf16t_tensor_op_f32_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     8293 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_cf32n_cf32t_cf32t_tensor_op_tf32_f32_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     8292 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_cf32t_cf32n_cf32t_tensor_op_tf32_f32_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     6714 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_cf64n_cf64t_cf64t_tensor_op_f64_gaussian_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     6747 2023-01-27 22:40:09.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_cf64n_cf64t_cf64t_tensor_op_f64_gaussian_sm90.cu
--rw-r--r--   0 root         (0) root         (0)     7895 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_cf64n_cf64t_cf64t_tensor_op_f64_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     7930 2023-01-27 22:40:09.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_cf64n_cf64t_cf64t_tensor_op_f64_sm90.cu
--rw-r--r--   0 root         (0) root         (0)     6516 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_cf64t_cf64n_cf64t_tensor_op_f64_gaussian_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     6549 2023-01-27 22:40:09.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_cf64t_cf64n_cf64t_tensor_op_f64_gaussian_sm90.cu
--rw-r--r--   0 root         (0) root         (0)     9016 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_cf64t_cf64n_cf64t_tensor_op_f64_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     9053 2023-01-27 22:40:09.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_cf64t_cf64n_cf64t_tensor_op_f64_sm90.cu
--rw-r--r--   0 root         (0) root         (0)     4628 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16n_f16n_f16n_direct_store_tensor_op_f32_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     6165 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16n_f16n_f16n_wmma_tensor_op_f16_sm70.cu
--rw-r--r--   0 root         (0) root         (0)     6124 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16n_f16n_f16n_wmma_tensor_op_f32_sm70.cu
--rw-r--r--   0 root         (0) root         (0)     9634 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16n_f16n_f16t_tensor_op_f32_sm75.cu
--rw-r--r--   0 root         (0) root         (0)    16357 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16n_f16n_f16t_tensor_op_f32_sm80.cu
--rw-r--r--   0 root         (0) root         (0)    13189 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16n_f16n_f16t_tensor_op_f32_sparse_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     8845 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16n_f16n_f16t_volta_tensor_op_f32_sm70.cu
--rw-r--r--   0 root         (0) root         (0)    13583 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16n_f16n_f16t_wmma_tensor_op_f16_sm70.cu
--rw-r--r--   0 root         (0) root         (0)    13464 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16n_f16n_f16t_wmma_tensor_op_f32_sm70.cu
--rw-r--r--   0 root         (0) root         (0)     9571 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16n_f16n_f32n_tensor_op_f32_sm75.cu
--rw-r--r--   0 root         (0) root         (0)    16239 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16n_f16n_f32n_tensor_op_f32_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     6140 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16n_f16n_f32n_wmma_tensor_op_f32_sm70.cu
--rw-r--r--   0 root         (0) root         (0)     9544 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16n_f16n_f32t_tensor_op_f32_sm75.cu
--rw-r--r--   0 root         (0) root         (0)    16417 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16n_f16n_f32t_tensor_op_f32_sm80.cu
--rw-r--r--   0 root         (0) root         (0)    13075 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16n_f16n_f32t_tensor_op_f32_sparse_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     8775 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16n_f16n_f32t_volta_tensor_op_f32_sm70.cu
--rw-r--r--   0 root         (0) root         (0)    11470 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16n_f16n_f32t_wmma_tensor_op_f32_sm70.cu
--rw-r--r--   0 root         (0) root         (0)     6156 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16n_f16t_f16n_wmma_tensor_op_f16_sm70.cu
--rw-r--r--   0 root         (0) root         (0)     6116 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16n_f16t_f16n_wmma_tensor_op_f32_sm70.cu
--rw-r--r--   0 root         (0) root         (0)     3528 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16n_f16t_f16t_tensor_op_f16_slicedk_sm75.cu
--rw-r--r--   0 root         (0) root         (0)     3539 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16n_f16t_f16t_tensor_op_f16_slicedk_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     7965 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16n_f16t_f16t_tensor_op_f16_sm75.cu
--rw-r--r--   0 root         (0) root         (0)    16470 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16n_f16t_f16t_tensor_op_f16_sm80.cu
--rw-r--r--   0 root         (0) root         (0)    13273 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16n_f16t_f16t_tensor_op_f16_sparse_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     3648 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16n_f16t_f16t_tensor_op_f32_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     8608 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16n_f16t_f16t_volta_tensor_op_f16_sm70.cu
--rw-r--r--   0 root         (0) root         (0)    13518 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16n_f16t_f16t_wmma_tensor_op_f16_sm70.cu
--rw-r--r--   0 root         (0) root         (0)     3645 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16n_f16t_f16t_wmma_tensor_op_f32_sm70.cu
--rw-r--r--   0 root         (0) root         (0)     6096 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16n_f16t_f32n_wmma_tensor_op_f32_sm70.cu
--rw-r--r--   0 root         (0) root         (0)     7845 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16n_f16t_f32t_tensor_op_f32_sm75.cu
--rw-r--r--   0 root         (0) root         (0)    16339 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16n_f16t_f32t_tensor_op_f32_sm80.cu
--rw-r--r--   0 root         (0) root         (0)    13008 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16n_f16t_f32t_tensor_op_f32_sparse_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     8505 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16n_f16t_f32t_volta_tensor_op_f32_sm70.cu
--rw-r--r--   0 root         (0) root         (0)    11497 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16n_f16t_f32t_wmma_tensor_op_f32_sm70.cu
--rw-r--r--   0 root         (0) root         (0)    11090 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16t_f16n_f16n_singlestage_wmma_tensor_op_f16_sm70.cu
--rw-r--r--   0 root         (0) root         (0)     6156 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16t_f16n_f16n_wmma_tensor_op_f16_sm70.cu
--rw-r--r--   0 root         (0) root         (0)     6116 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16t_f16n_f16n_wmma_tensor_op_f32_sm70.cu
--rw-r--r--   0 root         (0) root         (0)    11066 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16t_f16n_f16t_singlestage_wmma_tensor_op_f16_sm70.cu
--rw-r--r--   0 root         (0) root         (0)    17114 2023-01-27 22:40:09.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16t_f16n_f16t_tensor_op_f16_broadcast_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     3528 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16t_f16n_f16t_tensor_op_f16_slicedk_sm75.cu
--rw-r--r--   0 root         (0) root         (0)     3540 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16t_f16n_f16t_tensor_op_f16_slicedk_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     7964 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16t_f16n_f16t_tensor_op_f16_sm75.cu
--rw-r--r--   0 root         (0) root         (0)    16457 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16t_f16n_f16t_tensor_op_f16_sm80.cu
--rw-r--r--   0 root         (0) root         (0)    13266 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16t_f16n_f16t_tensor_op_f16_sparse_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     8933 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16t_f16n_f16t_volta_tensor_op_f16_sm70.cu
--rw-r--r--   0 root         (0) root         (0)    13551 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16t_f16n_f16t_wmma_tensor_op_f16_sm70.cu
--rw-r--r--   0 root         (0) root         (0)    13540 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16t_f16n_f16t_wmma_tensor_op_f32_sm70.cu
--rw-r--r--   0 root         (0) root         (0)     6130 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16t_f16n_f32n_wmma_tensor_op_f32_sm70.cu
--rw-r--r--   0 root         (0) root         (0)     8160 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16t_f16n_f32t_singlestage_wmma_tensor_op_f32_sm70.cu
--rw-r--r--   0 root         (0) root         (0)     7847 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16t_f16n_f32t_tensor_op_f32_sm75.cu
--rw-r--r--   0 root         (0) root         (0)    16131 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16t_f16n_f32t_tensor_op_f32_sm80.cu
--rw-r--r--   0 root         (0) root         (0)    13014 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16t_f16n_f32t_tensor_op_f32_sparse_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     8754 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16t_f16n_f32t_volta_tensor_op_f32_sm70.cu
--rw-r--r--   0 root         (0) root         (0)    11497 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16t_f16n_f32t_wmma_tensor_op_f32_sm70.cu
--rw-r--r--   0 root         (0) root         (0)     6147 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16t_f16t_f16n_wmma_tensor_op_f16_sm70.cu
--rw-r--r--   0 root         (0) root         (0)     6107 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16t_f16t_f16n_wmma_tensor_op_f32_sm70.cu
--rw-r--r--   0 root         (0) root         (0)    13518 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16t_f16t_f16t_wmma_tensor_op_f16_sm70.cu
--rw-r--r--   0 root         (0) root         (0)    13398 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16t_f16t_f16t_wmma_tensor_op_f32_sm70.cu
--rw-r--r--   0 root         (0) root         (0)     7845 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16t_f16t_f32n_tensor_op_f32_sm75.cu
--rw-r--r--   0 root         (0) root         (0)    16149 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16t_f16t_f32n_tensor_op_f32_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     6119 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16t_f16t_f32n_wmma_tensor_op_f32_sm70.cu
--rw-r--r--   0 root         (0) root         (0)     7827 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16t_f16t_f32t_tensor_op_f32_sm75.cu
--rw-r--r--   0 root         (0) root         (0)    16101 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16t_f16t_f32t_tensor_op_f32_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     9526 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16t_f16t_f32t_tensor_op_f32_sparse_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     7898 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16t_f16t_f32t_volta_tensor_op_f32_sm70.cu
--rw-r--r--   0 root         (0) root         (0)    11470 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16t_f16t_f32t_wmma_tensor_op_f32_sm70.cu
--rw-r--r--   0 root         (0) root         (0)     3584 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f32n_f32n_f32t_tensor_op_bf16_f32_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     3473 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f32n_f32n_f32t_tensor_op_f32_sm80.cu
--rw-r--r--   0 root         (0) root         (0)    12967 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f32n_f32n_f32t_tensor_op_f32_sparse_sm80.cu
--rw-r--r--   0 root         (0) root         (0)    12931 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f32n_f32t_f32t_tensor_op_f32_sparse_sm80.cu
--rw-r--r--   0 root         (0) root         (0)    12930 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f32t_f32n_f32t_tensor_op_f32_sparse_sm80.cu
--rw-r--r--   0 root         (0) root         (0)    12895 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f32t_f32t_f32t_tensor_op_f32_sparse_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     8349 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f64n_f64t_f64t_tensor_op_f64_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     7300 2023-01-27 22:40:09.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f64n_f64t_f64t_tensor_op_f64_sm90.cu
--rw-r--r--   0 root         (0) root         (0)     8348 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f64t_f64n_f64t_tensor_op_f64_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     7291 2023-01-27 22:40:09.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f64t_f64n_f64t_tensor_op_f64_sm90.cu
--rw-r--r--   0 root         (0) root         (0)    10240 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_grouped_scheduler_sm80.cu
--rw-r--r--   0 root         (0) root         (0)    26146 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_grouped_sm80.cu
--rw-r--r--   0 root         (0) root         (0)    11339 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_planar_complex_f16_f16_f32_tensor_op_sm70.cu
--rw-r--r--   0 root         (0) root         (0)     7346 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_planar_complex_f16_f16_f32_tensor_op_sm75.cu
--rw-r--r--   0 root         (0) root         (0)    12397 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_planar_complex_f16_f16_f32_tensor_op_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     6859 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_s4n_s4t_s4n_tensor_op_s32_sm75.cu
--rw-r--r--   0 root         (0) root         (0)     7239 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_s4n_s4t_s4n_tensor_op_s32_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     8121 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_s4t_s4n_s32n_tensor_op_s32_sm75.cu
--rw-r--r--   0 root         (0) root         (0)    16882 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_s4t_s4n_s32n_tensor_op_s32_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     8407 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_s4t_s4n_s32n_wmma_tensor_op_s32_sm75.cu
--rw-r--r--   0 root         (0) root         (0)     8103 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_s4t_s4n_s32t_tensor_op_s32_sm75.cu
--rw-r--r--   0 root         (0) root         (0)    17111 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_s4t_s4n_s32t_tensor_op_s32_sm80.cu
--rw-r--r--   0 root         (0) root         (0)    12637 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_s4t_s4n_s32t_tensor_op_s32_sparse_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     8388 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_s4t_s4n_s32t_wmma_tensor_op_s32_sm75.cu
--rw-r--r--   0 root         (0) root         (0)    10004 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_s4t_s4n_s4n_tensor_op_s32_sm75.cu
--rw-r--r--   0 root         (0) root         (0)    17544 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_s4t_s4n_s4n_tensor_op_s32_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     9980 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_s4t_s4n_s4t_tensor_op_s32_sm75.cu
--rw-r--r--   0 root         (0) root         (0)    17544 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_s4t_s4n_s4t_tensor_op_s32_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     9588 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_s8n_s8t_s8n_tensor_op_s32_sm75.cu
--rw-r--r--   0 root         (0) root         (0)    11288 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_s8n_s8t_s8n_tensor_op_s32_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     7977 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_s8t_s8n_s32n_tensor_op_s32_sm75.cu
--rw-r--r--   0 root         (0) root         (0)    16531 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_s8t_s8n_s32n_tensor_op_s32_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     5693 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_s8t_s8n_s32n_wmma_tensor_op_s32_sm72.cu
--rw-r--r--   0 root         (0) root         (0)     7959 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_s8t_s8n_s32t_tensor_op_s32_sm75.cu
--rw-r--r--   0 root         (0) root         (0)    16691 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_s8t_s8n_s32t_tensor_op_s32_sm80.cu
--rw-r--r--   0 root         (0) root         (0)    12408 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_s8t_s8n_s32t_tensor_op_s32_sparse_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     6864 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_s8t_s8n_s32t_wmma_tensor_op_s32_sm72.cu
--rw-r--r--   0 root         (0) root         (0)     7744 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_s8t_s8n_s8n_tensor_op_s32_sm75.cu
--rw-r--r--   0 root         (0) root         (0)    16531 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_s8t_s8n_s8n_tensor_op_s32_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     6675 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_s8t_s8n_s8n_wmma_tensor_op_s32_sm72.cu
--rw-r--r--   0 root         (0) root         (0)     7752 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_s8t_s8n_s8t_tensor_op_s32_sm75.cu
--rw-r--r--   0 root         (0) root         (0)    16484 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_s8t_s8n_s8t_tensor_op_s32_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     6663 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_s8t_s8n_s8t_wmma_tensor_op_s32_sm72.cu
--rw-r--r--   0 root         (0) root         (0)     4663 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_splitk_serial_tensor_op_sm75.cu
--rw-r--r--   0 root         (0) root         (0)     4945 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_splitk_simt_sm50.cu
--rw-r--r--   0 root         (0) root         (0)     6616 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_splitk_tensor_op_sm70.cu
--rw-r--r--   0 root         (0) root         (0)    10581 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_splitk_tensor_op_sm75.cu
--rw-r--r--   0 root         (0) root         (0)    16950 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_tf32n_tf32n_f32t_tensor_op_f32_sm80.cu
--rw-r--r--   0 root         (0) root         (0)    16902 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_tf32n_tf32t_f32t_tensor_op_f32_sm80.cu
--rw-r--r--   0 root         (0) root         (0)    15131 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_tf32t_tf32n_f32t_tensor_op_f32_sm80.cu
--rw-r--r--   0 root         (0) root         (0)    16855 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_tf32t_tf32t_f32t_tensor_op_f32_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     6854 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_u8t_u8n_s32t_wmma_tensor_op_s32_sm72.cu
--rw-r--r--   0 root         (0) root         (0)     6686 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_universal_cf32n_cf32n_cf32n_tensor_op_f32_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     6755 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_universal_cf64n_cf64t_cf64t_tensor_op_f64_gaussian_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     6687 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_universal_cf64n_cf64t_cf64t_tensor_op_f64_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     4726 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_universal_f16n_f16t_f32n_tensor_op_f32_sm75.cu
--rw-r--r--   0 root         (0) root         (0)     4718 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_universal_f16n_f16t_f32t_tensor_op_f32_sm75.cu
--rw-r--r--   0 root         (0) root         (0)    16715 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_with_broadcast_f16n_f16n_f16n_tensorop_f32_sm75.cu
--rw-r--r--   0 root         (0) root         (0)    12841 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_with_reduction_f16n_f16n_f16n_tensorop_f32_sm75.cu
--rw-r--r--   0 root         (0) root         (0)     4544 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_with_reduction_f16t_f16n_f16n_tensorop_f32_sm80.cu
--rw-r--r--   0 root         (0) root         (0)    13157 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemv.cu
--rw-r--r--   0 root         (0) root         (0)     6028 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/hemm_cf32h_cf32n_tensor_op_f32_ls_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     6031 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/hemm_cf32h_cf32n_tensor_op_f32_rs_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     6064 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/hemm_cf32h_cf32n_tensor_op_fast_f32_ls_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     6067 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/hemm_cf32h_cf32n_tensor_op_fast_f32_rs_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     4909 2023-01-27 22:40:09.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/hemm_cf64_cf64_cf64_tensor_op_f64_sm90.cu
--rw-r--r--   0 root         (0) root         (0)     6088 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/hemm_cf64h_cf64n_cf64n_tensor_op_ls_f64_gaussian_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     6037 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/hemm_cf64h_cf64n_cf64n_tensor_op_ls_f64_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     6040 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/hemm_cf64h_cf64n_cf64n_tensor_op_rs_f64_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     5382 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/her2k_cf32h_cf32n_tensor_op_f32_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     5406 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/her2k_cf32h_cf32n_tensor_op_fast_f32_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     5402 2023-01-27 22:40:09.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/her2k_cf64_cf64_tensor_op_f64_sm90.cu
--rw-r--r--   0 root         (0) root         (0)    13055 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/her2k_cf64h_cf64n_tensor_op_f64_grouped_sm80.cu
--rw-r--r--   0 root         (0) root         (0)    13027 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/her2k_cf64n_cf64n_tensor_op_f64_grouped_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     5388 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/her2k_cf64n_cf64n_tensor_op_f64_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     6939 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/her2k_cf64n_cf64t_tensor_op_f64_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     7677 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/herk_cf32h_cf32n_tensor_op_f32_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     7725 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/herk_cf32h_cf32n_tensor_op_fast_f32_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     3854 2023-01-27 22:40:09.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/herk_cf64_cf64_tensor_op_f64_sm90.cu
--rw-r--r--   0 root         (0) root         (0)     6396 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/herk_cf64h_cf64n_tensor_op_f64_sm80.cu
--rw-r--r--   0 root         (0) root         (0)    10027 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/multistage_testbed.h
--rw-r--r--   0 root         (0) root         (0)     9189 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/multistage_testbed_interleaved.h
--rw-r--r--   0 root         (0) root         (0)    11186 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/rank_2k_grouped_scheduler_sm80.cu
--rw-r--r--   0 root         (0) root         (0)    46795 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/simt_cgemm_nn_sm50.cu
--rw-r--r--   0 root         (0) root         (0)    54085 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/simt_cgemm_nt_sm50.cu
--rw-r--r--   0 root         (0) root         (0)     8318 2023-01-27 22:40:09.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/simt_cgemm_nt_sm80.cu
--rw-r--r--   0 root         (0) root         (0)    46687 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/simt_cgemm_tn_sm50.cu
--rw-r--r--   0 root         (0) root         (0)     8411 2023-01-27 22:40:09.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/simt_cgemm_tn_sm80.cu
--rw-r--r--   0 root         (0) root         (0)    46578 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/simt_cgemm_tt_sm50.cu
--rw-r--r--   0 root         (0) root         (0)    40533 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/simt_dgemm_nn_sm50.cu
--rw-r--r--   0 root         (0) root         (0)    47656 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/simt_dgemm_nt_sm50.cu
--rw-r--r--   0 root         (0) root         (0)    40441 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/simt_dgemm_tn_sm50.cu
--rw-r--r--   0 root         (0) root         (0)    40354 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/simt_dgemm_tt_sm50.cu
--rw-r--r--   0 root         (0) root         (0)     3513 2023-01-27 22:40:09.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/simt_f8gemm_tn_sm50.cu
--rw-r--r--   0 root         (0) root         (0)    89517 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/simt_hgemm_nn_sm50.cu
--rw-r--r--   0 root         (0) root         (0)    89304 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/simt_hgemm_nt_sm50.cu
--rw-r--r--   0 root         (0) root         (0)    89304 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/simt_hgemm_tn_sm50.cu
--rw-r--r--   0 root         (0) root         (0)    89091 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/simt_hgemm_tt_sm50.cu
--rw-r--r--   0 root         (0) root         (0)    69175 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/simt_igemm_nn_sm50.cu
--rw-r--r--   0 root         (0) root         (0)    71438 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/simt_igemm_nt_sm50.cu
--rw-r--r--   0 root         (0) root         (0)    67796 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/simt_igemm_tn_sm50.cu
--rw-r--r--   0 root         (0) root         (0)    70056 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/simt_igemm_tt_sm50.cu
--rw-r--r--   0 root         (0) root         (0)     7156 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/simt_int8_igemm_sm61.cu
--rw-r--r--   0 root         (0) root         (0)     6067 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/simt_int8_igemm_sm61_perf.cu
--rw-r--r--   0 root         (0) root         (0)     9063 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/simt_int8_igemm_sm61_sliced_k.cu
--rw-r--r--   0 root         (0) root         (0)    35759 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/simt_qgemm_nn_sm50.cu
--rw-r--r--   0 root         (0) root         (0)    35678 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/simt_qgemm_nt_sm50.cu
--rw-r--r--   0 root         (0) root         (0)    35678 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/simt_qgemm_tn_sm50.cu
--rw-r--r--   0 root         (0) root         (0)    35597 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/simt_qgemm_tt_sm50.cu
--rw-r--r--   0 root         (0) root         (0)    70872 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/simt_sgemm_nn_sm50.cu
--rw-r--r--   0 root         (0) root         (0)    73136 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/simt_sgemm_nt_sm50.cu
--rw-r--r--   0 root         (0) root         (0)     8870 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/simt_sgemm_nt_sm80.cu
--rw-r--r--   0 root         (0) root         (0)    69488 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/simt_sgemm_tn_sm50.cu
--rw-r--r--   0 root         (0) root         (0)     8865 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/simt_sgemm_tn_sm80.cu
--rw-r--r--   0 root         (0) root         (0)    71755 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/simt_sgemm_tt_sm50.cu
--rw-r--r--   0 root         (0) root         (0)    33231 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/simt_zgemm_nn_sm50.cu
--rw-r--r--   0 root         (0) root         (0)    33156 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/simt_zgemm_nt_sm50.cu
--rw-r--r--   0 root         (0) root         (0)    33156 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/simt_zgemm_tn_sm50.cu
--rw-r--r--   0 root         (0) root         (0)    33081 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/simt_zgemm_tt_sm50.cu
--rw-r--r--   0 root         (0) root         (0)     5238 2023-01-27 22:40:09.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/sm50_gemm_f32_f32_f32_simt.cu
--rw-r--r--   0 root         (0) root         (0)     5253 2023-01-27 22:40:09.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/sm50_gemm_f64_f64_f64_simt.cu
--rw-r--r--   0 root         (0) root         (0)     5357 2023-01-27 22:40:09.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/sm61_gemm_s8_s8_s32_simt.cu
--rw-r--r--   0 root         (0) root         (0)     5479 2023-01-27 22:40:09.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/sm80_gemm_f16_f16_f32_tensor_op_f32.cu
--rw-r--r--   0 root         (0) root         (0)     5238 2023-01-27 22:40:09.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/sm80_gemm_f32_f32_f32_simt.cu
--rw-r--r--   0 root         (0) root         (0)     5253 2023-01-27 22:40:09.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/sm80_gemm_f64_f64_f64_simt.cu
--rw-r--r--   0 root         (0) root         (0)     3875 2023-01-27 22:40:09.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/sm80_gemm_f64_f64_f64_tensor_op_f64.cu
--rw-r--r--   0 root         (0) root         (0)     3734 2023-01-27 22:40:09.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/sm80_gemm_s8_s8_s32_tensor_op.cu
--rw-r--r--   0 root         (0) root         (0)     5387 2023-01-27 22:40:09.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/sm80_gemm_tf32_tf32_f32_tensor_op_f32.cu
--rw-r--r--   0 root         (0) root         (0)     7436 2023-01-27 22:40:09.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/sm90_gemm_bf16_bf16_bf16_alignx_tensor_op_f32.cu
--rw-r--r--   0 root         (0) root         (0)     7409 2023-01-27 22:40:09.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/sm90_gemm_bf16_bf16_bf16_tensor_op_f32.cu
--rw-r--r--   0 root         (0) root         (0)    17391 2023-01-27 22:40:09.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/sm90_gemm_f16_f16_f16_alignx_tensor_op.cu
--rw-r--r--   0 root         (0) root         (0)    42504 2023-01-27 22:40:09.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/sm90_gemm_f16_f16_f16_tensor_op.cu
--rw-r--r--   0 root         (0) root         (0)    22602 2023-01-27 22:40:09.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/sm90_gemm_f16_f16_f16_tensor_op_f32_cluster_unspecialized.cu
--rw-r--r--   0 root         (0) root         (0)    22874 2023-01-27 22:40:09.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/sm90_gemm_f16_f16_f16_tensor_op_f32_cluster_warpspecialized.cu
--rw-r--r--   0 root         (0) root         (0)    42526 2023-01-27 22:40:09.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/sm90_gemm_f16_f16_f16_tensor_op_f32_cluster_warpspecialized_persistent.cu
--rw-r--r--   0 root         (0) root         (0)     3810 2023-01-27 22:40:09.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/sm90_gemm_f32_f32_f32_tensor_op_f32.cu
--rw-r--r--   0 root         (0) root         (0)     5976 2023-01-27 22:40:09.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/sm90_gemm_s8_s8_s8_alignx_tensor_op_s32.cu
--rw-r--r--   0 root         (0) root         (0)     9313 2023-01-27 22:40:09.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/sm90_gemm_s8_s8_s8_tensor_op_s32.cu
--rw-r--r--   0 root         (0) root         (0)     5986 2023-01-27 22:40:09.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/sm90_gemm_tf32_tf32_f32_alignx_tensor_op_f32.cu
--rw-r--r--   0 root         (0) root         (0)     7268 2023-01-27 22:40:09.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/sm90_gemm_tf32_tf32_f32_tensor_op_f32.cu
--rw-r--r--   0 root         (0) root         (0)     5923 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/symm_cf32n_cf32n_tensor_op_f32_ls_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     5926 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/symm_cf32n_cf32n_tensor_op_f32_rs_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     5959 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/symm_cf32n_cf32n_tensor_op_fast_f32_ls_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     5962 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/symm_cf32n_cf32n_tensor_op_fast_f32_rs_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     4839 2023-01-27 22:40:09.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/symm_cf64_cf64_cf64_tensor_op_f64_sm90.cu
--rw-r--r--   0 root         (0) root         (0)     5983 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/symm_cf64n_cf64n_cf64n_tensor_op_ls_f64_gaussian_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     5932 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/symm_cf64n_cf64n_cf64n_tensor_op_ls_f64_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     5935 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/symm_cf64n_cf64n_cf64n_tensor_op_rs_f64_sm80.cu
--rw-r--r--   0 root         (0) root         (0)    15203 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/symm_f32n_f32n_tensor_op_fast_f32_ls_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     8623 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/symm_f32n_f32n_tensor_op_fast_f32_rs_sm80.cu
--rw-r--r--   0 root         (0) root         (0)    15104 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/symm_f32t_f32t_tensor_op_fast_f32_ls_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     4777 2023-01-27 22:40:09.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/symm_f64_f64_tensor_op_f64_sm90.cu
--rw-r--r--   0 root         (0) root         (0)     8103 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/symm_f64n_f64n_tensor_op_f64_ls_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     8108 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/symm_f64n_f64n_tensor_op_f64_rs_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     8088 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/symm_f64n_f64t_tensor_op_f64_ls_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     8093 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/symm_f64n_f64t_tensor_op_f64_rs_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     8073 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/symm_f64t_f64n_tensor_op_f64_ls_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     8078 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/symm_f64t_f64n_tensor_op_f64_rs_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     8058 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/symm_f64t_f64t_tensor_op_f64_ls_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     8063 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/symm_f64t_f64t_tensor_op_f64_rs_sm80.cu
--rw-r--r--   0 root         (0) root         (0)    15071 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/symm_tf32n_f32n_tensor_op_f32_ls_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     8551 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/symm_tf32n_f32n_tensor_op_f32_rs_sm80.cu
--rw-r--r--   0 root         (0) root         (0)    14972 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/symm_tf32t_f32t_tensor_op_f32_ls_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     5362 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/syr2k_cf32n_cf32n_tensor_op_f32_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     5386 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/syr2k_cf32n_cf32n_tensor_op_fast_f32_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     5356 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/syr2k_cf32n_cf32t_tensor_op_f32_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     5380 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/syr2k_cf32n_cf32t_tensor_op_fast_f32_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     5379 2023-01-27 22:40:09.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/syr2k_cf64_cf64_tensor_op_f64_sm90.cu
--rw-r--r--   0 root         (0) root         (0)    12952 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/syr2k_cf64n_cf64n_tensor_op_f64_grouped_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     5368 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/syr2k_cf64n_cf64n_tensor_op_f64_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     7208 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/syr2k_cf64n_cf64t_tensor_op_f64_grouped_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     5362 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/syr2k_cf64n_cf64t_tensor_op_f64_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     7199 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/syr2k_cf64t_cf64n_tensor_op_f64_grouped_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     7190 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/syr2k_cf64t_cf64t_tensor_op_f64_grouped_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     4794 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/syr2k_f32n_f32n_tensor_op_fast_f32_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     4783 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/syr2k_f32t_f32n_tensor_op_fast_f32_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     4740 2023-01-27 22:40:09.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/syr2k_f64_f64_tensor_op_f64_sm90.cu
--rw-r--r--   0 root         (0) root         (0)    19145 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/syr2k_f64n_f64n_tensor_op_f64_grouped_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     7991 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/syr2k_f64n_f64n_tensor_op_f64_sm80.cu
--rw-r--r--   0 root         (0) root         (0)    11015 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/syr2k_f64n_f64t_tensor_op_f64_grouped_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     7976 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/syr2k_f64n_f64t_tensor_op_f64_sm80.cu
--rw-r--r--   0 root         (0) root         (0)    12342 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/syr2k_f64t_f64n_tensor_op_f64_grouped_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     7961 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/syr2k_f64t_f64n_tensor_op_f64_sm80.cu
--rw-r--r--   0 root         (0) root         (0)    12321 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/syr2k_f64t_f64t_tensor_op_f64_grouped_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     4786 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/syr2k_tf32n_f32n_tensor_op_f32_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     4775 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/syr2k_tf32t_f32n_tensor_op_f32_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     4993 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/syrk_cf32n_cf32n_tensor_op_f32_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     5017 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/syrk_cf32n_cf32n_tensor_op_fast_f32_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     4987 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/syrk_cf32n_cf32t_tensor_op_f32_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     5011 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/syrk_cf32n_cf32t_tensor_op_fast_f32_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     5024 2023-01-27 22:40:09.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/syrk_cf64_cf64_tensor_op_f64_sm90.cu
--rw-r--r--   0 root         (0) root         (0)     4996 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/syrk_cf64n_cf64n_tensor_op_f64_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     3793 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/syrk_cf64n_cf64t_tensor_op_f64_gaussian_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     4990 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/syrk_cf64n_cf64t_tensor_op_f64_sm80.cu
--rw-r--r--   0 root         (0) root         (0)    16083 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/syrk_f32n_f32t_tensor_op_fast_f32_sm80.cu
--rw-r--r--   0 root         (0) root         (0)    16041 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/syrk_f32t_f32t_tensor_op_fast_f32_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     4530 2023-01-27 22:40:09.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/syrk_f64_f64_tensor_op_f64_sm90.cu
--rw-r--r--   0 root         (0) root         (0)     7451 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/syrk_f64n_f64t_tensor_op_f64_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     9401 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/syrk_f64t_f64n_tensor_op_f64_sm80.cu
--rw-r--r--   0 root         (0) root         (0)    16027 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/syrk_tf32n_f32t_tensor_op_f32_sm80.cu
--rw-r--r--   0 root         (0) root         (0)    15985 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/syrk_tf32t_f32t_tensor_op_f32_sm80.cu
--rw-r--r--   0 root         (0) root         (0)    18193 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/testbed.h
--rw-r--r--   0 root         (0) root         (0)     8140 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/testbed_complex.h
--rw-r--r--   0 root         (0) root         (0)    20594 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/testbed_gemm_with_broadcast.h
--rw-r--r--   0 root         (0) root         (0)    19350 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/testbed_gemm_with_reduction.h
--rw-r--r--   0 root         (0) root         (0)    16502 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/testbed_grouped.h
--rw-r--r--   0 root         (0) root         (0)    16562 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/testbed_grouped_rank_2k.h
--rw-r--r--   0 root         (0) root         (0)    17002 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/testbed_grouped_rank_2k_scheduler.h
--rw-r--r--   0 root         (0) root         (0)    14651 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/testbed_grouped_scheduler.h
--rw-r--r--   0 root         (0) root         (0)    10134 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/testbed_interleaved.h
--rw-r--r--   0 root         (0) root         (0)     9485 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/testbed_planar_complex.h
--rw-r--r--   0 root         (0) root         (0)    20765 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/testbed_rank2k_universal.h
--rw-r--r--   0 root         (0) root         (0)    15566 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/testbed_rank_k_universal.h
--rw-r--r--   0 root         (0) root         (0)     8639 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/testbed_sanity.h
--rw-r--r--   0 root         (0) root         (0)    15773 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/testbed_sparse.h
--rw-r--r--   0 root         (0) root         (0)     6128 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/testbed_splitk.h
--rw-r--r--   0 root         (0) root         (0)    19865 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/testbed_symm_universal.h
--rw-r--r--   0 root         (0) root         (0)    20204 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/testbed_trmm_universal.h
--rw-r--r--   0 root         (0) root         (0)    16492 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/testbed_universal.h
--rw-r--r--   0 root         (0) root         (0)     2626 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/testbed_utils.h
--rw-r--r--   0 root         (0) root         (0)     9916 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/trmm_cf32n_cf32n_cf32t_tensor_op_f32_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     9988 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/trmm_cf32n_cf32n_cf32t_tensor_op_fast_f32_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     4989 2023-01-27 22:40:09.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/trmm_cf64_cf64_cf64_tensor_op_f64_sm90.cu
--rw-r--r--   0 root         (0) root         (0)     4992 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/trmm_cf64n_cf64n_cf64t_tensor_op_f64_gaussian_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     9762 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/trmm_cf64n_cf64n_cf64t_tensor_op_f64_sm80.cu
--rw-r--r--   0 root         (0) root         (0)    15614 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/trmm_f32n_f32t_f32t_tensor_op_fast_f32_ls_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     8733 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/trmm_f32n_f32t_f32t_tensor_op_fast_f32_rs_sm80.cu
--rw-r--r--   0 root         (0) root         (0)    14089 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/trmm_f32t_f32n_f32n_tensor_op_fast_f32_ls_sm80.cu
--rw-r--r--   0 root         (0) root         (0)    14444 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/trmm_f32t_f32n_f32t_tensor_op_fast_f32_ls_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     4608 2023-01-27 22:40:09.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/trmm_f64_f64_f64_tensor_op_f64_sm90.cu
--rw-r--r--   0 root         (0) root         (0)    12798 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/trmm_f64n_f64n_f64t_tensor_op_f64_ls_sm80.cu
--rw-r--r--   0 root         (0) root         (0)    12809 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/trmm_f64n_f64n_f64t_tensor_op_f64_rs_sm80.cu
--rw-r--r--   0 root         (0) root         (0)    12764 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/trmm_f64n_f64t_f64t_tensor_op_f64_rs_sm80.cu
--rw-r--r--   0 root         (0) root         (0)    12768 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/trmm_f64t_f64t_f64n_tensor_op_f64_ls_sm80.cu
--rw-r--r--   0 root         (0) root         (0)    12779 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/trmm_f64t_f64t_f64n_tensor_op_f64_rs_sm80.cu
--rw-r--r--   0 root         (0) root         (0)    15504 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/trmm_tf32n_tf32t_f32t_tensor_op_f32_ls_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     8673 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/trmm_tf32n_tf32t_f32t_tensor_op_f32_rs_sm80.cu
--rw-r--r--   0 root         (0) root         (0)    13989 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/trmm_tf32t_tf32n_f32n_tensor_op_f32_ls_sm80.cu
--rw-r--r--   0 root         (0) root         (0)    14344 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/trmm_tf32t_tf32n_f32t_tensor_op_f32_ls_sm80.cu
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 17:06:20.477059 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/kernel/
--rwxr-xr-x   0 root         (0) root         (0)    46470 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/kernel/batched_gemv.cu
--rwxr-xr-x   0 root         (0) root         (0)    14350 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/kernel/testbed_gemv.h
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 17:06:20.553671 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/thread/
--rw-r--r--   0 root         (0) root         (0)     4847 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/thread/gemm_sm50.cu
--rw-r--r--   0 root         (0) root         (0)    12503 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/thread/gemm_sm60.cu
--rw-r--r--   0 root         (0) root         (0)     3109 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/thread/gemm_sm61.cu
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 17:06:20.597100 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/thread/host/
--rw-r--r--   0 root         (0) root         (0)     5198 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/thread/host/gemm_sm60_host.cu
--rw-r--r--   0 root         (0) root         (0)     7161 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/thread/host/testbed_host.h
--rw-r--r--   0 root         (0) root         (0)     7124 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/thread/testbed.h
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 17:06:21.336925 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/threadblock/
--rw-r--r--   0 root         (0) root         (0)    24658 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/threadblock/batched_gemv.cu
--rw-r--r--   0 root         (0) root         (0)     4345 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/threadblock/epilogue_workspace.cu
--rw-r--r--   0 root         (0) root         (0)   135045 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/threadblock/mma_multistage.cu
--rw-r--r--   0 root         (0) root         (0)     4644 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/threadblock/mma_multistage_slicedk.cu
--rw-r--r--   0 root         (0) root         (0)    94442 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/threadblock/mma_multistage_sparse.cu
--rw-r--r--   0 root         (0) root         (0)    17113 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/threadblock/mma_multistage_sparse_testbed.h
--rw-r--r--   0 root         (0) root         (0)    13131 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/threadblock/mma_multistage_testbed.h
--rw-r--r--   0 root         (0) root         (0)    14539 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/threadblock/mma_multistage_testbed_slicedk.h
--rw-r--r--   0 root         (0) root         (0)    49052 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/threadblock/mma_pipelined_simt.cu
--rw-r--r--   0 root         (0) root         (0)     8407 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/threadblock/mma_pipelined_slicedk.cu
--rw-r--r--   0 root         (0) root         (0)    18705 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/threadblock/mma_pipelined_sm70.cu
--rw-r--r--   0 root         (0) root         (0)    78122 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/threadblock/mma_pipelined_sm75.cu
--rw-r--r--   0 root         (0) root         (0)    21051 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/threadblock/mma_pipelined_sm80.cu
--rw-r--r--   0 root         (0) root         (0)    13413 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/threadblock/mma_pipelined_testbed.h
--rw-r--r--   0 root         (0) root         (0)    14239 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/threadblock/mma_pipelined_testbed_slicedk.h
--rw-r--r--   0 root         (0) root         (0)    29772 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/threadblock/mma_pipelined_wmma_sm70.cu
--rw-r--r--   0 root         (0) root         (0)    12395 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/threadblock/mma_pipelined_wmma_sm75.cu
--rw-r--r--   0 root         (0) root         (0)     3502 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/threadblock/mma_planar_complex_sm80.cu
--rw-r--r--   0 root         (0) root         (0)    12138 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/threadblock/mma_planar_complex_testbed.h
--rw-r--r--   0 root         (0) root         (0)    16308 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/threadblock/mma_singlestage_wmma_sm70.cu
--rw-r--r--   0 root         (0) root         (0)    12502 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/threadblock/mma_singlestage_wmma_sm75.cu
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 17:06:21.628876 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/warp/
--rw-r--r--   0 root         (0) root         (0)    22029 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/warp/gemm_complex_sm80.cu
--rw-r--r--   0 root         (0) root         (0)    10916 2023-01-27 22:40:09.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/warp/gemm_complex_sm90.cu
--rw-r--r--   0 root         (0) root         (0)     9873 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/warp/gemm_gaussian_complex_sm80.cu
--rw-r--r--   0 root         (0) root         (0)    18220 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/warp/gemm_sm50.cu
--rw-r--r--   0 root         (0) root         (0)     4920 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/warp/gemm_sm60.cu
--rw-r--r--   0 root         (0) root         (0)     6291 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/warp/gemm_sm61.cu
--rw-r--r--   0 root         (0) root         (0)     9297 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/warp/gemm_sm70.cu
--rw-r--r--   0 root         (0) root         (0)    37942 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/warp/gemm_sm75.cu
--rw-r--r--   0 root         (0) root         (0)    81659 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/warp/gemm_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     9089 2023-01-27 22:40:09.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/warp/gemm_sm90.cu
--rw-r--r--   0 root         (0) root         (0)    48928 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/warp/gemm_sparse_sm80.cu
--rw-r--r--   0 root         (0) root         (0)    45327 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/warp/testbed.h
--rw-r--r--   0 root         (0) root         (0)    25780 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/warp/wmma_sm70.cu
--rw-r--r--   0 root         (0) root         (0)     7544 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/warp/wmma_sm72.cu
--rw-r--r--   0 root         (0) root         (0)     6487 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/warp/wmma_sm75.cu
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 17:06:21.789246 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/layout/
--rw-r--r--   0 root         (0) root         (0)     5788 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/layout/matrix.cu
--rw-r--r--   0 root         (0) root         (0)     5984 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/layout/tensor.cu
--rw-r--r--   0 root         (0) root         (0)     7081 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/layout/tensor_nhwc.cu
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 17:05:54.195559 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/nvrtc/
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 17:05:54.178056 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/nvrtc/cutlass/
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 17:06:21.820963 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/nvrtc/cutlass/nvrtc/
--rw-r--r--   0 root         (0) root         (0)     2096 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/nvrtc/cutlass/nvrtc/environment.h
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 17:05:54.190232 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/nvrtc/kernel/
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 17:06:21.852556 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/nvrtc/kernel/thread/
--rw-r--r--   0 root         (0) root         (0)     2915 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/nvrtc/kernel/thread/testbed_kernel.h
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 17:06:21.893736 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/nvrtc/stdlib/
--rw-rw-r--   0 root         (0) root         (0)        0 2022-06-02 16:47:41.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/nvrtc/stdlib/assert.h
--rw-r--r--   0 root         (0) root         (0)     4250 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/nvrtc/stdlib/stdint.h
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 17:06:21.936548 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/nvrtc/thread/
--rw-r--r--   0 root         (0) root         (0)     5727 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/nvrtc/thread/gemm_nvrtc.cu
--rw-r--r--   0 root         (0) root         (0)    10328 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/nvrtc/thread/testbed.h
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 17:06:22.067403 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/pipeline/
--rw-r--r--   0 root         (0) root         (0)    15532 2023-01-27 22:40:09.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/pipeline/pipeline_async.cu
--rw-r--r--   0 root         (0) root         (0)    15490 2023-01-27 22:40:09.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/pipeline/pipeline_tma_async.cu
--rw-r--r--   0 root         (0) root         (0)    17098 2023-01-27 22:40:09.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/pipeline/pipeline_tma_async_warp_specialized.cu
--rw-r--r--   0 root         (0) root         (0)    20252 2023-01-27 22:40:09.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/pipeline/pipeline_tma_async_warp_specialized_persistent.cu
--rw-r--r--   0 root         (0) root         (0)     7623 2023-01-27 22:40:09.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/pipeline/sequence_barrier.cu
--rw-r--r--   0 root         (0) root         (0)     4327 2023-01-27 22:40:09.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/pipeline/testbed.h
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 17:05:54.227745 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/reduction/
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 17:06:22.113067 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/reduction/device/
--rw-r--r--   0 root         (0) root         (0)    14684 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/reduction/device/tensor_reduce_contiguous.cu
--rw-r--r--   0 root         (0) root         (0)    15609 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/reduction/device/tensor_reduce_strided.cu
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 17:06:22.156332 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/reduction/kernel/
--rw-r--r--   0 root         (0) root         (0)    11350 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/reduction/kernel/reduce_splitk.cu
--rw-r--r--   0 root         (0) root         (0)     2228 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/reduction/kernel/reduce_splitk_testbed.h
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 17:06:22.201797 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/reduction/thread/
--rw-r--r--   0 root         (0) root         (0)     3110 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/reduction/thread/reduction_thread.cu
--rw-r--r--   0 root         (0) root         (0)     6657 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/reduction/thread/testbed.h
--rw-r--r--   0 root         (0) root         (0)     2047 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/test_unit.cpp
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 17:05:54.239268 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/transform/
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 17:06:22.354721 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/transform/threadblock/
--rw-r--r--   0 root         (0) root         (0)    25527 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/transform/threadblock/predicated_tile_iterator.cu
--rw-r--r--   0 root         (0) root         (0)     9501 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/transform/threadblock/regular_tile_iterator_tensor_op.cu
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 17:06:22.398216 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/util/
--rw-r--r--   0 root         (0) root         (0)     2663 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/util/cutlass_test_levels.cu
--rw-r--r--   0 root         (0) root         (0)     7474 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/util/tensor_reduce.cu
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 17:05:54.390377 flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 17:05:54.358558 flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/library/
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 17:05:54.270548 flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/library/include/
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 17:05:54.276418 flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/library/include/cutlass/
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 17:06:22.644849 flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/library/include/cutlass/library/
--rw-r--r--   0 root         (0) root         (0)     3974 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/library/include/cutlass/library/arch_mappings.h
--rw-r--r--   0 root         (0) root         (0)    16013 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/library/include/cutlass/library/handle.h
--rw-r--r--   0 root         (0) root         (0)    38274 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/library/include/cutlass/library/library.h
--rw-r--r--   0 root         (0) root         (0)     4070 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/library/include/cutlass/library/manifest.h
--rw-r--r--   0 root         (0) root         (0)    17934 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/library/include/cutlass/library/operation_table.h
--rw-r--r--   0 root         (0) root         (0)     2724 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/library/include/cutlass/library/singleton.h
--rw-r--r--   0 root         (0) root         (0)     7904 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/library/include/cutlass/library/util.h
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 17:05:54.290002 flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/library/scripts/
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 17:05:54.295868 flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 17:05:54.301682 flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 17:06:22.830469 flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/
--rw-r--r--   0 root         (0) root         (0)     2788 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/compiler.h
--rw-r--r--   0 root         (0) root         (0)     2460 2023-01-27 22:40:09.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/cute.cpp
--rw-r--r--   0 root         (0) root         (0)     6223 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/cutlass.cpp
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 17:06:22.926451 flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/include/
--rw-r--r--   0 root         (0) root         (0)     2851 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/include/arch.h
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 17:06:23.100886 flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/include/conv/
--rw-r--r--   0 root         (0) root         (0)     5897 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/include/conv/conv_problem_size.h
--rw-r--r--   0 root         (0) root         (0)     4763 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/include/conv/convolution.h
--rw-r--r--   0 root         (0) root         (0)     2650 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/include/conv/host.h
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 17:06:23.141775 flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/include/epilogue/
--rw-r--r--   0 root         (0) root         (0)     7072 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/include/epilogue/epilogue_visitor_generic.h
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 17:06:23.374139 flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/include/epilogue/epilogue_visitor_op/
--rw-r--r--   0 root         (0) root         (0)     3003 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/include/epilogue/epilogue_visitor_op/binary_ops.h
--rw-r--r--   0 root         (0) root         (0)     6103 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/include/epilogue/epilogue_visitor_op/unary_ops.h
--rw-r--r--   0 root         (0) root         (0)     4698 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/include/epilogue/epilogue_visitor_op/visitor_op_accumulator.h
--rw-r--r--   0 root         (0) root         (0)     8442 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/include/epilogue/epilogue_visitor_op/visitor_op_binary.h
--rw-r--r--   0 root         (0) root         (0)     8830 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/include/epilogue/epilogue_visitor_op/visitor_op_column_broadcast.h
--rw-r--r--   0 root         (0) root         (0)    13040 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/include/epilogue/epilogue_visitor_op/visitor_op_column_reduction.h
--rw-r--r--   0 root         (0) root         (0)     9476 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/include/epilogue/epilogue_visitor_op/visitor_op_linear_combination.h
--rw-r--r--   0 root         (0) root         (0)     9085 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/include/epilogue/epilogue_visitor_op/visitor_op_row_broadcast.h
--rw-r--r--   0 root         (0) root         (0)    12017 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/include/epilogue/epilogue_visitor_op/visitor_op_row_reduction.h
--rw-r--r--   0 root         (0) root         (0)     6177 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/include/epilogue/epilogue_visitor_op/visitor_op_tensor_input.h
--rw-r--r--   0 root         (0) root         (0)     8017 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/include/epilogue/epilogue_visitor_op/visitor_op_tensor_output.h
--rw-r--r--   0 root         (0) root         (0)     7223 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/include/epilogue/epilogue_visitor_op/visitor_op_unary.h
--rw-r--r--   0 root         (0) root         (0)    16985 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/include/epilogue/epilogue_visitor_with_layernorm.h
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 17:06:23.440561 flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/include/gemm/
--rw-r--r--   0 root         (0) root         (0)     3673 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/include/gemm/gemm.h
--rw-r--r--   0 root         (0) root         (0)    23600 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/include/gemm/gemm_universal_with_visitor.h
--rw-r--r--   0 root         (0) root         (0)     2328 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/include/gemm/host.h
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 17:06:23.501599 flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/include/layout/
--rw-r--r--   0 root         (0) root         (0)     2115 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/include/layout/layout.h
--rw-r--r--   0 root         (0) root         (0)     4337 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/include/layout/matrix.h
--rw-r--r--   0 root         (0) root         (0)     3694 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/include/layout/tensor.h
--rw-r--r--   0 root         (0) root         (0)     8445 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/include/swizzling.h
--rw-r--r--   0 root         (0) root         (0)     3902 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/include/tensor_coord.h
--rw-r--r--   0 root         (0) root         (0)     5563 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/include/tensor_ref_view.h
--rw-r--r--   0 root         (0) root         (0)     4855 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/include/types.h
--rw-r--r--   0 root         (0) root         (0)      811 2022-09-05 01:02:49.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/library.h
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 17:05:54.348665 flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/test/
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 17:06:23.566236 flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/test/conv/
--rw-r--r--   0 root         (0) root         (0)     2651 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/test/conv/conv_problems.h
--rw-r--r--   0 root         (0) root         (0)     2253 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/test/conv/convolution.h
--rw-r--r--   0 root         (0) root         (0)     8826 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/test/conv/host.h
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 17:06:23.608797 flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/test/gemm/
--rw-r--r--   0 root         (0) root         (0)     2139 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/test/gemm/gemm.h
--rw-r--r--   0 root         (0) root         (0)    18930 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/test/gemm/host.h
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 17:06:23.855928 flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/library/src/
--rw-r--r--   0 root         (0) root         (0)    13931 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/library/src/conv2d_operation.h
--rw-r--r--   0 root         (0) root         (0)    13857 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/library/src/conv3d_operation.h
--rw-r--r--   0 root         (0) root         (0)    42264 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/library/src/gemm_operation.h
--rw-r--r--   0 root         (0) root         (0)    35709 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/library/src/handle.cu
--rw-r--r--   0 root         (0) root         (0)    12616 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/library/src/library_internal.h
--rw-r--r--   0 root         (0) root         (0)     3782 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/library/src/manifest.cpp
--rw-r--r--   0 root         (0) root         (0)     5468 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/library/src/operation_table.cu
--rw-r--r--   0 root         (0) root         (0)    12873 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/library/src/rank_2k_operation.h
--rw-r--r--   0 root         (0) root         (0)    11367 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/library/src/rank_k_operation.h
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 17:06:24.015101 flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/library/src/reduction/
--rw-r--r--   0 root         (0) root         (0)     3190 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/library/src/reduction/init_reduction_operations.cu
--rw-r--r--   0 root         (0) root         (0)     6367 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/library/src/reduction/reduction_device.cu
--rw-r--r--   0 root         (0) root         (0)    10270 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/library/src/reduction/reduction_operation.h
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 17:06:24.152668 flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/library/src/reference/
--rw-r--r--   0 root         (0) root         (0)     6746 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/library/src/reference/conv2d.cu
--rw-r--r--   0 root         (0) root         (0)     6286 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/library/src/reference/conv3d.cu
--rw-r--r--   0 root         (0) root         (0)    17191 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/library/src/reference/conv_reference_operation.h
--rw-r--r--   0 root         (0) root         (0)     7199 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/library/src/reference/gemm.cu
--rw-r--r--   0 root         (0) root         (0)    14732 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/library/src/reference/gemm_reference_operation.h
--rw-r--r--   0 root         (0) root         (0)     2857 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/library/src/reference/initialize_reference_operations.cu
--rw-r--r--   0 root         (0) root         (0)     2855 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/library/src/singleton.cu
--rw-r--r--   0 root         (0) root         (0)    13134 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/library/src/symm_operation.h
--rw-r--r--   0 root         (0) root         (0)    11698 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/library/src/trmm_operation.h
--rw-r--r--   0 root         (0) root         (0)    43704 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/library/src/util.cu
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 17:05:54.382531 flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/profiler/
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 17:06:25.298429 flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/profiler/src/
--rw-r--r--   0 root         (0) root         (0)    53205 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/profiler/src/conv2d_operation_profiler.cu
--rw-r--r--   0 root         (0) root         (0)    18014 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/profiler/src/conv2d_operation_profiler.h
--rw-r--r--   0 root         (0) root         (0)    48659 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/profiler/src/conv3d_operation_profiler.cu
--rw-r--r--   0 root         (0) root         (0)    16043 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/profiler/src/conv3d_operation_profiler.h
--rw-r--r--   0 root         (0) root         (0)    36462 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/profiler/src/cublas_helpers.cu
--rw-r--r--   0 root         (0) root         (0)    10627 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/profiler/src/cublas_helpers.h
--rw-r--r--   0 root         (0) root         (0)    16877 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/profiler/src/cudnn_helpers.cpp
--rw-r--r--   0 root         (0) root         (0)    20397 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/profiler/src/cudnn_helpers.h
--rw-r--r--   0 root         (0) root         (0)     7233 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/profiler/src/cutlass_profiler.cu
--rw-r--r--   0 root         (0) root         (0)     3233 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/profiler/src/cutlass_profiler.h
--rw-r--r--   0 root         (0) root         (0)     2453 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/profiler/src/debug.h
--rw-r--r--   0 root         (0) root         (0)    53643 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/profiler/src/device_allocation.cu
--rw-r--r--   0 root         (0) root         (0)     7217 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/profiler/src/device_allocation.h
--rw-r--r--   0 root         (0) root         (0)     6841 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/profiler/src/device_context.cu
--rw-r--r--   0 root         (0) root         (0)     4300 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/profiler/src/device_context.h
--rw-r--r--   0 root         (0) root         (0)     8296 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/profiler/src/enumerated_types.cpp
--rw-r--r--   0 root         (0) root         (0)     6421 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/profiler/src/enumerated_types.h
--rw-r--r--   0 root         (0) root         (0)    41919 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/profiler/src/gemm_operation_profiler.cu
--rw-r--r--   0 root         (0) root         (0)     8544 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/profiler/src/gemm_operation_profiler.h
--rw-r--r--   0 root         (0) root         (0)     3874 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/profiler/src/gpu_timer.cpp
--rw-r--r--   0 root         (0) root         (0)     2724 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/profiler/src/gpu_timer.h
--rw-r--r--   0 root         (0) root         (0)     2340 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/profiler/src/main.cpp
--rw-r--r--   0 root         (0) root         (0)    20944 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/profiler/src/operation_profiler.cu
--rw-r--r--   0 root         (0) root         (0)     7876 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/profiler/src/operation_profiler.h
--rw-r--r--   0 root         (0) root         (0)    27172 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/profiler/src/options.cu
--rw-r--r--   0 root         (0) root         (0)     8773 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/profiler/src/options.h
--rw-r--r--   0 root         (0) root         (0)    14192 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/profiler/src/performance_report.cpp
--rw-r--r--   0 root         (0) root         (0)     4337 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/profiler/src/performance_report.h
--rw-r--r--   0 root         (0) root         (0)     2494 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/profiler/src/performance_result.cu
--rw-r--r--   0 root         (0) root         (0)     3941 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/profiler/src/performance_result.h
--rw-r--r--   0 root         (0) root         (0)    37487 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/profiler/src/problem_space.cpp
--rw-r--r--   0 root         (0) root         (0)    27747 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/profiler/src/problem_space.h
--rw-r--r--   0 root         (0) root         (0)    25014 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/profiler/src/rank_2k_operation_profiler.cu
--rw-r--r--   0 root         (0) root         (0)     6891 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/profiler/src/rank_2k_operation_profiler.h
--rw-r--r--   0 root         (0) root         (0)    24253 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/profiler/src/rank_k_operation_profiler.cu
--rw-r--r--   0 root         (0) root         (0)     6830 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/profiler/src/rank_k_operation_profiler.h
--rw-r--r--   0 root         (0) root         (0)     5452 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/profiler/src/reduction_operation_profiler.h
--rw-r--r--   0 root         (0) root         (0)    20688 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/profiler/src/sparse_gemm_operation_profiler.cu
--rw-r--r--   0 root         (0) root         (0)     6471 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/profiler/src/sparse_gemm_operation_profiler.h
--rw-r--r--   0 root         (0) root         (0)    26610 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/profiler/src/symm_operation_profiler.cu
--rw-r--r--   0 root         (0) root         (0)     6933 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/profiler/src/symm_operation_profiler.h
--rw-r--r--   0 root         (0) root         (0)    24431 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/profiler/src/trmm_operation_profiler.cu
--rw-r--r--   0 root         (0) root         (0)     6599 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/profiler/src/trmm_operation_profiler.h
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 17:05:54.396404 flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/util/
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 17:05:54.403038 flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/util/include/
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 17:05:54.409158 flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/util/include/cutlass/
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 17:06:25.695963 flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/
--rw-r--r--   0 root         (0) root         (0)     9774 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/command_line.h
--rw-r--r--   0 root         (0) root         (0)     5104 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/debug.h
--rw-r--r--   0 root         (0) root         (0)     5953 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/device_dump.h
--rw-r--r--   0 root         (0) root         (0)    17696 2023-03-13 04:24:26.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/device_groupnorm.h
--rw-r--r--   0 root         (0) root         (0)    20880 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/device_layernorm.h
--rw-r--r--   0 root         (0) root         (0)    10561 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/device_memory.h
--rw-r--r--   0 root         (0) root         (0)     5219 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/device_nchw_to_nhwc.h
--rw-r--r--   0 root         (0) root         (0)    11067 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/device_nhwc_padding.h
--rw-r--r--   0 root         (0) root         (0)    18653 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/device_nhwc_pooling.h
--rw-r--r--   0 root         (0) root         (0)     5214 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/device_nhwc_to_nchw.h
--rw-r--r--   0 root         (0) root         (0)     4007 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/device_utils.h
--rw-r--r--   0 root         (0) root         (0)     4597 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/distribution.h
--rw-r--r--   0 root         (0) root         (0)     2674 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/exceptions.h
--rw-r--r--   0 root         (0) root         (0)     4821 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/host_reorder.h
--rw-r--r--   0 root         (0) root         (0)    16745 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/host_tensor.h
--rw-r--r--   0 root         (0) root         (0)    20354 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/host_tensor_planar_complex.h
--rw-r--r--   0 root         (0) root         (0)     4760 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/host_uncompress.h
--rw-r--r--   0 root         (0) root         (0)     1962 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/index_sequence.h
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 17:05:54.452150 flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 17:06:25.741069 flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/detail/
--rw-r--r--   0 root         (0) root         (0)     4606 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/detail/inner_product.h
--rw-r--r--   0 root         (0) root         (0)     3527 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/detail/linear_to_coordinate.h
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 17:06:25.929435 flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/device/
--rw-r--r--   0 root         (0) root         (0)    48350 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/device/convolution.h
--rw-r--r--   0 root         (0) root         (0)    14296 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/device/gemm.h
--rw-r--r--   0 root         (0) root         (0)    10524 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/device/gemm_complex.h
--rw-r--r--   0 root         (0) root         (0)     9652 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/device/gemm_planar_complex.h
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 17:06:25.990855 flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/device/kernel/
--rw-r--r--   0 root         (0) root         (0)     5381 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/device/kernel/gemm.h
--rw-r--r--   0 root         (0) root         (0)     6198 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/device/kernel/tensor_elementwise.h
--rw-r--r--   0 root         (0) root         (0)     5126 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/device/kernel/tensor_foreach.h
--rw-r--r--   0 root         (0) root         (0)    11615 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/device/rank_2k_complex.h
--rw-r--r--   0 root         (0) root         (0)     7278 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/device/tensor_compare.h
--rw-r--r--   0 root         (0) root         (0)    46444 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/device/tensor_fill.h
--rw-r--r--   0 root         (0) root         (0)     5293 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/device/tensor_foreach.h
--rw-r--r--   0 root         (0) root         (0)    15964 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/device/tensor_reduce.h
--rw-r--r--   0 root         (0) root         (0)     4589 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/device/tensor_relu.h
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 17:06:26.017679 flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/device/thread/
--rw-r--r--   0 root         (0) root         (0)     5872 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/device/thread/gemm.h
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 17:06:26.499755 flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/host/
--rw-r--r--   0 root         (0) root         (0)    28439 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/host/convolution.h
--rw-r--r--   0 root         (0) root         (0)     2766 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/host/error_metrics.h
--rw-r--r--   0 root         (0) root         (0)    17163 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/host/gemm.h
--rw-r--r--   0 root         (0) root         (0)     7062 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/host/gemm_complex.h
--rw-r--r--   0 root         (0) root         (0)     7708 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/host/gemm_planar_complex.h
--rw-r--r--   0 root         (0) root         (0)     9441 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/host/rank_2k.h
--rw-r--r--   0 root         (0) root         (0)    11444 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/host/rank_2k_complex.h
--rw-r--r--   0 root         (0) root         (0)     8148 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/host/rank_k_complex.h
--rw-r--r--   0 root         (0) root         (0)    10509 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/host/symm.h
--rw-r--r--   0 root         (0) root         (0)    12296 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/host/symm_complex.h
--rw-r--r--   0 root         (0) root         (0)     8440 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/host/tensor_compare.h
--rw-r--r--   0 root         (0) root         (0)     8317 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/host/tensor_copy.h
--rw-r--r--   0 root         (0) root         (0)     9023 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/host/tensor_elementwise.h
--rw-r--r--   0 root         (0) root         (0)    41867 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/host/tensor_fill.h
--rw-r--r--   0 root         (0) root         (0)     4756 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/host/tensor_foreach.h
--rw-r--r--   0 root         (0) root         (0)     2133 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/host/tensor_norm.h
--rw-r--r--   0 root         (0) root         (0)     6111 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/host/tensor_reduce.h
--rw-r--r--   0 root         (0) root         (0)     7670 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/host/trmm.h
--rw-r--r--   0 root         (0) root         (0)     9874 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/host/trmm_complex.h
--rw-r--r--   0 root         (0) root         (0)     8285 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/tensor_view_io.h
--rw-r--r--   0 root         (0) root         (0)     8809 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/type_traits.h
--rw-r--r--   0 root         (0) root         (0)    30683 2023-04-11 20:32:31.000000 flash_attn-1.0.1/csrc/flash_attn/flash_api.cpp
--rw-r--r--   0 root         (0) root         (0)    32519 2023-04-12 06:28:29.000000 flash_attn-1.0.1/csrc/flash_attn/fmha_api.cpp
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 17:06:28.184711 flash_attn-1.0.1/csrc/flash_attn/src/
--rw-r--r--   0 root         (0) root         (0)     1664 2023-04-08 17:10:55.000000 flash_attn-1.0.1/csrc/flash_attn/src/block_info.h
--rw-r--r--   0 root         (0) root         (0)     3920 2023-04-09 19:53:14.000000 flash_attn-1.0.1/csrc/flash_attn/src/flash.h
--rw-r--r--   0 root         (0) root         (0)      983 2023-04-07 15:25:56.000000 flash_attn-1.0.1/csrc/flash_attn/src/flash_bwd_hdim128.cu
--rw-r--r--   0 root         (0) root         (0)      574 2023-04-12 03:22:08.000000 flash_attn-1.0.1/csrc/flash_attn/src/flash_bwd_hdim128_bf16_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     1124 2023-04-12 03:09:57.000000 flash_attn-1.0.1/csrc/flash_attn/src/flash_bwd_hdim128_fp16_sm80.cu
--rw-r--r--   0 root         (0) root         (0)      983 2023-04-07 15:25:56.000000 flash_attn-1.0.1/csrc/flash_attn/src/flash_bwd_hdim128_sm80_fp16.cu
--rw-r--r--   0 root         (0) root         (0)      425 2023-04-07 15:25:56.000000 flash_attn-1.0.1/csrc/flash_attn/src/flash_bwd_hdim32.cu
--rw-r--r--   0 root         (0) root         (0)      450 2023-04-08 07:29:00.000000 flash_attn-1.0.1/csrc/flash_attn/src/flash_bwd_hdim32_bf16_sm80.cu
--rw-r--r--   0 root         (0) root         (0)      442 2023-04-08 07:26:34.000000 flash_attn-1.0.1/csrc/flash_attn/src/flash_bwd_hdim32_fp16_sm80.cu
--rw-r--r--   0 root         (0) root         (0)      425 2023-04-07 15:25:56.000000 flash_attn-1.0.1/csrc/flash_attn/src/flash_bwd_hdim32_sm80_fp16.cu
--rw-r--r--   0 root         (0) root         (0)     2074 2023-04-07 15:25:56.000000 flash_attn-1.0.1/csrc/flash_attn/src/flash_bwd_hdim64.cu
--rw-r--r--   0 root         (0) root         (0)      450 2023-04-08 07:29:20.000000 flash_attn-1.0.1/csrc/flash_attn/src/flash_bwd_hdim64_bf16_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     2458 2023-04-11 20:53:27.000000 flash_attn-1.0.1/csrc/flash_attn/src/flash_bwd_hdim64_fp16_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     2074 2023-04-07 15:25:56.000000 flash_attn-1.0.1/csrc/flash_attn/src/flash_bwd_hdim64_sm80_fp16.cu
--rw-r--r--   0 root         (0) root         (0)      581 2023-04-07 15:25:56.000000 flash_attn-1.0.1/csrc/flash_attn/src/flash_bwd_hdim96.cu
--rw-r--r--   0 root         (0) root         (0)      449 2023-04-08 07:29:33.000000 flash_attn-1.0.1/csrc/flash_attn/src/flash_bwd_hdim96_bf16_sm80.cu
--rw-r--r--   0 root         (0) root         (0)      598 2023-04-08 07:26:34.000000 flash_attn-1.0.1/csrc/flash_attn/src/flash_bwd_hdim96_fp16_sm80.cu
--rw-r--r--   0 root         (0) root         (0)      581 2023-04-07 15:25:56.000000 flash_attn-1.0.1/csrc/flash_attn/src/flash_bwd_hdim96_sm80_fp16.cu
--rw-r--r--   0 root         (0) root         (0)    82323 2023-04-12 05:47:44.000000 flash_attn-1.0.1/csrc/flash_attn/src/flash_bwd_kernel.h
--rw-r--r--   0 root         (0) root         (0)    41051 2023-03-25 22:30:12.000000 flash_attn-1.0.1/csrc/flash_attn/src/flash_bwd_kernel_bak.h
--rw-r--r--   0 root         (0) root         (0)    39628 2023-03-25 21:25:30.000000 flash_attn-1.0.1/csrc/flash_attn/src/flash_bwd_kernel_new.h
--rw-r--r--   0 root         (0) root         (0)    39403 2023-04-08 17:10:55.000000 flash_attn-1.0.1/csrc/flash_attn/src/flash_bwd_kernel_reverse.h
--rw-r--r--   0 root         (0) root         (0)     8751 2023-04-12 03:19:57.000000 flash_attn-1.0.1/csrc/flash_attn/src/flash_bwd_launch_template.h
--rw-r--r--   0 root         (0) root         (0)      645 2023-04-07 21:42:50.000000 flash_attn-1.0.1/csrc/flash_attn/src/flash_fwd_hdim128.cu
--rw-r--r--   0 root         (0) root         (0)      592 2023-04-08 07:07:58.000000 flash_attn-1.0.1/csrc/flash_attn/src/flash_fwd_hdim128_bf16_sm80.cu
--rw-r--r--   0 root         (0) root         (0)      662 2023-04-08 06:50:03.000000 flash_attn-1.0.1/csrc/flash_attn/src/flash_fwd_hdim128_fp16_sm80.cu
--rw-r--r--   0 root         (0) root         (0)      662 2023-04-08 06:50:03.000000 flash_attn-1.0.1/csrc/flash_attn/src/flash_fwd_hdim128_sm80_fp16.cu
--rw-r--r--   0 root         (0) root         (0)      990 2023-04-07 18:39:34.000000 flash_attn-1.0.1/csrc/flash_attn/src/flash_fwd_hdim160.cu
--rw-r--r--   0 root         (0) root         (0)      495 2023-04-08 07:08:17.000000 flash_attn-1.0.1/csrc/flash_attn/src/flash_fwd_hdim160_bf16_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     1007 2023-04-08 06:50:03.000000 flash_attn-1.0.1/csrc/flash_attn/src/flash_fwd_hdim160_fp16_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     1007 2023-04-08 06:50:03.000000 flash_attn-1.0.1/csrc/flash_attn/src/flash_fwd_hdim160_sm80_fp16.cu
--rw-r--r--   0 root         (0) root         (0)     1041 2023-04-07 18:40:27.000000 flash_attn-1.0.1/csrc/flash_attn/src/flash_fwd_hdim192.cu
--rw-r--r--   0 root         (0) root         (0)      494 2023-04-08 07:08:35.000000 flash_attn-1.0.1/csrc/flash_attn/src/flash_fwd_hdim192_bf16_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     1058 2023-04-08 06:50:03.000000 flash_attn-1.0.1/csrc/flash_attn/src/flash_fwd_hdim192_fp16_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     1058 2023-04-08 06:50:03.000000 flash_attn-1.0.1/csrc/flash_attn/src/flash_fwd_hdim192_sm80_fp16.cu
--rw-r--r--   0 root         (0) root         (0)      945 2023-04-07 18:37:45.000000 flash_attn-1.0.1/csrc/flash_attn/src/flash_fwd_hdim32.cu
--rw-r--r--   0 root         (0) root         (0)      494 2023-04-08 07:06:55.000000 flash_attn-1.0.1/csrc/flash_attn/src/flash_fwd_hdim32_bf16_sm80.cu
--rw-r--r--   0 root         (0) root         (0)      962 2023-04-08 06:50:03.000000 flash_attn-1.0.1/csrc/flash_attn/src/flash_fwd_hdim32_fp16_sm80.cu
--rw-r--r--   0 root         (0) root         (0)      962 2023-04-08 06:50:03.000000 flash_attn-1.0.1/csrc/flash_attn/src/flash_fwd_hdim32_sm80_fp16.cu
--rw-r--r--   0 root         (0) root         (0)     1149 2023-04-07 22:02:41.000000 flash_attn-1.0.1/csrc/flash_attn/src/flash_fwd_hdim64.cu
--rw-r--r--   0 root         (0) root         (0)      588 2023-04-08 07:07:20.000000 flash_attn-1.0.1/csrc/flash_attn/src/flash_fwd_hdim64_bf16_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     1166 2023-04-08 06:50:03.000000 flash_attn-1.0.1/csrc/flash_attn/src/flash_fwd_hdim64_fp16_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     1166 2023-04-08 06:50:03.000000 flash_attn-1.0.1/csrc/flash_attn/src/flash_fwd_hdim64_sm80_fp16.cu
--rw-r--r--   0 root         (0) root         (0)      711 2023-04-07 22:17:50.000000 flash_attn-1.0.1/csrc/flash_attn/src/flash_fwd_hdim96.cu
--rw-r--r--   0 root         (0) root         (0)      492 2023-04-08 07:07:36.000000 flash_attn-1.0.1/csrc/flash_attn/src/flash_fwd_hdim96_bf16_sm80.cu
--rw-r--r--   0 root         (0) root         (0)      728 2023-04-08 06:50:03.000000 flash_attn-1.0.1/csrc/flash_attn/src/flash_fwd_hdim96_fp16_sm80.cu
--rw-r--r--   0 root         (0) root         (0)      728 2023-04-08 06:50:03.000000 flash_attn-1.0.1/csrc/flash_attn/src/flash_fwd_hdim96_sm80_fp16.cu
--rw-r--r--   0 root         (0) root         (0)    30219 2023-04-12 05:25:26.000000 flash_attn-1.0.1/csrc/flash_attn/src/flash_fwd_kernel.h
--rw-r--r--   0 root         (0) root         (0)    27253 2023-04-08 17:10:55.000000 flash_attn-1.0.1/csrc/flash_attn/src/flash_fwd_kernel_old.h
--rw-r--r--   0 root         (0) root         (0)     2346 2023-04-09 16:47:24.000000 flash_attn-1.0.1/csrc/flash_attn/src/flash_fwd_launch_template.h
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 17:06:28.399535 flash_attn-1.0.1/csrc/flash_attn/src/fmha/
--rw-r--r--   0 root         (0) root         (0)    17999 2023-04-12 06:28:29.000000 flash_attn-1.0.1/csrc/flash_attn/src/fmha/gemm.h
--rw-r--r--   0 root         (0) root         (0)    22872 2023-04-12 06:28:29.000000 flash_attn-1.0.1/csrc/flash_attn/src/fmha/gmem_tile.h
--rw-r--r--   0 root         (0) root         (0)     5997 2023-04-12 06:28:29.000000 flash_attn-1.0.1/csrc/flash_attn/src/fmha/kernel_traits.h
--rw-r--r--   0 root         (0) root         (0)     4362 2023-04-12 06:28:29.000000 flash_attn-1.0.1/csrc/flash_attn/src/fmha/mask.h
--rw-r--r--   0 root         (0) root         (0)    74010 2023-04-12 06:28:29.000000 flash_attn-1.0.1/csrc/flash_attn/src/fmha/smem_tile.h
--rw-r--r--   0 root         (0) root         (0)    25514 2023-04-12 06:28:29.000000 flash_attn-1.0.1/csrc/flash_attn/src/fmha/softmax.h
--rw-r--r--   0 root         (0) root         (0)    41059 2023-04-12 06:28:29.000000 flash_attn-1.0.1/csrc/flash_attn/src/fmha/utils.h
--rw-r--r--   0 root         (0) root         (0)     7152 2023-04-12 06:28:29.000000 flash_attn-1.0.1/csrc/flash_attn/src/fmha.h
--rw-r--r--   0 root         (0) root         (0)     4118 2023-04-12 06:28:29.000000 flash_attn-1.0.1/csrc/flash_attn/src/fmha_block_dgrad_fp16_kernel_loop.sm80.cu
--rw-r--r--   0 root         (0) root         (0)    33506 2023-04-12 06:28:29.000000 flash_attn-1.0.1/csrc/flash_attn/src/fmha_block_dgrad_kernel_1xN_loop.h
--rw-r--r--   0 root         (0) root         (0)     5292 2023-04-12 06:28:29.000000 flash_attn-1.0.1/csrc/flash_attn/src/fmha_block_fprop_fp16_kernel.sm80.cu
--rw-r--r--   0 root         (0) root         (0)    23207 2023-04-12 06:28:29.000000 flash_attn-1.0.1/csrc/flash_attn/src/fmha_block_fprop_kernel_1xN.h
--rw-r--r--   0 root         (0) root         (0)     2502 2023-04-12 06:28:29.000000 flash_attn-1.0.1/csrc/flash_attn/src/fmha_blockmask.h
--rw-r--r--   0 root         (0) root         (0)      465 2023-04-12 06:28:29.000000 flash_attn-1.0.1/csrc/flash_attn/src/fmha_bwd_hdim128.cu
--rw-r--r--   0 root         (0) root         (0)      727 2023-04-12 06:28:29.000000 flash_attn-1.0.1/csrc/flash_attn/src/fmha_bwd_hdim32.cu
--rw-r--r--   0 root         (0) root         (0)     1713 2023-04-12 06:28:29.000000 flash_attn-1.0.1/csrc/flash_attn/src/fmha_bwd_hdim64.cu
--rw-r--r--   0 root         (0) root         (0)     6453 2023-04-12 06:28:29.000000 flash_attn-1.0.1/csrc/flash_attn/src/fmha_bwd_launch_template.h
--rw-r--r--   0 root         (0) root         (0)    37194 2023-04-12 06:28:29.000000 flash_attn-1.0.1/csrc/flash_attn/src/fmha_dgrad_kernel_1xN_loop.h
--rw-r--r--   0 root         (0) root         (0)    30832 2023-04-12 06:28:29.000000 flash_attn-1.0.1/csrc/flash_attn/src/fmha_fprop_kernel_1xN.h
--rw-r--r--   0 root         (0) root         (0)      445 2023-04-12 06:28:29.000000 flash_attn-1.0.1/csrc/flash_attn/src/fmha_fwd_hdim128.cu
--rw-r--r--   0 root         (0) root         (0)      724 2023-04-12 06:28:29.000000 flash_attn-1.0.1/csrc/flash_attn/src/fmha_fwd_hdim32.cu
--rw-r--r--   0 root         (0) root         (0)      725 2023-04-12 06:28:29.000000 flash_attn-1.0.1/csrc/flash_attn/src/fmha_fwd_hdim64.cu
--rw-r--r--   0 root         (0) root         (0)     4393 2023-04-12 06:28:29.000000 flash_attn-1.0.1/csrc/flash_attn/src/fmha_fwd_launch_template.h
--rw-r--r--   0 root         (0) root         (0)     3104 2023-04-12 06:28:29.000000 flash_attn-1.0.1/csrc/flash_attn/src/fmha_kernel.h
--rw-r--r--   0 root         (0) root         (0)     4892 2023-04-12 06:28:29.000000 flash_attn-1.0.1/csrc/flash_attn/src/fmha_utils.h
--rw-r--r--   0 root         (0) root         (0)    18374 2023-04-12 05:46:15.000000 flash_attn-1.0.1/csrc/flash_attn/src/kernel_traits.h
--rw-r--r--   0 root         (0) root         (0)     2927 2023-03-31 21:49:35.000000 flash_attn-1.0.1/csrc/flash_attn/src/mask.h
--rw-r--r--   0 root         (0) root         (0)     5462 2023-04-12 06:28:29.000000 flash_attn-1.0.1/csrc/flash_attn/src/philox.cuh
--rw-r--r--   0 root         (0) root         (0)    14205 2023-04-07 21:07:49.000000 flash_attn-1.0.1/csrc/flash_attn/src/softmax.h
--rw-r--r--   0 root         (0) root         (0)     1686 2023-04-12 06:28:29.000000 flash_attn-1.0.1/csrc/flash_attn/src/static_switch.h
--rw-r--r--   0 root         (0) root         (0)    13123 2023-04-12 03:28:52.000000 flash_attn-1.0.1/csrc/flash_attn/src/utils.h
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 17:06:28.425040 flash_attn-1.0.1/csrc/flash_gen/
--rw-r--r--   0 root         (0) root         (0)     7018 2022-11-21 06:35:03.000000 flash_attn-1.0.1/csrc/flash_gen/decoder_masked_multihead_attention.cu
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 17:06:28.789476 flash_attn-1.0.1/csrc/ft_attention/
--rw-r--r--   0 root         (0) root         (0)     8253 2023-04-12 06:28:29.000000 flash_attn-1.0.1/csrc/ft_attention/cuda_bf16_fallbacks.cuh
--rw-r--r--   0 root         (0) root         (0)      867 2023-04-12 06:28:29.000000 flash_attn-1.0.1/csrc/ft_attention/cuda_bf16_wrapper.h
--rw-r--r--   0 root         (0) root         (0)     7243 2023-04-12 06:28:29.000000 flash_attn-1.0.1/csrc/ft_attention/decoder_masked_multihead_attention.cu
--rw-r--r--   0 root         (0) root         (0)     7463 2023-04-12 06:28:29.000000 flash_attn-1.0.1/csrc/ft_attention/decoder_masked_multihead_attention.h
--rw-r--r--   0 root         (0) root         (0)    52690 2023-04-12 06:28:29.000000 flash_attn-1.0.1/csrc/ft_attention/decoder_masked_multihead_attention_utils.h
--rw-r--r--   0 root         (0) root         (0)     7423 2023-04-12 06:28:29.000000 flash_attn-1.0.1/csrc/ft_attention/ft_attention.cpp
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 17:06:28.921366 flash_attn-1.0.1/csrc/fused_dense_lib/
--rw-r--r--   0 root         (0) root         (0)     8215 2023-04-12 06:28:29.000000 flash_attn-1.0.1/csrc/fused_dense_lib/fused_dense.cpp
--rw-r--r--   0 root         (0) root         (0)    25273 2023-04-12 06:28:29.000000 flash_attn-1.0.1/csrc/fused_dense_lib/fused_dense_cuda.cu
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 17:06:29.417411 flash_attn-1.0.1/csrc/fused_softmax/
--rw-r--r--   0 root         (0) root         (0)     5037 2023-04-12 06:28:29.000000 flash_attn-1.0.1/csrc/fused_softmax/fused_softmax.cpp
--rw-r--r--   0 root         (0) root         (0)    23616 2023-04-12 06:28:29.000000 flash_attn-1.0.1/csrc/fused_softmax/scaled_masked_softmax.h
--rw-r--r--   0 root         (0) root         (0)     4209 2023-04-12 06:28:29.000000 flash_attn-1.0.1/csrc/fused_softmax/scaled_masked_softmax_cuda.cu
--rw-r--r--   0 root         (0) root         (0)    24659 2023-04-12 06:28:29.000000 flash_attn-1.0.1/csrc/fused_softmax/scaled_upper_triang_masked_softmax.h
--rw-r--r--   0 root         (0) root         (0)     3154 2023-04-12 06:28:29.000000 flash_attn-1.0.1/csrc/fused_softmax/scaled_upper_triang_masked_softmax_cuda.cu
--rw-r--r--   0 root         (0) root         (0)     1216 2023-04-12 06:28:29.000000 flash_attn-1.0.1/csrc/fused_softmax/type_shim.h
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 17:06:33.876586 flash_attn-1.0.1/csrc/layer_norm/
--rw-r--r--   0 root         (0) root         (0)     7248 2023-04-12 06:28:29.000000 flash_attn-1.0.1/csrc/layer_norm/ln.h
--rw-r--r--   0 root         (0) root         (0)    36418 2023-04-12 06:28:29.000000 flash_attn-1.0.1/csrc/layer_norm/ln_api.cpp
--rw-r--r--   0 root         (0) root         (0)      987 2023-04-12 06:28:29.000000 flash_attn-1.0.1/csrc/layer_norm/ln_bwd_1024.cu
--rw-r--r--   0 root         (0) root         (0)      987 2023-04-12 06:28:29.000000 flash_attn-1.0.1/csrc/layer_norm/ln_bwd_1280.cu
--rw-r--r--   0 root         (0) root         (0)      977 2023-04-12 06:28:29.000000 flash_attn-1.0.1/csrc/layer_norm/ln_bwd_1536.cu
--rw-r--r--   0 root         (0) root         (0)      976 2023-04-12 06:28:29.000000 flash_attn-1.0.1/csrc/layer_norm/ln_bwd_2048.cu
--rw-r--r--   0 root         (0) root         (0)      977 2023-04-12 06:28:29.000000 flash_attn-1.0.1/csrc/layer_norm/ln_bwd_256.cu
--rw-r--r--   0 root         (0) root         (0)      977 2023-04-12 06:28:29.000000 flash_attn-1.0.1/csrc/layer_norm/ln_bwd_2560.cu
--rw-r--r--   0 root         (0) root         (0)      976 2023-04-12 06:28:29.000000 flash_attn-1.0.1/csrc/layer_norm/ln_bwd_3072.cu
--rw-r--r--   0 root         (0) root         (0)      976 2023-04-12 06:28:29.000000 flash_attn-1.0.1/csrc/layer_norm/ln_bwd_4096.cu
--rw-r--r--   0 root         (0) root         (0)      977 2023-04-12 06:28:29.000000 flash_attn-1.0.1/csrc/layer_norm/ln_bwd_512.cu
--rw-r--r--   0 root         (0) root         (0)      976 2023-04-12 06:28:29.000000 flash_attn-1.0.1/csrc/layer_norm/ln_bwd_5120.cu
--rw-r--r--   0 root         (0) root         (0)      976 2023-04-12 06:28:29.000000 flash_attn-1.0.1/csrc/layer_norm/ln_bwd_6144.cu
--rw-r--r--   0 root         (0) root         (0)      976 2023-04-12 06:28:29.000000 flash_attn-1.0.1/csrc/layer_norm/ln_bwd_7168.cu
--rw-r--r--   0 root         (0) root         (0)      977 2023-04-12 06:28:29.000000 flash_attn-1.0.1/csrc/layer_norm/ln_bwd_768.cu
--rw-r--r--   0 root         (0) root         (0)      976 2023-04-12 06:28:29.000000 flash_attn-1.0.1/csrc/layer_norm/ln_bwd_8192.cu
--rw-r--r--   0 root         (0) root         (0)    25647 2023-04-12 06:28:29.000000 flash_attn-1.0.1/csrc/layer_norm/ln_bwd_kernels.cuh
--rw-r--r--   0 root         (0) root         (0)    19944 2023-01-19 07:34:02.000000 flash_attn-1.0.1/csrc/layer_norm/ln_bwd_semi_cuda_kernel_old.cu
--rw-r--r--   0 root         (0) root         (0)      925 2023-04-12 06:28:29.000000 flash_attn-1.0.1/csrc/layer_norm/ln_fwd_1024.cu
--rw-r--r--   0 root         (0) root         (0)      925 2023-01-22 09:05:55.000000 flash_attn-1.0.1/csrc/layer_norm/ln_fwd_10240.cu
--rw-r--r--   0 root         (0) root         (0)      925 2023-01-22 09:07:15.000000 flash_attn-1.0.1/csrc/layer_norm/ln_fwd_12288.cu
--rw-r--r--   0 root         (0) root         (0)      925 2022-12-05 08:45:58.000000 flash_attn-1.0.1/csrc/layer_norm/ln_fwd_128.cu
--rw-r--r--   0 root         (0) root         (0)      925 2023-04-12 06:28:29.000000 flash_attn-1.0.1/csrc/layer_norm/ln_fwd_1280.cu
--rw-r--r--   0 root         (0) root         (0)      925 2023-04-12 06:28:29.000000 flash_attn-1.0.1/csrc/layer_norm/ln_fwd_1536.cu
--rw-r--r--   0 root         (0) root         (0)      925 2023-04-12 06:28:29.000000 flash_attn-1.0.1/csrc/layer_norm/ln_fwd_2048.cu
--rw-r--r--   0 root         (0) root         (0)      925 2023-04-12 06:28:29.000000 flash_attn-1.0.1/csrc/layer_norm/ln_fwd_256.cu
--rw-r--r--   0 root         (0) root         (0)      925 2023-04-12 06:28:29.000000 flash_attn-1.0.1/csrc/layer_norm/ln_fwd_2560.cu
--rw-r--r--   0 root         (0) root         (0)      925 2023-04-12 06:28:29.000000 flash_attn-1.0.1/csrc/layer_norm/ln_fwd_3072.cu
--rw-r--r--   0 root         (0) root         (0)      925 2022-12-05 08:50:57.000000 flash_attn-1.0.1/csrc/layer_norm/ln_fwd_384.cu
--rw-r--r--   0 root         (0) root         (0)      925 2023-04-12 06:28:29.000000 flash_attn-1.0.1/csrc/layer_norm/ln_fwd_4096.cu
--rw-r--r--   0 root         (0) root         (0)      925 2023-04-12 06:28:29.000000 flash_attn-1.0.1/csrc/layer_norm/ln_fwd_512.cu
--rw-r--r--   0 root         (0) root         (0)      925 2023-04-12 06:28:29.000000 flash_attn-1.0.1/csrc/layer_norm/ln_fwd_5120.cu
--rw-r--r--   0 root         (0) root         (0)      925 2023-04-12 06:28:29.000000 flash_attn-1.0.1/csrc/layer_norm/ln_fwd_6144.cu
--rw-r--r--   0 root         (0) root         (0)      925 2023-04-12 06:28:29.000000 flash_attn-1.0.1/csrc/layer_norm/ln_fwd_7168.cu
--rw-r--r--   0 root         (0) root         (0)      925 2023-04-12 06:28:29.000000 flash_attn-1.0.1/csrc/layer_norm/ln_fwd_768.cu
--rw-r--r--   0 root         (0) root         (0)      925 2023-04-12 06:28:29.000000 flash_attn-1.0.1/csrc/layer_norm/ln_fwd_8192.cu
--rw-r--r--   0 root         (0) root         (0)      925 2023-01-22 08:41:06.000000 flash_attn-1.0.1/csrc/layer_norm/ln_fwd_9216.cu
--rw-r--r--   0 root         (0) root         (0)    18000 2022-12-06 21:18:58.000000 flash_attn-1.0.1/csrc/layer_norm/ln_fwd_cuda_kernel_old.cu
--rw-r--r--   0 root         (0) root         (0)    12721 2023-04-12 06:28:29.000000 flash_attn-1.0.1/csrc/layer_norm/ln_fwd_kernels.cuh
--rw-r--r--   0 root         (0) root         (0)     6655 2023-04-12 06:28:29.000000 flash_attn-1.0.1/csrc/layer_norm/ln_kernel_traits.h
--rw-r--r--   0 root         (0) root         (0)     1095 2023-04-12 06:28:29.000000 flash_attn-1.0.1/csrc/layer_norm/ln_parallel_bwd_1024.cu
--rw-r--r--   0 root         (0) root         (0)     1095 2023-04-12 06:28:29.000000 flash_attn-1.0.1/csrc/layer_norm/ln_parallel_bwd_1280.cu
--rw-r--r--   0 root         (0) root         (0)     1085 2023-04-12 06:28:29.000000 flash_attn-1.0.1/csrc/layer_norm/ln_parallel_bwd_1536.cu
--rw-r--r--   0 root         (0) root         (0)     1084 2023-04-12 06:28:29.000000 flash_attn-1.0.1/csrc/layer_norm/ln_parallel_bwd_2048.cu
--rw-r--r--   0 root         (0) root         (0)     1085 2023-04-12 06:28:29.000000 flash_attn-1.0.1/csrc/layer_norm/ln_parallel_bwd_256.cu
--rw-r--r--   0 root         (0) root         (0)     1085 2023-04-12 06:28:29.000000 flash_attn-1.0.1/csrc/layer_norm/ln_parallel_bwd_2560.cu
--rw-r--r--   0 root         (0) root         (0)     1084 2023-04-12 06:28:29.000000 flash_attn-1.0.1/csrc/layer_norm/ln_parallel_bwd_3072.cu
--rw-r--r--   0 root         (0) root         (0)     1145 2023-04-12 06:28:29.000000 flash_attn-1.0.1/csrc/layer_norm/ln_parallel_bwd_4096.cu
--rw-r--r--   0 root         (0) root         (0)     1085 2023-04-12 06:28:29.000000 flash_attn-1.0.1/csrc/layer_norm/ln_parallel_bwd_512.cu
--rw-r--r--   0 root         (0) root         (0)     1145 2023-04-12 06:28:29.000000 flash_attn-1.0.1/csrc/layer_norm/ln_parallel_bwd_5120.cu
--rw-r--r--   0 root         (0) root         (0)     1084 2023-04-12 06:28:29.000000 flash_attn-1.0.1/csrc/layer_norm/ln_parallel_bwd_6144.cu
--rw-r--r--   0 root         (0) root         (0)     1084 2023-04-12 06:28:29.000000 flash_attn-1.0.1/csrc/layer_norm/ln_parallel_bwd_7168.cu
--rw-r--r--   0 root         (0) root         (0)     1085 2023-04-12 06:28:29.000000 flash_attn-1.0.1/csrc/layer_norm/ln_parallel_bwd_768.cu
--rw-r--r--   0 root         (0) root         (0)     1084 2023-04-12 06:28:29.000000 flash_attn-1.0.1/csrc/layer_norm/ln_parallel_bwd_8192.cu
--rw-r--r--   0 root         (0) root         (0)     1033 2023-04-12 06:28:29.000000 flash_attn-1.0.1/csrc/layer_norm/ln_parallel_fwd_1024.cu
--rw-r--r--   0 root         (0) root         (0)     1033 2023-03-29 20:52:04.000000 flash_attn-1.0.1/csrc/layer_norm/ln_parallel_fwd_128.cu
--rw-r--r--   0 root         (0) root         (0)     1033 2023-04-12 06:28:29.000000 flash_attn-1.0.1/csrc/layer_norm/ln_parallel_fwd_1280.cu
--rw-r--r--   0 root         (0) root         (0)     1033 2023-04-12 06:28:29.000000 flash_attn-1.0.1/csrc/layer_norm/ln_parallel_fwd_1536.cu
--rw-r--r--   0 root         (0) root         (0)     1033 2023-04-12 06:28:29.000000 flash_attn-1.0.1/csrc/layer_norm/ln_parallel_fwd_2048.cu
--rw-r--r--   0 root         (0) root         (0)     1032 2023-04-12 06:28:29.000000 flash_attn-1.0.1/csrc/layer_norm/ln_parallel_fwd_256.cu
--rw-r--r--   0 root         (0) root         (0)     1033 2023-04-12 06:28:29.000000 flash_attn-1.0.1/csrc/layer_norm/ln_parallel_fwd_2560.cu
--rw-r--r--   0 root         (0) root         (0)     1033 2023-04-12 06:28:29.000000 flash_attn-1.0.1/csrc/layer_norm/ln_parallel_fwd_3072.cu
--rw-r--r--   0 root         (0) root         (0)     1033 2023-04-12 06:28:29.000000 flash_attn-1.0.1/csrc/layer_norm/ln_parallel_fwd_4096.cu
--rw-r--r--   0 root         (0) root         (0)     1033 2023-04-12 06:28:29.000000 flash_attn-1.0.1/csrc/layer_norm/ln_parallel_fwd_512.cu
--rw-r--r--   0 root         (0) root         (0)     1033 2023-04-12 06:28:29.000000 flash_attn-1.0.1/csrc/layer_norm/ln_parallel_fwd_5120.cu
--rw-r--r--   0 root         (0) root         (0)     1033 2023-04-12 06:28:29.000000 flash_attn-1.0.1/csrc/layer_norm/ln_parallel_fwd_6144.cu
--rw-r--r--   0 root         (0) root         (0)     1033 2023-04-12 06:28:29.000000 flash_attn-1.0.1/csrc/layer_norm/ln_parallel_fwd_7168.cu
--rw-r--r--   0 root         (0) root         (0)     1033 2023-04-12 06:28:29.000000 flash_attn-1.0.1/csrc/layer_norm/ln_parallel_fwd_768.cu
--rw-r--r--   0 root         (0) root         (0)     1033 2023-04-12 06:28:29.000000 flash_attn-1.0.1/csrc/layer_norm/ln_parallel_fwd_8192.cu
--rw-r--r--   0 root         (0) root         (0)    11720 2023-03-29 19:53:46.000000 flash_attn-1.0.1/csrc/layer_norm/ln_parallel_res_fwd_kernel.cuh
--rw-r--r--   0 root         (0) root         (0)      977 2023-03-27 04:43:57.000000 flash_attn-1.0.1/csrc/layer_norm/ln_parallel_residual_bwd_512.cu
--rw-r--r--   0 root         (0) root         (0)    24916 2023-04-12 06:28:29.000000 flash_attn-1.0.1/csrc/layer_norm/ln_parallel_residual_bwd_kernels.cuh
--rw-r--r--   0 root         (0) root         (0)    11515 2023-03-29 20:50:46.000000 flash_attn-1.0.1/csrc/layer_norm/ln_parallel_residual_fwd_kernel.cuh
--rw-r--r--   0 root         (0) root         (0)    12530 2023-04-12 06:28:29.000000 flash_attn-1.0.1/csrc/layer_norm/ln_parallel_residual_fwd_kernels.cuh
--rw-r--r--   0 root         (0) root         (0)    29989 2023-04-12 06:28:29.000000 flash_attn-1.0.1/csrc/layer_norm/ln_utils.cuh
--rw-r--r--   0 root         (0) root         (0)     1278 2023-04-12 06:28:29.000000 flash_attn-1.0.1/csrc/layer_norm/static_switch.h
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 17:06:33.970751 flash_attn-1.0.1/csrc/rotary/
--rw-r--r--   0 root         (0) root         (0)     1806 2023-04-12 06:28:29.000000 flash_attn-1.0.1/csrc/rotary/rotary.cpp
--rw-r--r--   0 root         (0) root         (0)     1984 2023-04-12 06:28:29.000000 flash_attn-1.0.1/csrc/rotary/rotary_cuda.cu
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 17:06:34.012112 flash_attn-1.0.1/csrc/xentropy/
--rw-r--r--   0 root         (0) root         (0)     2290 2023-04-12 06:28:29.000000 flash_attn-1.0.1/csrc/xentropy/interface.cpp
--rw-r--r--   0 root         (0) root         (0)    25783 2023-04-12 06:28:29.000000 flash_attn-1.0.1/csrc/xentropy/xentropy_kernel.cu
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 17:06:34.239596 flash_attn-1.0.1/flash_attn/
--rw-r--r--   0 root         (0) root         (0)        0 2022-07-04 00:53:37.000000 flash_attn-1.0.1/flash_attn/__init__.py
--rw-rw-r--   0 root         (0) root         (0)    20845 2022-10-31 02:25:05.000000 flash_attn-1.0.1/flash_attn/attention_kernl.py
--rw-r--r--   0 root         (0) root         (0)     5898 2023-04-12 06:28:29.000000 flash_attn-1.0.1/flash_attn/bert_padding.py
--rw-r--r--   0 root         (0) root         (0)     4722 2023-04-12 06:28:29.000000 flash_attn-1.0.1/flash_attn/flash_attention.py
--rw-r--r--   0 root         (0) root         (0)    21496 2023-04-12 06:28:29.000000 flash_attn-1.0.1/flash_attn/flash_attn_interface.py
--rw-r--r--   0 root         (0) root         (0)    38148 2023-04-12 06:28:29.000000 flash_attn-1.0.1/flash_attn/flash_attn_triton.py
--rw-r--r--   0 root         (0) root         (0)    10593 2023-04-12 06:28:29.000000 flash_attn-1.0.1/flash_attn/flash_attn_triton_og.py
--rw-r--r--   0 root         (0) root         (0)     8255 2022-11-18 03:30:00.000000 flash_attn-1.0.1/flash_attn/flash_attn_triton_single_query.py
--rw-r--r--   0 root         (0) root         (0)    37797 2023-03-17 09:16:10.000000 flash_attn-1.0.1/flash_attn/flash_attn_triton_tmp.py
--rw-r--r--   0 root         (0) root         (0)    10640 2023-03-12 08:48:14.000000 flash_attn-1.0.1/flash_attn/flash_attn_triton_tmp_og.py
--rw-rw-r--   0 root         (0) root         (0)    22919 2022-10-31 00:28:55.000000 flash_attn-1.0.1/flash_attn/flash_attn_triton_varlen.py
--rw-r--r--   0 root         (0) root         (0)     6819 2022-06-26 00:59:43.000000 flash_attn-1.0.1/flash_attn/flash_blocksparse_attention.py
--rw-r--r--   0 root         (0) root         (0)     7036 2022-06-26 00:59:43.000000 flash_attn-1.0.1/flash_attn/flash_blocksparse_attn_interface.py
--rw-r--r--   0 root         (0) root         (0)     7902 2023-04-12 06:28:29.000000 flash_attn-1.0.1/flash_attn/fused_softmax.py
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 17:06:34.375742 flash_attn-1.0.1/flash_attn/layers/
--rw-r--r--   0 root         (0) root         (0)        0 2023-04-12 06:28:29.000000 flash_attn-1.0.1/flash_attn/layers/__init__.py
--rw-r--r--   0 root         (0) root         (0)     2039 2023-04-12 06:28:29.000000 flash_attn-1.0.1/flash_attn/layers/patch_embed.py
--rw-r--r--   0 root         (0) root         (0)    10656 2023-04-12 06:28:29.000000 flash_attn-1.0.1/flash_attn/layers/rotary.py
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 17:06:34.439739 flash_attn-1.0.1/flash_attn/losses/
--rw-r--r--   0 root         (0) root         (0)        0 2023-04-12 06:28:29.000000 flash_attn-1.0.1/flash_attn/losses/__init__.py
--rw-r--r--   0 root         (0) root         (0)     6697 2023-04-12 06:28:29.000000 flash_attn-1.0.1/flash_attn/losses/cross_entropy.py
--rw-r--r--   0 root         (0) root         (0)     2122 2022-12-18 05:19:38.000000 flash_attn-1.0.1/flash_attn/losses/cross_entropy_apex.py
--rw-r--r--   0 root         (0) root         (0)     6649 2022-12-23 22:38:19.000000 flash_attn-1.0.1/flash_attn/losses/cross_entropy_parallel.py
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 17:06:34.664465 flash_attn-1.0.1/flash_attn/models/
--rw-r--r--   0 root         (0) root         (0)        0 2023-04-12 06:28:29.000000 flash_attn-1.0.1/flash_attn/models/__init__.py
--rw-r--r--   0 root         (0) root         (0)    26630 2023-04-12 06:28:29.000000 flash_attn-1.0.1/flash_attn/models/bert.py
--rw-r--r--   0 root         (0) root         (0)    34989 2023-04-12 06:28:29.000000 flash_attn-1.0.1/flash_attn/models/gpt.py
--rw-r--r--   0 root         (0) root         (0)     4863 2023-03-22 21:08:53.000000 flash_attn-1.0.1/flash_attn/models/gpt_j.py
--rw-r--r--   0 root         (0) root         (0)     5025 2023-04-12 06:28:29.000000 flash_attn-1.0.1/flash_attn/models/gpt_neox.py
--rw-r--r--   0 root         (0) root         (0)     4387 2023-04-12 06:28:29.000000 flash_attn-1.0.1/flash_attn/models/gptj.py
--rw-r--r--   0 root         (0) root         (0)     5174 2023-04-12 06:28:29.000000 flash_attn-1.0.1/flash_attn/models/opt.py
--rw-r--r--   0 root         (0) root         (0)    13621 2023-04-12 06:28:29.000000 flash_attn-1.0.1/flash_attn/models/vit.py
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 17:06:34.742905 flash_attn-1.0.1/flash_attn/modules/
--rw-r--r--   0 root         (0) root         (0)        0 2023-04-12 06:28:29.000000 flash_attn-1.0.1/flash_attn/modules/__init__.py
--rw-r--r--   0 root         (0) root         (0)    15244 2023-04-12 06:28:29.000000 flash_attn-1.0.1/flash_attn/modules/block.py
--rw-r--r--   0 root         (0) root         (0)     8620 2023-04-12 06:28:29.000000 flash_attn-1.0.1/flash_attn/modules/embedding.py
--rw-r--r--   0 root         (0) root         (0)    32394 2023-04-12 06:28:29.000000 flash_attn-1.0.1/flash_attn/modules/mha.py
--rw-r--r--   0 root         (0) root         (0)     1023 2023-04-12 06:28:29.000000 flash_attn-1.0.1/flash_attn/modules/mlp.py
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 17:06:34.822671 flash_attn-1.0.1/flash_attn/ops/
--rw-r--r--   0 root         (0) root         (0)        0 2023-04-12 06:28:29.000000 flash_attn-1.0.1/flash_attn/ops/__init__.py
--rw-r--r--   0 root         (0) root         (0)    25573 2023-04-12 06:28:29.000000 flash_attn-1.0.1/flash_attn/ops/fused_dense.py
--rw-r--r--   0 root         (0) root         (0)     2685 2023-04-12 06:28:29.000000 flash_attn-1.0.1/flash_attn/ops/gelu_activation.py
--rw-r--r--   0 root         (0) root         (0)    18374 2023-04-12 06:28:29.000000 flash_attn-1.0.1/flash_attn/ops/layer_norm.py
--rw-r--r--   0 root         (0) root         (0)     3159 2023-04-12 06:28:29.000000 flash_attn-1.0.1/flash_attn/ops/rms_norm.py
--rw-r--r--   0 root         (0) root         (0)     5855 2023-04-07 15:25:56.000000 flash_attn-1.0.1/flash_attn/rotary.py
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 17:06:34.857568 flash_attn-1.0.1/flash_attn/triton/
--rw-rw-r--   0 root         (0) root         (0)        0 2022-11-18 00:51:48.000000 flash_attn-1.0.1/flash_attn/triton/__init__.py
--rw-rw-r--   0 root         (0) root         (0)    14332 2022-10-23 23:52:09.000000 flash_attn-1.0.1/flash_attn/triton/fused_attention.py
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 17:06:34.936363 flash_attn-1.0.1/flash_attn/utils/
--rw-r--r--   0 root         (0) root         (0)        0 2023-04-12 06:28:29.000000 flash_attn-1.0.1/flash_attn/utils/__init__.py
--rw-r--r--   0 root         (0) root         (0)     5909 2023-04-12 06:28:29.000000 flash_attn-1.0.1/flash_attn/utils/benchmark.py
--rw-r--r--   0 root         (0) root         (0)     5545 2023-04-12 06:28:29.000000 flash_attn-1.0.1/flash_attn/utils/distributed.py
--rw-r--r--   0 root         (0) root         (0)    13140 2023-04-12 06:28:29.000000 flash_attn-1.0.1/flash_attn/utils/generation.py
--rw-r--r--   0 root         (0) root         (0)     1824 2023-04-12 06:28:29.000000 flash_attn-1.0.1/flash_attn/utils/pretrained.py
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 17:06:34.324397 flash_attn-1.0.1/flash_attn.egg-info/
--rw-rw-r--   0 root         (0) root         (0)    10037 2023-04-12 17:05:39.000000 flash_attn-1.0.1/flash_attn.egg-info/PKG-INFO
--rw-rw-r--   0 root         (0) root         (0)   113147 2023-04-12 17:05:53.000000 flash_attn-1.0.1/flash_attn.egg-info/SOURCES.txt
--rw-rw-r--   0 root         (0) root         (0)        1 2023-04-12 17:05:39.000000 flash_attn-1.0.1/flash_attn.egg-info/dependency_links.txt
--rw-rw-r--   0 root         (0) root         (0)       13 2023-04-12 17:05:39.000000 flash_attn-1.0.1/flash_attn.egg-info/requires.txt
--rw-rw-r--   0 root         (0) root         (0)       27 2023-04-12 17:05:39.000000 flash_attn-1.0.1/flash_attn.egg-info/top_level.txt
--rw-rw-r--   0 root         (0) root         (0)       38 2023-04-12 17:06:34.955113 flash_attn-1.0.1/setup.cfg
--rw-r--r--   0 root         (0) root         (0)     7541 2023-04-12 17:04:32.000000 flash_attn-1.0.1/setup.py
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-16 05:00:30.173370 flash_attn-1.0.2/
+-rw-r--r--   0 root         (0) root         (0)       56 2022-11-17 23:40:55.000000 flash_attn-1.0.2/AUTHORS
+-rw-r--r--   0 root         (0) root         (0)     1558 2022-09-09 19:08:03.000000 flash_attn-1.0.2/LICENSE
+-rw-r--r--   0 root         (0) root         (0)      251 2023-04-16 00:48:36.000000 flash_attn-1.0.2/MANIFEST.in
+-rw-rw-r--   0 root         (0) root         (0)    10037 2023-04-16 05:00:30.170120 flash_attn-1.0.2/PKG-INFO
+-rw-r--r--   0 root         (0) root         (0)     9529 2023-04-16 00:48:36.000000 flash_attn-1.0.2/README.md
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-16 04:59:39.761710 flash_attn-1.0.2/csrc/
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-16 04:59:39.903629 flash_attn-1.0.2/csrc/flash_attn/
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-16 04:59:39.527008 flash_attn-1.0.2/csrc/flash_attn/cutlass/
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-16 04:59:40.022990 flash_attn-1.0.2/csrc/flash_attn/cutlass/cmake/
+-rw-r--r--   0 root         (0) root         (0)     2023 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/cmake/nop.cu
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-16 04:59:39.060400 flash_attn-1.0.2/csrc/flash_attn/cutlass/examples/
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-16 04:59:40.145795 flash_attn-1.0.2/csrc/flash_attn/cutlass/examples/00_basic_gemm/
+-rw-r--r--   0 root         (0) root         (0)    14698 2023-04-15 23:28:02.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/examples/00_basic_gemm/basic_gemm.cu
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-16 04:59:40.264669 flash_attn-1.0.2/csrc/flash_attn/cutlass/examples/01_cutlass_utilities/
+-rw-r--r--   0 root         (0) root         (0)    13255 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/examples/01_cutlass_utilities/cutlass_utilities.cu
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-16 04:59:40.285472 flash_attn-1.0.2/csrc/flash_attn/cutlass/examples/02_dump_reg_shmem/
+-rw-r--r--   0 root         (0) root         (0)     7157 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/examples/02_dump_reg_shmem/dump_reg_shmem.cu
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-16 04:59:40.376371 flash_attn-1.0.2/csrc/flash_attn/cutlass/examples/03_visualize_layout/
+-rw-r--r--   0 root         (0) root         (0)     4478 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/examples/03_visualize_layout/options.h
+-rw-r--r--   0 root         (0) root         (0)     7081 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/examples/03_visualize_layout/register_layout.cu
+-rw-r--r--   0 root         (0) root         (0)     2691 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/examples/03_visualize_layout/register_layout.h
+-rw-r--r--   0 root         (0) root         (0)     5819 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/examples/03_visualize_layout/visualize_layout.cpp
+-rw-r--r--   0 root         (0) root         (0)    11415 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/examples/03_visualize_layout/visualize_layout.h
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-16 04:59:40.396959 flash_attn-1.0.2/csrc/flash_attn/cutlass/examples/04_tile_iterator/
+-rw-r--r--   0 root         (0) root         (0)     8226 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/examples/04_tile_iterator/tile_iterator.cu
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-16 04:59:40.423652 flash_attn-1.0.2/csrc/flash_attn/cutlass/examples/05_batched_gemm/
+-rw-r--r--   0 root         (0) root         (0)    15161 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/examples/05_batched_gemm/batched_gemm.cu
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-16 04:59:40.443720 flash_attn-1.0.2/csrc/flash_attn/cutlass/examples/06_splitK_gemm/
+-rw-r--r--   0 root         (0) root         (0)    17570 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/examples/06_splitK_gemm/splitk_gemm.cu
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-16 04:59:40.464512 flash_attn-1.0.2/csrc/flash_attn/cutlass/examples/07_volta_tensorop_gemm/
+-rw-r--r--   0 root         (0) root         (0)    18280 2023-04-15 23:28:02.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/examples/07_volta_tensorop_gemm/volta_tensorop_gemm.cu
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-16 04:59:40.485139 flash_attn-1.0.2/csrc/flash_attn/cutlass/examples/08_turing_tensorop_gemm/
+-rw-r--r--   0 root         (0) root         (0)    18226 2023-04-15 23:28:02.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/examples/08_turing_tensorop_gemm/turing_tensorop_gemm.cu
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-16 04:59:40.505304 flash_attn-1.0.2/csrc/flash_attn/cutlass/examples/09_turing_tensorop_conv2dfprop/
+-rw-r--r--   0 root         (0) root         (0)    28124 2023-04-15 23:28:02.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/examples/09_turing_tensorop_conv2dfprop/turing_tensorop_conv2dfprop.cu
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-16 04:59:40.525928 flash_attn-1.0.2/csrc/flash_attn/cutlass/examples/10_planar_complex/
+-rw-r--r--   0 root         (0) root         (0)    21947 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/examples/10_planar_complex/planar_complex.cu
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-16 04:59:40.646217 flash_attn-1.0.2/csrc/flash_attn/cutlass/examples/11_planar_complex_array/
+-rw-r--r--   0 root         (0) root         (0)    23244 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/examples/11_planar_complex_array/planar_complex_array.cu
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-16 04:59:40.670046 flash_attn-1.0.2/csrc/flash_attn/cutlass/examples/12_gemm_bias_relu/
+-rw-r--r--   0 root         (0) root         (0)    13151 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/examples/12_gemm_bias_relu/gemm_bias_relu.cu
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-16 04:59:41.381887 flash_attn-1.0.2/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/
+-rw-r--r--   0 root         (0) root         (0)    26102 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/b2b_conv2d_run.h
+-rw-r--r--   0 root         (0) root         (0)    22877 2023-04-15 23:28:02.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/b2b_gemm_run.h
+-rw-r--r--   0 root         (0) root         (0)    28268 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/b2b_interleaved_conv2d_run.h
+-rw-r--r--   0 root         (0) root         (0)    24493 2023-04-15 23:28:02.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/b2b_interleaved_gemm_run.h
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-16 04:59:41.416981 flash_attn-1.0.2/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/device/
+-rw-r--r--   0 root         (0) root         (0)    15552 2023-04-15 23:28:02.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/device/b2b_gemm.h
+-rw-r--r--   0 root         (0) root         (0)    11520 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/device/b2b_implicit_gemm_convolution.h
+-rw-r--r--   0 root         (0) root         (0)     8756 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/fused_two_convs_f16_sm75_rf.cu
+-rw-r--r--   0 root         (0) root         (0)     8759 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/fused_two_convs_f16_sm75_shmem.cu
+-rw-r--r--   0 root         (0) root         (0)     8712 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/fused_two_convs_f16_sm80_rf.cu
+-rw-r--r--   0 root         (0) root         (0)     8762 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/fused_two_convs_f16_sm80_shmem.cu
+-rw-r--r--   0 root         (0) root         (0)     8787 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/fused_two_convs_s8_sm75_rf.cu
+-rw-r--r--   0 root         (0) root         (0)     8793 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/fused_two_convs_s8_sm75_shmem.cu
+-rw-r--r--   0 root         (0) root         (0)     8711 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/fused_two_convs_s8_sm80_rf.cu
+-rw-r--r--   0 root         (0) root         (0)     8775 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/fused_two_convs_s8_sm80_shmem.cu
+-rw-r--r--   0 root         (0) root         (0)     7269 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/fused_two_gemms_f16_sm75_rf.cu
+-rw-r--r--   0 root         (0) root         (0)     7338 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/fused_two_gemms_f16_sm75_shmem.cu
+-rw-r--r--   0 root         (0) root         (0)     7294 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/fused_two_gemms_f16_sm80_rf.cu
+-rw-r--r--   0 root         (0) root         (0)     7359 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/fused_two_gemms_f16_sm80_shmem.cu
+-rw-r--r--   0 root         (0) root         (0)     7362 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/fused_two_gemms_s8_sm75_rf.cu
+-rw-r--r--   0 root         (0) root         (0)     7430 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/fused_two_gemms_s8_sm75_shmem.cu
+-rw-r--r--   0 root         (0) root         (0)     7627 2023-04-15 23:28:02.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/fused_two_gemms_s8_sm80_rf.cu
+-rw-r--r--   0 root         (0) root         (0)     7634 2023-04-15 23:28:02.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/fused_two_gemms_s8_sm80_shmem.cu
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-16 04:59:41.961835 flash_attn-1.0.2/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/kernel/
+-rw-r--r--   0 root         (0) root         (0)    16152 2023-04-15 23:28:02.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/kernel/b2b_gemm.h
+-rw-r--r--   0 root         (0) root         (0)    18151 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/kernel/b2b_implicit_gemm_convolution.h
+-rw-r--r--   0 root         (0) root         (0)     3973 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/kernel/default_b2b_conv2d_fprop.h
+-rw-r--r--   0 root         (0) root         (0)    26762 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/kernel/default_b2b_conv2d_fprop_sm75.h
+-rw-r--r--   0 root         (0) root         (0)    26775 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/kernel/default_b2b_conv2d_fprop_sm80.h
+-rw-r--r--   0 root         (0) root         (0)    28422 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/kernel/default_b2b_conv2d_fprop_smem_accumulator_sm75.h
+-rw-r--r--   0 root         (0) root         (0)    28073 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/kernel/default_b2b_conv2d_fprop_smem_accumulator_sm80.h
+-rw-r--r--   0 root         (0) root         (0)    17111 2023-04-15 23:28:02.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/kernel/default_b2b_gemm.h
+-rw-r--r--   0 root         (0) root         (0)    15658 2023-04-15 23:28:02.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/kernel/default_b2b_gemm_smem_accumulator.h
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-16 04:59:38.763744 flash_attn-1.0.2/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/reference/
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-16 04:59:41.987529 flash_attn-1.0.2/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/reference/device/
+-rw-r--r--   0 root         (0) root         (0)    10368 2023-04-15 23:28:02.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/reference/device/tensor_scale_bias.h
+-rw-r--r--   0 root         (0) root         (0)     3577 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/test_run.h
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-16 04:59:43.389113 flash_attn-1.0.2/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/threadblock/
+-rw-r--r--   0 root         (0) root         (0)    31616 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/threadblock/b2b_implicit_gemm_multistage.h
+-rw-r--r--   0 root         (0) root         (0)    31443 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/threadblock/b2b_implicit_gemm_multistage_smem_accumulator.h
+-rw-r--r--   0 root         (0) root         (0)    21010 2023-04-15 23:28:02.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/threadblock/b2b_implicit_gemm_pipelined.h
+-rw-r--r--   0 root         (0) root         (0)    20493 2023-04-15 23:28:02.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/threadblock/b2b_implicit_gemm_pipelined_smem_accumulator.h
+-rw-r--r--   0 root         (0) root         (0)     7983 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/threadblock/b2b_mma_base.h
+-rw-r--r--   0 root         (0) root         (0)     6047 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/threadblock/b2b_mma_base_smem_accumulator.h
+-rw-r--r--   0 root         (0) root         (0)    33788 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/threadblock/b2b_mma_multistage.h
+-rw-r--r--   0 root         (0) root         (0)    33506 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/threadblock/b2b_mma_multistage_smem_accumulator.h
+-rw-r--r--   0 root         (0) root         (0)    21451 2023-04-15 23:28:02.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/threadblock/b2b_mma_pipelined.h
+-rw-r--r--   0 root         (0) root         (0)    21065 2023-04-15 23:28:02.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/threadblock/b2b_mma_pipelined_smem_accumulator.h
+-rw-r--r--   0 root         (0) root         (0)    27144 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/threadblock/default_b2b_mma.h
+-rw-r--r--   0 root         (0) root         (0)    27400 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/threadblock/default_b2b_mma_smem_accumulator.h
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-16 04:59:43.409483 flash_attn-1.0.2/csrc/flash_attn/cutlass/examples/14_ampere_tf32_tensorop_gemm/
+-rw-r--r--   0 root         (0) root         (0)    18020 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/examples/14_ampere_tf32_tensorop_gemm/ampere_tf32_tensorop_gemm.cu
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-16 04:59:43.429264 flash_attn-1.0.2/csrc/flash_attn/cutlass/examples/15_ampere_sparse_tensorop_gemm/
+-rw-r--r--   0 root         (0) root         (0)    15042 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/examples/15_ampere_sparse_tensorop_gemm/ampere_sparse_tensorop_gemm.cu
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-16 04:59:43.449371 flash_attn-1.0.2/csrc/flash_attn/cutlass/examples/16_ampere_tensorop_conv2dfprop/
+-rw-r--r--   0 root         (0) root         (0)    27755 2023-04-15 23:28:02.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/examples/16_ampere_tensorop_conv2dfprop/ampere_tensorop_conv2dfprop.cu
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-16 04:59:43.469640 flash_attn-1.0.2/csrc/flash_attn/cutlass/examples/17_fprop_per_channel_bias/
+-rw-r--r--   0 root         (0) root         (0)    12580 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/examples/17_fprop_per_channel_bias/fprop_per_channel_bias.cu
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-16 04:59:43.490114 flash_attn-1.0.2/csrc/flash_attn/cutlass/examples/18_ampere_fp64_tensorop_affine2_gemm/
+-rw-r--r--   0 root         (0) root         (0)    14007 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/examples/18_ampere_fp64_tensorop_affine2_gemm/ampere_fp64_tensorop_affine2_gemm.cu
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-16 04:59:43.510219 flash_attn-1.0.2/csrc/flash_attn/cutlass/examples/19_tensorop_canonical/
+-rw-r--r--   0 root         (0) root         (0)    13401 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/examples/19_tensorop_canonical/tensorop_canonical.cu
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-16 04:59:43.530829 flash_attn-1.0.2/csrc/flash_attn/cutlass/examples/20_simt_canonical/
+-rw-r--r--   0 root         (0) root         (0)    12556 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/examples/20_simt_canonical/simt_canonical.cu
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-16 04:59:43.551292 flash_attn-1.0.2/csrc/flash_attn/cutlass/examples/21_quaternion_gemm/
+-rw-r--r--   0 root         (0) root         (0)    17319 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/examples/21_quaternion_gemm/quaternion_gemm.cu
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-16 04:59:43.571754 flash_attn-1.0.2/csrc/flash_attn/cutlass/examples/22_quaternion_conv/
+-rw-r--r--   0 root         (0) root         (0)    21495 2023-04-15 23:28:02.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/examples/22_quaternion_conv/quaternion_conv.cu
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-16 04:59:43.591914 flash_attn-1.0.2/csrc/flash_attn/cutlass/examples/23_ampere_gemm_operand_reduction_fusion/
+-rw-r--r--   0 root         (0) root         (0)    27530 2023-04-15 23:28:02.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/examples/23_ampere_gemm_operand_reduction_fusion/ampere_gemm_operand_reduction_fusion.cu
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-16 04:59:43.618375 flash_attn-1.0.2/csrc/flash_attn/cutlass/examples/24_gemm_grouped/
+-rw-r--r--   0 root         (0) root         (0)    50996 2023-04-16 04:29:29.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/examples/24_gemm_grouped/gemm_grouped.cu
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-16 04:59:43.751026 flash_attn-1.0.2/csrc/flash_attn/cutlass/examples/25_ampere_fprop_mainloop_fusion/
+-rw-r--r--   0 root         (0) root         (0)    26547 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/examples/25_ampere_fprop_mainloop_fusion/ampere_3d_fprop_mainloop_fusion.cu
+-rw-r--r--   0 root         (0) root         (0)    25628 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/examples/25_ampere_fprop_mainloop_fusion/ampere_fprop_mainloop_fusion.cu
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-16 04:59:43.869762 flash_attn-1.0.2/csrc/flash_attn/cutlass/examples/26_ampere_wgrad_mainloop_fusion/
+-rw-r--r--   0 root         (0) root         (0)    25538 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/examples/26_ampere_wgrad_mainloop_fusion/ampere_wgrad_mainloop_fusion.cu
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-16 04:59:43.988959 flash_attn-1.0.2/csrc/flash_attn/cutlass/examples/27_ampere_3xtf32_fast_accurate_tensorop_gemm/
+-rw-r--r--   0 root         (0) root         (0)    30446 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/examples/27_ampere_3xtf32_fast_accurate_tensorop_gemm/27_ampere_3xtf32_fast_accurate_tensorop_gemm.cu
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-16 04:59:44.009309 flash_attn-1.0.2/csrc/flash_attn/cutlass/examples/28_ampere_3xtf32_fast_accurate_tensorop_fprop/
+-rw-r--r--   0 root         (0) root         (0)    28159 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/examples/28_ampere_3xtf32_fast_accurate_tensorop_fprop/ampere_3xtf32_fast_accurate_tensorop_fprop.cu
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-16 04:59:44.058394 flash_attn-1.0.2/csrc/flash_attn/cutlass/examples/29_ampere_3xtf32_fast_accurate_tensorop_complex_gemm/
+-rw-r--r--   0 root         (0) root         (0)    28382 2023-04-15 18:47:44.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/examples/29_ampere_3xtf32_fast_accurate_tensorop_complex_gemm/29_3xtf32_complex_gemm.cu
+-rw-r--r--   0 root         (0) root         (0)    28403 2023-04-15 23:28:02.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/examples/29_ampere_3xtf32_fast_accurate_tensorop_complex_gemm/29_ampere_3xtf32_fast_accurate_tensorop_complex_gemm.cu
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-16 04:59:44.176600 flash_attn-1.0.2/csrc/flash_attn/cutlass/examples/30_wgrad_split_k/
+-rw-r--r--   0 root         (0) root         (0)    27329 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/examples/30_wgrad_split_k/30_wgrad_split_k.cu
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-16 04:59:44.197323 flash_attn-1.0.2/csrc/flash_attn/cutlass/examples/31_basic_syrk/
+-rw-r--r--   0 root         (0) root         (0)    15206 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/examples/31_basic_syrk/basic_syrk.cu
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-16 04:59:44.316710 flash_attn-1.0.2/csrc/flash_attn/cutlass/examples/32_basic_trmm/
+-rw-r--r--   0 root         (0) root         (0)    15907 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/examples/32_basic_trmm/basic_trmm.cu
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-16 04:59:44.336833 flash_attn-1.0.2/csrc/flash_attn/cutlass/examples/33_ampere_3xtf32_tensorop_symm/
+-rw-r--r--   0 root         (0) root         (0)    31803 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/examples/33_ampere_3xtf32_tensorop_symm/ampere_3xtf32_tensorop_symm.cu
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-16 04:59:44.455854 flash_attn-1.0.2/csrc/flash_attn/cutlass/examples/34_transposed_conv2d/
+-rw-r--r--   0 root         (0) root         (0)    22378 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/examples/34_transposed_conv2d/34_transposed_conv2d.cu
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-16 04:59:44.703444 flash_attn-1.0.2/csrc/flash_attn/cutlass/examples/35_gemm_softmax/
+-rw-r--r--   0 root         (0) root         (0)    23114 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/examples/35_gemm_softmax/gemm_softmax.cu
+-rw-r--r--   0 root         (0) root         (0)    16723 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/examples/35_gemm_softmax/gemm_with_epilogue_visitor.h
+-rw-r--r--   0 root         (0) root         (0)    18713 2023-04-15 23:28:02.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/examples/35_gemm_softmax/gemm_with_softmax.h
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-16 04:59:44.723315 flash_attn-1.0.2/csrc/flash_attn/cutlass/examples/36_gather_scatter_fusion/
+-rw-r--r--   0 root         (0) root         (0)    20795 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/examples/36_gather_scatter_fusion/gather_scatter_fusion.cu
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-16 04:59:44.773817 flash_attn-1.0.2/csrc/flash_attn/cutlass/examples/37_gemm_layernorm_gemm_fusion/
+-rw-r--r--   0 root         (0) root         (0)    31111 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/examples/37_gemm_layernorm_gemm_fusion/gemm_layernorm.cu
+-rw-r--r--   0 root         (0) root         (0)    13982 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/examples/37_gemm_layernorm_gemm_fusion/gemm_with_epilogue_visitor.h
+-rw-r--r--   0 root         (0) root         (0)    33916 2023-04-16 04:29:29.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/examples/37_gemm_layernorm_gemm_fusion/gemm_with_layernorm.h
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-16 04:59:44.793821 flash_attn-1.0.2/csrc/flash_attn/cutlass/examples/38_syr2k_grouped/
+-rw-r--r--   0 root         (0) root         (0)    47455 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/examples/38_syr2k_grouped/syr2k_grouped.cu
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-16 04:59:45.056452 flash_attn-1.0.2/csrc/flash_attn/cutlass/examples/39_gemm_permute/
+-rw-r--r--   0 root         (0) root         (0)    37896 2023-04-15 23:28:02.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/examples/39_gemm_permute/gemm_permute.cu
+-rw-r--r--   0 root         (0) root         (0)    15309 2023-04-15 18:47:44.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/examples/39_gemm_permute/layouts.h
+-rw-r--r--   0 root         (0) root         (0)    11985 2023-04-15 18:47:44.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/examples/39_gemm_permute/permute_info.h
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-16 04:59:45.590993 flash_attn-1.0.2/csrc/flash_attn/cutlass/examples/41_fused_multi_head_attention/
+-rw-r--r--   0 root         (0) root         (0)    18389 2023-04-16 04:29:29.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/examples/41_fused_multi_head_attention/attention_scaling_coefs_updater.h
+-rw-r--r--   0 root         (0) root         (0)     8286 2023-04-16 04:29:29.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/examples/41_fused_multi_head_attention/debug_utils.h
+-rw-r--r--   0 root         (0) root         (0)     9888 2023-04-16 04:29:29.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/examples/41_fused_multi_head_attention/default_fmha_grouped.h
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-16 04:59:45.640919 flash_attn-1.0.2/csrc/flash_attn/cutlass/examples/41_fused_multi_head_attention/epilogue/
+-rw-r--r--   0 root         (0) root         (0)    22349 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/examples/41_fused_multi_head_attention/epilogue/epilogue_pipelined.h
+-rw-r--r--   0 root         (0) root         (0)     9162 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/examples/41_fused_multi_head_attention/epilogue/epilogue_rescale_output.h
+-rw-r--r--   0 root         (0) root         (0)     6111 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/examples/41_fused_multi_head_attention/epilogue/epilogue_thread_apply_logsumexp.h
+-rw-r--r--   0 root         (0) root         (0)    22349 2023-04-16 04:29:29.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/examples/41_fused_multi_head_attention/epilogue_pipelined.h
+-rw-r--r--   0 root         (0) root         (0)     9162 2023-04-16 04:29:29.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/examples/41_fused_multi_head_attention/epilogue_rescale_output.h
+-rw-r--r--   0 root         (0) root         (0)     6111 2023-04-16 04:29:29.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/examples/41_fused_multi_head_attention/epilogue_thread_apply_logsumexp.h
+-rw-r--r--   0 root         (0) root         (0)     6768 2023-04-16 04:29:29.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/examples/41_fused_multi_head_attention/find_default_mma.h
+-rw-r--r--   0 root         (0) root         (0)    29972 2023-04-16 04:29:29.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/examples/41_fused_multi_head_attention/fmha_grouped.h
+-rw-r--r--   0 root         (0) root         (0)     6666 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/examples/41_fused_multi_head_attention/fmha_grouped_problem_visitor.h
+-rw-r--r--   0 root         (0) root         (0)    11078 2023-04-15 18:47:44.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/examples/41_fused_multi_head_attention/fused_multi_head_attention_backward.cu
+-rw-r--r--   0 root         (0) root         (0)    37104 2023-04-16 04:29:29.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/examples/41_fused_multi_head_attention/fused_multihead_attention_fixed_seqlen.cu
+-rw-r--r--   0 root         (0) root         (0)    39975 2023-04-16 04:29:29.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/examples/41_fused_multi_head_attention/fused_multihead_attention_variable_seqlen.cu
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-16 04:59:45.748541 flash_attn-1.0.2/csrc/flash_attn/cutlass/examples/41_fused_multi_head_attention/gemm/
+-rw-r--r--   0 root         (0) root         (0)     3994 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/examples/41_fused_multi_head_attention/gemm/custom_mma.h
+-rw-r--r--   0 root         (0) root         (0)     6241 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/examples/41_fused_multi_head_attention/gemm/custom_mma_base.h
+-rw-r--r--   0 root         (0) root         (0)    27198 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/examples/41_fused_multi_head_attention/gemm/custom_mma_multistage.h
+-rw-r--r--   0 root         (0) root         (0)    14090 2023-04-15 23:28:02.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/examples/41_fused_multi_head_attention/gemm/custom_mma_pipelined.h
+-rw-r--r--   0 root         (0) root         (0)     6782 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/examples/41_fused_multi_head_attention/gemm/find_default_mma.h
+-rw-r--r--   0 root         (0) root         (0)    13959 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/examples/41_fused_multi_head_attention/gemm/mma_accum_lambda_iterator.h
+-rw-r--r--   0 root         (0) root         (0)    72983 2023-04-15 23:28:02.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/examples/41_fused_multi_head_attention/gemm/mma_from_smem.h
+-rw-r--r--   0 root         (0) root         (0)    12089 2023-04-16 04:29:29.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/examples/41_fused_multi_head_attention/gemm_kernel_utils.h
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-16 04:59:45.942756 flash_attn-1.0.2/csrc/flash_attn/cutlass/examples/41_fused_multi_head_attention/iterators/
+-rw-r--r--   0 root         (0) root         (0)    23805 2023-04-16 04:29:29.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/examples/41_fused_multi_head_attention/iterators/epilogue_predicated_tile_iterator.h
+-rw-r--r--   0 root         (0) root         (0)     3142 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/examples/41_fused_multi_head_attention/iterators/make_residual_last.h
+-rw-r--r--   0 root         (0) root         (0)    64480 2023-04-15 23:28:02.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/examples/41_fused_multi_head_attention/iterators/predicated_tile_access_iterator_residual_last.h
+-rw-r--r--   0 root         (0) root         (0)    64500 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/examples/41_fused_multi_head_attention/iterators/predicated_tile_iterator_residual_last.h
+-rw-r--r--   0 root         (0) root         (0)     2435 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/examples/41_fused_multi_head_attention/iterators/transpose_warp_iterator.h
+-rw-r--r--   0 root         (0) root         (0)     9497 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/examples/41_fused_multi_head_attention/iterators/warp_iterator_from_smem.h
+-rw-r--r--   0 root         (0) root         (0)    86943 2023-04-15 18:47:44.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/examples/41_fused_multi_head_attention/kernel_backward.h
+-rw-r--r--   0 root         (0) root         (0)    37905 2023-04-16 04:29:29.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/examples/41_fused_multi_head_attention/kernel_forward.h
+-rw-r--r--   0 root         (0) root         (0)    61195 2023-04-16 04:29:29.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/examples/41_fused_multi_head_attention/mma_from_smem.h
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-16 04:59:45.964281 flash_attn-1.0.2/csrc/flash_attn/cutlass/examples/41_fused_multi_head_attention/transform/
+-rw-r--r--   0 root         (0) root         (0)     3747 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/examples/41_fused_multi_head_attention/transform/tile_smem_loader.h
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-16 04:59:46.131706 flash_attn-1.0.2/csrc/flash_attn/cutlass/examples/41_multi_head_attention/
+-rw-r--r--   0 root         (0) root         (0)    37396 2023-04-12 16:23:45.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/examples/41_multi_head_attention/fused_multihead_attention.cu
+-rw-r--r--   0 root         (0) root         (0)    17839 2023-04-12 16:23:45.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/examples/41_multi_head_attention/gemm_attention.h
+-rw-r--r--   0 root         (0) root         (0)    16152 2023-04-12 16:23:45.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/examples/41_multi_head_attention/gemm_grouped_with_softmax_visitor.h
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-16 04:59:46.154624 flash_attn-1.0.2/csrc/flash_attn/cutlass/examples/42_ampere_tensorop_group_conv/
+-rw-r--r--   0 root         (0) root         (0)    23901 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/examples/42_ampere_tensorop_group_conv/ampere_tensorop_group_conv.cu
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-16 04:59:46.175256 flash_attn-1.0.2/csrc/flash_attn/cutlass/examples/43_ell_block_sparse_gemm/
+-rw-r--r--   0 root         (0) root         (0)    23867 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/examples/43_ell_block_sparse_gemm/ell_block_sparse_gemm.cu
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-16 04:59:46.306028 flash_attn-1.0.2/csrc/flash_attn/cutlass/examples/44_multi_gemm_ir_and_codegen/
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-16 04:59:38.977063 flash_attn-1.0.2/csrc/flash_attn/cutlass/examples/44_multi_gemm_ir_and_codegen/fixed_impl/
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-16 04:59:38.966225 flash_attn-1.0.2/csrc/flash_attn/cutlass/examples/44_multi_gemm_ir_and_codegen/fixed_impl/epilogue/
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-16 04:59:46.378082 flash_attn-1.0.2/csrc/flash_attn/cutlass/examples/44_multi_gemm_ir_and_codegen/fixed_impl/epilogue/threadblock/
+-rw-r--r--   0 root         (0) root         (0)     6370 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/examples/44_multi_gemm_ir_and_codegen/fixed_impl/epilogue/threadblock/default_bias_act_epilogue_tensor_op.h
+-rw-r--r--   0 root         (0) root         (0)     4099 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/examples/44_multi_gemm_ir_and_codegen/fixed_impl/epilogue/threadblock/default_thread_map_tensor_op_for_fused_bias.h
+-rw-r--r--   0 root         (0) root         (0)     8285 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/examples/44_multi_gemm_ir_and_codegen/fixed_impl/epilogue/threadblock/fused_bias_act_epilogue.h
+-rw-r--r--   0 root         (0) root         (0)    10439 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/examples/44_multi_gemm_ir_and_codegen/fixed_impl/epilogue/threadblock/output_tile_thread_map_for_fused_bias.h
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-16 04:59:46.497778 flash_attn-1.0.2/csrc/flash_attn/cutlass/examples/44_multi_gemm_ir_and_codegen/fixed_impl/epilogue/warp/
+-rw-r--r--   0 root         (0) root         (0)     6848 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/examples/44_multi_gemm_ir_and_codegen/fixed_impl/epilogue/warp/fused_bias_act_fragment_iterator_tensor_op.h
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-16 04:59:38.982700 flash_attn-1.0.2/csrc/flash_attn/cutlass/examples/44_multi_gemm_ir_and_codegen/fixed_impl/gemm/
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-16 04:59:46.521168 flash_attn-1.0.2/csrc/flash_attn/cutlass/examples/44_multi_gemm_ir_and_codegen/fixed_impl/gemm/warp/
+-rw-r--r--   0 root         (0) root         (0)    14747 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/examples/44_multi_gemm_ir_and_codegen/fixed_impl/gemm/warp/mma_tensor_op_fragment_iterator_without_output_op.h
+-rw-r--r--   0 root         (0) root         (0)    10231 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/examples/44_multi_gemm_ir_and_codegen/leaky_bias.h
+-rw-r--r--   0 root         (0) root         (0)     3745 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/examples/44_multi_gemm_ir_and_codegen/utils.h
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-16 04:59:46.583947 flash_attn-1.0.2/csrc/flash_attn/cutlass/examples/45_dual_gemm/
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-16 04:59:46.603529 flash_attn-1.0.2/csrc/flash_attn/cutlass/examples/45_dual_gemm/device/
+-rw-r--r--   0 root         (0) root         (0)    15362 2023-04-16 04:29:29.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/examples/45_dual_gemm/device/dual_gemm.h
+-rw-r--r--   0 root         (0) root         (0)     8109 2023-04-16 04:29:29.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/examples/45_dual_gemm/dual_gemm.cu
+-rw-r--r--   0 root         (0) root         (0)     2366 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/examples/45_dual_gemm/dual_gemm_common.h
+-rw-r--r--   0 root         (0) root         (0)    26478 2023-04-16 04:29:29.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/examples/45_dual_gemm/dual_gemm_run.h
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-16 04:59:46.623291 flash_attn-1.0.2/csrc/flash_attn/cutlass/examples/45_dual_gemm/kernel/
+-rw-r--r--   0 root         (0) root         (0)    16424 2023-04-16 04:29:29.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/examples/45_dual_gemm/kernel/dual_gemm.h
+-rw-r--r--   0 root         (0) root         (0)     3577 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/examples/45_dual_gemm/test_run.h
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-16 04:59:46.741679 flash_attn-1.0.2/csrc/flash_attn/cutlass/examples/45_dual_gemm/thread/
+-rw-r--r--   0 root         (0) root         (0)     5818 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/examples/45_dual_gemm/thread/left_silu_and_mul.h
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-16 04:59:46.790279 flash_attn-1.0.2/csrc/flash_attn/cutlass/examples/45_dual_gemm/threadblock/
+-rw-r--r--   0 root         (0) root         (0)    15613 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/examples/45_dual_gemm/threadblock/dual_epilogue.h
+-rw-r--r--   0 root         (0) root         (0)     7264 2023-04-16 04:29:29.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/examples/45_dual_gemm/threadblock/dual_mma_base.h
+-rw-r--r--   0 root         (0) root         (0)    28984 2023-04-16 04:29:29.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/examples/45_dual_gemm/threadblock/dual_mma_multistage.h
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-16 04:59:46.812045 flash_attn-1.0.2/csrc/flash_attn/cutlass/examples/46_depthwise_simt_conv2dfprop/
+-rw-r--r--   0 root         (0) root         (0)    24464 2023-04-15 23:28:02.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/examples/46_depthwise_simt_conv2dfprop/depthwise_simt_conv2dfprop.cu
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-16 04:59:46.832492 flash_attn-1.0.2/csrc/flash_attn/cutlass/examples/47_ampere_gemm_universal_streamk/
+-rw-r--r--   0 root         (0) root         (0)    22677 2023-04-16 04:29:29.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/examples/47_ampere_gemm_universal_streamk/ampere_gemm_universal_streamk.cu
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-16 04:59:46.959854 flash_attn-1.0.2/csrc/flash_attn/cutlass/examples/48_hopper_warp_specialized_gemm/
+-rw-r--r--   0 root         (0) root         (0)    16736 2023-04-15 23:28:02.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/examples/48_hopper_warp_specialized_gemm/48_hopper_warp_specialized_gemm.cu
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-16 04:59:46.979949 flash_attn-1.0.2/csrc/flash_attn/cutlass/examples/49_hopper_gemm_schedules_with_collective_builder/
+-rw-r--r--   0 root         (0) root         (0)    22631 2023-04-15 23:28:02.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/examples/49_hopper_gemm_schedules_with_collective_builder/49_hopper_gemm_schedules_with_collective_builder.cu
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-16 04:59:47.113870 flash_attn-1.0.2/csrc/flash_attn/cutlass/examples/49_hopper_gemm_with_collective_builder/
+-rw-r--r--   0 root         (0) root         (0)    24957 2023-04-15 18:47:44.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/examples/49_hopper_gemm_with_collective_builder/49_collective_builder.cu
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-16 04:59:47.135172 flash_attn-1.0.2/csrc/flash_attn/cutlass/examples/50_hopper_gemm_with_epilogue_swizzle/
+-rw-r--r--   0 root         (0) root         (0)    18635 2023-04-15 23:28:02.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/examples/50_hopper_gemm_with_epilogue_swizzle/50_hopper_gemm_with_epilogue_swizzle.cu
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-16 04:59:47.169992 flash_attn-1.0.2/csrc/flash_attn/cutlass/examples/51_hopper_gett/
+-rw-r--r--   0 root         (0) root         (0)    17256 2023-04-15 18:47:44.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/examples/51_hopper_gett/51_hopper_gett.cu
+-rw-r--r--   0 root         (0) root         (0)     5572 2023-04-15 18:47:44.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/examples/51_hopper_gett/gett_kernel.cuh
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-16 04:59:47.193209 flash_attn-1.0.2/csrc/flash_attn/cutlass/examples/60_cutlass_import/
+-rw-r--r--   0 root         (0) root         (0)     2849 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/examples/60_cutlass_import/main.cpp
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-16 04:59:47.213499 flash_attn-1.0.2/csrc/flash_attn/cutlass/examples/common/
+-rw-r--r--   0 root         (0) root         (0)     2621 2023-04-16 04:29:29.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/examples/common/helper.h
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-16 04:59:39.066096 flash_attn-1.0.2/csrc/flash_attn/cutlass/examples/cute/
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-16 04:59:47.236756 flash_attn-1.0.2/csrc/flash_attn/cutlass/examples/cute/tutorial/
+-rw-r--r--   0 root         (0) root         (0)    14342 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/examples/cute/tutorial/sgemm_nt_1.cu
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-16 04:59:39.077724 flash_attn-1.0.2/csrc/flash_attn/cutlass/include/
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-16 04:59:49.057690 flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/
+-rw-r--r--   0 root         (0) root         (0)     3793 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/aligned_buffer.h
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-16 04:59:49.709097 flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/arch/
+-rw-r--r--   0 root         (0) root         (0)     3538 2023-04-15 23:28:02.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/arch/arch.h
+-rw-r--r--   0 root         (0) root         (0)    12127 2023-04-15 23:28:02.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/arch/barrier.h
+-rw-r--r--   0 root         (0) root         (0)     2691 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/arch/cache_operation.h
+-rw-r--r--   0 root         (0) root         (0)    14313 2023-04-15 23:28:02.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/arch/memory.h
+-rw-r--r--   0 root         (0) root         (0)    10490 2023-04-16 04:29:29.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/arch/memory_sm75.h
+-rw-r--r--   0 root         (0) root         (0)    15166 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/arch/memory_sm80.h
+-rw-r--r--   0 root         (0) root         (0)     8073 2023-04-16 04:29:29.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/arch/mma.h
+-rw-r--r--   0 root         (0) root         (0)    11096 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/arch/mma_sm50.h
+-rw-r--r--   0 root         (0) root         (0)     7040 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/arch/mma_sm60.h
+-rw-r--r--   0 root         (0) root         (0)     4193 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/arch/mma_sm61.h
+-rw-r--r--   0 root         (0) root         (0)    16554 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/arch/mma_sm70.h
+-rw-r--r--   0 root         (0) root         (0)    31682 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/arch/mma_sm75.h
+-rw-r--r--   0 root         (0) root         (0)    55573 2023-04-16 04:29:29.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/arch/mma_sm80.h
+-rw-r--r--   0 root         (0) root         (0)     4430 2023-04-16 04:29:29.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/arch/mma_sm90.h
+-rw-r--r--   0 root         (0) root         (0)    43978 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/arch/mma_sparse_sm80.h
+-rw-r--r--   0 root         (0) root         (0)     2622 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/arch/reg_reconfig.h
+-rw-r--r--   0 root         (0) root         (0)     3998 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/arch/simd.h
+-rw-r--r--   0 root         (0) root         (0)     3656 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/arch/simd_sm60.h
+-rw-r--r--   0 root         (0) root         (0)     5102 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/arch/simd_sm61.h
+-rw-r--r--   0 root         (0) root         (0)     8473 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/arch/wmma.h
+-rw-r--r--   0 root         (0) root         (0)     5286 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/arch/wmma_sm70.h
+-rw-r--r--   0 root         (0) root         (0)     7746 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/arch/wmma_sm72.h
+-rw-r--r--   0 root         (0) root         (0)     7616 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/arch/wmma_sm75.h
+-rw-r--r--   0 root         (0) root         (0)    62709 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/array.h
+-rw-r--r--   0 root         (0) root         (0)     3662 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/array_planar_complex.h
+-rw-r--r--   0 root         (0) root         (0)    13154 2023-04-16 04:29:29.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/array_subbyte.h
+-rw-r--r--   0 root         (0) root         (0)     6371 2023-04-15 23:28:02.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/barrier.h
+-rw-r--r--   0 root         (0) root         (0)    13371 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/bfloat16.h
+-rw-r--r--   0 root         (0) root         (0)     6338 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/blas3.h
+-rw-r--r--   0 root         (0) root         (0)     9372 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/block_striped.h
+-rw-r--r--   0 root         (0) root         (0)    19422 2023-04-15 23:28:02.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/complex.h
+-rw-r--r--   0 root         (0) root         (0)    47943 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/constants.h
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-16 04:59:49.856006 flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/conv/
+-rw-r--r--   0 root         (0) root         (0)    22725 2023-04-15 23:28:02.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/conv/conv2d_problem_size.h
+-rw-r--r--   0 root         (0) root         (0)    16292 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/conv/conv3d_problem_size.h
+-rw-r--r--   0 root         (0) root         (0)     6664 2023-04-15 23:28:02.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/conv/convolution.h
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-16 04:59:50.934563 flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/conv/device/
+-rw-r--r--   0 root         (0) root         (0)     9744 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/conv/device/direct_convolution.h
+-rw-r--r--   0 root         (0) root         (0)    12078 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/conv/device/implicit_gemm_convolution.h
+-rw-r--r--   0 root         (0) root         (0)    10044 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/conv/device/implicit_gemm_convolution_fusion.h
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-16 04:59:51.216303 flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/conv/kernel/
+-rw-r--r--   0 root         (0) root         (0)     7671 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/conv/kernel/default_conv2d.h
+-rw-r--r--   0 root         (0) root         (0)    53546 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/conv/kernel/default_conv2d_dgrad.h
+-rw-r--r--   0 root         (0) root         (0)    56838 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/conv/kernel/default_conv2d_fprop.h
+-rw-r--r--   0 root         (0) root         (0)    11953 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/conv/kernel/default_conv2d_fprop_fusion.h
+-rw-r--r--   0 root         (0) root         (0)     4658 2023-04-16 04:29:29.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/conv/kernel/default_conv2d_fprop_with_broadcast.h
+-rw-r--r--   0 root         (0) root         (0)     4660 2023-04-15 23:28:02.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/conv/kernel/default_conv2d_fprop_with_reduction.h
+-rw-r--r--   0 root         (0) root         (0)    15891 2023-04-15 23:28:02.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/conv/kernel/default_conv2d_group_fprop.h
+-rw-r--r--   0 root         (0) root         (0)    28745 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/conv/kernel/default_conv2d_wgrad.h
+-rw-r--r--   0 root         (0) root         (0)    10459 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/conv/kernel/default_conv2d_wgrad_fusion.h
+-rw-r--r--   0 root         (0) root         (0)     9324 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/conv/kernel/default_conv3d_dgrad.h
+-rw-r--r--   0 root         (0) root         (0)    14864 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/conv/kernel/default_conv3d_fprop.h
+-rw-r--r--   0 root         (0) root         (0)    11980 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/conv/kernel/default_conv3d_fprop_fusion.h
+-rw-r--r--   0 root         (0) root         (0)    14883 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/conv/kernel/default_conv3d_wgrad.h
+-rw-r--r--   0 root         (0) root         (0)    19294 2023-04-15 23:28:02.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/conv/kernel/default_depthwise_fprop.h
+-rw-r--r--   0 root         (0) root         (0)    18048 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/conv/kernel/direct_convolution.h
+-rw-r--r--   0 root         (0) root         (0)    15454 2023-04-16 04:29:29.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/conv/kernel/implicit_gemm_convolution.h
+-rw-r--r--   0 root         (0) root         (0)    15709 2023-04-16 04:29:29.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/conv/kernel/implicit_gemm_convolution_fusion.h
+-rw-r--r--   0 root         (0) root         (0)    17131 2023-04-16 04:29:29.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/conv/kernel/implicit_gemm_convolution_strided_dgrad.h
+-rw-r--r--   0 root         (0) root         (0)    16749 2023-04-16 04:29:29.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/conv/kernel/implicit_gemm_convolution_with_fused_epilogue.h
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-16 04:59:51.239286 flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/conv/thread/
+-rw-r--r--   0 root         (0) root         (0)     9689 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/conv/thread/depthwise_mma.h
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-16 04:59:53.459872 flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/
+-rw-r--r--   0 root         (0) root         (0)    15306 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/conv2d_dgrad_filter_tile_access_iterator_analytic.h
+-rw-r--r--   0 root         (0) root         (0)    19735 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/conv2d_dgrad_filter_tile_access_iterator_optimized.h
+-rw-r--r--   0 root         (0) root         (0)    18940 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/conv2d_dgrad_output_gradient_tile_access_iterator_analytic.h
+-rw-r--r--   0 root         (0) root         (0)    26137 2023-04-15 23:28:02.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/conv2d_dgrad_output_gradient_tile_access_iterator_optimized.h
+-rw-r--r--   0 root         (0) root         (0)    10953 2023-04-15 23:28:02.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/conv2d_fprop_activation_tile_access_iterator_analytic.h
+-rw-r--r--   0 root         (0) root         (0)    11529 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/conv2d_fprop_activation_tile_access_iterator_few_channels.h
+-rw-r--r--   0 root         (0) root         (0)    11333 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/conv2d_fprop_activation_tile_access_iterator_fixed_channels.h
+-rw-r--r--   0 root         (0) root         (0)    13664 2023-04-15 23:28:02.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/conv2d_fprop_activation_tile_access_iterator_optimized.h
+-rw-r--r--   0 root         (0) root         (0)    10627 2023-04-15 23:28:02.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/conv2d_fprop_filter_tile_access_iterator_analytic.h
+-rw-r--r--   0 root         (0) root         (0)     9314 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/conv2d_fprop_filter_tile_access_iterator_few_channels.h
+-rw-r--r--   0 root         (0) root         (0)     9018 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/conv2d_fprop_filter_tile_access_iterator_fixed_channels.h
+-rw-r--r--   0 root         (0) root         (0)    10387 2023-04-15 23:28:02.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/conv2d_fprop_filter_tile_access_iterator_optimized.h
+-rw-r--r--   0 root         (0) root         (0)    30197 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/conv2d_params.h
+-rw-r--r--   0 root         (0) root         (0)    11202 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/conv2d_tile_iterator.h
+-rw-r--r--   0 root         (0) root         (0)    10350 2023-04-15 23:28:02.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/conv2d_wgrad_activation_tile_access_iterator_analytic.h
+-rw-r--r--   0 root         (0) root         (0)    11520 2023-04-15 23:28:02.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/conv2d_wgrad_activation_tile_access_iterator_optimized.h
+-rw-r--r--   0 root         (0) root         (0)     9043 2023-04-15 23:28:02.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/conv2d_wgrad_output_gradient_tile_access_iterator_analytic.h
+-rw-r--r--   0 root         (0) root         (0)    10832 2023-04-15 23:28:02.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/conv2d_wgrad_output_gradient_tile_access_iterator_optimized.h
+-rw-r--r--   0 root         (0) root         (0)     8450 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/conv3d_dgrad_filter_tile_access_iterator_analytic.h
+-rw-r--r--   0 root         (0) root         (0)     9569 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/conv3d_dgrad_filter_tile_access_iterator_optimized.h
+-rw-r--r--   0 root         (0) root         (0)    11020 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/conv3d_dgrad_output_gradient_tile_access_iterator_analytic.h
+-rw-r--r--   0 root         (0) root         (0)    15014 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/conv3d_dgrad_output_gradient_tile_access_iterator_optimized.h
+-rw-r--r--   0 root         (0) root         (0)     9634 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/conv3d_fprop_activation_tile_access_iterator_analytic.h
+-rw-r--r--   0 root         (0) root         (0)    15132 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/conv3d_fprop_activation_tile_access_iterator_optimized.h
+-rw-r--r--   0 root         (0) root         (0)     7945 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/conv3d_fprop_filter_tile_access_iterator_analytic.h
+-rw-r--r--   0 root         (0) root         (0)     8891 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/conv3d_fprop_filter_tile_access_iterator_optimized.h
+-rw-r--r--   0 root         (0) root         (0)    18249 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/conv3d_params.h
+-rw-r--r--   0 root         (0) root         (0)     9971 2023-04-15 23:28:02.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/conv3d_wgrad_activation_tile_access_iterator_analytic.h
+-rw-r--r--   0 root         (0) root         (0)    12024 2023-04-15 23:28:02.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/conv3d_wgrad_activation_tile_access_iterator_optimized.h
+-rw-r--r--   0 root         (0) root         (0)     8821 2023-04-15 23:28:02.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/conv3d_wgrad_output_gradient_tile_access_iterator_analytic.h
+-rw-r--r--   0 root         (0) root         (0)    10744 2023-04-15 23:28:02.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/conv3d_wgrad_output_gradient_tile_access_iterator_optimized.h
+-rw-r--r--   0 root         (0) root         (0)     8871 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/depthwise_direct_conv_params.h
+-rw-r--r--   0 root         (0) root         (0)    10747 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/depthwise_fprop_activation_tile_access_iterator_direct_conv_fixed_stride_dilation.h
+-rw-r--r--   0 root         (0) root         (0)     9899 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/depthwise_fprop_activation_tile_access_iterator_direct_conv_optimized.h
+-rw-r--r--   0 root         (0) root         (0)    20899 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/depthwise_fprop_direct_conv_multistage.h
+-rw-r--r--   0 root         (0) root         (0)     8921 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/depthwise_fprop_filter_tile_access_iterator_direct_conv_optimized.h
+-rw-r--r--   0 root         (0) root         (0)    12744 2023-04-15 23:28:02.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/depthwise_fprop_pipelined.h
+-rw-r--r--   0 root         (0) root         (0)     8097 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/depthwise_mma_base.h
+-rw-r--r--   0 root         (0) root         (0)    36697 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/depthwise_mma_core_with_lane_access_size.h
+-rw-r--r--   0 root         (0) root         (0)    30106 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/implicit_gemm_fprop_fusion_multistage.h
+-rw-r--r--   0 root         (0) root         (0)    20086 2023-04-15 23:28:02.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/implicit_gemm_multistage.h
+-rw-r--r--   0 root         (0) root         (0)    12174 2023-04-15 23:28:02.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/implicit_gemm_pipelined.h
+-rw-r--r--   0 root         (0) root         (0)    26320 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/implicit_gemm_wgrad_fusion_multistage.h
+-rw-r--r--   0 root         (0) root         (0)    16915 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/predicated_scale_bias_vector_access_iterator.h
+-rw-r--r--   0 root         (0) root         (0)    12476 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/predicated_scale_bias_vector_iterator.h
+-rw-r--r--   0 root         (0) root         (0)     8050 2023-04-15 23:28:02.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/threadblock_swizzle.h
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-16 04:59:53.608734 flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/conv/warp/
+-rw-r--r--   0 root         (0) root         (0)    12419 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/conv/warp/mma_depthwise_simt.h
+-rw-r--r--   0 root         (0) root         (0)    30655 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/conv/warp/mma_depthwise_simt_tile_iterator.h
+-rw-r--r--   0 root         (0) root         (0)     8772 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/conv/warp/scale_bias_relu_transform.h
+-rw-r--r--   0 root         (0) root         (0)    11827 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/coord.h
+-rw-r--r--   0 root         (0) root         (0)    11077 2023-04-15 23:28:02.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/core_io.h
+-rw-r--r--   0 root         (0) root         (0)     7838 2023-04-16 04:29:29.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/cutlass.h
+-rw-r--r--   0 root         (0) root         (0)     3108 2023-04-16 04:29:29.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/device_kernel.h
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-16 04:59:39.138178 flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/epilogue/
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-16 04:59:54.452234 flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/epilogue/thread/
+-rw-r--r--   0 root         (0) root         (0)    18909 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/epilogue/thread/activation.h
+-rw-r--r--   0 root         (0) root         (0)     4691 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/epilogue/thread/conversion_op.h
+-rw-r--r--   0 root         (0) root         (0)     9349 2023-04-16 04:29:29.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/epilogue/thread/linear_combination.h
+-rw-r--r--   0 root         (0) root         (0)     8344 2023-04-16 04:29:29.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/epilogue/thread/linear_combination_bias_elementwise.h
+-rw-r--r--   0 root         (0) root         (0)    13490 2023-04-16 04:29:29.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/epilogue/thread/linear_combination_bias_relu.h
+-rw-r--r--   0 root         (0) root         (0)    23649 2023-04-15 23:28:02.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/epilogue/thread/linear_combination_clamp.h
+-rw-r--r--   0 root         (0) root         (0)     9067 2023-04-15 23:28:02.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/epilogue/thread/linear_combination_dgelu.h
+-rw-r--r--   0 root         (0) root         (0)    15195 2023-04-15 23:28:02.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/epilogue/thread/linear_combination_drelu.h
+-rw-r--r--   0 root         (0) root         (0)     3669 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/epilogue/thread/linear_combination_gelu.h
+-rw-r--r--   0 root         (0) root         (0)     8065 2023-04-15 23:28:02.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/epilogue/thread/linear_combination_generic.h
+-rw-r--r--   0 root         (0) root         (0)     3693 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/epilogue/thread/linear_combination_hardswish.h
+-rw-r--r--   0 root         (0) root         (0)     8344 2023-04-15 23:28:02.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/epilogue/thread/linear_combination_leaky_relu.h
+-rw-r--r--   0 root         (0) root         (0)     3058 2023-04-15 23:28:02.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/epilogue/thread/linear_combination_params.h
+-rw-r--r--   0 root         (0) root         (0)     9351 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/epilogue/thread/linear_combination_planar_complex.h
+-rw-r--r--   0 root         (0) root         (0)    20486 2023-04-15 23:28:02.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/epilogue/thread/linear_combination_relu.h
+-rw-r--r--   0 root         (0) root         (0)    19348 2023-04-15 23:28:02.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/epilogue/thread/linear_combination_relu0.h
+-rw-r--r--   0 root         (0) root         (0)    11855 2023-04-16 04:29:29.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/epilogue/thread/linear_combination_residual_block.h
+-rw-r--r--   0 root         (0) root         (0)     3688 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/epilogue/thread/linear_combination_sigmoid.h
+-rw-r--r--   0 root         (0) root         (0)     3669 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/epilogue/thread/linear_combination_silu.h
+-rw-r--r--   0 root         (0) root         (0)     8662 2023-04-15 23:28:02.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/epilogue/thread/linear_combination_with_elementwise.h
+-rw-r--r--   0 root         (0) root         (0)     3416 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/epilogue/thread/reduction_op.h
+-rw-r--r--   0 root         (0) root         (0)     2656 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/epilogue/thread/scale_type.h
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-16 04:59:55.918972 flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/
+-rw-r--r--   0 root         (0) root         (0)     9142 2023-04-15 23:28:02.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/default_epilogue_complex_tensor_op.h
+-rw-r--r--   0 root         (0) root         (0)     9441 2023-04-15 23:28:02.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/default_epilogue_complex_tensor_op_blas3.h
+-rw-r--r--   0 root         (0) root         (0)     3234 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/default_epilogue_direct_store.h
+-rw-r--r--   0 root         (0) root         (0)     7209 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/default_epilogue_planar_complex.h
+-rw-r--r--   0 root         (0) root         (0)    13385 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/default_epilogue_simt.h
+-rw-r--r--   0 root         (0) root         (0)    27150 2023-04-16 04:29:29.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/default_epilogue_tensor_op.h
+-rw-r--r--   0 root         (0) root         (0)     7129 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/default_epilogue_tensor_op_blas3.h
+-rw-r--r--   0 root         (0) root         (0)    10846 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/default_epilogue_volta_tensor_op.h
+-rw-r--r--   0 root         (0) root         (0)     5817 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/default_epilogue_with_broadcast.h
+-rw-r--r--   0 root         (0) root         (0)     5763 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/default_epilogue_with_reduction.h
+-rw-r--r--   0 root         (0) root         (0)     5947 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/default_epilogue_wmma_tensor_op.h
+-rw-r--r--   0 root         (0) root         (0)     4409 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/default_thread_map_simt.h
+-rw-r--r--   0 root         (0) root         (0)     7398 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/default_thread_map_tensor_op.h
+-rw-r--r--   0 root         (0) root         (0)     7303 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/default_thread_map_volta_tensor_op.h
+-rw-r--r--   0 root         (0) root         (0)     4098 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/default_thread_map_wmma_tensor_op.h
+-rw-r--r--   0 root         (0) root         (0)     4678 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/direct_store_epilogue_iterator.h
+-rw-r--r--   0 root         (0) root         (0)    19214 2023-04-16 04:29:29.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/epilogue.h
+-rw-r--r--   0 root         (0) root         (0)     8279 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/epilogue_base.h
+-rw-r--r--   0 root         (0) root         (0)     7455 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/epilogue_base_streamk.h
+-rw-r--r--   0 root         (0) root         (0)    13424 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/epilogue_depthwise.h
+-rw-r--r--   0 root         (0) root         (0)    13933 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/epilogue_direct_store.h
+-rw-r--r--   0 root         (0) root         (0)     7401 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/epilogue_gemm_k_reduction.h
+-rw-r--r--   0 root         (0) root         (0)    14610 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/epilogue_planar_complex.h
+-rw-r--r--   0 root         (0) root         (0)     9073 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/epilogue_smem_accumulator.h
+-rw-r--r--   0 root         (0) root         (0)    16804 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/epilogue_visitor_with_softmax.h
+-rw-r--r--   0 root         (0) root         (0)    52430 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/epilogue_with_broadcast.h
+-rw-r--r--   0 root         (0) root         (0)    29199 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/epilogue_with_reduction.h
+-rw-r--r--   0 root         (0) root         (0)    13454 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/epilogue_with_visitor.h
+-rw-r--r--   0 root         (0) root         (0)     7308 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/epilogue_workspace.h
+-rw-r--r--   0 root         (0) root         (0)    14359 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/interleaved_epilogue.h
+-rw-rw-r--   0 root         (0) root         (0)     2912 2022-06-02 16:47:41.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/output_iterator_parameter.h
+-rw-r--r--   0 root         (0) root         (0)    19750 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/output_tile_thread_map.h
+-rw-r--r--   0 root         (0) root         (0)    40870 2023-04-15 23:28:02.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/predicated_tile_iterator.h
+-rw-r--r--   0 root         (0) root         (0)    18821 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/predicated_tile_iterator_affine.h
+-rw-r--r--   0 root         (0) root         (0)     5636 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/predicated_tile_iterator_affine_layout_params.h
+-rw-r--r--   0 root         (0) root         (0)    21249 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/predicated_tile_iterator_blas3.h
+-rw-r--r--   0 root         (0) root         (0)    13872 2023-04-15 23:28:02.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/predicated_tile_iterator_direct_conv.h
+-rw-r--r--   0 root         (0) root         (0)    14496 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/predicated_tile_iterator_params.h
+-rw-r--r--   0 root         (0) root         (0)     9146 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/predicated_tile_iterator_predicates.h
+-rw-r--r--   0 root         (0) root         (0)    15536 2023-04-15 23:28:02.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/predicated_tile_iterator_strided_dgrad.h
+-rw-r--r--   0 root         (0) root         (0)     7487 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/shared_load_iterator.h
+-rw-r--r--   0 root         (0) root         (0)    17683 2023-04-16 04:29:29.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/shared_load_iterator_mixed.h
+-rw-r--r--   0 root         (0) root         (0)     7394 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/shared_load_iterator_pitch_liner.h
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-16 04:59:56.144837 flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/epilogue/warp/
+-rw-r--r--   0 root         (0) root         (0)     7055 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/epilogue/warp/fragment_iterator_complex_tensor_op.h
+-rw-r--r--   0 root         (0) root         (0)     7736 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/epilogue/warp/fragment_iterator_gaussian_complex_tensor_op.h
+-rw-r--r--   0 root         (0) root         (0)     5880 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/epilogue/warp/fragment_iterator_simt.h
+-rw-r--r--   0 root         (0) root         (0)     9883 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/epilogue/warp/fragment_iterator_tensor_op.h
+-rw-r--r--   0 root         (0) root         (0)     8924 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/epilogue/warp/fragment_iterator_volta_tensor_op.h
+-rw-r--r--   0 root         (0) root         (0)     6045 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/epilogue/warp/fragment_iterator_wmma_tensor_op.h
+-rw-r--r--   0 root         (0) root         (0)     4864 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/epilogue/warp/simt_policy.h
+-rw-r--r--   0 root         (0) root         (0)     5979 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/epilogue/warp/tensor_op_policy.h
+-rw-r--r--   0 root         (0) root         (0)    25658 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/epilogue/warp/tile_iterator_simt.h
+-rw-r--r--   0 root         (0) root         (0)    20290 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/epilogue/warp/tile_iterator_tensor_op.h
+-rw-r--r--   0 root         (0) root         (0)    22857 2023-04-16 04:29:29.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/epilogue/warp/tile_iterator_tensor_op_mixed.h
+-rw-r--r--   0 root         (0) root         (0)    14258 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/epilogue/warp/tile_iterator_volta_tensor_op.h
+-rw-r--r--   0 root         (0) root         (0)     7704 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/epilogue/warp/tile_iterator_wmma_tensor_op.h
+-rw-r--r--   0 root         (0) root         (0)     7485 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/epilogue/warp/volta_tensor_op_policy.h
+-rw-r--r--   0 root         (0) root         (0)     3916 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/epilogue/warp/wmma_tensor_op_policy.h
+-rw-r--r--   0 root         (0) root         (0)    26026 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/fast_math.h
+-rw-r--r--   0 root         (0) root         (0)    35325 2023-04-16 04:29:29.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/float8.h
+-rw-r--r--   0 root         (0) root         (0)     2645 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/floating_point_nvrtc.h
+-rw-r--r--   0 root         (0) root         (0)    11242 2023-04-16 04:29:29.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/functional.h
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-16 04:59:56.162059 flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/gemm/
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-16 04:59:56.801915 flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/gemm/device/
+-rw-r--r--   0 root         (0) root         (0)    17023 2023-04-16 04:29:29.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/gemm/device/base_grouped.h
+-rw-r--r--   0 root         (0) root         (0)    24413 2023-04-15 23:28:02.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/gemm/device/default_gemm_configuration.h
+-rw-r--r--   0 root         (0) root         (0)    27616 2023-04-15 23:28:02.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/gemm/device/ell_gemm.h
+-rw-r--r--   0 root         (0) root         (0)    25202 2023-04-15 23:28:02.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/gemm/device/gemm.h
+-rw-r--r--   0 root         (0) root         (0)    22367 2023-04-15 23:28:02.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/gemm/device/gemm_array.h
+-rw-r--r--   0 root         (0) root         (0)    22375 2023-04-15 23:28:02.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/gemm/device/gemm_batched.h
+-rw-r--r--   0 root         (0) root         (0)    22725 2023-04-15 23:28:02.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/gemm/device/gemm_complex.h
+-rw-r--r--   0 root         (0) root         (0)     2591 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/gemm/device/gemm_grouped.h
+-rw-r--r--   0 root         (0) root         (0)    13736 2023-04-15 23:28:02.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/gemm/device/gemm_layernorm_mainloop_fusion.h
+-rw-r--r--   0 root         (0) root         (0)    17329 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/gemm/device/gemm_sparse.h
+-rw-r--r--   0 root         (0) root         (0)    20450 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/gemm/device/gemm_splitk_parallel.h
+-rw-r--r--   0 root         (0) root         (0)    14902 2023-04-15 23:28:02.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/gemm/device/gemm_universal.h
+-rw-r--r--   0 root         (0) root         (0)     7444 2023-04-16 04:29:29.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/gemm/device/gemm_universal_adapter.h
+-rw-r--r--   0 root         (0) root         (0)    13352 2023-04-16 04:29:29.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/gemm/device/gemm_universal_base.h
+-rw-r--r--   0 root         (0) root         (0)    13968 2023-04-15 23:28:02.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/gemm/device/gemm_universal_with_broadcast.h
+-rw-r--r--   0 root         (0) root         (0)    14853 2023-04-15 23:28:02.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/gemm/device/gemm_with_k_reduction.h
+-rw-r--r--   0 root         (0) root         (0)     5690 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/gemm/device/gemv.h
+-rw-r--r--   0 root         (0) root         (0)    18127 2023-04-15 23:28:02.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/gemm/device/rank_2k.h
+-rw-r--r--   0 root         (0) root         (0)     2747 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/gemm/device/rank_2k_grouped.h
+-rw-r--r--   0 root         (0) root         (0)    16719 2023-04-15 23:28:02.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/gemm/device/rank_k.h
+-rwxr-xr-x   0 root         (0) root         (0)    21050 2023-04-15 23:28:02.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/gemm/device/symm.h
+-rw-r--r--   0 root         (0) root         (0)    26464 2023-04-15 23:28:02.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/gemm/device/trmm.h
+-rw-r--r--   0 root         (0) root         (0)    11570 2023-04-16 04:29:29.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/gemm/gemm.h
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-16 04:59:58.166301 flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/
+-rw-r--r--   0 root         (0) root         (0)    29360 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/default_ell_gemm.h
+-rw-r--r--   0 root         (0) root         (0)    37752 2023-04-16 04:29:29.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/default_gemm.h
+-rw-r--r--   0 root         (0) root         (0)    16130 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/default_gemm_complex.h
+-rw-r--r--   0 root         (0) root         (0)    12385 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/default_gemm_grouped.h
+-rw-r--r--   0 root         (0) root         (0)     6592 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/default_gemm_grouped_softmax_mainloop_fusion.h
+-rw-r--r--   0 root         (0) root         (0)     5848 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/default_gemm_layernorm_mainloop_fusion.h
+-rw-r--r--   0 root         (0) root         (0)    11104 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/default_gemm_planar_complex_universal.h
+-rw-r--r--   0 root         (0) root         (0)     7983 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/default_gemm_sparse.h
+-rw-r--r--   0 root         (0) root         (0)     4932 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/default_gemm_splitk_parallel.h
+-rw-r--r--   0 root         (0) root         (0)    11951 2023-04-15 23:28:02.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/default_gemm_universal.h
+-rw-r--r--   0 root         (0) root         (0)     8063 2023-04-16 04:29:29.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/default_gemm_with_broadcast.h
+-rw-r--r--   0 root         (0) root         (0)     6457 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/default_gemm_with_k_reduction.h
+-rw-r--r--   0 root         (0) root         (0)     8086 2023-04-15 23:28:02.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/default_gemm_with_reduction.h
+-rwxr-xr-x   0 root         (0) root         (0)     5349 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/default_gemv.h
+-rw-r--r--   0 root         (0) root         (0)    11560 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/default_rank_2k.h
+-rw-r--r--   0 root         (0) root         (0)    20509 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/default_rank_2k_complex.h
+-rw-r--r--   0 root         (0) root         (0)    12470 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/default_rank_2k_grouped.h
+-rw-r--r--   0 root         (0) root         (0)    10620 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/default_rank_2k_universal.h
+-rw-r--r--   0 root         (0) root         (0)     9872 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/default_rank_k.h
+-rw-r--r--   0 root         (0) root         (0)    16990 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/default_rank_k_complex.h
+-rw-r--r--   0 root         (0) root         (0)     9444 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/default_rank_k_universal.h
+-rwxr-xr-x   0 root         (0) root         (0)    13375 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/default_symm.h
+-rwxr-xr-x   0 root         (0) root         (0)    21830 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/default_symm_complex.h
+-rwxr-xr-x   0 root         (0) root         (0)    10315 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/default_symm_universal.h
+-rw-r--r--   0 root         (0) root         (0)    10873 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/default_trmm.h
+-rw-r--r--   0 root         (0) root         (0)    10730 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/default_trmm_complex.h
+-rw-r--r--   0 root         (0) root         (0)    10850 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/default_trmm_universal.h
+-rw-r--r--   0 root         (0) root         (0)    28916 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/ell_gemm.h
+-rw-r--r--   0 root         (0) root         (0)    13381 2023-04-16 04:29:29.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/gemm.h
+-rw-r--r--   0 root         (0) root         (0)     8717 2023-04-16 04:29:29.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/gemm_array.h
+-rw-r--r--   0 root         (0) root         (0)     8785 2023-04-16 04:29:29.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/gemm_batched.h
+-rw-r--r--   0 root         (0) root         (0)    14711 2023-04-16 04:29:29.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/gemm_grouped.h
+-rw-r--r--   0 root         (0) root         (0)     4691 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/gemm_grouped_problem_visitor.h
+-rw-r--r--   0 root         (0) root         (0)    15623 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/gemm_grouped_softmax_mainloop_fusion.h
+-rw-r--r--   0 root         (0) root         (0)    27281 2023-04-15 23:28:02.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/gemm_layernorm_mainloop_fusion.h
+-rwxr-xr-x   0 root         (0) root         (0)     6144 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/gemm_params.h
+-rw-r--r--   0 root         (0) root         (0)     5165 2023-04-16 04:29:29.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/gemm_pipelined.h
+-rw-r--r--   0 root         (0) root         (0)    22973 2023-04-16 04:29:29.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/gemm_planar_complex.h
+-rw-r--r--   0 root         (0) root         (0)    18961 2023-04-16 04:29:29.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/gemm_planar_complex_array.h
+-rw-r--r--   0 root         (0) root         (0)     8142 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/gemm_splitk_parallel.h
+-rw-r--r--   0 root         (0) root         (0)     4291 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/gemm_transpose_operands.h
+-rw-r--r--   0 root         (0) root         (0)    22913 2023-04-16 04:29:29.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/gemm_universal.h
+-rw-r--r--   0 root         (0) root         (0)    41469 2023-04-16 04:29:29.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/gemm_universal_streamk.h
+-rw-r--r--   0 root         (0) root         (0)    47222 2023-04-16 04:29:29.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/gemm_with_fused_epilogue.h
+-rw-r--r--   0 root         (0) root         (0)    23629 2023-04-16 04:29:29.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/gemm_with_k_reduction.h
+-rw-r--r--   0 root         (0) root         (0)     8090 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/gemv.h
+-rwxr-xr-x   0 root         (0) root         (0)     8979 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/gemv_batched_strided.h
+-rw-r--r--   0 root         (0) root         (0)    16849 2023-04-15 23:28:02.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/grouped_problem_visitor.h
+-rw-r--r--   0 root         (0) root         (0)     7148 2023-04-16 04:29:29.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/params_universal_base.h
+-rw-r--r--   0 root         (0) root         (0)    22962 2023-04-16 04:29:29.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/rank_2k_grouped.h
+-rw-r--r--   0 root         (0) root         (0)    16100 2023-04-15 15:41:02.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/rank_2k_grouped_problem_visitor.h
+-rw-r--r--   0 root         (0) root         (0)     4334 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/rank_2k_transpose_operands.h
+-rw-r--r--   0 root         (0) root         (0)    24162 2023-04-16 04:29:29.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/rank_2k_universal.h
+-rw-r--r--   0 root         (0) root         (0)    17567 2023-04-16 04:29:29.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/rank_k_universal.h
+-rw-r--r--   0 root         (0) root         (0)    13610 2023-04-16 04:29:29.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/sparse_gemm.h
+-rwxr-xr-x   0 root         (0) root         (0)    23900 2023-04-16 04:29:29.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/symm_universal.h
+-rw-r--r--   0 root         (0) root         (0)    19537 2023-04-16 04:29:29.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/trmm_universal.h
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-16 04:59:58.231487 flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/gemm/thread/
+-rw-r--r--   0 root         (0) root         (0)     3567 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/gemm/thread/mma.h
+-rw-r--r--   0 root         (0) root         (0)    15373 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/gemm/thread/mma_sm50.h
+-rw-r--r--   0 root         (0) root         (0)    29987 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/gemm/thread/mma_sm60.h
+-rw-r--r--   0 root         (0) root         (0)     8142 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/gemm/thread/mma_sm61.h
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-16 04:59:59.686714 flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/
+-rw-r--r--   0 root         (0) root         (0)    31930 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/default_ell_mma.h
+-rwxr-xr-x   0 root         (0) root         (0)     6979 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/default_gemv_core.h
+-rw-r--r--   0 root         (0) root         (0)    34241 2023-04-15 23:28:02.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/default_mma.h
+-rw-r--r--   0 root         (0) root         (0)     5123 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/default_mma_core.h
+-rw-r--r--   0 root         (0) root         (0)    57426 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/default_mma_core_simt.h
+-rw-r--r--   0 root         (0) root         (0)    19257 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/default_mma_core_sm70.h
+-rw-r--r--   0 root         (0) root         (0)    42310 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/default_mma_core_sm75.h
+-rw-r--r--   0 root         (0) root         (0)   103000 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/default_mma_core_sm80.h
+-rw-r--r--   0 root         (0) root         (0)    32106 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/default_mma_core_sparse_sm80.h
+-rw-r--r--   0 root         (0) root         (0)    12645 2023-04-15 23:28:02.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/default_mma_core_with_access_size.h
+-rw-r--r--   0 root         (0) root         (0)     7387 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/default_mma_core_with_reduction.h
+-rw-r--r--   0 root         (0) root         (0)    20975 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/default_mma_core_wmma.h
+-rw-r--r--   0 root         (0) root         (0)     7998 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/default_mma_layernorm_mainloop_fusion.h
+-rw-r--r--   0 root         (0) root         (0)     5110 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/default_mma_planar_complex_multistage.h
+-rw-r--r--   0 root         (0) root         (0)     4627 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/default_mma_planar_complex_pipelined.h
+-rw-r--r--   0 root         (0) root         (0)     7113 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/default_mma_softmax_mainloop_fusion.h
+-rw-r--r--   0 root         (0) root         (0)     6323 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/default_mma_with_reduction.h
+-rw-r--r--   0 root         (0) root         (0)     7121 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/default_multistage_mma_complex.h
+-rw-r--r--   0 root         (0) root         (0)     4959 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/default_multistage_mma_complex_core.h
+-rw-r--r--   0 root         (0) root         (0)    65201 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/default_multistage_mma_complex_core_sm80.h
+-rw-r--r--   0 root         (0) root         (0)    25495 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/default_multistage_trmm_complex.h
+-rw-r--r--   0 root         (0) root         (0)     8509 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/default_sparse_mma.h
+-rw-r--r--   0 root         (0) root         (0)    19515 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/default_trmm.h
+-rw-r--r--   0 root         (0) root         (0)    24047 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/ell_mma_multistage.h
+-rw-r--r--   0 root         (0) root         (0)    13836 2023-04-15 23:28:02.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/ell_mma_pipelined.h
+-rwxr-xr-x   0 root         (0) root         (0)     4726 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/gemv.h
+-rw-r--r--   0 root         (0) root         (0)     3652 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/index_remat.h
+-rw-r--r--   0 root         (0) root         (0)     7823 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/mma_base.h
+-rw-r--r--   0 root         (0) root         (0)    27415 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/mma_blas3_multistage.h
+-rw-r--r--   0 root         (0) root         (0)    32894 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/mma_layernorm_mainloop_fusion_multistage.h
+-rw-r--r--   0 root         (0) root         (0)    28015 2023-04-15 23:28:02.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/mma_multistage.h
+-rw-r--r--   0 root         (0) root         (0)    15995 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/mma_pipelined.h
+-rw-r--r--   0 root         (0) root         (0)     6901 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/mma_planar_complex_base.h
+-rw-r--r--   0 root         (0) root         (0)    22653 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/mma_planar_complex_multistage.h
+-rw-r--r--   0 root         (0) root         (0)    14746 2023-04-15 23:28:02.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/mma_planar_complex_pipelined.h
+-rw-r--r--   0 root         (0) root         (0)     9864 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/mma_singlestage.h
+-rw-r--r--   0 root         (0) root         (0)    27061 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/mma_softmax_mainloop_fusion_multistage.h
+-rw-r--r--   0 root         (0) root         (0)     9210 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/mma_sparse_base.h
+-rw-r--r--   0 root         (0) root         (0)    25333 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/mma_sparse_multistage.h
+-rw-r--r--   0 root         (0) root         (0)    20473 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/mma_with_reduction_multistage.h
+-rw-r--r--   0 root         (0) root         (0)    15007 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/threadblock_swizzle.h
+-rw-r--r--   0 root         (0) root         (0)    26621 2023-04-16 04:29:29.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/threadblock_swizzle_streamk.h
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-16 05:00:00.911400 flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/gemm/warp/
+-rw-r--r--   0 root         (0) root         (0)    20553 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/gemm/warp/default_mma_complex_tensor_op.h
+-rw-r--r--   0 root         (0) root         (0)     6684 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/gemm/warp/default_mma_sparse_tensor_op.h
+-rw-r--r--   0 root         (0) root         (0)     5160 2023-04-15 23:28:02.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/gemm/warp/default_mma_tensor_op.h
+-rw-r--r--   0 root         (0) root         (0)     9026 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/gemm/warp/default_mma_tensor_op_sm80.h
+-rw-r--r--   0 root         (0) root         (0)     4053 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/gemm/warp/default_mma_with_reduction_tensor_op.h
+-rw-r--r--   0 root         (0) root         (0)     4685 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/gemm/warp/default_mma_wmma_tensor_op.h
+-rw-r--r--   0 root         (0) root         (0)     5725 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/gemm/warp/layernorm_scale_bias_transform.h
+-rw-r--r--   0 root         (0) root         (0)     2619 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/gemm/warp/mma.h
+-rw-r--r--   0 root         (0) root         (0)    37705 2023-04-15 23:28:02.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/gemm/warp/mma_complex_tensor_op.h
+-rw-r--r--   0 root         (0) root         (0)    23132 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/gemm/warp/mma_complex_tensor_op_fast_f32.h
+-rw-r--r--   0 root         (0) root         (0)    78615 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/gemm/warp/mma_complex_tensor_op_tile_iterator_sm80.h
+-rw-r--r--   0 root         (0) root         (0)    21205 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/gemm/warp/mma_gaussian_complex_tensor_op.h
+-rw-r--r--   0 root         (0) root         (0)    14589 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/gemm/warp/mma_gaussian_complex_tensor_op_tile_iterator_sm80.h
+-rw-r--r--   0 root         (0) root         (0)     6144 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/gemm/warp/mma_planar_complex.h
+-rw-r--r--   0 root         (0) root         (0)     8446 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/gemm/warp/mma_simt.h
+-rw-r--r--   0 root         (0) root         (0)     3079 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/gemm/warp/mma_simt_policy.h
+-rw-r--r--   0 root         (0) root         (0)    59793 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/gemm/warp/mma_simt_tile_iterator.h
+-rw-r--r--   0 root         (0) root         (0)    11758 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/gemm/warp/mma_sparse_tensor_op.h
+-rw-r--r--   0 root         (0) root         (0)    14407 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/gemm/warp/mma_tensor_op.h
+-rw-r--r--   0 root         (0) root         (0)    15721 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/gemm/warp/mma_tensor_op_fast_f32.h
+-rw-rw-r--   0 root         (0) root         (0)    18643 2022-06-02 16:47:41.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/gemm/warp/mma_tensor_op_fragment_iterator.h
+-rw-r--r--   0 root         (0) root         (0)     2939 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/gemm/warp/mma_tensor_op_policy.h
+-rw-r--r--   0 root         (0) root         (0)     8966 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/gemm/warp/mma_tensor_op_sm70.h
+-rw-r--r--   0 root         (0) root         (0)    11017 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/gemm/warp/mma_tensor_op_tile_access_iterator.h
+-rw-r--r--   0 root         (0) root         (0)   136033 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/gemm/warp/mma_tensor_op_tile_iterator.h
+-rw-r--r--   0 root         (0) root         (0)    99649 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/gemm/warp/mma_tensor_op_tile_iterator_sm70.h
+-rw-r--r--   0 root         (0) root         (0)    75179 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/gemm/warp/mma_tensor_op_tile_iterator_sm80.h
+-rw-r--r--   0 root         (0) root         (0)    13151 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/gemm/warp/mma_tensor_op_tile_iterator_sparse.h
+-rw-r--r--   0 root         (0) root         (0)    27101 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/gemm/warp/mma_tensor_op_tile_iterator_wmma.h
+-rw-r--r--   0 root         (0) root         (0)     7241 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/gemm/warp/mma_tensor_op_wmma.h
+-rw-r--r--   0 root         (0) root         (0)    17271 2023-04-16 04:29:29.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/gemm/warp/mma_with_reduction_tensor_op.h
+-rw-r--r--   0 root         (0) root         (0)    19125 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/gemm/warp/scale_bias_tile_iterator.h
+-rw-r--r--   0 root         (0) root         (0)     4610 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/gemm/warp/softmax_scale_bias_transform.h
+-rw-r--r--   0 root         (0) root         (0)     8728 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/gemm/warp/tile_iterator_planar_complex.h
+-rw-r--r--   0 root         (0) root         (0)    23615 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/half.h
+-rw-r--r--   0 root         (0) root         (0)     6893 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/integer_subbyte.h
+-rw-r--r--   0 root         (0) root         (0)     2801 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/kernel_launch.h
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-16 05:00:01.149922 flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/layout/
+-rw-r--r--   0 root         (0) root         (0)     3020 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/layout/layout.h
+-rw-r--r--   0 root         (0) root         (0)    34712 2023-04-16 04:29:29.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/layout/matrix.h
+-rw-r--r--   0 root         (0) root         (0)     9133 2023-04-15 23:28:02.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/layout/permute.h
+-rw-r--r--   0 root         (0) root         (0)     4696 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/layout/pitch_linear.h
+-rw-r--r--   0 root         (0) root         (0)    18295 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/layout/tensor.h
+-rw-r--r--   0 root         (0) root         (0)    29599 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/layout/tensor_op_multiplicand_sm70.h
+-rw-r--r--   0 root         (0) root         (0)    33137 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/layout/tensor_op_multiplicand_sm75.h
+-rw-r--r--   0 root         (0) root         (0)    29336 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/layout/tensor_op_multiplicand_sm80.h
+-rw-r--r--   0 root         (0) root         (0)     3328 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/layout/vector.h
+-rw-r--r--   0 root         (0) root         (0)   364115 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/matrix.h
+-rw-r--r--   0 root         (0) root         (0)     4991 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/matrix_coord.h
+-rw-r--r--   0 root         (0) root         (0)     2726 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/matrix_shape.h
+-rw-r--r--   0 root         (0) root         (0)    71278 2023-04-15 23:28:02.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/numeric_conversion.h
+-rw-r--r--   0 root         (0) root         (0)     3505 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/numeric_types.h
+-rw-r--r--   0 root         (0) root         (0)     5492 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/pitch_linear_coord.h
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-16 05:00:01.171053 flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/platform/
+-rw-r--r--   0 root         (0) root         (0)    26097 2023-04-15 23:28:02.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/platform/platform.h
+-rw-r--r--   0 root         (0) root         (0)    15565 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/predicate_vector.h
+-rw-r--r--   0 root         (0) root         (0)    20901 2023-04-16 04:29:29.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/quaternion.h
+-rw-r--r--   0 root         (0) root         (0)     2369 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/real.h
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-16 05:00:01.189336 flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/reduction/
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-16 05:00:01.370103 flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/reduction/device/
+-rw-r--r--   0 root         (0) root         (0)     6823 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/reduction/device/reduce_split_k.h
+-rw-r--r--   0 root         (0) root         (0)     8152 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/reduction/device/tensor_reduce.h
+-rw-r--r--   0 root         (0) root         (0)    11579 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/reduction/device/tensor_reduce_affine_contiguous.h
+-rw-r--r--   0 root         (0) root         (0)    11448 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/reduction/device/tensor_reduce_affine_strided.h
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-16 05:00:01.549863 flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/reduction/kernel/
+-rw-r--r--   0 root         (0) root         (0)     8762 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/reduction/kernel/reduce_softmax_final.h
+-rw-r--r--   0 root         (0) root         (0)     7897 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/reduction/kernel/reduce_split_k.h
+-rw-r--r--   0 root         (0) root         (0)    20685 2023-04-15 23:28:02.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/reduction/kernel/tensor_reduce_affine_contiguous.h
+-rw-r--r--   0 root         (0) root         (0)    21662 2023-04-15 23:28:02.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/reduction/kernel/tensor_reduce_affine_strided.h
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-16 05:00:01.683215 flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/reduction/thread/
+-rw-r--r--   0 root         (0) root         (0)     7208 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/reduction/thread/reduce.h
+-rw-r--r--   0 root         (0) root         (0)     6790 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/reduction/thread/reduction_operators.h
+-rw-r--r--   0 root         (0) root         (0)     2936 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/reduction/threadblock_swizzle.h
+-rw-r--r--   0 root         (0) root         (0)     5929 2023-04-15 23:28:02.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/relatively_equal.h
+-rw-r--r--   0 root         (0) root         (0)     4186 2023-04-15 23:28:02.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/semaphore.h
+-rw-r--r--   0 root         (0) root         (0)    17243 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/subbyte_reference.h
+-rw-r--r--   0 root         (0) root         (0)     8964 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/tensor_coord.h
+-rw-r--r--   0 root         (0) root         (0)    12207 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/tensor_ref.h
+-rw-r--r--   0 root         (0) root         (0)    11201 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/tensor_ref_planar_complex.h
+-rw-r--r--   0 root         (0) root         (0)     9509 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/tensor_view.h
+-rw-r--r--   0 root         (0) root         (0)    10250 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/tensor_view_planar_complex.h
+-rw-r--r--   0 root         (0) root         (0)    13017 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/tfloat32.h
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-16 05:00:01.803274 flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/thread/
+-rw-r--r--   0 root         (0) root         (0)     5931 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/thread/matrix.h
+-rw-r--r--   0 root         (0) root         (0)     2581 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/trace.h
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-16 05:00:01.820973 flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/transform/
+-rw-r--r--   0 root         (0) root         (0)    33392 2023-04-16 04:29:29.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/transform/pitch_linear_thread_map.h
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-16 05:00:01.953895 flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/transform/thread/
+-rw-r--r--   0 root         (0) root         (0)     3835 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/transform/thread/transpose.h
+-rw-r--r--   0 root         (0) root         (0)     4309 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/transform/thread/unary_op.h
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-16 05:00:03.143629 flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/transform/threadblock/
+-rw-r--r--   0 root         (0) root         (0)     6181 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/transform/threadblock/ell_iterator.h
+-rw-r--r--   0 root         (0) root         (0)    44443 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/transform/threadblock/ell_predicated_tile_access_iterator.h
+-rw-r--r--   0 root         (0) root         (0)    44309 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/transform/threadblock/ell_predicated_tile_iterator.h
+-rw-r--r--   0 root         (0) root         (0)    12890 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/transform/threadblock/predicated_scale_bias_vector_access_iterator.h
+-rw-r--r--   0 root         (0) root         (0)    11097 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/transform/threadblock/predicated_scale_bias_vector_iterator.h
+-rw-r--r--   0 root         (0) root         (0)    70684 2023-04-15 23:28:02.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/transform/threadblock/predicated_tile_access_iterator.h
+-rw-r--r--   0 root         (0) root         (0)    28232 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/transform/threadblock/predicated_tile_access_iterator_2dthreadtile.h
+-rwxr-xr-x   0 root         (0) root         (0)    10243 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/transform/threadblock/predicated_tile_access_iterator_params.h
+-rw-r--r--   0 root         (0) root         (0)    31412 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/transform/threadblock/predicated_tile_access_iterator_triangular_matrix.h
+-rw-r--r--   0 root         (0) root         (0)    62672 2023-04-15 23:28:02.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/transform/threadblock/predicated_tile_iterator.h
+-rw-r--r--   0 root         (0) root         (0)    27175 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/transform/threadblock/predicated_tile_iterator_2dthreadtile.h
+-rw-r--r--   0 root         (0) root         (0)    28064 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/transform/threadblock/predicated_tile_iterator_triangular_matrix.h
+-rw-r--r--   0 root         (0) root         (0)    13088 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/transform/threadblock/predicated_vector_access_iterator.h
+-rw-r--r--   0 root         (0) root         (0)     8232 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/transform/threadblock/regular_scale_bias_vector_access_iterator.h
+-rw-r--r--   0 root         (0) root         (0)     2638 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/transform/threadblock/regular_tile_access_iterator.h
+-rw-r--r--   0 root         (0) root         (0)    13283 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/transform/threadblock/regular_tile_access_iterator_pitch_linear.h
+-rw-r--r--   0 root         (0) root         (0)    18623 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/transform/threadblock/regular_tile_access_iterator_pitch_linear_direct_conv.h
+-rw-r--r--   0 root         (0) root         (0)    27922 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/transform/threadblock/regular_tile_access_iterator_tensor_op.h
+-rw-r--r--   0 root         (0) root         (0)    47789 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/transform/threadblock/regular_tile_access_iterator_tensor_op_sm80.h
+-rw-r--r--   0 root         (0) root         (0)     2616 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/transform/threadblock/regular_tile_iterator.h
+-rw-r--r--   0 root         (0) root         (0)    16510 2023-04-15 23:28:02.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/transform/threadblock/regular_tile_iterator_pitch_linear.h
+-rw-r--r--   0 root         (0) root         (0)    15486 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/transform/threadblock/regular_tile_iterator_pitch_linear_2dthreadtile.h
+-rw-r--r--   0 root         (0) root         (0)    36050 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/transform/threadblock/regular_tile_iterator_tensor_op.h
+-rw-r--r--   0 root         (0) root         (0)    43663 2023-04-15 23:28:02.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/transform/threadblock/regular_tile_iterator_tensor_op_sm70.h
+-rw-r--r--   0 root         (0) root         (0)     5226 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/transform/threadblock/vector_iterator.h
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-16 05:00:03.164260 flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/transform/warp/
+-rw-r--r--   0 root         (0) root         (0)     8828 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/transform/warp/vector_fragment_iterator.h
+-rw-r--r--   0 root         (0) root         (0)     8166 2023-04-16 04:29:29.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/uint128.h
+-rw-r--r--   0 root         (0) root         (0)     3359 2023-04-16 04:29:29.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/wmma_array.h
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-16 04:59:39.239497 flash_attn-1.0.2/csrc/flash_attn/cutlass/python/
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-16 04:59:39.245220 flash_attn-1.0.2/csrc/flash_attn/cutlass/python/cutlass/
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-16 05:00:03.221266 flash_attn-1.0.2/csrc/flash_attn/cutlass/python/cutlass/cpp/
+-rw-r--r--   0 root         (0) root         (0)     2788 2023-04-15 18:47:44.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/python/cutlass/cpp/compiler.h
+-rw-r--r--   0 root         (0) root         (0)     6233 2023-04-15 18:47:44.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/python/cutlass/cpp/cutlass_bindings.cpp
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-16 05:00:03.395130 flash_attn-1.0.2/csrc/flash_attn/cutlass/python/cutlass/cpp/include/
+-rw-r--r--   0 root         (0) root         (0)     2854 2023-04-15 18:47:44.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/python/cutlass/cpp/include/arch.h
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-16 05:00:03.541925 flash_attn-1.0.2/csrc/flash_attn/cutlass/python/cutlass/cpp/include/conv/
+-rw-r--r--   0 root         (0) root         (0)     5897 2023-04-15 18:47:44.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/python/cutlass/cpp/include/conv/conv_problem_size.h
+-rw-r--r--   0 root         (0) root         (0)     4763 2023-04-15 18:47:44.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/python/cutlass/cpp/include/conv/convolution.h
+-rw-r--r--   0 root         (0) root         (0)     2650 2023-04-15 18:47:44.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/python/cutlass/cpp/include/conv/host.h
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-16 05:00:03.574748 flash_attn-1.0.2/csrc/flash_attn/cutlass/python/cutlass/cpp/include/epilogue/
+-rw-r--r--   0 root         (0) root         (0)     6845 2023-04-15 18:47:44.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/python/cutlass/cpp/include/epilogue/epilogue_visitor_generic.h
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-16 05:00:03.769082 flash_attn-1.0.2/csrc/flash_attn/cutlass/python/cutlass/cpp/include/epilogue/epilogue_visitor_op/
+-rw-r--r--   0 root         (0) root         (0)     3003 2023-04-15 18:47:44.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/python/cutlass/cpp/include/epilogue/epilogue_visitor_op/binary_ops.h
+-rw-r--r--   0 root         (0) root         (0)     6103 2023-04-15 18:47:44.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/python/cutlass/cpp/include/epilogue/epilogue_visitor_op/unary_ops.h
+-rw-r--r--   0 root         (0) root         (0)     4698 2023-04-15 18:47:44.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/python/cutlass/cpp/include/epilogue/epilogue_visitor_op/visitor_op_accumulator.h
+-rw-r--r--   0 root         (0) root         (0)     8397 2023-04-15 18:47:44.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/python/cutlass/cpp/include/epilogue/epilogue_visitor_op/visitor_op_binary.h
+-rw-r--r--   0 root         (0) root         (0)     8830 2023-04-15 18:47:44.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/python/cutlass/cpp/include/epilogue/epilogue_visitor_op/visitor_op_column_broadcast.h
+-rw-r--r--   0 root         (0) root         (0)    12998 2023-04-15 18:47:44.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/python/cutlass/cpp/include/epilogue/epilogue_visitor_op/visitor_op_column_reduction.h
+-rw-r--r--   0 root         (0) root         (0)     9454 2023-04-15 18:47:44.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/python/cutlass/cpp/include/epilogue/epilogue_visitor_op/visitor_op_linear_combination.h
+-rw-r--r--   0 root         (0) root         (0)     9085 2023-04-15 18:47:44.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/python/cutlass/cpp/include/epilogue/epilogue_visitor_op/visitor_op_row_broadcast.h
+-rw-r--r--   0 root         (0) root         (0)    11975 2023-04-15 18:47:44.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/python/cutlass/cpp/include/epilogue/epilogue_visitor_op/visitor_op_row_reduction.h
+-rw-r--r--   0 root         (0) root         (0)     6177 2023-04-15 18:47:44.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/python/cutlass/cpp/include/epilogue/epilogue_visitor_op/visitor_op_tensor_input.h
+-rw-r--r--   0 root         (0) root         (0)     8017 2023-04-15 18:47:44.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/python/cutlass/cpp/include/epilogue/epilogue_visitor_op/visitor_op_tensor_output.h
+-rw-r--r--   0 root         (0) root         (0)     7201 2023-04-15 18:47:44.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/python/cutlass/cpp/include/epilogue/epilogue_visitor_op/visitor_op_unary.h
+-rw-r--r--   0 root         (0) root         (0)    16970 2023-04-15 18:47:44.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/python/cutlass/cpp/include/epilogue/epilogue_visitor_with_layernorm.h
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-16 05:00:03.818946 flash_attn-1.0.2/csrc/flash_attn/cutlass/python/cutlass/cpp/include/gemm/
+-rw-r--r--   0 root         (0) root         (0)     3674 2023-04-15 18:47:44.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/python/cutlass/cpp/include/gemm/gemm.h
+-rw-r--r--   0 root         (0) root         (0)    21710 2023-04-15 18:47:44.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/python/cutlass/cpp/include/gemm/gemm_universal_with_visitor.h
+-rw-r--r--   0 root         (0) root         (0)     2328 2023-04-15 18:47:44.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/python/cutlass/cpp/include/gemm/host.h
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-16 05:00:03.868667 flash_attn-1.0.2/csrc/flash_attn/cutlass/python/cutlass/cpp/include/layout/
+-rw-r--r--   0 root         (0) root         (0)     2115 2023-04-15 18:47:44.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/python/cutlass/cpp/include/layout/layout.h
+-rw-r--r--   0 root         (0) root         (0)     4337 2023-04-15 18:47:44.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/python/cutlass/cpp/include/layout/matrix.h
+-rw-r--r--   0 root         (0) root         (0)     3694 2023-04-15 18:47:44.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/python/cutlass/cpp/include/layout/tensor.h
+-rw-r--r--   0 root         (0) root         (0)     9039 2023-04-15 18:47:44.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/python/cutlass/cpp/include/swizzling.h
+-rw-r--r--   0 root         (0) root         (0)     3902 2023-04-15 18:47:44.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/python/cutlass/cpp/include/tensor_coord.h
+-rw-r--r--   0 root         (0) root         (0)     5543 2023-04-15 18:47:44.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/python/cutlass/cpp/include/tensor_ref_view.h
+-rw-r--r--   0 root         (0) root         (0)     4855 2023-04-15 18:47:44.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/python/cutlass/cpp/include/types.h
+-rw-r--r--   0 root         (0) root         (0)      811 2023-04-15 18:47:44.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/python/cutlass/cpp/library.h
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-16 04:59:39.291649 flash_attn-1.0.2/csrc/flash_attn/cutlass/python/cutlass/cpp/test/
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-16 05:00:03.922147 flash_attn-1.0.2/csrc/flash_attn/cutlass/python/cutlass/cpp/test/conv/
+-rw-r--r--   0 root         (0) root         (0)     2651 2023-04-15 18:47:44.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/python/cutlass/cpp/test/conv/conv_problems.h
+-rw-r--r--   0 root         (0) root         (0)     2253 2023-04-15 18:47:44.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/python/cutlass/cpp/test/conv/convolution.h
+-rw-r--r--   0 root         (0) root         (0)     8786 2023-04-15 18:47:44.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/python/cutlass/cpp/test/conv/host.h
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-16 05:00:03.956530 flash_attn-1.0.2/csrc/flash_attn/cutlass/python/cutlass/cpp/test/gemm/
+-rw-r--r--   0 root         (0) root         (0)     2139 2023-04-15 18:47:44.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/python/cutlass/cpp/test/gemm/gemm.h
+-rw-r--r--   0 root         (0) root         (0)    18930 2023-04-15 18:47:44.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/python/cutlass/cpp/test/gemm/host.h
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-16 04:59:39.307900 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-16 05:00:03.977877 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-16 05:00:03.997565 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/cluster_launch/
+-rw-r--r--   0 root         (0) root         (0)    12088 2023-04-15 18:47:44.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/cluster_launch/cluster_launch.cu
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-16 05:00:04.131140 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/common/
+-rw-r--r--   0 root         (0) root         (0)     4273 2023-04-16 04:29:29.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/common/cutlass_unit_test.h
+-rw-r--r--   0 root         (0) root         (0)     4341 2023-04-16 04:29:29.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/common/filter_architecture.cpp
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-16 04:59:39.331173 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/conv/
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-16 05:00:05.923092 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/conv/device/
+-rw-r--r--   0 root         (0) root         (0)    21797 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/conv/device/cache_testbed_output.h
+-rw-r--r--   0 root         (0) root         (0)     5344 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_dgrad_implicit_gemm_cf32nhwc_cf32nhwc_cf32nhwc_simt_f32_sm50.cu
+-rw-r--r--   0 root         (0) root         (0)     5443 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_dgrad_implicit_gemm_cf32nhwc_cf32nhwc_cf32nhwc_simt_f32_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)    11470 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_dgrad_implicit_gemm_f16nhwc_f16nhwc_f16nhwc_tensor_op_f16_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)     5239 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_dgrad_implicit_gemm_f16nhwc_f16nhwc_f32nhwc_tensor_op_f32_sm70.cu
+-rw-r--r--   0 root         (0) root         (0)     9110 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_dgrad_implicit_gemm_f16nhwc_f16nhwc_f32nhwc_tensor_op_f32_sm75.cu
+-rw-r--r--   0 root         (0) root         (0)     8485 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_dgrad_implicit_gemm_f16nhwc_f16nhwc_f32nhwc_tensor_op_f32_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)     5243 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_dgrad_implicit_gemm_f32nhwc_f32nhwc_f32nhwc_simt_f32_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)     5378 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_dgrad_implicit_gemm_tf32nhwc_tf32nhwc_f32nhwc_tensor_op_f32_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)    12054 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_fprop_few_channels_f16nhwc_f16nhwc_f16nhwc_tensor_op_f32_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)     9603 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_fprop_fixed_channels_f16nhwc_f16nhwc_f16nhwc_tensor_op_f32_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)     5267 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_fprop_implicit_gemm_cf32nhwc_cf32nhwc_cf32nhwc_simt_f32_sm50.cu
+-rw-r--r--   0 root         (0) root         (0)     5357 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_fprop_implicit_gemm_cf32nhwc_cf32nhwc_cf32nhwc_simt_f32_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)     5089 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_fprop_implicit_gemm_f16nhwc_f16nhwc_f16nhwc_simt_f16_sm60.cu
+-rw-r--r--   0 root         (0) root         (0)    13690 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_fprop_implicit_gemm_f16nhwc_f16nhwc_f16nhwc_tensor_op_f16_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)     5390 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_fprop_implicit_gemm_f16nhwc_f16nhwc_f16nhwc_tensor_op_f32_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)     5191 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_fprop_implicit_gemm_f16nhwc_f16nhwc_f32nhwc_tensor_op_f32_sm70.cu
+-rw-r--r--   0 root         (0) root         (0)    11136 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_fprop_implicit_gemm_f16nhwc_f16nhwc_f32nhwc_tensor_op_f32_sm75.cu
+-rw-r--r--   0 root         (0) root         (0)     5291 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_fprop_implicit_gemm_f16nhwc_f16nhwc_f32nhwc_tensor_op_f32_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)     3551 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_fprop_implicit_gemm_f32nhwc_f32nhwc_f32nhwc_simt_f32_sm50.cu
+-rw-r--r--   0 root         (0) root         (0)     5157 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_fprop_implicit_gemm_f32nhwc_f32nhwc_f32nhwc_simt_f32_sm80.cu
+-rwxr-xr-x   0 root         (0) root         (0)     8278 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_fprop_implicit_gemm_qf32nhwc_qf32nhwc_qf32nhwc_simt_f32_sm50.cu
+-rw-r--r--   0 root         (0) root         (0)    20555 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_fprop_implicit_gemm_s4ncxhwx_s4cxrskx_s4ncxhwx_tensor_op_s32_sm75.cu
+-rw-r--r--   0 root         (0) root         (0)    20647 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_fprop_implicit_gemm_s4ncxhwx_s4cxrskx_s4ncxhwx_tensor_op_s32_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)     5155 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_fprop_implicit_gemm_s4nhwc_s4nhwc_s32nhwc_tensor_op_s32_sm75.cu
+-rw-r--r--   0 root         (0) root         (0)     5239 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_fprop_implicit_gemm_s4nhwc_s4nhwc_s32nhwc_tensor_op_s32_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)    26114 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_fprop_implicit_gemm_s8ncxhwx_s8cxrskx_s8ncxhwx_tensor_op_s32_sm75.cu
+-rw-r--r--   0 root         (0) root         (0)    26210 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_fprop_implicit_gemm_s8ncxhwx_s8cxrskx_s8ncxhwx_tensor_op_s32_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)     5111 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_fprop_implicit_gemm_s8nhwc_s8nhwc_s32nhwc_tensor_op_s32_sm75.cu
+-rw-r--r--   0 root         (0) root         (0)     5194 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_fprop_implicit_gemm_s8nhwc_s8nhwc_s32nhwc_tensor_op_s32_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)     5738 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_fprop_implicit_gemm_tf32nhwc_tf32nhwc_f32nhwc_tensor_op_f32_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)     5439 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_fprop_with_broadcast_sm70.cu
+-rw-r--r--   0 root         (0) root         (0)     7363 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_fprop_with_broadcast_sm75.cu
+-rw-r--r--   0 root         (0) root         (0)     3984 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_fprop_with_reduction_sm75.cu
+-rw-r--r--   0 root         (0) root         (0)    39452 2023-04-15 23:28:02.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_problems.h
+-rw-r--r--   0 root         (0) root         (0)    14471 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_strided_dgrad_implicit_gemm_f16nhwc_f16nhwc_f32nhwc_tensor_op_f32_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)     4662 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_strided_dgrad_implicit_gemm_tf32nhwc_tf32nhwc_f32nhwc_tensor_op_f32_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)    26224 2023-04-15 23:28:02.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_testbed.h
+-rw-r--r--   0 root         (0) root         (0)    21174 2023-04-16 04:29:29.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_testbed_interleaved.h
+-rw-r--r--   0 root         (0) root         (0)     5179 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_wgrad_implicit_gemm_cf32nhwc_cf32nhwc_cf32nhwc_simt_f32_sm50.cu
+-rw-r--r--   0 root         (0) root         (0)     5358 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_wgrad_implicit_gemm_cf32nhwc_cf32nhwc_cf32nhwc_simt_f32_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)     5264 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_wgrad_implicit_gemm_f16nhwc_f16nhwc_f16nhwc_tensor_op_f16_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)     3615 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_wgrad_implicit_gemm_f16nhwc_f16nhwc_f32nhwc_tensor_op_f32_sm70.cu
+-rw-r--r--   0 root         (0) root         (0)     7591 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_wgrad_implicit_gemm_f16nhwc_f16nhwc_f32nhwc_tensor_op_f32_sm75.cu
+-rw-r--r--   0 root         (0) root         (0)    10514 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_wgrad_implicit_gemm_f16nhwc_f16nhwc_f32nhwc_tensor_op_f32_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)     5157 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_wgrad_implicit_gemm_f32nhwc_f32nhwc_f32nhwc_simt_f32_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)     5772 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_wgrad_implicit_gemm_tf32nhwc_tf32nhwc_f32nhwc_tensor_op_f32_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)    23460 2023-04-16 04:29:29.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_with_broadcast_testbed.h
+-rw-r--r--   0 root         (0) root         (0)    21512 2023-04-15 23:28:02.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_with_reduction_testbed.h
+-rw-r--r--   0 root         (0) root         (0)     5135 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/conv/device/conv3d_dgrad_implicit_gemm_f16ndhwc_f16ndhwc_f32ndhwc_tensor_op_f32_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)     5347 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/conv/device/conv3d_dgrad_implicit_gemm_tf32ndhwc_tf32ndhwc_f32ndhwc_tensor_op_f32_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)     3736 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/conv/device/conv3d_fprop_implicit_gemm_f16ndhwc_f16ndhwc_f32ndhwc_tensor_op_f32_sm75.cu
+-rw-r--r--   0 root         (0) root         (0)     6560 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/conv/device/conv3d_fprop_implicit_gemm_f16ndhwc_f16ndhwc_f32ndhwc_tensor_op_f32_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)     5257 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/conv/device/conv3d_fprop_implicit_gemm_tf32ndhwc_tf32ndhwc_f32ndhwc_tensor_op_f32_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)    12276 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/conv/device/conv3d_problems.h
+-rw-r--r--   0 root         (0) root         (0)    21643 2023-04-15 23:28:02.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/conv/device/conv3d_testbed.h
+-rw-r--r--   0 root         (0) root         (0)     3622 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/conv/device/conv3d_wgrad_implicit_gemm_f16ndhwc_f16ndhwc_f32ndhwc_tensor_op_f32_sm75.cu
+-rw-r--r--   0 root         (0) root         (0)     6560 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/conv/device/conv3d_wgrad_implicit_gemm_f16ndhwc_f16ndhwc_f32ndhwc_tensor_op_f32_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)     5256 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/conv/device/conv3d_wgrad_implicit_gemm_tf32ndhwc_tf32ndhwc_f32ndhwc_tensor_op_f32_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)    17700 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/conv/device/depthwise_conv2d_direct_conv_testbed.h
+-rw-r--r--   0 root         (0) root         (0)    18437 2023-04-16 04:29:29.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/conv/device/depthwise_conv2d_fprop_direct_conv_f16nhwc_f16nhwc_f16nhwc_simt_f16_sm60.cu
+-rw-r--r--   0 root         (0) root         (0)    22194 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/conv/device/depthwise_conv2d_fprop_direct_conv_fixed_stride_dilation_f16nhwc_f16nhwc_f16nhwc_simt_f16_sm60.cu
+-rw-r--r--   0 root         (0) root         (0)     9383 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/conv/device/depthwise_conv2d_fprop_implicit_gemm_f16nhwc_f16nhwc_f16nhwc_simt_f16_sm60.cu
+-rw-r--r--   0 root         (0) root         (0)     9387 2023-04-12 16:23:45.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/conv/device/depthwise_fprop_implicit_gemm_f16nhwc_f16nhwc_f16nhwc_simt_f16_sm60.cu
+-rw-r--r--   0 root         (0) root         (0)    16100 2023-04-15 23:28:02.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/conv/device/group_conv2d_fprop_implicit_gemm_f16nhwc_f16nhwc_f16nhwc_tensor_op_f32_sm80.cu
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-16 05:00:06.153933 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/core/
+-rw-r--r--   0 root         (0) root         (0)     7365 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/core/array.cu
+-rw-r--r--   0 root         (0) root         (0)     7353 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/core/bfloat16.cu
+-rw-r--r--   0 root         (0) root         (0)     6981 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/core/complex.cu
+-rw-r--r--   0 root         (0) root         (0)     4009 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/core/float8.cu
+-rw-r--r--   0 root         (0) root         (0)    13001 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/core/functional.cu
+-rw-r--r--   0 root         (0) root         (0)     3553 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/core/half.cu
+-rw-r--r--   0 root         (0) root         (0)     5295 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/core/matrix.cu
+-rw-r--r--   0 root         (0) root         (0)     8592 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/core/matrix_coord.cu
+-rw-r--r--   0 root         (0) root         (0)    11508 2023-04-16 04:29:29.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/core/numeric_conversion.cu
+-rw-r--r--   0 root         (0) root         (0)     8148 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/core/predicate_vector.cu
+-rw-r--r--   0 root         (0) root         (0)     5777 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/core/quaternion.cu
+-rw-r--r--   0 root         (0) root         (0)     6746 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/core/tensor_ref.cu
+-rw-r--r--   0 root         (0) root         (0)     8885 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/core/tensor_view.cu
+-rw-r--r--   0 root         (0) root         (0)     2050 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/core/test_unit_core.cpp
+-rw-r--r--   0 root         (0) root         (0)     7088 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/core/tfloat32.cu
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-16 04:59:39.367600 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/cute/
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-16 05:00:06.193513 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/cute/ampere/
+-rw-r--r--   0 root         (0) root         (0)     3527 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/cute/ampere/cp_async.cu
+-rw-r--r--   0 root         (0) root         (0)    14320 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/cute/ampere/ldsm.cu
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-16 05:00:06.601394 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/cute/core/
+-rw-r--r--   0 root         (0) root         (0)     3838 2023-04-15 18:47:44.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/cute/core/array_subbyte.cpp
+-rw-r--r--   0 root         (0) root         (0)     3332 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/cute/core/bitfield.cpp
+-rw-r--r--   0 root         (0) root         (0)     4861 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/cute/core/coalesce.cpp
+-rw-r--r--   0 root         (0) root         (0)     8195 2023-04-15 18:47:44.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/cute/core/compact_xmajor.cpp
+-rw-r--r--   0 root         (0) root         (0)     5620 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/cute/core/compare.cpp
+-rw-r--r--   0 root         (0) root         (0)     7178 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/cute/core/complement.cpp
+-rw-r--r--   0 root         (0) root         (0)    12569 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/cute/core/composition.cpp
+-rw-r--r--   0 root         (0) root         (0)     4856 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/cute/core/inverse_left.cpp
+-rw-r--r--   0 root         (0) root         (0)     6702 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/cute/core/inverse_right.cpp
+-rw-r--r--   0 root         (0) root         (0)     6734 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/cute/core/logical_divide.cpp
+-rw-r--r--   0 root         (0) root         (0)     5914 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/cute/core/logical_product.cpp
+-rw-r--r--   0 root         (0) root         (0)     3488 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/cute/core/mixedbits.cpp
+-rw-r--r--   0 root         (0) root         (0)     2342 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/cute/core/transform.cpp
+-rw-r--r--   0 root         (0) root         (0)    13304 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/cute/core/tuple.cpp
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-16 05:00:06.792063 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/cute/hopper/
+-rw-r--r--   0 root         (0) root         (0)     7030 2023-04-15 18:47:44.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/cute/hopper/bulk_load.cu
+-rw-r--r--   0 root         (0) root         (0)     6407 2023-04-15 18:47:44.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/cute/hopper/bulk_store.cu
+-rw-r--r--   0 root         (0) root         (0)    14365 2023-04-15 23:28:02.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/cute/hopper/stsm.cu
+-rw-r--r--   0 root         (0) root         (0)    18990 2023-04-15 23:28:02.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/cute/hopper/tma_load.cu
+-rw-r--r--   0 root         (0) root         (0)    13875 2023-04-15 23:28:02.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/cute/hopper/tma_store.cu
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-16 05:00:06.812232 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/cute/layout/
+-rw-r--r--   0 root         (0) root         (0)     4544 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/cute/layout/layout_operator.cu
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-16 05:00:06.832422 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/cute/msvc_compilation/
+-rw-r--r--   0 root         (0) root         (0)     6533 2023-04-15 18:47:44.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/cute/msvc_compilation/tuple.cpp
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-16 04:59:39.389239 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/epilogue/
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-16 05:00:06.884625 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/epilogue/thread/
+-rw-r--r--   0 root         (0) root         (0)    15818 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/epilogue/thread/activation.cu
+-rw-r--r--   0 root         (0) root         (0)     6534 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/epilogue/thread/linear_combination.cu
+-rw-r--r--   0 root         (0) root         (0)     9964 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/epilogue/thread/linear_combination_planar_complex.cu
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-16 05:00:07.084161 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/epilogue/threadblock/
+-rw-r--r--   0 root         (0) root         (0)    13824 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/epilogue/threadblock/epilogue_planar_complex.cu
+-rw-r--r--   0 root         (0) root         (0)    27176 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/epilogue/threadblock/epilogue_simt.cu
+-rw-r--r--   0 root         (0) root         (0)    12061 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/epilogue/threadblock/epilogue_simt_sm60.cu
+-rw-r--r--   0 root         (0) root         (0)    25275 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/epilogue/threadblock/epilogue_simt_sm61.cu
+-rw-r--r--   0 root         (0) root         (0)    84612 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/epilogue/threadblock/epilogue_tensor_op.cu
+-rw-r--r--   0 root         (0) root         (0)    70486 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/epilogue/threadblock/epilogue_volta_tensor_op.cu
+-rw-r--r--   0 root         (0) root         (0)    25293 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/epilogue/threadblock/epilogue_with_reduction_tensor_op.cu
+-rw-r--r--   0 root         (0) root         (0)    13012 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/epilogue/threadblock/epilogue_with_reduction_testbed.h
+-rw-r--r--   0 root         (0) root         (0)     7743 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/epilogue/threadblock/epilogue_wmma_tensor_op_sm70.cu
+-rw-r--r--   0 root         (0) root         (0)    19178 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/epilogue/threadblock/output_tile_threadmap.cu
+-rw-r--r--   0 root         (0) root         (0)    28433 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/epilogue/threadblock/predicated_tile_iterator.cu
+-rw-r--r--   0 root         (0) root         (0)    11038 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/epilogue/threadblock/testbed.h
+-rw-r--r--   0 root         (0) root         (0)    11734 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/epilogue/threadblock/testbed_planar_complex.h
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-16 05:00:07.133148 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/epilogue/warp/
+-rw-r--r--   0 root         (0) root         (0)     6783 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/epilogue/warp/fragment_iterator_tensor_op.cu
+-rw-r--r--   0 root         (0) root         (0)     7275 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/epilogue/warp/fragment_iterator_volta_tensor_op.cu
+-rw-r--r--   0 root         (0) root         (0)     6616 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/epilogue/warp/fragment_iterator_wmma_tensor_op.cu
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-16 04:59:39.422340 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-16 05:00:18.688187 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/
+-rw-r--r--   0 root         (0) root         (0)    10189 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_b1t_b1n_s32n_tensor_op_s32_sm75.cu
+-rw-r--r--   0 root         (0) root         (0)    17899 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_b1t_b1n_s32n_tensor_op_s32_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)     8933 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_b1t_b1n_s32n_wmma_tensor_op_s32_sm75.cu
+-rw-r--r--   0 root         (0) root         (0)    10164 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_b1t_b1n_s32t_tensor_op_s32_sm75.cu
+-rw-r--r--   0 root         (0) root         (0)    17984 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_b1t_b1n_s32t_tensor_op_s32_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)     8915 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_b1t_b1n_s32t_wmma_tensor_op_s32_sm75.cu
+-rw-r--r--   0 root         (0) root         (0)    16447 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_bf16n_bf16n_f32t_tensor_op_f32_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)    16575 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_bf16t_bf16t_bf16t_tensor_op_f32_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)     8318 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_cf32n_cf32t_cf32t_tensor_op_tf32_f32_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)     8317 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_cf32t_cf32n_cf32t_tensor_op_tf32_f32_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)     6714 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_cf64n_cf64t_cf64t_tensor_op_f64_gaussian_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)     6735 2023-04-16 04:29:29.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_cf64n_cf64t_cf64t_tensor_op_f64_gaussian_sm90.cu
+-rw-r--r--   0 root         (0) root         (0)     7895 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_cf64n_cf64t_cf64t_tensor_op_f64_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)     7918 2023-04-16 04:29:29.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_cf64n_cf64t_cf64t_tensor_op_f64_sm90.cu
+-rw-r--r--   0 root         (0) root         (0)     6516 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_cf64t_cf64n_cf64t_tensor_op_f64_gaussian_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)     6537 2023-04-16 04:29:29.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_cf64t_cf64n_cf64t_tensor_op_f64_gaussian_sm90.cu
+-rw-r--r--   0 root         (0) root         (0)     9016 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_cf64t_cf64n_cf64t_tensor_op_f64_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)     9041 2023-04-16 04:29:29.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_cf64t_cf64n_cf64t_tensor_op_f64_sm90.cu
+-rw-r--r--   0 root         (0) root         (0)     4628 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16n_f16n_f16n_direct_store_tensor_op_f32_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)     6165 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16n_f16n_f16n_wmma_tensor_op_f16_sm70.cu
+-rw-r--r--   0 root         (0) root         (0)     6124 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16n_f16n_f16n_wmma_tensor_op_f32_sm70.cu
+-rw-r--r--   0 root         (0) root         (0)     9634 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16n_f16n_f16t_tensor_op_f32_sm75.cu
+-rw-r--r--   0 root         (0) root         (0)    16357 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16n_f16n_f16t_tensor_op_f32_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)    13189 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16n_f16n_f16t_tensor_op_f32_sparse_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)     8845 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16n_f16n_f16t_volta_tensor_op_f32_sm70.cu
+-rw-r--r--   0 root         (0) root         (0)    13583 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16n_f16n_f16t_wmma_tensor_op_f16_sm70.cu
+-rw-r--r--   0 root         (0) root         (0)    13464 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16n_f16n_f16t_wmma_tensor_op_f32_sm70.cu
+-rw-r--r--   0 root         (0) root         (0)     9571 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16n_f16n_f32n_tensor_op_f32_sm75.cu
+-rw-r--r--   0 root         (0) root         (0)    16239 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16n_f16n_f32n_tensor_op_f32_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)     6140 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16n_f16n_f32n_wmma_tensor_op_f32_sm70.cu
+-rw-r--r--   0 root         (0) root         (0)     9544 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16n_f16n_f32t_tensor_op_f32_sm75.cu
+-rw-r--r--   0 root         (0) root         (0)    16417 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16n_f16n_f32t_tensor_op_f32_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)    13075 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16n_f16n_f32t_tensor_op_f32_sparse_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)     8775 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16n_f16n_f32t_volta_tensor_op_f32_sm70.cu
+-rw-r--r--   0 root         (0) root         (0)    11470 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16n_f16n_f32t_wmma_tensor_op_f32_sm70.cu
+-rw-r--r--   0 root         (0) root         (0)     6156 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16n_f16t_f16n_wmma_tensor_op_f16_sm70.cu
+-rw-r--r--   0 root         (0) root         (0)     6116 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16n_f16t_f16n_wmma_tensor_op_f32_sm70.cu
+-rw-r--r--   0 root         (0) root         (0)     3528 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16n_f16t_f16t_tensor_op_f16_slicedk_sm75.cu
+-rw-r--r--   0 root         (0) root         (0)     3539 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16n_f16t_f16t_tensor_op_f16_slicedk_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)     7965 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16n_f16t_f16t_tensor_op_f16_sm75.cu
+-rw-r--r--   0 root         (0) root         (0)    16470 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16n_f16t_f16t_tensor_op_f16_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)    13273 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16n_f16t_f16t_tensor_op_f16_sparse_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)     3648 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16n_f16t_f16t_tensor_op_f32_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)     8608 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16n_f16t_f16t_volta_tensor_op_f16_sm70.cu
+-rw-r--r--   0 root         (0) root         (0)    13518 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16n_f16t_f16t_wmma_tensor_op_f16_sm70.cu
+-rw-r--r--   0 root         (0) root         (0)     3645 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16n_f16t_f16t_wmma_tensor_op_f32_sm70.cu
+-rw-r--r--   0 root         (0) root         (0)     6096 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16n_f16t_f32n_wmma_tensor_op_f32_sm70.cu
+-rw-r--r--   0 root         (0) root         (0)     7845 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16n_f16t_f32t_tensor_op_f32_sm75.cu
+-rw-r--r--   0 root         (0) root         (0)    18135 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16n_f16t_f32t_tensor_op_f32_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)    13008 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16n_f16t_f32t_tensor_op_f32_sparse_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)     8505 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16n_f16t_f32t_volta_tensor_op_f32_sm70.cu
+-rw-r--r--   0 root         (0) root         (0)    11497 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16n_f16t_f32t_wmma_tensor_op_f32_sm70.cu
+-rw-r--r--   0 root         (0) root         (0)    11090 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16t_f16n_f16n_singlestage_wmma_tensor_op_f16_sm70.cu
+-rw-r--r--   0 root         (0) root         (0)     6156 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16t_f16n_f16n_wmma_tensor_op_f16_sm70.cu
+-rw-r--r--   0 root         (0) root         (0)     6116 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16t_f16n_f16n_wmma_tensor_op_f32_sm70.cu
+-rw-r--r--   0 root         (0) root         (0)    11066 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16t_f16n_f16t_singlestage_wmma_tensor_op_f16_sm70.cu
+-rw-r--r--   0 root         (0) root         (0)    17114 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16t_f16n_f16t_tensor_op_f16_broadcast_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)     3528 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16t_f16n_f16t_tensor_op_f16_slicedk_sm75.cu
+-rw-r--r--   0 root         (0) root         (0)     3540 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16t_f16n_f16t_tensor_op_f16_slicedk_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)     7964 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16t_f16n_f16t_tensor_op_f16_sm75.cu
+-rw-r--r--   0 root         (0) root         (0)    16457 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16t_f16n_f16t_tensor_op_f16_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)    13266 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16t_f16n_f16t_tensor_op_f16_sparse_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)     8933 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16t_f16n_f16t_volta_tensor_op_f16_sm70.cu
+-rw-r--r--   0 root         (0) root         (0)    13551 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16t_f16n_f16t_wmma_tensor_op_f16_sm70.cu
+-rw-r--r--   0 root         (0) root         (0)    13540 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16t_f16n_f16t_wmma_tensor_op_f32_sm70.cu
+-rw-r--r--   0 root         (0) root         (0)     6130 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16t_f16n_f32n_wmma_tensor_op_f32_sm70.cu
+-rw-r--r--   0 root         (0) root         (0)     8160 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16t_f16n_f32t_singlestage_wmma_tensor_op_f32_sm70.cu
+-rw-r--r--   0 root         (0) root         (0)     7847 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16t_f16n_f32t_tensor_op_f32_sm75.cu
+-rw-r--r--   0 root         (0) root         (0)    16131 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16t_f16n_f32t_tensor_op_f32_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)    13014 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16t_f16n_f32t_tensor_op_f32_sparse_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)     8754 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16t_f16n_f32t_volta_tensor_op_f32_sm70.cu
+-rw-r--r--   0 root         (0) root         (0)    11497 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16t_f16n_f32t_wmma_tensor_op_f32_sm70.cu
+-rw-r--r--   0 root         (0) root         (0)     6147 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16t_f16t_f16n_wmma_tensor_op_f16_sm70.cu
+-rw-r--r--   0 root         (0) root         (0)     6107 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16t_f16t_f16n_wmma_tensor_op_f32_sm70.cu
+-rw-r--r--   0 root         (0) root         (0)    13518 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16t_f16t_f16t_wmma_tensor_op_f16_sm70.cu
+-rw-r--r--   0 root         (0) root         (0)    13398 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16t_f16t_f16t_wmma_tensor_op_f32_sm70.cu
+-rw-r--r--   0 root         (0) root         (0)     7845 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16t_f16t_f32n_tensor_op_f32_sm75.cu
+-rw-r--r--   0 root         (0) root         (0)    16149 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16t_f16t_f32n_tensor_op_f32_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)     6119 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16t_f16t_f32n_wmma_tensor_op_f32_sm70.cu
+-rw-r--r--   0 root         (0) root         (0)     7827 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16t_f16t_f32t_tensor_op_f32_sm75.cu
+-rw-r--r--   0 root         (0) root         (0)    16101 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16t_f16t_f32t_tensor_op_f32_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)     9526 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16t_f16t_f32t_tensor_op_f32_sparse_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)     7898 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16t_f16t_f32t_volta_tensor_op_f32_sm70.cu
+-rw-r--r--   0 root         (0) root         (0)    11470 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16t_f16t_f32t_wmma_tensor_op_f32_sm70.cu
+-rw-r--r--   0 root         (0) root         (0)     3584 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f32n_f32n_f32t_tensor_op_bf16_f32_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)     3473 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f32n_f32n_f32t_tensor_op_f32_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)    12967 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f32n_f32n_f32t_tensor_op_f32_sparse_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)    12931 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f32n_f32t_f32t_tensor_op_f32_sparse_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)    12930 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f32t_f32n_f32t_tensor_op_f32_sparse_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)    12895 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f32t_f32t_f32t_tensor_op_f32_sparse_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)     8349 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f64n_f64t_f64t_tensor_op_f64_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)     7288 2023-04-16 04:29:29.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f64n_f64t_f64t_tensor_op_f64_sm90.cu
+-rw-r--r--   0 root         (0) root         (0)     8348 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f64t_f64n_f64t_tensor_op_f64_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)     7279 2023-04-16 04:29:29.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f64t_f64n_f64t_tensor_op_f64_sm90.cu
+-rw-r--r--   0 root         (0) root         (0)    10240 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_grouped_scheduler_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)    26146 2023-04-15 23:28:02.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_grouped_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)    11339 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_planar_complex_f16_f16_f32_tensor_op_sm70.cu
+-rw-r--r--   0 root         (0) root         (0)     7346 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_planar_complex_f16_f16_f32_tensor_op_sm75.cu
+-rw-r--r--   0 root         (0) root         (0)    12397 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_planar_complex_f16_f16_f32_tensor_op_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)     6859 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_s4n_s4t_s4n_tensor_op_s32_sm75.cu
+-rw-r--r--   0 root         (0) root         (0)     7239 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_s4n_s4t_s4n_tensor_op_s32_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)     8121 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_s4t_s4n_s32n_tensor_op_s32_sm75.cu
+-rw-r--r--   0 root         (0) root         (0)    16882 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_s4t_s4n_s32n_tensor_op_s32_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)     8407 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_s4t_s4n_s32n_wmma_tensor_op_s32_sm75.cu
+-rw-r--r--   0 root         (0) root         (0)     8103 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_s4t_s4n_s32t_tensor_op_s32_sm75.cu
+-rw-r--r--   0 root         (0) root         (0)    17111 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_s4t_s4n_s32t_tensor_op_s32_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)    12637 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_s4t_s4n_s32t_tensor_op_s32_sparse_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)     8388 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_s4t_s4n_s32t_wmma_tensor_op_s32_sm75.cu
+-rw-r--r--   0 root         (0) root         (0)    10044 2023-04-15 23:28:02.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_s4t_s4n_s4n_tensor_op_s32_sm75.cu
+-rw-r--r--   0 root         (0) root         (0)    17544 2023-04-15 23:28:02.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_s4t_s4n_s4n_tensor_op_s32_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)    10020 2023-04-15 23:28:02.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_s4t_s4n_s4t_tensor_op_s32_sm75.cu
+-rw-r--r--   0 root         (0) root         (0)    17544 2023-04-15 23:28:02.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_s4t_s4n_s4t_tensor_op_s32_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)     9588 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_s8n_s8t_s8n_tensor_op_s32_sm75.cu
+-rw-r--r--   0 root         (0) root         (0)    11288 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_s8n_s8t_s8n_tensor_op_s32_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)     3514 2023-04-15 18:47:44.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_s8t_s8n_f16t_tensor_op_s32_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)     7977 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_s8t_s8n_s32n_tensor_op_s32_sm75.cu
+-rw-r--r--   0 root         (0) root         (0)    16531 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_s8t_s8n_s32n_tensor_op_s32_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)     5693 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_s8t_s8n_s32n_wmma_tensor_op_s32_sm72.cu
+-rw-r--r--   0 root         (0) root         (0)     7959 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_s8t_s8n_s32t_tensor_op_s32_sm75.cu
+-rw-r--r--   0 root         (0) root         (0)    16691 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_s8t_s8n_s32t_tensor_op_s32_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)    12408 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_s8t_s8n_s32t_tensor_op_s32_sparse_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)     6864 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_s8t_s8n_s32t_wmma_tensor_op_s32_sm72.cu
+-rw-r--r--   0 root         (0) root         (0)     7744 2023-04-15 23:28:02.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_s8t_s8n_s8n_tensor_op_s32_sm75.cu
+-rw-r--r--   0 root         (0) root         (0)    16531 2023-04-15 23:28:02.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_s8t_s8n_s8n_tensor_op_s32_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)     6675 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_s8t_s8n_s8n_wmma_tensor_op_s32_sm72.cu
+-rw-r--r--   0 root         (0) root         (0)     7752 2023-04-15 23:28:02.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_s8t_s8n_s8t_tensor_op_s32_sm75.cu
+-rw-r--r--   0 root         (0) root         (0)    16484 2023-04-15 23:28:02.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_s8t_s8n_s8t_tensor_op_s32_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)     6663 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_s8t_s8n_s8t_wmma_tensor_op_s32_sm72.cu
+-rw-r--r--   0 root         (0) root         (0)     4663 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_splitk_serial_tensor_op_sm75.cu
+-rw-r--r--   0 root         (0) root         (0)     4945 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_splitk_simt_sm50.cu
+-rw-r--r--   0 root         (0) root         (0)     6616 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_splitk_tensor_op_sm70.cu
+-rw-r--r--   0 root         (0) root         (0)    10581 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_splitk_tensor_op_sm75.cu
+-rw-r--r--   0 root         (0) root         (0)    16950 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_tf32n_tf32n_f32t_tensor_op_f32_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)    16902 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_tf32n_tf32t_f32t_tensor_op_f32_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)    15131 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_tf32t_tf32n_f32t_tensor_op_f32_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)    16855 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_tf32t_tf32t_f32t_tensor_op_f32_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)     6854 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_u8t_u8n_s32t_wmma_tensor_op_s32_sm72.cu
+-rw-r--r--   0 root         (0) root         (0)     6686 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_universal_cf32n_cf32n_cf32n_tensor_op_f32_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)     6755 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_universal_cf64n_cf64t_cf64t_tensor_op_f64_gaussian_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)     6687 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_universal_cf64n_cf64t_cf64t_tensor_op_f64_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)     4726 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_universal_f16n_f16t_f32n_tensor_op_f32_sm75.cu
+-rw-r--r--   0 root         (0) root         (0)     4718 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_universal_f16n_f16t_f32t_tensor_op_f32_sm75.cu
+-rw-r--r--   0 root         (0) root         (0)    16715 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_with_broadcast_f16n_f16n_f16n_tensorop_f32_sm75.cu
+-rw-r--r--   0 root         (0) root         (0)    12841 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_with_reduction_f16n_f16n_f16n_tensorop_f32_sm75.cu
+-rw-r--r--   0 root         (0) root         (0)     4544 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_with_reduction_f16t_f16n_f16n_tensorop_f32_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)    13157 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/gemv.cu
+-rw-r--r--   0 root         (0) root         (0)     6028 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/hemm_cf32h_cf32n_tensor_op_f32_ls_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)     6031 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/hemm_cf32h_cf32n_tensor_op_f32_rs_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)     6064 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/hemm_cf32h_cf32n_tensor_op_fast_f32_ls_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)     6067 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/hemm_cf32h_cf32n_tensor_op_fast_f32_rs_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)     4897 2023-04-16 04:29:29.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/hemm_cf64_cf64_cf64_tensor_op_f64_sm90.cu
+-rw-r--r--   0 root         (0) root         (0)     6088 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/hemm_cf64h_cf64n_cf64n_tensor_op_ls_f64_gaussian_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)     6037 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/hemm_cf64h_cf64n_cf64n_tensor_op_ls_f64_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)     6040 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/hemm_cf64h_cf64n_cf64n_tensor_op_rs_f64_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)     5382 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/her2k_cf32h_cf32n_tensor_op_f32_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)     5406 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/her2k_cf32h_cf32n_tensor_op_fast_f32_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)     5390 2023-04-16 04:29:29.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/her2k_cf64_cf64_tensor_op_f64_sm90.cu
+-rw-r--r--   0 root         (0) root         (0)    13055 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/her2k_cf64h_cf64n_tensor_op_f64_grouped_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)    13027 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/her2k_cf64n_cf64n_tensor_op_f64_grouped_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)     5388 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/her2k_cf64n_cf64n_tensor_op_f64_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)     6939 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/her2k_cf64n_cf64t_tensor_op_f64_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)     7677 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/herk_cf32h_cf32n_tensor_op_f32_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)     7725 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/herk_cf32h_cf32n_tensor_op_fast_f32_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)     3842 2023-04-16 04:29:29.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/herk_cf64_cf64_tensor_op_f64_sm90.cu
+-rw-r--r--   0 root         (0) root         (0)     6396 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/herk_cf64h_cf64n_tensor_op_f64_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)    10023 2023-04-16 04:29:29.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/multistage_testbed.h
+-rw-r--r--   0 root         (0) root         (0)     9189 2023-04-16 04:29:29.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/multistage_testbed_interleaved.h
+-rw-r--r--   0 root         (0) root         (0)    11186 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/rank_2k_grouped_scheduler_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)    46795 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/simt_cgemm_nn_sm50.cu
+-rw-r--r--   0 root         (0) root         (0)    54085 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/simt_cgemm_nt_sm50.cu
+-rw-r--r--   0 root         (0) root         (0)     8318 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/simt_cgemm_nt_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)    46687 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/simt_cgemm_tn_sm50.cu
+-rw-r--r--   0 root         (0) root         (0)     8411 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/simt_cgemm_tn_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)    46578 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/simt_cgemm_tt_sm50.cu
+-rw-r--r--   0 root         (0) root         (0)    40533 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/simt_dgemm_nn_sm50.cu
+-rw-r--r--   0 root         (0) root         (0)    47656 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/simt_dgemm_nt_sm50.cu
+-rw-r--r--   0 root         (0) root         (0)    40441 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/simt_dgemm_tn_sm50.cu
+-rw-r--r--   0 root         (0) root         (0)    40354 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/simt_dgemm_tt_sm50.cu
+-rw-r--r--   0 root         (0) root         (0)     3513 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/simt_f8gemm_tn_sm50.cu
+-rw-r--r--   0 root         (0) root         (0)    89517 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/simt_hgemm_nn_sm50.cu
+-rw-r--r--   0 root         (0) root         (0)    89304 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/simt_hgemm_nt_sm50.cu
+-rw-r--r--   0 root         (0) root         (0)    89304 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/simt_hgemm_tn_sm50.cu
+-rw-r--r--   0 root         (0) root         (0)    89091 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/simt_hgemm_tt_sm50.cu
+-rw-r--r--   0 root         (0) root         (0)    69175 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/simt_igemm_nn_sm50.cu
+-rw-r--r--   0 root         (0) root         (0)    71438 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/simt_igemm_nt_sm50.cu
+-rw-r--r--   0 root         (0) root         (0)    67796 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/simt_igemm_tn_sm50.cu
+-rw-r--r--   0 root         (0) root         (0)    70056 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/simt_igemm_tt_sm50.cu
+-rw-r--r--   0 root         (0) root         (0)     7156 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/simt_int8_igemm_sm61.cu
+-rw-r--r--   0 root         (0) root         (0)     6067 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/simt_int8_igemm_sm61_perf.cu
+-rw-r--r--   0 root         (0) root         (0)     9063 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/simt_int8_igemm_sm61_sliced_k.cu
+-rw-r--r--   0 root         (0) root         (0)    35894 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/simt_qgemm_nn_sm50.cu
+-rw-r--r--   0 root         (0) root         (0)    35813 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/simt_qgemm_nt_sm50.cu
+-rw-r--r--   0 root         (0) root         (0)    35813 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/simt_qgemm_tn_sm50.cu
+-rw-r--r--   0 root         (0) root         (0)    35732 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/simt_qgemm_tt_sm50.cu
+-rw-r--r--   0 root         (0) root         (0)    70872 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/simt_sgemm_nn_sm50.cu
+-rw-r--r--   0 root         (0) root         (0)    73136 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/simt_sgemm_nt_sm50.cu
+-rw-r--r--   0 root         (0) root         (0)     8870 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/simt_sgemm_nt_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)    69488 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/simt_sgemm_tn_sm50.cu
+-rw-r--r--   0 root         (0) root         (0)     8865 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/simt_sgemm_tn_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)    71755 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/simt_sgemm_tt_sm50.cu
+-rw-r--r--   0 root         (0) root         (0)    33231 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/simt_zgemm_nn_sm50.cu
+-rw-r--r--   0 root         (0) root         (0)    33156 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/simt_zgemm_nt_sm50.cu
+-rw-r--r--   0 root         (0) root         (0)    33156 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/simt_zgemm_tn_sm50.cu
+-rw-r--r--   0 root         (0) root         (0)    33081 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/simt_zgemm_tt_sm50.cu
+-rw-r--r--   0 root         (0) root         (0)     5238 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/sm50_gemm_f32_f32_f32_simt.cu
+-rw-r--r--   0 root         (0) root         (0)     5253 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/sm50_gemm_f64_f64_f64_simt.cu
+-rw-r--r--   0 root         (0) root         (0)     5357 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/sm61_gemm_s8_s8_s32_simt.cu
+-rw-r--r--   0 root         (0) root         (0)     5479 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/sm80_gemm_f16_f16_f32_tensor_op_f32.cu
+-rw-r--r--   0 root         (0) root         (0)     5238 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/sm80_gemm_f32_f32_f32_simt.cu
+-rw-r--r--   0 root         (0) root         (0)     5253 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/sm80_gemm_f64_f64_f64_simt.cu
+-rw-r--r--   0 root         (0) root         (0)     3875 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/sm80_gemm_f64_f64_f64_tensor_op_f64.cu
+-rw-r--r--   0 root         (0) root         (0)     3734 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/sm80_gemm_s8_s8_s32_tensor_op.cu
+-rw-r--r--   0 root         (0) root         (0)     5387 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/sm80_gemm_tf32_tf32_f32_tensor_op_f32.cu
+-rw-r--r--   0 root         (0) root         (0)     7436 2023-04-15 23:28:02.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/sm90_gemm_bf16_bf16_bf16_alignx_tensor_op_f32.cu
+-rw-r--r--   0 root         (0) root         (0)     7409 2023-04-15 23:28:02.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/sm90_gemm_bf16_bf16_bf16_tensor_op_f32.cu
+-rw-r--r--   0 root         (0) root         (0)    17391 2023-04-15 23:28:02.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/sm90_gemm_f16_f16_f16_alignx_tensor_op.cu
+-rw-r--r--   0 root         (0) root         (0)    42504 2023-04-15 23:28:02.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/sm90_gemm_f16_f16_f16_tensor_op.cu
+-rw-r--r--   0 root         (0) root         (0)    22602 2023-04-15 23:28:02.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/sm90_gemm_f16_f16_f16_tensor_op_f32_cluster_unspecialized.cu
+-rw-r--r--   0 root         (0) root         (0)    22874 2023-04-15 23:28:02.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/sm90_gemm_f16_f16_f16_tensor_op_f32_cluster_warpspecialized.cu
+-rw-r--r--   0 root         (0) root         (0)    32884 2023-04-15 18:47:44.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/sm90_gemm_f16_f16_f16_tensor_op_f32_cluster_warpspecialized_cooperative.cu
+-rw-r--r--   0 root         (0) root         (0)    14716 2023-04-15 18:47:44.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/sm90_gemm_f16_f16_f16_tensor_op_f32_cluster_warpspecialized_cooperative_bias_elementwise.cu
+-rw-r--r--   0 root         (0) root         (0)    42526 2023-04-15 23:28:02.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/sm90_gemm_f16_f16_f16_tensor_op_f32_cluster_warpspecialized_persistent.cu
+-rw-r--r--   0 root         (0) root         (0)    49311 2023-04-15 18:47:44.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/sm90_gemm_f16_f16_f16_tensor_op_f32_cluster_warpspecialized_pingpong.cu
+-rw-r--r--   0 root         (0) root         (0)    14621 2023-04-15 18:47:44.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/sm90_gemm_f16_f16_f16_tensor_op_f32_cluster_warpspecialized_pingpong_bias_elementwise.cu
+-rw-r--r--   0 root         (0) root         (0)    12049 2023-04-15 18:47:44.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/sm90_gemm_f16_f16_f16_tensor_op_f32_tensor_broadcast.cu
+-rw-r--r--   0 root         (0) root         (0)     3810 2023-04-15 23:28:02.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/sm90_gemm_f32_f32_f32_tensor_op_f32.cu
+-rw-r--r--   0 root         (0) root         (0)     4507 2023-04-15 18:47:44.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/sm90_gemm_f32_f32_f32_tensor_op_f32_tensor_broadcast.cu
+-rw-r--r--   0 root         (0) root         (0)     5976 2023-04-15 23:28:02.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/sm90_gemm_s8_s8_s8_alignx_tensor_op_s32.cu
+-rw-r--r--   0 root         (0) root         (0)     9313 2023-04-15 23:28:02.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/sm90_gemm_s8_s8_s8_tensor_op_s32.cu
+-rw-r--r--   0 root         (0) root         (0)     4515 2023-04-15 18:47:44.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/sm90_gemm_s8_s8_s8_tensor_op_s32_tensor_broadcast.cu
+-rw-r--r--   0 root         (0) root         (0)     5986 2023-04-15 23:28:02.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/sm90_gemm_tf32_tf32_f32_alignx_tensor_op_f32.cu
+-rw-r--r--   0 root         (0) root         (0)     7268 2023-04-15 23:28:02.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/sm90_gemm_tf32_tf32_f32_tensor_op_f32.cu
+-rw-r--r--   0 root         (0) root         (0)    21505 2023-04-15 18:47:44.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/sm90_gemm_tf32_tf32_f32_tensor_op_f32_gmma_rs_cluster_warpspecialized.cu
+-rw-r--r--   0 root         (0) root         (0)     5923 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/symm_cf32n_cf32n_tensor_op_f32_ls_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)     5926 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/symm_cf32n_cf32n_tensor_op_f32_rs_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)     5959 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/symm_cf32n_cf32n_tensor_op_fast_f32_ls_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)     5962 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/symm_cf32n_cf32n_tensor_op_fast_f32_rs_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)     4827 2023-04-16 04:29:29.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/symm_cf64_cf64_cf64_tensor_op_f64_sm90.cu
+-rw-r--r--   0 root         (0) root         (0)     5983 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/symm_cf64n_cf64n_cf64n_tensor_op_ls_f64_gaussian_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)     5932 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/symm_cf64n_cf64n_cf64n_tensor_op_ls_f64_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)     5935 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/symm_cf64n_cf64n_cf64n_tensor_op_rs_f64_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)    15203 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/symm_f32n_f32n_tensor_op_fast_f32_ls_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)     8623 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/symm_f32n_f32n_tensor_op_fast_f32_rs_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)    15104 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/symm_f32t_f32t_tensor_op_fast_f32_ls_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)     4765 2023-04-16 04:29:29.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/symm_f64_f64_tensor_op_f64_sm90.cu
+-rw-r--r--   0 root         (0) root         (0)     8103 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/symm_f64n_f64n_tensor_op_f64_ls_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)     8108 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/symm_f64n_f64n_tensor_op_f64_rs_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)     8088 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/symm_f64n_f64t_tensor_op_f64_ls_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)     8093 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/symm_f64n_f64t_tensor_op_f64_rs_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)     8073 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/symm_f64t_f64n_tensor_op_f64_ls_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)     8078 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/symm_f64t_f64n_tensor_op_f64_rs_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)     8058 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/symm_f64t_f64t_tensor_op_f64_ls_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)     8063 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/symm_f64t_f64t_tensor_op_f64_rs_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)    15071 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/symm_tf32n_f32n_tensor_op_f32_ls_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)     8551 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/symm_tf32n_f32n_tensor_op_f32_rs_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)    14972 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/symm_tf32t_f32t_tensor_op_f32_ls_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)     5362 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/syr2k_cf32n_cf32n_tensor_op_f32_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)     5386 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/syr2k_cf32n_cf32n_tensor_op_fast_f32_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)     5356 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/syr2k_cf32n_cf32t_tensor_op_f32_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)     5380 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/syr2k_cf32n_cf32t_tensor_op_fast_f32_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)     5367 2023-04-16 04:29:29.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/syr2k_cf64_cf64_tensor_op_f64_sm90.cu
+-rw-r--r--   0 root         (0) root         (0)    12952 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/syr2k_cf64n_cf64n_tensor_op_f64_grouped_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)     5368 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/syr2k_cf64n_cf64n_tensor_op_f64_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)     7208 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/syr2k_cf64n_cf64t_tensor_op_f64_grouped_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)     5362 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/syr2k_cf64n_cf64t_tensor_op_f64_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)     7199 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/syr2k_cf64t_cf64n_tensor_op_f64_grouped_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)     7190 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/syr2k_cf64t_cf64t_tensor_op_f64_grouped_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)     4794 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/syr2k_f32n_f32n_tensor_op_fast_f32_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)     4783 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/syr2k_f32t_f32n_tensor_op_fast_f32_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)     4728 2023-04-16 04:29:29.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/syr2k_f64_f64_tensor_op_f64_sm90.cu
+-rw-r--r--   0 root         (0) root         (0)    19145 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/syr2k_f64n_f64n_tensor_op_f64_grouped_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)     7991 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/syr2k_f64n_f64n_tensor_op_f64_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)    11015 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/syr2k_f64n_f64t_tensor_op_f64_grouped_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)     7976 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/syr2k_f64n_f64t_tensor_op_f64_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)    12342 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/syr2k_f64t_f64n_tensor_op_f64_grouped_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)     7961 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/syr2k_f64t_f64n_tensor_op_f64_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)    12321 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/syr2k_f64t_f64t_tensor_op_f64_grouped_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)     4786 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/syr2k_tf32n_f32n_tensor_op_f32_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)     4775 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/syr2k_tf32t_f32n_tensor_op_f32_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)     4993 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/syrk_cf32n_cf32n_tensor_op_f32_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)     5017 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/syrk_cf32n_cf32n_tensor_op_fast_f32_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)     4987 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/syrk_cf32n_cf32t_tensor_op_f32_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)     5011 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/syrk_cf32n_cf32t_tensor_op_fast_f32_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)     5012 2023-04-16 04:29:29.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/syrk_cf64_cf64_tensor_op_f64_sm90.cu
+-rw-r--r--   0 root         (0) root         (0)     4996 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/syrk_cf64n_cf64n_tensor_op_f64_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)     3793 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/syrk_cf64n_cf64t_tensor_op_f64_gaussian_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)     4990 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/syrk_cf64n_cf64t_tensor_op_f64_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)    16083 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/syrk_f32n_f32t_tensor_op_fast_f32_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)    16041 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/syrk_f32t_f32t_tensor_op_fast_f32_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)     4518 2023-04-16 04:29:29.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/syrk_f64_f64_tensor_op_f64_sm90.cu
+-rw-r--r--   0 root         (0) root         (0)     7451 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/syrk_f64n_f64t_tensor_op_f64_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)     9401 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/syrk_f64t_f64n_tensor_op_f64_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)    16027 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/syrk_tf32n_f32t_tensor_op_f32_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)    15985 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/syrk_tf32t_f32t_tensor_op_f32_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)    20333 2023-04-16 04:29:29.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/testbed.h
+-rw-r--r--   0 root         (0) root         (0)     8136 2023-04-16 04:29:29.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/testbed_complex.h
+-rw-r--r--   0 root         (0) root         (0)    20590 2023-04-16 04:29:29.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/testbed_gemm_with_broadcast.h
+-rw-r--r--   0 root         (0) root         (0)    19346 2023-04-16 04:29:29.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/testbed_gemm_with_reduction.h
+-rw-r--r--   0 root         (0) root         (0)    16502 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/testbed_grouped.h
+-rw-r--r--   0 root         (0) root         (0)    16562 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/testbed_grouped_rank_2k.h
+-rw-r--r--   0 root         (0) root         (0)    17002 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/testbed_grouped_rank_2k_scheduler.h
+-rw-r--r--   0 root         (0) root         (0)    14698 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/testbed_grouped_scheduler.h
+-rw-r--r--   0 root         (0) root         (0)    10130 2023-04-16 04:29:29.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/testbed_interleaved.h
+-rw-r--r--   0 root         (0) root         (0)     9481 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/testbed_planar_complex.h
+-rw-r--r--   0 root         (0) root         (0)    20761 2023-04-16 04:29:29.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/testbed_rank2k_universal.h
+-rw-r--r--   0 root         (0) root         (0)    15562 2023-04-16 04:29:29.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/testbed_rank_k_universal.h
+-rw-r--r--   0 root         (0) root         (0)     8639 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/testbed_sanity.h
+-rw-r--r--   0 root         (0) root         (0)    15769 2023-04-16 04:29:29.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/testbed_sparse.h
+-rw-r--r--   0 root         (0) root         (0)     6124 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/testbed_splitk.h
+-rw-r--r--   0 root         (0) root         (0)    19861 2023-04-16 04:29:29.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/testbed_symm_universal.h
+-rw-r--r--   0 root         (0) root         (0)    20200 2023-04-16 04:29:29.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/testbed_trmm_universal.h
+-rw-r--r--   0 root         (0) root         (0)    17311 2023-04-16 04:29:29.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/testbed_universal.h
+-rw-r--r--   0 root         (0) root         (0)     2626 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/testbed_utils.h
+-rw-r--r--   0 root         (0) root         (0)     9916 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/trmm_cf32n_cf32n_cf32t_tensor_op_f32_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)     9988 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/trmm_cf32n_cf32n_cf32t_tensor_op_fast_f32_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)     4977 2023-04-16 04:29:29.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/trmm_cf64_cf64_cf64_tensor_op_f64_sm90.cu
+-rw-r--r--   0 root         (0) root         (0)     4992 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/trmm_cf64n_cf64n_cf64t_tensor_op_f64_gaussian_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)     9762 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/trmm_cf64n_cf64n_cf64t_tensor_op_f64_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)    15614 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/trmm_f32n_f32t_f32t_tensor_op_fast_f32_ls_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)     8733 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/trmm_f32n_f32t_f32t_tensor_op_fast_f32_rs_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)    14089 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/trmm_f32t_f32n_f32n_tensor_op_fast_f32_ls_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)    14444 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/trmm_f32t_f32n_f32t_tensor_op_fast_f32_ls_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)     4596 2023-04-16 04:29:29.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/trmm_f64_f64_f64_tensor_op_f64_sm90.cu
+-rw-r--r--   0 root         (0) root         (0)    12798 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/trmm_f64n_f64n_f64t_tensor_op_f64_ls_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)    12809 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/trmm_f64n_f64n_f64t_tensor_op_f64_rs_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)    12764 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/trmm_f64n_f64t_f64t_tensor_op_f64_rs_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)    12768 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/trmm_f64t_f64t_f64n_tensor_op_f64_ls_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)    12779 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/trmm_f64t_f64t_f64n_tensor_op_f64_rs_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)    15504 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/trmm_tf32n_tf32t_f32t_tensor_op_f32_ls_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)     8673 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/trmm_tf32n_tf32t_f32t_tensor_op_f32_rs_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)    13989 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/trmm_tf32t_tf32n_f32n_tensor_op_f32_ls_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)    14344 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/trmm_tf32t_tf32n_f32t_tensor_op_f32_ls_sm80.cu
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-16 05:00:18.723418 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/kernel/
+-rwxr-xr-x   0 root         (0) root         (0)    46470 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/kernel/batched_gemv.cu
+-rwxr-xr-x   0 root         (0) root         (0)    14362 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/kernel/testbed_gemv.h
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-16 05:00:18.806846 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/thread/
+-rw-r--r--   0 root         (0) root         (0)     4847 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/thread/gemm_sm50.cu
+-rw-r--r--   0 root         (0) root         (0)    12503 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/thread/gemm_sm60.cu
+-rw-r--r--   0 root         (0) root         (0)     3109 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/thread/gemm_sm61.cu
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-16 05:00:19.039068 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/thread/host/
+-rw-r--r--   0 root         (0) root         (0)     5198 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/thread/host/gemm_sm60_host.cu
+-rw-r--r--   0 root         (0) root         (0)     7161 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/thread/host/testbed_host.h
+-rw-r--r--   0 root         (0) root         (0)     7124 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/thread/testbed.h
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-16 05:00:19.456702 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/threadblock/
+-rw-r--r--   0 root         (0) root         (0)    25036 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/threadblock/batched_gemv.cu
+-rw-r--r--   0 root         (0) root         (0)     4345 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/threadblock/epilogue_workspace.cu
+-rw-r--r--   0 root         (0) root         (0)   135045 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/threadblock/mma_multistage.cu
+-rw-r--r--   0 root         (0) root         (0)     4644 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/threadblock/mma_multistage_slicedk.cu
+-rw-r--r--   0 root         (0) root         (0)    94442 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/threadblock/mma_multistage_sparse.cu
+-rw-r--r--   0 root         (0) root         (0)    17109 2023-04-16 04:29:29.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/threadblock/mma_multistage_sparse_testbed.h
+-rw-r--r--   0 root         (0) root         (0)    13131 2023-04-16 04:29:29.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/threadblock/mma_multistage_testbed.h
+-rw-r--r--   0 root         (0) root         (0)    14539 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/threadblock/mma_multistage_testbed_slicedk.h
+-rw-r--r--   0 root         (0) root         (0)    49052 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/threadblock/mma_pipelined_simt.cu
+-rw-r--r--   0 root         (0) root         (0)     8407 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/threadblock/mma_pipelined_slicedk.cu
+-rw-r--r--   0 root         (0) root         (0)    18705 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/threadblock/mma_pipelined_sm70.cu
+-rw-r--r--   0 root         (0) root         (0)    78122 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/threadblock/mma_pipelined_sm75.cu
+-rw-r--r--   0 root         (0) root         (0)    21051 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/threadblock/mma_pipelined_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)    13413 2023-04-16 04:29:29.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/threadblock/mma_pipelined_testbed.h
+-rw-r--r--   0 root         (0) root         (0)    14239 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/threadblock/mma_pipelined_testbed_slicedk.h
+-rw-r--r--   0 root         (0) root         (0)    29772 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/threadblock/mma_pipelined_wmma_sm70.cu
+-rw-r--r--   0 root         (0) root         (0)    12395 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/threadblock/mma_pipelined_wmma_sm75.cu
+-rw-r--r--   0 root         (0) root         (0)     3502 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/threadblock/mma_planar_complex_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)    12138 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/threadblock/mma_planar_complex_testbed.h
+-rw-r--r--   0 root         (0) root         (0)    16308 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/threadblock/mma_singlestage_wmma_sm70.cu
+-rw-r--r--   0 root         (0) root         (0)    12502 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/threadblock/mma_singlestage_wmma_sm75.cu
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-16 05:00:19.906021 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/warp/
+-rw-r--r--   0 root         (0) root         (0)    22128 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/warp/gemm_complex_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)    10904 2023-04-16 04:29:29.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/warp/gemm_complex_sm90.cu
+-rw-r--r--   0 root         (0) root         (0)     9873 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/warp/gemm_gaussian_complex_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)    18220 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/warp/gemm_sm50.cu
+-rw-r--r--   0 root         (0) root         (0)     4920 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/warp/gemm_sm60.cu
+-rw-r--r--   0 root         (0) root         (0)     6291 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/warp/gemm_sm61.cu
+-rw-r--r--   0 root         (0) root         (0)     9297 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/warp/gemm_sm70.cu
+-rw-r--r--   0 root         (0) root         (0)    37942 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/warp/gemm_sm75.cu
+-rw-r--r--   0 root         (0) root         (0)    81659 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/warp/gemm_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)     9077 2023-04-16 04:29:29.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/warp/gemm_sm90.cu
+-rw-r--r--   0 root         (0) root         (0)    48928 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/warp/gemm_sparse_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)    45327 2023-04-16 04:29:29.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/warp/testbed.h
+-rw-r--r--   0 root         (0) root         (0)    25780 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/warp/wmma_sm70.cu
+-rw-r--r--   0 root         (0) root         (0)     7544 2023-04-15 23:28:02.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/warp/wmma_sm72.cu
+-rw-r--r--   0 root         (0) root         (0)     6487 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/warp/wmma_sm75.cu
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-16 05:00:19.956899 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/layout/
+-rw-r--r--   0 root         (0) root         (0)     5788 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/layout/matrix.cu
+-rw-r--r--   0 root         (0) root         (0)     5984 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/layout/tensor.cu
+-rw-r--r--   0 root         (0) root         (0)     7081 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/layout/tensor_nhwc.cu
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-16 04:59:39.466508 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/nvrtc/
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-16 04:59:39.449918 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/nvrtc/cutlass/
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-16 05:00:19.983610 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/nvrtc/cutlass/nvrtc/
+-rw-r--r--   0 root         (0) root         (0)     2096 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/nvrtc/cutlass/nvrtc/environment.h
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-16 04:59:39.461483 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/nvrtc/kernel/
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-16 05:00:20.007470 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/nvrtc/kernel/thread/
+-rw-r--r--   0 root         (0) root         (0)     2915 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/nvrtc/kernel/thread/testbed_kernel.h
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-16 05:00:20.040414 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/nvrtc/stdlib/
+-rw-rw-r--   0 root         (0) root         (0)        0 2022-06-02 16:47:41.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/nvrtc/stdlib/assert.h
+-rw-r--r--   0 root         (0) root         (0)     4250 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/nvrtc/stdlib/stdint.h
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-16 05:00:20.075406 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/nvrtc/thread/
+-rw-r--r--   0 root         (0) root         (0)     5727 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/nvrtc/thread/gemm_nvrtc.cu
+-rw-r--r--   0 root         (0) root         (0)    10328 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/nvrtc/thread/testbed.h
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-16 05:00:20.265015 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/pipeline/
+-rw-r--r--   0 root         (0) root         (0)    15532 2023-04-15 23:28:02.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/pipeline/pipeline_async.cu
+-rw-r--r--   0 root         (0) root         (0)    15490 2023-04-15 23:28:02.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/pipeline/pipeline_tma_async.cu
+-rw-r--r--   0 root         (0) root         (0)    17098 2023-04-15 23:28:02.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/pipeline/pipeline_tma_async_warp_specialized.cu
+-rw-r--r--   0 root         (0) root         (0)    20252 2023-04-15 23:28:02.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/pipeline/pipeline_tma_async_warp_specialized_persistent.cu
+-rw-r--r--   0 root         (0) root         (0)     7623 2023-04-15 23:28:03.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/pipeline/sequence_barrier.cu
+-rw-r--r--   0 root         (0) root         (0)     4327 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/pipeline/testbed.h
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-16 04:59:39.497358 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/reduction/
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-16 05:00:20.303146 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/reduction/device/
+-rw-r--r--   0 root         (0) root         (0)    14684 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/reduction/device/tensor_reduce_contiguous.cu
+-rw-r--r--   0 root         (0) root         (0)    15609 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/reduction/device/tensor_reduce_strided.cu
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-16 05:00:20.436539 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/reduction/kernel/
+-rw-r--r--   0 root         (0) root         (0)    11350 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/reduction/kernel/reduce_splitk.cu
+-rw-r--r--   0 root         (0) root         (0)     2228 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/reduction/kernel/reduce_splitk_testbed.h
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-16 05:00:20.470511 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/reduction/thread/
+-rw-r--r--   0 root         (0) root         (0)     3110 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/reduction/thread/reduction_thread.cu
+-rw-r--r--   0 root         (0) root         (0)     6657 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/reduction/thread/testbed.h
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-16 05:00:20.490224 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/substrate/
+-rw-r--r--   0 root         (0) root         (0)     3312 2023-04-15 18:47:44.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/substrate/dependent_false.cpp
+-rw-r--r--   0 root         (0) root         (0)     2047 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/test_unit.cpp
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-16 04:59:39.513525 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/transform/
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-16 05:00:20.529005 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/transform/threadblock/
+-rw-r--r--   0 root         (0) root         (0)    25527 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/transform/threadblock/predicated_tile_iterator.cu
+-rw-r--r--   0 root         (0) root         (0)     9501 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/transform/threadblock/regular_tile_iterator_tensor_op.cu
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-16 05:00:20.662068 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/util/
+-rw-r--r--   0 root         (0) root         (0)     2663 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/util/cutlass_test_levels.cu
+-rw-r--r--   0 root         (0) root         (0)     7474 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/util/tensor_reduce.cu
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-16 04:59:39.666687 flash_attn-1.0.2/csrc/flash_attn/cutlass/tools/
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-16 04:59:39.629284 flash_attn-1.0.2/csrc/flash_attn/cutlass/tools/library/
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-16 04:59:39.543610 flash_attn-1.0.2/csrc/flash_attn/cutlass/tools/library/include/
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-16 04:59:39.549358 flash_attn-1.0.2/csrc/flash_attn/cutlass/tools/library/include/cutlass/
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-16 05:00:20.788616 flash_attn-1.0.2/csrc/flash_attn/cutlass/tools/library/include/cutlass/library/
+-rw-r--r--   0 root         (0) root         (0)     4118 2023-04-15 23:28:03.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/tools/library/include/cutlass/library/arch_mappings.h
+-rw-r--r--   0 root         (0) root         (0)    16013 2023-04-15 23:28:03.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/tools/library/include/cutlass/library/handle.h
+-rw-r--r--   0 root         (0) root         (0)    38340 2023-04-16 04:29:29.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/tools/library/include/cutlass/library/library.h
+-rw-r--r--   0 root         (0) root         (0)     4070 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/tools/library/include/cutlass/library/manifest.h
+-rw-r--r--   0 root         (0) root         (0)    17934 2023-04-15 23:28:03.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/tools/library/include/cutlass/library/operation_table.h
+-rw-r--r--   0 root         (0) root         (0)     2724 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/tools/library/include/cutlass/library/singleton.h
+-rw-r--r--   0 root         (0) root         (0)     7904 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/tools/library/include/cutlass/library/util.h
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-16 04:59:39.562390 flash_attn-1.0.2/csrc/flash_attn/cutlass/tools/library/scripts/
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-16 04:59:39.568065 flash_attn-1.0.2/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-16 04:59:39.573680 flash_attn-1.0.2/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-16 05:00:20.860163 flash_attn-1.0.2/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/
+-rw-r--r--   0 root         (0) root         (0)     2788 2023-04-15 23:28:03.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/compiler.h
+-rw-r--r--   0 root         (0) root         (0)     2460 2023-04-15 23:28:03.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/cute.cpp
+-rw-r--r--   0 root         (0) root         (0)     6223 2023-04-16 04:29:29.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/cutlass.cpp
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-16 05:00:20.938670 flash_attn-1.0.2/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/include/
+-rw-r--r--   0 root         (0) root         (0)     2851 2023-04-15 23:28:03.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/include/arch.h
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-16 05:00:20.987775 flash_attn-1.0.2/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/include/conv/
+-rw-r--r--   0 root         (0) root         (0)     5897 2023-04-15 23:28:03.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/include/conv/conv_problem_size.h
+-rw-r--r--   0 root         (0) root         (0)     4763 2023-04-15 23:28:03.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/include/conv/convolution.h
+-rw-r--r--   0 root         (0) root         (0)     2650 2023-04-15 23:28:03.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/include/conv/host.h
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-16 05:00:21.119380 flash_attn-1.0.2/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/include/epilogue/
+-rw-r--r--   0 root         (0) root         (0)     6844 2023-04-16 04:29:29.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/include/epilogue/epilogue_visitor_generic.h
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-16 05:00:21.402101 flash_attn-1.0.2/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/include/epilogue/epilogue_visitor_op/
+-rw-r--r--   0 root         (0) root         (0)     3047 2023-04-16 04:29:29.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/include/epilogue/epilogue_visitor_op/binary_ops.h
+-rw-r--r--   0 root         (0) root         (0)     6134 2023-04-16 04:29:29.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/include/epilogue/epilogue_visitor_op/unary_ops.h
+-rw-r--r--   0 root         (0) root         (0)     4702 2023-04-16 04:29:29.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/include/epilogue/epilogue_visitor_op/visitor_op_accumulator.h
+-rw-r--r--   0 root         (0) root         (0)     8467 2023-04-16 04:29:29.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/include/epilogue/epilogue_visitor_op/visitor_op_binary.h
+-rw-r--r--   0 root         (0) root         (0)     8815 2023-04-16 04:29:29.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/include/epilogue/epilogue_visitor_op/visitor_op_column_broadcast.h
+-rw-r--r--   0 root         (0) root         (0)    13049 2023-04-16 04:29:29.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/include/epilogue/epilogue_visitor_op/visitor_op_column_reduction.h
+-rw-r--r--   0 root         (0) root         (0)     9493 2023-04-16 04:29:29.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/include/epilogue/epilogue_visitor_op/visitor_op_linear_combination.h
+-rw-r--r--   0 root         (0) root         (0)     9070 2023-04-16 04:29:29.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/include/epilogue/epilogue_visitor_op/visitor_op_row_broadcast.h
+-rw-r--r--   0 root         (0) root         (0)    12029 2023-04-16 04:29:29.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/include/epilogue/epilogue_visitor_op/visitor_op_row_reduction.h
+-rw-r--r--   0 root         (0) root         (0)     6177 2023-04-15 23:28:03.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/include/epilogue/epilogue_visitor_op/visitor_op_tensor_input.h
+-rw-r--r--   0 root         (0) root         (0)     8017 2023-04-15 23:28:03.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/include/epilogue/epilogue_visitor_op/visitor_op_tensor_output.h
+-rw-r--r--   0 root         (0) root         (0)     7235 2023-04-16 04:29:29.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/include/epilogue/epilogue_visitor_op/visitor_op_unary.h
+-rw-r--r--   0 root         (0) root         (0)    16970 2023-04-15 23:28:03.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/include/epilogue/epilogue_visitor_with_layernorm.h
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-16 05:00:21.451797 flash_attn-1.0.2/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/include/gemm/
+-rw-r--r--   0 root         (0) root         (0)     3673 2023-04-15 23:28:03.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/include/gemm/gemm.h
+-rw-r--r--   0 root         (0) root         (0)    22378 2023-04-16 04:29:29.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/include/gemm/gemm_universal_with_visitor.h
+-rw-r--r--   0 root         (0) root         (0)     2328 2023-04-15 23:28:03.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/include/gemm/host.h
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-16 05:00:21.600289 flash_attn-1.0.2/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/include/layout/
+-rw-r--r--   0 root         (0) root         (0)     2115 2023-04-15 23:28:03.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/include/layout/layout.h
+-rw-r--r--   0 root         (0) root         (0)     4337 2023-04-15 23:28:03.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/include/layout/matrix.h
+-rw-r--r--   0 root         (0) root         (0)     3694 2023-04-15 23:28:03.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/include/layout/tensor.h
+-rw-r--r--   0 root         (0) root         (0)     8624 2023-04-16 04:29:29.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/include/swizzling.h
+-rw-r--r--   0 root         (0) root         (0)     3902 2023-04-15 23:28:03.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/include/tensor_coord.h
+-rw-r--r--   0 root         (0) root         (0)     5563 2023-04-15 23:28:03.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/include/tensor_ref_view.h
+-rw-r--r--   0 root         (0) root         (0)     4855 2023-04-15 23:28:03.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/include/types.h
+-rw-r--r--   0 root         (0) root         (0)      811 2023-04-15 23:28:03.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/library.h
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-16 04:59:39.619304 flash_attn-1.0.2/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/test/
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-16 05:00:21.653381 flash_attn-1.0.2/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/test/conv/
+-rw-r--r--   0 root         (0) root         (0)     2651 2023-04-15 23:28:03.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/test/conv/conv_problems.h
+-rw-r--r--   0 root         (0) root         (0)     2253 2023-04-15 23:28:03.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/test/conv/convolution.h
+-rw-r--r--   0 root         (0) root         (0)     8826 2023-04-15 23:28:03.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/test/conv/host.h
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-16 05:00:21.799456 flash_attn-1.0.2/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/test/gemm/
+-rw-r--r--   0 root         (0) root         (0)     2139 2023-04-15 23:28:03.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/test/gemm/gemm.h
+-rw-r--r--   0 root         (0) root         (0)    18930 2023-04-15 23:28:03.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/test/gemm/host.h
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-16 05:00:22.113349 flash_attn-1.0.2/csrc/flash_attn/cutlass/tools/library/src/
+-rw-r--r--   0 root         (0) root         (0)    22377 2023-04-15 23:28:03.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/tools/library/src/conv2d_operation.h
+-rw-r--r--   0 root         (0) root         (0)    13851 2023-04-15 23:28:03.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/tools/library/src/conv3d_operation.h
+-rw-r--r--   0 root         (0) root         (0)    42129 2023-04-15 23:28:03.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/tools/library/src/gemm_operation.h
+-rw-r--r--   0 root         (0) root         (0)    35709 2023-04-16 04:29:29.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/tools/library/src/handle.cu
+-rw-r--r--   0 root         (0) root         (0)    12616 2023-04-15 23:28:03.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/tools/library/src/library_internal.h
+-rw-r--r--   0 root         (0) root         (0)     3782 2023-04-15 23:28:03.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/tools/library/src/manifest.cpp
+-rw-r--r--   0 root         (0) root         (0)     5468 2023-04-15 23:28:03.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/tools/library/src/operation_table.cu
+-rw-r--r--   0 root         (0) root         (0)    12873 2023-04-15 23:28:03.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/tools/library/src/rank_2k_operation.h
+-rw-r--r--   0 root         (0) root         (0)    11367 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/tools/library/src/rank_k_operation.h
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-16 05:00:22.261300 flash_attn-1.0.2/csrc/flash_attn/cutlass/tools/library/src/reduction/
+-rw-r--r--   0 root         (0) root         (0)     3190 2023-04-15 23:28:03.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/tools/library/src/reduction/init_reduction_operations.cu
+-rw-r--r--   0 root         (0) root         (0)     6367 2023-04-15 23:28:03.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/tools/library/src/reduction/reduction_device.cu
+-rw-r--r--   0 root         (0) root         (0)    10270 2023-04-15 23:28:03.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/tools/library/src/reduction/reduction_operation.h
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-16 05:00:22.360617 flash_attn-1.0.2/csrc/flash_attn/cutlass/tools/library/src/reference/
+-rw-r--r--   0 root         (0) root         (0)     6746 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/tools/library/src/reference/conv2d.cu
+-rw-r--r--   0 root         (0) root         (0)     6286 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/tools/library/src/reference/conv3d.cu
+-rw-r--r--   0 root         (0) root         (0)    17191 2023-04-15 23:28:03.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/tools/library/src/reference/conv_reference_operation.h
+-rw-r--r--   0 root         (0) root         (0)     7199 2023-04-15 23:28:03.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/tools/library/src/reference/gemm.cu
+-rw-r--r--   0 root         (0) root         (0)    14732 2023-04-15 23:28:03.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/tools/library/src/reference/gemm_reference_operation.h
+-rw-r--r--   0 root         (0) root         (0)     2857 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/tools/library/src/reference/initialize_reference_operations.cu
+-rw-r--r--   0 root         (0) root         (0)     2669 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/tools/library/src/singleton.cu
+-rw-r--r--   0 root         (0) root         (0)    13134 2023-04-15 23:28:03.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/tools/library/src/symm_operation.h
+-rw-r--r--   0 root         (0) root         (0)    11698 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/tools/library/src/trmm_operation.h
+-rw-r--r--   0 root         (0) root         (0)    43704 2023-04-15 23:28:03.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/tools/library/src/util.cu
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-16 04:59:39.657508 flash_attn-1.0.2/csrc/flash_attn/cutlass/tools/profiler/
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-16 05:00:23.612282 flash_attn-1.0.2/csrc/flash_attn/cutlass/tools/profiler/src/
+-rw-r--r--   0 root         (0) root         (0)    54128 2023-04-15 23:28:03.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/tools/profiler/src/conv2d_operation_profiler.cu
+-rw-r--r--   0 root         (0) root         (0)    18170 2023-04-15 23:28:03.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/tools/profiler/src/conv2d_operation_profiler.h
+-rw-r--r--   0 root         (0) root         (0)    48659 2023-04-15 23:28:03.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/tools/profiler/src/conv3d_operation_profiler.cu
+-rw-r--r--   0 root         (0) root         (0)    16043 2023-04-15 23:28:03.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/tools/profiler/src/conv3d_operation_profiler.h
+-rw-r--r--   0 root         (0) root         (0)    36462 2023-04-15 23:28:03.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/tools/profiler/src/cublas_helpers.cu
+-rw-r--r--   0 root         (0) root         (0)    10627 2023-04-15 23:28:03.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/tools/profiler/src/cublas_helpers.h
+-rw-r--r--   0 root         (0) root         (0)    17049 2023-04-15 23:28:03.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/tools/profiler/src/cudnn_helpers.cpp
+-rw-r--r--   0 root         (0) root         (0)    20433 2023-04-15 23:28:03.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/tools/profiler/src/cudnn_helpers.h
+-rw-r--r--   0 root         (0) root         (0)     7233 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/tools/profiler/src/cutlass_profiler.cu
+-rw-r--r--   0 root         (0) root         (0)     3233 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/tools/profiler/src/cutlass_profiler.h
+-rw-r--r--   0 root         (0) root         (0)     2453 2023-04-15 23:28:03.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/tools/profiler/src/debug.h
+-rw-r--r--   0 root         (0) root         (0)    53643 2023-04-15 23:28:03.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/tools/profiler/src/device_allocation.cu
+-rw-r--r--   0 root         (0) root         (0)     7217 2023-04-15 23:28:03.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/tools/profiler/src/device_allocation.h
+-rw-r--r--   0 root         (0) root         (0)     6841 2023-04-15 23:28:03.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/tools/profiler/src/device_context.cu
+-rw-r--r--   0 root         (0) root         (0)     4300 2023-04-15 23:28:03.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/tools/profiler/src/device_context.h
+-rw-r--r--   0 root         (0) root         (0)     8296 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/tools/profiler/src/enumerated_types.cpp
+-rw-r--r--   0 root         (0) root         (0)     6421 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/tools/profiler/src/enumerated_types.h
+-rw-r--r--   0 root         (0) root         (0)    41919 2023-04-16 04:29:29.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/tools/profiler/src/gemm_operation_profiler.cu
+-rw-r--r--   0 root         (0) root         (0)     8544 2023-04-15 23:28:03.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/tools/profiler/src/gemm_operation_profiler.h
+-rw-r--r--   0 root         (0) root         (0)     3874 2023-04-15 23:28:03.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/tools/profiler/src/gpu_timer.cpp
+-rw-r--r--   0 root         (0) root         (0)     2724 2023-04-15 23:28:03.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/tools/profiler/src/gpu_timer.h
+-rw-r--r--   0 root         (0) root         (0)     2340 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/tools/profiler/src/main.cpp
+-rw-r--r--   0 root         (0) root         (0)    20944 2023-04-16 04:29:29.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/tools/profiler/src/operation_profiler.cu
+-rw-r--r--   0 root         (0) root         (0)     7876 2023-04-15 23:28:03.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/tools/profiler/src/operation_profiler.h
+-rw-r--r--   0 root         (0) root         (0)    27172 2023-04-15 23:28:03.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/tools/profiler/src/options.cu
+-rw-r--r--   0 root         (0) root         (0)     8773 2023-04-15 23:28:03.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/tools/profiler/src/options.h
+-rw-r--r--   0 root         (0) root         (0)    14192 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/tools/profiler/src/performance_report.cpp
+-rw-r--r--   0 root         (0) root         (0)     4337 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/tools/profiler/src/performance_report.h
+-rw-r--r--   0 root         (0) root         (0)     2494 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/tools/profiler/src/performance_result.cu
+-rw-r--r--   0 root         (0) root         (0)     3941 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/tools/profiler/src/performance_result.h
+-rw-r--r--   0 root         (0) root         (0)    37487 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/tools/profiler/src/problem_space.cpp
+-rw-r--r--   0 root         (0) root         (0)    27747 2023-04-15 23:28:03.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/tools/profiler/src/problem_space.h
+-rw-r--r--   0 root         (0) root         (0)    25014 2023-04-15 23:28:03.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/tools/profiler/src/rank_2k_operation_profiler.cu
+-rw-r--r--   0 root         (0) root         (0)     6891 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/tools/profiler/src/rank_2k_operation_profiler.h
+-rw-r--r--   0 root         (0) root         (0)    24253 2023-04-15 23:28:03.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/tools/profiler/src/rank_k_operation_profiler.cu
+-rw-r--r--   0 root         (0) root         (0)     6830 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/tools/profiler/src/rank_k_operation_profiler.h
+-rw-r--r--   0 root         (0) root         (0)     5452 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/tools/profiler/src/reduction_operation_profiler.h
+-rw-r--r--   0 root         (0) root         (0)    20688 2023-04-15 23:28:03.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/tools/profiler/src/sparse_gemm_operation_profiler.cu
+-rw-r--r--   0 root         (0) root         (0)     6471 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/tools/profiler/src/sparse_gemm_operation_profiler.h
+-rw-r--r--   0 root         (0) root         (0)    26610 2023-04-15 23:28:03.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/tools/profiler/src/symm_operation_profiler.cu
+-rw-r--r--   0 root         (0) root         (0)     6933 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/tools/profiler/src/symm_operation_profiler.h
+-rw-r--r--   0 root         (0) root         (0)    24431 2023-04-15 23:28:03.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/tools/profiler/src/trmm_operation_profiler.cu
+-rw-r--r--   0 root         (0) root         (0)     6599 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/tools/profiler/src/trmm_operation_profiler.h
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-16 04:59:39.672401 flash_attn-1.0.2/csrc/flash_attn/cutlass/tools/util/
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-16 04:59:39.678017 flash_attn-1.0.2/csrc/flash_attn/cutlass/tools/util/include/
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-16 04:59:39.683584 flash_attn-1.0.2/csrc/flash_attn/cutlass/tools/util/include/cutlass/
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-16 05:00:24.225760 flash_attn-1.0.2/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/
+-rw-r--r--   0 root         (0) root         (0)     9774 2023-04-15 23:28:03.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/command_line.h
+-rw-r--r--   0 root         (0) root         (0)     5104 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/debug.h
+-rw-r--r--   0 root         (0) root         (0)     5953 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/device_dump.h
+-rw-r--r--   0 root         (0) root         (0)    17695 2023-04-15 23:28:03.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/device_groupnorm.h
+-rw-r--r--   0 root         (0) root         (0)    20880 2023-04-16 04:29:29.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/device_layernorm.h
+-rw-r--r--   0 root         (0) root         (0)    10561 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/device_memory.h
+-rw-r--r--   0 root         (0) root         (0)     5219 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/device_nchw_to_nhwc.h
+-rw-r--r--   0 root         (0) root         (0)    11067 2023-04-15 23:28:03.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/device_nhwc_padding.h
+-rw-r--r--   0 root         (0) root         (0)    18653 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/device_nhwc_pooling.h
+-rw-r--r--   0 root         (0) root         (0)     5214 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/device_nhwc_to_nchw.h
+-rw-r--r--   0 root         (0) root         (0)     4007 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/device_utils.h
+-rw-r--r--   0 root         (0) root         (0)     4597 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/distribution.h
+-rw-r--r--   0 root         (0) root         (0)     2674 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/exceptions.h
+-rw-r--r--   0 root         (0) root         (0)     4821 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/host_reorder.h
+-rw-r--r--   0 root         (0) root         (0)    16745 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/host_tensor.h
+-rw-r--r--   0 root         (0) root         (0)    20354 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/host_tensor_planar_complex.h
+-rw-r--r--   0 root         (0) root         (0)     5890 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/host_uncompress.h
+-rw-r--r--   0 root         (0) root         (0)     1962 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/index_sequence.h
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-16 04:59:39.717580 flash_attn-1.0.2/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-16 05:00:24.362123 flash_attn-1.0.2/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/detail/
+-rw-r--r--   0 root         (0) root         (0)     4606 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/detail/inner_product.h
+-rw-r--r--   0 root         (0) root         (0)     3527 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/detail/linear_to_coordinate.h
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-16 05:00:24.931703 flash_attn-1.0.2/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/device/
+-rw-r--r--   0 root         (0) root         (0)    48350 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/device/convolution.h
+-rw-r--r--   0 root         (0) root         (0)    14296 2023-04-15 23:28:03.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/device/gemm.h
+-rw-r--r--   0 root         (0) root         (0)    10524 2023-04-15 23:28:03.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/device/gemm_complex.h
+-rw-r--r--   0 root         (0) root         (0)     9652 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/device/gemm_planar_complex.h
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-16 05:00:25.086838 flash_attn-1.0.2/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/device/kernel/
+-rw-r--r--   0 root         (0) root         (0)     5381 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/device/kernel/gemm.h
+-rw-r--r--   0 root         (0) root         (0)     6198 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/device/kernel/tensor_elementwise.h
+-rw-r--r--   0 root         (0) root         (0)     5126 2023-04-15 23:28:03.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/device/kernel/tensor_foreach.h
+-rw-r--r--   0 root         (0) root         (0)    11615 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/device/rank_2k_complex.h
+-rw-r--r--   0 root         (0) root         (0)     7278 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/device/tensor_compare.h
+-rw-r--r--   0 root         (0) root         (0)    46444 2023-04-15 23:28:03.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/device/tensor_fill.h
+-rw-r--r--   0 root         (0) root         (0)     5293 2023-04-15 23:28:03.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/device/tensor_foreach.h
+-rw-r--r--   0 root         (0) root         (0)    15964 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/device/tensor_reduce.h
+-rw-r--r--   0 root         (0) root         (0)     4589 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/device/tensor_relu.h
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-16 05:00:25.107200 flash_attn-1.0.2/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/device/thread/
+-rw-r--r--   0 root         (0) root         (0)     5872 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/device/thread/gemm.h
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-16 05:00:25.589849 flash_attn-1.0.2/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/host/
+-rw-r--r--   0 root         (0) root         (0)    28439 2023-04-15 23:28:03.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/host/convolution.h
+-rw-r--r--   0 root         (0) root         (0)     2766 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/host/error_metrics.h
+-rw-r--r--   0 root         (0) root         (0)    17163 2023-04-15 23:28:03.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/host/gemm.h
+-rw-r--r--   0 root         (0) root         (0)     7097 2023-04-15 23:28:03.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/host/gemm_complex.h
+-rw-r--r--   0 root         (0) root         (0)     7708 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/host/gemm_planar_complex.h
+-rw-r--r--   0 root         (0) root         (0)     9441 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/host/rank_2k.h
+-rw-r--r--   0 root         (0) root         (0)    11444 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/host/rank_2k_complex.h
+-rw-r--r--   0 root         (0) root         (0)     8148 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/host/rank_k_complex.h
+-rw-r--r--   0 root         (0) root         (0)    10509 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/host/symm.h
+-rw-r--r--   0 root         (0) root         (0)    12296 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/host/symm_complex.h
+-rw-r--r--   0 root         (0) root         (0)     8440 2023-04-15 23:28:03.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/host/tensor_compare.h
+-rw-r--r--   0 root         (0) root         (0)     8317 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/host/tensor_copy.h
+-rw-r--r--   0 root         (0) root         (0)     9027 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/host/tensor_elementwise.h
+-rw-r--r--   0 root         (0) root         (0)    43961 2023-04-15 23:28:03.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/host/tensor_fill.h
+-rw-r--r--   0 root         (0) root         (0)     4756 2023-04-15 23:28:03.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/host/tensor_foreach.h
+-rw-r--r--   0 root         (0) root         (0)     2133 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/host/tensor_norm.h
+-rw-r--r--   0 root         (0) root         (0)     6111 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/host/tensor_reduce.h
+-rw-r--r--   0 root         (0) root         (0)     7670 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/host/trmm.h
+-rw-r--r--   0 root         (0) root         (0)     9874 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/host/trmm_complex.h
+-rw-r--r--   0 root         (0) root         (0)     8285 2023-04-15 23:28:03.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/tensor_view_io.h
+-rw-r--r--   0 root         (0) root         (0)     8809 2023-04-12 22:58:14.000000 flash_attn-1.0.2/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/type_traits.h
+-rw-r--r--   0 root         (0) root         (0)    32526 2023-04-16 00:20:27.000000 flash_attn-1.0.2/csrc/flash_attn/flash_api.cpp
+-rw-r--r--   0 root         (0) root         (0)    33206 2023-04-16 04:28:21.000000 flash_attn-1.0.2/csrc/flash_attn/fmha_api.cpp
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-16 05:00:27.194308 flash_attn-1.0.2/csrc/flash_attn/src/
+-rw-r--r--   0 root         (0) root         (0)     1664 2023-04-16 00:20:27.000000 flash_attn-1.0.2/csrc/flash_attn/src/block_info.h
+-rw-r--r--   0 root         (0) root         (0)     3931 2023-04-16 00:20:27.000000 flash_attn-1.0.2/csrc/flash_attn/src/flash.h
+-rw-r--r--   0 root         (0) root         (0)      983 2023-04-07 15:25:56.000000 flash_attn-1.0.2/csrc/flash_attn/src/flash_bwd_hdim128.cu
+-rw-r--r--   0 root         (0) root         (0)      574 2023-04-16 00:20:27.000000 flash_attn-1.0.2/csrc/flash_attn/src/flash_bwd_hdim128_bf16_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)     1124 2023-04-16 00:20:27.000000 flash_attn-1.0.2/csrc/flash_attn/src/flash_bwd_hdim128_fp16_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)      983 2023-04-07 15:25:56.000000 flash_attn-1.0.2/csrc/flash_attn/src/flash_bwd_hdim128_sm80_fp16.cu
+-rw-r--r--   0 root         (0) root         (0)      425 2023-04-07 15:25:56.000000 flash_attn-1.0.2/csrc/flash_attn/src/flash_bwd_hdim32.cu
+-rw-r--r--   0 root         (0) root         (0)      450 2023-04-16 00:20:27.000000 flash_attn-1.0.2/csrc/flash_attn/src/flash_bwd_hdim32_bf16_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)      442 2023-04-16 00:20:27.000000 flash_attn-1.0.2/csrc/flash_attn/src/flash_bwd_hdim32_fp16_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)      425 2023-04-07 15:25:56.000000 flash_attn-1.0.2/csrc/flash_attn/src/flash_bwd_hdim32_sm80_fp16.cu
+-rw-r--r--   0 root         (0) root         (0)     2074 2023-04-07 15:25:56.000000 flash_attn-1.0.2/csrc/flash_attn/src/flash_bwd_hdim64.cu
+-rw-r--r--   0 root         (0) root         (0)      450 2023-04-16 00:20:27.000000 flash_attn-1.0.2/csrc/flash_attn/src/flash_bwd_hdim64_bf16_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)     2458 2023-04-16 00:20:27.000000 flash_attn-1.0.2/csrc/flash_attn/src/flash_bwd_hdim64_fp16_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)     2074 2023-04-07 15:25:56.000000 flash_attn-1.0.2/csrc/flash_attn/src/flash_bwd_hdim64_sm80_fp16.cu
+-rw-r--r--   0 root         (0) root         (0)      581 2023-04-07 15:25:56.000000 flash_attn-1.0.2/csrc/flash_attn/src/flash_bwd_hdim96.cu
+-rw-r--r--   0 root         (0) root         (0)      449 2023-04-16 00:20:27.000000 flash_attn-1.0.2/csrc/flash_attn/src/flash_bwd_hdim96_bf16_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)      598 2023-04-16 00:20:27.000000 flash_attn-1.0.2/csrc/flash_attn/src/flash_bwd_hdim96_fp16_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)      581 2023-04-07 15:25:56.000000 flash_attn-1.0.2/csrc/flash_attn/src/flash_bwd_hdim96_sm80_fp16.cu
+-rw-r--r--   0 root         (0) root         (0)    83190 2023-04-16 00:20:27.000000 flash_attn-1.0.2/csrc/flash_attn/src/flash_bwd_kernel.h
+-rw-r--r--   0 root         (0) root         (0)    41051 2023-03-25 22:30:12.000000 flash_attn-1.0.2/csrc/flash_attn/src/flash_bwd_kernel_bak.h
+-rw-r--r--   0 root         (0) root         (0)    39628 2023-03-25 21:25:30.000000 flash_attn-1.0.2/csrc/flash_attn/src/flash_bwd_kernel_new.h
+-rw-r--r--   0 root         (0) root         (0)    39403 2023-04-08 17:10:55.000000 flash_attn-1.0.2/csrc/flash_attn/src/flash_bwd_kernel_reverse.h
+-rw-r--r--   0 root         (0) root         (0)     8911 2023-04-16 00:20:27.000000 flash_attn-1.0.2/csrc/flash_attn/src/flash_bwd_launch_template.h
+-rw-r--r--   0 root         (0) root         (0)      645 2023-04-07 21:42:50.000000 flash_attn-1.0.2/csrc/flash_attn/src/flash_fwd_hdim128.cu
+-rw-r--r--   0 root         (0) root         (0)      592 2023-04-16 00:20:27.000000 flash_attn-1.0.2/csrc/flash_attn/src/flash_fwd_hdim128_bf16_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)      662 2023-04-16 00:20:27.000000 flash_attn-1.0.2/csrc/flash_attn/src/flash_fwd_hdim128_fp16_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)      662 2023-04-08 06:50:03.000000 flash_attn-1.0.2/csrc/flash_attn/src/flash_fwd_hdim128_sm80_fp16.cu
+-rw-r--r--   0 root         (0) root         (0)      990 2023-04-07 18:39:34.000000 flash_attn-1.0.2/csrc/flash_attn/src/flash_fwd_hdim160.cu
+-rw-r--r--   0 root         (0) root         (0)      495 2023-04-16 00:20:27.000000 flash_attn-1.0.2/csrc/flash_attn/src/flash_fwd_hdim160_bf16_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)     1007 2023-04-16 00:20:27.000000 flash_attn-1.0.2/csrc/flash_attn/src/flash_fwd_hdim160_fp16_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)     1007 2023-04-08 06:50:03.000000 flash_attn-1.0.2/csrc/flash_attn/src/flash_fwd_hdim160_sm80_fp16.cu
+-rw-r--r--   0 root         (0) root         (0)     1041 2023-04-07 18:40:27.000000 flash_attn-1.0.2/csrc/flash_attn/src/flash_fwd_hdim192.cu
+-rw-r--r--   0 root         (0) root         (0)      494 2023-04-16 00:20:27.000000 flash_attn-1.0.2/csrc/flash_attn/src/flash_fwd_hdim192_bf16_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)     1058 2023-04-16 00:20:27.000000 flash_attn-1.0.2/csrc/flash_attn/src/flash_fwd_hdim192_fp16_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)     1058 2023-04-08 06:50:03.000000 flash_attn-1.0.2/csrc/flash_attn/src/flash_fwd_hdim192_sm80_fp16.cu
+-rw-r--r--   0 root         (0) root         (0)      945 2023-04-07 18:37:45.000000 flash_attn-1.0.2/csrc/flash_attn/src/flash_fwd_hdim32.cu
+-rw-r--r--   0 root         (0) root         (0)      494 2023-04-16 00:20:27.000000 flash_attn-1.0.2/csrc/flash_attn/src/flash_fwd_hdim32_bf16_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)      962 2023-04-16 00:20:27.000000 flash_attn-1.0.2/csrc/flash_attn/src/flash_fwd_hdim32_fp16_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)      962 2023-04-08 06:50:03.000000 flash_attn-1.0.2/csrc/flash_attn/src/flash_fwd_hdim32_sm80_fp16.cu
+-rw-r--r--   0 root         (0) root         (0)     1149 2023-04-07 22:02:41.000000 flash_attn-1.0.2/csrc/flash_attn/src/flash_fwd_hdim64.cu
+-rw-r--r--   0 root         (0) root         (0)      588 2023-04-16 00:20:27.000000 flash_attn-1.0.2/csrc/flash_attn/src/flash_fwd_hdim64_bf16_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)     1166 2023-04-16 00:20:27.000000 flash_attn-1.0.2/csrc/flash_attn/src/flash_fwd_hdim64_fp16_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)     1166 2023-04-08 06:50:03.000000 flash_attn-1.0.2/csrc/flash_attn/src/flash_fwd_hdim64_sm80_fp16.cu
+-rw-r--r--   0 root         (0) root         (0)      711 2023-04-07 22:17:50.000000 flash_attn-1.0.2/csrc/flash_attn/src/flash_fwd_hdim96.cu
+-rw-r--r--   0 root         (0) root         (0)      492 2023-04-16 00:20:27.000000 flash_attn-1.0.2/csrc/flash_attn/src/flash_fwd_hdim96_bf16_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)      728 2023-04-16 00:20:27.000000 flash_attn-1.0.2/csrc/flash_attn/src/flash_fwd_hdim96_fp16_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)      728 2023-04-08 06:50:03.000000 flash_attn-1.0.2/csrc/flash_attn/src/flash_fwd_hdim96_sm80_fp16.cu
+-rw-r--r--   0 root         (0) root         (0)    30072 2023-04-16 00:20:27.000000 flash_attn-1.0.2/csrc/flash_attn/src/flash_fwd_kernel.h
+-rw-r--r--   0 root         (0) root         (0)    27253 2023-04-08 17:10:55.000000 flash_attn-1.0.2/csrc/flash_attn/src/flash_fwd_kernel_old.h
+-rw-r--r--   0 root         (0) root         (0)     2564 2023-04-16 00:20:27.000000 flash_attn-1.0.2/csrc/flash_attn/src/flash_fwd_launch_template.h
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-16 05:00:27.309216 flash_attn-1.0.2/csrc/flash_attn/src/fmha/
+-rw-r--r--   0 root         (0) root         (0)    17999 2023-04-16 00:48:36.000000 flash_attn-1.0.2/csrc/flash_attn/src/fmha/gemm.h
+-rw-r--r--   0 root         (0) root         (0)    22872 2023-04-16 00:48:36.000000 flash_attn-1.0.2/csrc/flash_attn/src/fmha/gmem_tile.h
+-rw-r--r--   0 root         (0) root         (0)     5997 2023-04-16 00:48:36.000000 flash_attn-1.0.2/csrc/flash_attn/src/fmha/kernel_traits.h
+-rw-r--r--   0 root         (0) root         (0)     4362 2023-04-16 00:48:36.000000 flash_attn-1.0.2/csrc/flash_attn/src/fmha/mask.h
+-rw-r--r--   0 root         (0) root         (0)    74010 2023-04-16 00:48:36.000000 flash_attn-1.0.2/csrc/flash_attn/src/fmha/smem_tile.h
+-rw-r--r--   0 root         (0) root         (0)    25514 2023-04-16 00:48:36.000000 flash_attn-1.0.2/csrc/flash_attn/src/fmha/softmax.h
+-rw-r--r--   0 root         (0) root         (0)    41059 2023-04-16 00:48:36.000000 flash_attn-1.0.2/csrc/flash_attn/src/fmha/utils.h
+-rw-r--r--   0 root         (0) root         (0)     7237 2023-04-16 04:28:21.000000 flash_attn-1.0.2/csrc/flash_attn/src/fmha.h
+-rw-r--r--   0 root         (0) root         (0)     4118 2023-04-16 00:48:36.000000 flash_attn-1.0.2/csrc/flash_attn/src/fmha_block_dgrad_fp16_kernel_loop.sm80.cu
+-rw-r--r--   0 root         (0) root         (0)    33506 2023-04-16 00:48:36.000000 flash_attn-1.0.2/csrc/flash_attn/src/fmha_block_dgrad_kernel_1xN_loop.h
+-rw-r--r--   0 root         (0) root         (0)     5292 2023-04-16 00:48:36.000000 flash_attn-1.0.2/csrc/flash_attn/src/fmha_block_fprop_fp16_kernel.sm80.cu
+-rw-r--r--   0 root         (0) root         (0)    23207 2023-04-16 00:48:36.000000 flash_attn-1.0.2/csrc/flash_attn/src/fmha_block_fprop_kernel_1xN.h
+-rw-r--r--   0 root         (0) root         (0)     2502 2023-04-16 00:48:36.000000 flash_attn-1.0.2/csrc/flash_attn/src/fmha_blockmask.h
+-rw-r--r--   0 root         (0) root         (0)      465 2023-04-16 00:48:36.000000 flash_attn-1.0.2/csrc/flash_attn/src/fmha_bwd_hdim128.cu
+-rw-r--r--   0 root         (0) root         (0)      727 2023-04-16 00:48:36.000000 flash_attn-1.0.2/csrc/flash_attn/src/fmha_bwd_hdim32.cu
+-rw-r--r--   0 root         (0) root         (0)     1713 2023-04-16 00:48:36.000000 flash_attn-1.0.2/csrc/flash_attn/src/fmha_bwd_hdim64.cu
+-rw-r--r--   0 root         (0) root         (0)     6453 2023-04-16 00:48:36.000000 flash_attn-1.0.2/csrc/flash_attn/src/fmha_bwd_launch_template.h
+-rw-r--r--   0 root         (0) root         (0)    37168 2023-04-16 04:28:21.000000 flash_attn-1.0.2/csrc/flash_attn/src/fmha_dgrad_kernel_1xN_loop.h
+-rw-r--r--   0 root         (0) root         (0)    31042 2023-04-16 04:28:21.000000 flash_attn-1.0.2/csrc/flash_attn/src/fmha_fprop_kernel_1xN.h
+-rw-r--r--   0 root         (0) root         (0)      445 2023-04-16 00:48:36.000000 flash_attn-1.0.2/csrc/flash_attn/src/fmha_fwd_hdim128.cu
+-rw-r--r--   0 root         (0) root         (0)      724 2023-04-16 00:48:36.000000 flash_attn-1.0.2/csrc/flash_attn/src/fmha_fwd_hdim32.cu
+-rw-r--r--   0 root         (0) root         (0)      725 2023-04-16 00:48:36.000000 flash_attn-1.0.2/csrc/flash_attn/src/fmha_fwd_hdim64.cu
+-rw-r--r--   0 root         (0) root         (0)     4393 2023-04-16 00:48:36.000000 flash_attn-1.0.2/csrc/flash_attn/src/fmha_fwd_launch_template.h
+-rw-r--r--   0 root         (0) root         (0)     3104 2023-04-16 00:48:36.000000 flash_attn-1.0.2/csrc/flash_attn/src/fmha_kernel.h
+-rw-r--r--   0 root         (0) root         (0)     4892 2023-04-16 00:48:36.000000 flash_attn-1.0.2/csrc/flash_attn/src/fmha_utils.h
+-rw-r--r--   0 root         (0) root         (0)    17820 2023-04-16 00:20:27.000000 flash_attn-1.0.2/csrc/flash_attn/src/kernel_traits.h
+-rw-r--r--   0 root         (0) root         (0)     2927 2023-03-31 21:49:35.000000 flash_attn-1.0.2/csrc/flash_attn/src/mask.h
+-rw-r--r--   0 root         (0) root         (0)     5462 2023-04-16 00:48:36.000000 flash_attn-1.0.2/csrc/flash_attn/src/philox.cuh
+-rw-r--r--   0 root         (0) root         (0)    14205 2023-04-16 00:20:27.000000 flash_attn-1.0.2/csrc/flash_attn/src/softmax.h
+-rw-r--r--   0 root         (0) root         (0)     1686 2023-04-16 00:48:36.000000 flash_attn-1.0.2/csrc/flash_attn/src/static_switch.h
+-rw-r--r--   0 root         (0) root         (0)    16378 2023-04-16 00:20:27.000000 flash_attn-1.0.2/csrc/flash_attn/src/utils.h
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-16 05:00:27.428122 flash_attn-1.0.2/csrc/flash_gen/
+-rw-r--r--   0 root         (0) root         (0)     7018 2022-11-21 06:35:03.000000 flash_attn-1.0.2/csrc/flash_gen/decoder_masked_multihead_attention.cu
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-16 05:00:27.521743 flash_attn-1.0.2/csrc/ft_attention/
+-rw-r--r--   0 root         (0) root         (0)     8253 2023-04-16 00:48:36.000000 flash_attn-1.0.2/csrc/ft_attention/cuda_bf16_fallbacks.cuh
+-rw-r--r--   0 root         (0) root         (0)      867 2023-04-16 00:48:36.000000 flash_attn-1.0.2/csrc/ft_attention/cuda_bf16_wrapper.h
+-rw-r--r--   0 root         (0) root         (0)     7243 2023-04-16 00:48:36.000000 flash_attn-1.0.2/csrc/ft_attention/decoder_masked_multihead_attention.cu
+-rw-r--r--   0 root         (0) root         (0)     7463 2023-04-16 00:48:36.000000 flash_attn-1.0.2/csrc/ft_attention/decoder_masked_multihead_attention.h
+-rw-r--r--   0 root         (0) root         (0)    52690 2023-04-16 00:48:36.000000 flash_attn-1.0.2/csrc/ft_attention/decoder_masked_multihead_attention_utils.h
+-rw-r--r--   0 root         (0) root         (0)     7423 2023-04-16 00:48:36.000000 flash_attn-1.0.2/csrc/ft_attention/ft_attention.cpp
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-16 05:00:27.556961 flash_attn-1.0.2/csrc/fused_dense_lib/
+-rw-r--r--   0 root         (0) root         (0)     8215 2023-04-16 00:48:36.000000 flash_attn-1.0.2/csrc/fused_dense_lib/fused_dense.cpp
+-rw-r--r--   0 root         (0) root         (0)    25273 2023-04-16 00:48:36.000000 flash_attn-1.0.2/csrc/fused_dense_lib/fused_dense_cuda.cu
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-16 05:00:27.749639 flash_attn-1.0.2/csrc/fused_softmax/
+-rw-r--r--   0 root         (0) root         (0)     5037 2023-04-16 00:48:36.000000 flash_attn-1.0.2/csrc/fused_softmax/fused_softmax.cpp
+-rw-r--r--   0 root         (0) root         (0)    23616 2023-04-16 00:48:36.000000 flash_attn-1.0.2/csrc/fused_softmax/scaled_masked_softmax.h
+-rw-r--r--   0 root         (0) root         (0)     4209 2023-04-16 00:48:36.000000 flash_attn-1.0.2/csrc/fused_softmax/scaled_masked_softmax_cuda.cu
+-rw-r--r--   0 root         (0) root         (0)    24659 2023-04-16 00:48:36.000000 flash_attn-1.0.2/csrc/fused_softmax/scaled_upper_triang_masked_softmax.h
+-rw-r--r--   0 root         (0) root         (0)     3154 2023-04-16 00:48:36.000000 flash_attn-1.0.2/csrc/fused_softmax/scaled_upper_triang_masked_softmax_cuda.cu
+-rw-r--r--   0 root         (0) root         (0)     1216 2023-04-16 00:48:36.000000 flash_attn-1.0.2/csrc/fused_softmax/type_shim.h
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-16 05:00:29.066153 flash_attn-1.0.2/csrc/layer_norm/
+-rw-r--r--   0 root         (0) root         (0)     7248 2023-04-16 00:48:36.000000 flash_attn-1.0.2/csrc/layer_norm/ln.h
+-rw-r--r--   0 root         (0) root         (0)    36418 2023-04-16 00:48:36.000000 flash_attn-1.0.2/csrc/layer_norm/ln_api.cpp
+-rw-r--r--   0 root         (0) root         (0)      987 2023-04-16 00:48:36.000000 flash_attn-1.0.2/csrc/layer_norm/ln_bwd_1024.cu
+-rw-r--r--   0 root         (0) root         (0)      987 2023-04-16 00:48:36.000000 flash_attn-1.0.2/csrc/layer_norm/ln_bwd_1280.cu
+-rw-r--r--   0 root         (0) root         (0)      977 2023-04-16 00:48:36.000000 flash_attn-1.0.2/csrc/layer_norm/ln_bwd_1536.cu
+-rw-r--r--   0 root         (0) root         (0)      976 2023-04-16 00:48:37.000000 flash_attn-1.0.2/csrc/layer_norm/ln_bwd_2048.cu
+-rw-r--r--   0 root         (0) root         (0)      977 2023-04-16 00:48:37.000000 flash_attn-1.0.2/csrc/layer_norm/ln_bwd_256.cu
+-rw-r--r--   0 root         (0) root         (0)      977 2023-04-16 00:48:37.000000 flash_attn-1.0.2/csrc/layer_norm/ln_bwd_2560.cu
+-rw-r--r--   0 root         (0) root         (0)      976 2023-04-16 00:48:37.000000 flash_attn-1.0.2/csrc/layer_norm/ln_bwd_3072.cu
+-rw-r--r--   0 root         (0) root         (0)      976 2023-04-16 00:48:37.000000 flash_attn-1.0.2/csrc/layer_norm/ln_bwd_4096.cu
+-rw-r--r--   0 root         (0) root         (0)      977 2023-04-16 00:48:37.000000 flash_attn-1.0.2/csrc/layer_norm/ln_bwd_512.cu
+-rw-r--r--   0 root         (0) root         (0)      976 2023-04-16 00:48:37.000000 flash_attn-1.0.2/csrc/layer_norm/ln_bwd_5120.cu
+-rw-r--r--   0 root         (0) root         (0)      976 2023-04-16 00:48:37.000000 flash_attn-1.0.2/csrc/layer_norm/ln_bwd_6144.cu
+-rw-r--r--   0 root         (0) root         (0)      976 2023-04-16 00:48:37.000000 flash_attn-1.0.2/csrc/layer_norm/ln_bwd_7168.cu
+-rw-r--r--   0 root         (0) root         (0)      977 2023-04-16 00:48:37.000000 flash_attn-1.0.2/csrc/layer_norm/ln_bwd_768.cu
+-rw-r--r--   0 root         (0) root         (0)      976 2023-04-16 00:48:37.000000 flash_attn-1.0.2/csrc/layer_norm/ln_bwd_8192.cu
+-rw-r--r--   0 root         (0) root         (0)    25647 2023-04-16 00:48:37.000000 flash_attn-1.0.2/csrc/layer_norm/ln_bwd_kernels.cuh
+-rw-r--r--   0 root         (0) root         (0)    19944 2023-01-19 07:34:02.000000 flash_attn-1.0.2/csrc/layer_norm/ln_bwd_semi_cuda_kernel_old.cu
+-rw-r--r--   0 root         (0) root         (0)      925 2023-04-16 00:48:37.000000 flash_attn-1.0.2/csrc/layer_norm/ln_fwd_1024.cu
+-rw-r--r--   0 root         (0) root         (0)      925 2023-01-22 09:05:55.000000 flash_attn-1.0.2/csrc/layer_norm/ln_fwd_10240.cu
+-rw-r--r--   0 root         (0) root         (0)      925 2023-01-22 09:07:15.000000 flash_attn-1.0.2/csrc/layer_norm/ln_fwd_12288.cu
+-rw-r--r--   0 root         (0) root         (0)      925 2022-12-05 08:45:58.000000 flash_attn-1.0.2/csrc/layer_norm/ln_fwd_128.cu
+-rw-r--r--   0 root         (0) root         (0)      925 2023-04-16 00:48:37.000000 flash_attn-1.0.2/csrc/layer_norm/ln_fwd_1280.cu
+-rw-r--r--   0 root         (0) root         (0)      925 2023-04-16 00:48:37.000000 flash_attn-1.0.2/csrc/layer_norm/ln_fwd_1536.cu
+-rw-r--r--   0 root         (0) root         (0)      925 2023-04-16 00:48:37.000000 flash_attn-1.0.2/csrc/layer_norm/ln_fwd_2048.cu
+-rw-r--r--   0 root         (0) root         (0)      925 2023-04-16 00:48:37.000000 flash_attn-1.0.2/csrc/layer_norm/ln_fwd_256.cu
+-rw-r--r--   0 root         (0) root         (0)      925 2023-04-16 00:48:37.000000 flash_attn-1.0.2/csrc/layer_norm/ln_fwd_2560.cu
+-rw-r--r--   0 root         (0) root         (0)      925 2023-04-16 00:48:37.000000 flash_attn-1.0.2/csrc/layer_norm/ln_fwd_3072.cu
+-rw-r--r--   0 root         (0) root         (0)      925 2022-12-05 08:50:57.000000 flash_attn-1.0.2/csrc/layer_norm/ln_fwd_384.cu
+-rw-r--r--   0 root         (0) root         (0)      925 2023-04-16 00:48:37.000000 flash_attn-1.0.2/csrc/layer_norm/ln_fwd_4096.cu
+-rw-r--r--   0 root         (0) root         (0)      925 2023-04-16 00:48:37.000000 flash_attn-1.0.2/csrc/layer_norm/ln_fwd_512.cu
+-rw-r--r--   0 root         (0) root         (0)      925 2023-04-16 00:48:37.000000 flash_attn-1.0.2/csrc/layer_norm/ln_fwd_5120.cu
+-rw-r--r--   0 root         (0) root         (0)      925 2023-04-16 00:48:37.000000 flash_attn-1.0.2/csrc/layer_norm/ln_fwd_6144.cu
+-rw-r--r--   0 root         (0) root         (0)      925 2023-04-16 00:48:37.000000 flash_attn-1.0.2/csrc/layer_norm/ln_fwd_7168.cu
+-rw-r--r--   0 root         (0) root         (0)      925 2023-04-16 00:48:37.000000 flash_attn-1.0.2/csrc/layer_norm/ln_fwd_768.cu
+-rw-r--r--   0 root         (0) root         (0)      925 2023-04-16 00:48:37.000000 flash_attn-1.0.2/csrc/layer_norm/ln_fwd_8192.cu
+-rw-r--r--   0 root         (0) root         (0)      925 2023-01-22 08:41:06.000000 flash_attn-1.0.2/csrc/layer_norm/ln_fwd_9216.cu
+-rw-r--r--   0 root         (0) root         (0)    18000 2022-12-06 21:18:58.000000 flash_attn-1.0.2/csrc/layer_norm/ln_fwd_cuda_kernel_old.cu
+-rw-r--r--   0 root         (0) root         (0)    12721 2023-04-16 00:48:37.000000 flash_attn-1.0.2/csrc/layer_norm/ln_fwd_kernels.cuh
+-rw-r--r--   0 root         (0) root         (0)     6655 2023-04-16 00:48:37.000000 flash_attn-1.0.2/csrc/layer_norm/ln_kernel_traits.h
+-rw-r--r--   0 root         (0) root         (0)     1095 2023-04-16 00:48:37.000000 flash_attn-1.0.2/csrc/layer_norm/ln_parallel_bwd_1024.cu
+-rw-r--r--   0 root         (0) root         (0)     1095 2023-04-16 00:48:37.000000 flash_attn-1.0.2/csrc/layer_norm/ln_parallel_bwd_1280.cu
+-rw-r--r--   0 root         (0) root         (0)     1085 2023-04-16 00:48:37.000000 flash_attn-1.0.2/csrc/layer_norm/ln_parallel_bwd_1536.cu
+-rw-r--r--   0 root         (0) root         (0)     1084 2023-04-16 00:48:37.000000 flash_attn-1.0.2/csrc/layer_norm/ln_parallel_bwd_2048.cu
+-rw-r--r--   0 root         (0) root         (0)     1085 2023-04-16 00:48:37.000000 flash_attn-1.0.2/csrc/layer_norm/ln_parallel_bwd_256.cu
+-rw-r--r--   0 root         (0) root         (0)     1085 2023-04-16 00:48:37.000000 flash_attn-1.0.2/csrc/layer_norm/ln_parallel_bwd_2560.cu
+-rw-r--r--   0 root         (0) root         (0)     1084 2023-04-16 00:48:37.000000 flash_attn-1.0.2/csrc/layer_norm/ln_parallel_bwd_3072.cu
+-rw-r--r--   0 root         (0) root         (0)     1145 2023-04-16 00:48:37.000000 flash_attn-1.0.2/csrc/layer_norm/ln_parallel_bwd_4096.cu
+-rw-r--r--   0 root         (0) root         (0)     1085 2023-04-16 00:48:37.000000 flash_attn-1.0.2/csrc/layer_norm/ln_parallel_bwd_512.cu
+-rw-r--r--   0 root         (0) root         (0)     1145 2023-04-16 00:48:37.000000 flash_attn-1.0.2/csrc/layer_norm/ln_parallel_bwd_5120.cu
+-rw-r--r--   0 root         (0) root         (0)     1084 2023-04-16 00:48:37.000000 flash_attn-1.0.2/csrc/layer_norm/ln_parallel_bwd_6144.cu
+-rw-r--r--   0 root         (0) root         (0)     1084 2023-04-16 00:48:37.000000 flash_attn-1.0.2/csrc/layer_norm/ln_parallel_bwd_7168.cu
+-rw-r--r--   0 root         (0) root         (0)     1085 2023-04-16 00:48:37.000000 flash_attn-1.0.2/csrc/layer_norm/ln_parallel_bwd_768.cu
+-rw-r--r--   0 root         (0) root         (0)     1084 2023-04-16 00:48:37.000000 flash_attn-1.0.2/csrc/layer_norm/ln_parallel_bwd_8192.cu
+-rw-r--r--   0 root         (0) root         (0)     1033 2023-04-16 00:48:37.000000 flash_attn-1.0.2/csrc/layer_norm/ln_parallel_fwd_1024.cu
+-rw-r--r--   0 root         (0) root         (0)     1033 2023-03-29 20:52:04.000000 flash_attn-1.0.2/csrc/layer_norm/ln_parallel_fwd_128.cu
+-rw-r--r--   0 root         (0) root         (0)     1033 2023-04-16 00:48:37.000000 flash_attn-1.0.2/csrc/layer_norm/ln_parallel_fwd_1280.cu
+-rw-r--r--   0 root         (0) root         (0)     1033 2023-04-16 00:48:37.000000 flash_attn-1.0.2/csrc/layer_norm/ln_parallel_fwd_1536.cu
+-rw-r--r--   0 root         (0) root         (0)     1033 2023-04-16 00:48:37.000000 flash_attn-1.0.2/csrc/layer_norm/ln_parallel_fwd_2048.cu
+-rw-r--r--   0 root         (0) root         (0)     1032 2023-04-16 00:48:37.000000 flash_attn-1.0.2/csrc/layer_norm/ln_parallel_fwd_256.cu
+-rw-r--r--   0 root         (0) root         (0)     1033 2023-04-16 00:48:37.000000 flash_attn-1.0.2/csrc/layer_norm/ln_parallel_fwd_2560.cu
+-rw-r--r--   0 root         (0) root         (0)     1033 2023-04-16 00:48:37.000000 flash_attn-1.0.2/csrc/layer_norm/ln_parallel_fwd_3072.cu
+-rw-r--r--   0 root         (0) root         (0)     1033 2023-04-16 00:48:37.000000 flash_attn-1.0.2/csrc/layer_norm/ln_parallel_fwd_4096.cu
+-rw-r--r--   0 root         (0) root         (0)     1033 2023-04-16 00:48:37.000000 flash_attn-1.0.2/csrc/layer_norm/ln_parallel_fwd_512.cu
+-rw-r--r--   0 root         (0) root         (0)     1033 2023-04-16 00:48:37.000000 flash_attn-1.0.2/csrc/layer_norm/ln_parallel_fwd_5120.cu
+-rw-r--r--   0 root         (0) root         (0)     1033 2023-04-16 00:48:37.000000 flash_attn-1.0.2/csrc/layer_norm/ln_parallel_fwd_6144.cu
+-rw-r--r--   0 root         (0) root         (0)     1033 2023-04-16 00:48:37.000000 flash_attn-1.0.2/csrc/layer_norm/ln_parallel_fwd_7168.cu
+-rw-r--r--   0 root         (0) root         (0)     1033 2023-04-16 00:48:37.000000 flash_attn-1.0.2/csrc/layer_norm/ln_parallel_fwd_768.cu
+-rw-r--r--   0 root         (0) root         (0)     1033 2023-04-16 00:48:37.000000 flash_attn-1.0.2/csrc/layer_norm/ln_parallel_fwd_8192.cu
+-rw-r--r--   0 root         (0) root         (0)    11720 2023-03-29 19:53:46.000000 flash_attn-1.0.2/csrc/layer_norm/ln_parallel_res_fwd_kernel.cuh
+-rw-r--r--   0 root         (0) root         (0)      977 2023-03-27 04:43:57.000000 flash_attn-1.0.2/csrc/layer_norm/ln_parallel_residual_bwd_512.cu
+-rw-r--r--   0 root         (0) root         (0)    24916 2023-04-16 00:48:37.000000 flash_attn-1.0.2/csrc/layer_norm/ln_parallel_residual_bwd_kernels.cuh
+-rw-r--r--   0 root         (0) root         (0)    11515 2023-03-29 20:50:46.000000 flash_attn-1.0.2/csrc/layer_norm/ln_parallel_residual_fwd_kernel.cuh
+-rw-r--r--   0 root         (0) root         (0)    12530 2023-04-16 00:48:37.000000 flash_attn-1.0.2/csrc/layer_norm/ln_parallel_residual_fwd_kernels.cuh
+-rw-r--r--   0 root         (0) root         (0)    29989 2023-04-16 00:48:37.000000 flash_attn-1.0.2/csrc/layer_norm/ln_utils.cuh
+-rw-r--r--   0 root         (0) root         (0)     1278 2023-04-16 00:48:37.000000 flash_attn-1.0.2/csrc/layer_norm/static_switch.h
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-16 05:00:29.101748 flash_attn-1.0.2/csrc/rotary/
+-rw-r--r--   0 root         (0) root         (0)     1806 2023-04-16 00:48:37.000000 flash_attn-1.0.2/csrc/rotary/rotary.cpp
+-rw-r--r--   0 root         (0) root         (0)     1984 2023-04-16 00:48:37.000000 flash_attn-1.0.2/csrc/rotary/rotary_cuda.cu
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-16 05:00:29.138894 flash_attn-1.0.2/csrc/xentropy/
+-rw-r--r--   0 root         (0) root         (0)     2290 2023-04-16 00:48:37.000000 flash_attn-1.0.2/csrc/xentropy/interface.cpp
+-rw-r--r--   0 root         (0) root         (0)    25783 2023-04-16 00:48:37.000000 flash_attn-1.0.2/csrc/xentropy/xentropy_kernel.cu
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-16 05:00:29.460695 flash_attn-1.0.2/flash_attn/
+-rw-r--r--   0 root         (0) root         (0)        0 2022-07-04 00:53:37.000000 flash_attn-1.0.2/flash_attn/__init__.py
+-rw-rw-r--   0 root         (0) root         (0)    20845 2022-10-31 02:25:05.000000 flash_attn-1.0.2/flash_attn/attention_kernl.py
+-rw-r--r--   0 root         (0) root         (0)     5898 2023-04-16 00:48:37.000000 flash_attn-1.0.2/flash_attn/bert_padding.py
+-rw-r--r--   0 root         (0) root         (0)     4722 2023-04-16 00:48:37.000000 flash_attn-1.0.2/flash_attn/flash_attention.py
+-rw-r--r--   0 root         (0) root         (0)    20506 2023-04-16 04:28:21.000000 flash_attn-1.0.2/flash_attn/flash_attn_interface.py
+-rw-r--r--   0 root         (0) root         (0)    38148 2023-04-16 00:48:37.000000 flash_attn-1.0.2/flash_attn/flash_attn_triton.py
+-rw-r--r--   0 root         (0) root         (0)    10593 2023-04-16 00:48:37.000000 flash_attn-1.0.2/flash_attn/flash_attn_triton_og.py
+-rw-r--r--   0 root         (0) root         (0)     8255 2022-11-18 03:30:00.000000 flash_attn-1.0.2/flash_attn/flash_attn_triton_single_query.py
+-rw-r--r--   0 root         (0) root         (0)    37797 2023-03-17 09:16:10.000000 flash_attn-1.0.2/flash_attn/flash_attn_triton_tmp.py
+-rw-r--r--   0 root         (0) root         (0)    10640 2023-03-12 08:48:14.000000 flash_attn-1.0.2/flash_attn/flash_attn_triton_tmp_og.py
+-rw-rw-r--   0 root         (0) root         (0)    22919 2022-10-31 00:28:55.000000 flash_attn-1.0.2/flash_attn/flash_attn_triton_varlen.py
+-rw-r--r--   0 root         (0) root         (0)     6819 2022-06-26 00:59:43.000000 flash_attn-1.0.2/flash_attn/flash_blocksparse_attention.py
+-rw-r--r--   0 root         (0) root         (0)     7036 2022-06-26 00:59:43.000000 flash_attn-1.0.2/flash_attn/flash_blocksparse_attn_interface.py
+-rw-r--r--   0 root         (0) root         (0)     7902 2023-04-16 00:48:37.000000 flash_attn-1.0.2/flash_attn/fused_softmax.py
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-16 05:00:29.590150 flash_attn-1.0.2/flash_attn/layers/
+-rw-r--r--   0 root         (0) root         (0)        0 2023-04-16 00:48:37.000000 flash_attn-1.0.2/flash_attn/layers/__init__.py
+-rw-r--r--   0 root         (0) root         (0)     2039 2023-04-16 00:48:37.000000 flash_attn-1.0.2/flash_attn/layers/patch_embed.py
+-rw-r--r--   0 root         (0) root         (0)    10656 2023-04-16 00:48:37.000000 flash_attn-1.0.2/flash_attn/layers/rotary.py
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-16 05:00:29.652230 flash_attn-1.0.2/flash_attn/losses/
+-rw-r--r--   0 root         (0) root         (0)        0 2023-04-16 00:48:37.000000 flash_attn-1.0.2/flash_attn/losses/__init__.py
+-rw-r--r--   0 root         (0) root         (0)     6697 2023-04-16 00:48:37.000000 flash_attn-1.0.2/flash_attn/losses/cross_entropy.py
+-rw-r--r--   0 root         (0) root         (0)     2122 2022-12-18 05:19:38.000000 flash_attn-1.0.2/flash_attn/losses/cross_entropy_apex.py
+-rw-r--r--   0 root         (0) root         (0)     6649 2022-12-23 22:38:19.000000 flash_attn-1.0.2/flash_attn/losses/cross_entropy_parallel.py
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-16 05:00:29.874289 flash_attn-1.0.2/flash_attn/models/
+-rw-r--r--   0 root         (0) root         (0)        0 2023-04-16 00:48:37.000000 flash_attn-1.0.2/flash_attn/models/__init__.py
+-rw-r--r--   0 root         (0) root         (0)    26630 2023-04-16 00:48:37.000000 flash_attn-1.0.2/flash_attn/models/bert.py
+-rw-r--r--   0 root         (0) root         (0)    35019 2023-04-16 00:48:37.000000 flash_attn-1.0.2/flash_attn/models/gpt.py
+-rw-r--r--   0 root         (0) root         (0)     4863 2023-03-22 21:08:53.000000 flash_attn-1.0.2/flash_attn/models/gpt_j.py
+-rw-r--r--   0 root         (0) root         (0)     5025 2023-04-16 00:48:37.000000 flash_attn-1.0.2/flash_attn/models/gpt_neox.py
+-rw-r--r--   0 root         (0) root         (0)     4387 2023-04-16 00:48:37.000000 flash_attn-1.0.2/flash_attn/models/gptj.py
+-rw-r--r--   0 root         (0) root         (0)     5174 2023-04-16 00:48:37.000000 flash_attn-1.0.2/flash_attn/models/opt.py
+-rw-r--r--   0 root         (0) root         (0)    13621 2023-04-16 00:48:37.000000 flash_attn-1.0.2/flash_attn/models/vit.py
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-16 05:00:29.952741 flash_attn-1.0.2/flash_attn/modules/
+-rw-r--r--   0 root         (0) root         (0)        0 2023-04-16 00:48:37.000000 flash_attn-1.0.2/flash_attn/modules/__init__.py
+-rw-r--r--   0 root         (0) root         (0)    15244 2023-04-16 00:48:37.000000 flash_attn-1.0.2/flash_attn/modules/block.py
+-rw-r--r--   0 root         (0) root         (0)     8620 2023-04-16 00:48:37.000000 flash_attn-1.0.2/flash_attn/modules/embedding.py
+-rw-r--r--   0 root         (0) root         (0)    32514 2023-04-16 00:48:37.000000 flash_attn-1.0.2/flash_attn/modules/mha.py
+-rw-r--r--   0 root         (0) root         (0)     1027 2023-04-16 00:48:37.000000 flash_attn-1.0.2/flash_attn/modules/mlp.py
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-16 05:00:30.046145 flash_attn-1.0.2/flash_attn/ops/
+-rw-r--r--   0 root         (0) root         (0)        0 2023-04-16 00:48:37.000000 flash_attn-1.0.2/flash_attn/ops/__init__.py
+-rw-r--r--   0 root         (0) root         (0)     3002 2023-04-16 00:48:37.000000 flash_attn-1.0.2/flash_attn/ops/activations.py
+-rw-r--r--   0 root         (0) root         (0)    25997 2023-04-16 00:48:37.000000 flash_attn-1.0.2/flash_attn/ops/fused_dense.py
+-rw-r--r--   0 root         (0) root         (0)     2685 2023-04-13 20:57:51.000000 flash_attn-1.0.2/flash_attn/ops/gelu_activation.py
+-rw-r--r--   0 root         (0) root         (0)    18374 2023-04-16 00:48:37.000000 flash_attn-1.0.2/flash_attn/ops/layer_norm.py
+-rw-r--r--   0 root         (0) root         (0)     3159 2023-04-16 00:48:37.000000 flash_attn-1.0.2/flash_attn/ops/rms_norm.py
+-rw-r--r--   0 root         (0) root         (0)     5855 2023-04-16 00:20:27.000000 flash_attn-1.0.2/flash_attn/rotary.py
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-16 05:00:30.081411 flash_attn-1.0.2/flash_attn/triton/
+-rw-rw-r--   0 root         (0) root         (0)        0 2022-11-18 00:51:48.000000 flash_attn-1.0.2/flash_attn/triton/__init__.py
+-rw-rw-r--   0 root         (0) root         (0)    14332 2022-10-23 23:52:09.000000 flash_attn-1.0.2/flash_attn/triton/fused_attention.py
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-16 05:00:30.162575 flash_attn-1.0.2/flash_attn/utils/
+-rw-r--r--   0 root         (0) root         (0)        0 2023-04-16 00:48:37.000000 flash_attn-1.0.2/flash_attn/utils/__init__.py
+-rw-r--r--   0 root         (0) root         (0)     5909 2023-04-16 00:48:37.000000 flash_attn-1.0.2/flash_attn/utils/benchmark.py
+-rw-r--r--   0 root         (0) root         (0)     5545 2023-04-16 00:48:37.000000 flash_attn-1.0.2/flash_attn/utils/distributed.py
+-rw-r--r--   0 root         (0) root         (0)    13645 2023-04-16 00:48:37.000000 flash_attn-1.0.2/flash_attn/utils/generation.py
+-rw-r--r--   0 root         (0) root         (0)     1824 2023-04-16 00:48:37.000000 flash_attn-1.0.2/flash_attn/utils/pretrained.py
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-16 05:00:29.542435 flash_attn-1.0.2/flash_attn.egg-info/
+-rw-rw-r--   0 root         (0) root         (0)    10037 2023-04-16 04:59:28.000000 flash_attn-1.0.2/flash_attn.egg-info/PKG-INFO
+-rw-rw-r--   0 root         (0) root         (0)   118136 2023-04-16 04:59:38.000000 flash_attn-1.0.2/flash_attn.egg-info/SOURCES.txt
+-rw-rw-r--   0 root         (0) root         (0)        1 2023-04-16 04:59:28.000000 flash_attn-1.0.2/flash_attn.egg-info/dependency_links.txt
+-rw-rw-r--   0 root         (0) root         (0)       23 2023-04-16 04:59:28.000000 flash_attn-1.0.2/flash_attn.egg-info/requires.txt
+-rw-rw-r--   0 root         (0) root         (0)       27 2023-04-16 04:59:28.000000 flash_attn-1.0.2/flash_attn.egg-info/top_level.txt
+-rw-rw-r--   0 root         (0) root         (0)       38 2023-04-16 05:00:30.174120 flash_attn-1.0.2/setup.cfg
+-rw-r--r--   0 root         (0) root         (0)     7562 2023-04-16 04:28:34.000000 flash_attn-1.0.2/setup.py
```

### Comparing `flash_attn-1.0.1/LICENSE` & `flash_attn-1.0.2/LICENSE`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.1/PKG-INFO` & `flash_attn-1.0.2/PKG-INFO`

 * *Files 0% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 Metadata-Version: 2.1
 Name: flash_attn
-Version: 1.0.1
+Version: 1.0.2
 Summary: Flash Attention: Fast and Memory-Efficient Exact Attention
 Home-page: https://github.com/HazyResearch/flash-attention
 Author: Tri Dao
 Author-email: trid@stanford.edu
 License: UNKNOWN
 Platform: UNKNOWN
 Classifier: Programming Language :: Python :: 3
```

### Comparing `flash_attn-1.0.1/README.md` & `flash_attn-1.0.2/README.md`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/cmake/nop.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/cmake/nop.cu`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/00_basic_gemm/basic_gemm.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/examples/00_basic_gemm/basic_gemm.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/01_cutlass_utilities/cutlass_utilities.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/examples/01_cutlass_utilities/cutlass_utilities.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/02_dump_reg_shmem/dump_reg_shmem.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/examples/02_dump_reg_shmem/dump_reg_shmem.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/03_visualize_layout/options.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/examples/03_visualize_layout/options.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/03_visualize_layout/register_layout.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/examples/03_visualize_layout/register_layout.cu`

 * *Files 11% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -60,23 +60,23 @@
       {"ColumnMajorInterleaved<4>",
        new VisualizeLayout<cutlass::layout::ColumnMajorInterleaved<4>>},
       {"RowMajorInterleaved<4>",
        new VisualizeLayout<cutlass::layout::RowMajorInterleaved<4>>},
       // All Ampere/Turing H/Integer matrix multiply tensor core kernels uses the same swizzling
       // layout implementation with different templates.
       //
-      // BMMA 88128  Interleaved-256
-      // BMMA 168256 Interleaved-256
+      // mma.sync.aligned.m8n8k128.s32.b1.b1.s32 Interleaved-256
+      // mma.sync.aligned.m16n8k256.s32.b1.b1.s32 Interleaved-256
       {"TensorOpMultiplicand<1,256>",
        new VisualizeLayout<cutlass::layout::TensorOpMultiplicand<1, 256>>},
-      // BMMA 88128  TN kblock512
-      // BMMA 168256 TN kblock512
+      // mma.sync.aligned.m8n8k128.s32.b1.b1.s32 TN kblock512
+      // mma.sync.aligned.m16n8k256.s32.b1.b1.s32 TN kblock512
       {"TensorOpMultiplicand<1,512>",
        new VisualizeLayout<cutlass::layout::TensorOpMultiplicand<1, 512>>},
-      // BMMA 168256 TN kblock1024
+      // mma.sync.aligned.m16n8k256.s32.b1.b1.s32 TN kblock1024
       {"TensorOpMultiplicand<1,1024>",
        new VisualizeLayout<cutlass::layout::TensorOpMultiplicand<1, 1024>>},
       // Integer matrix multiply.int4 8832  Interleaved-64
       // Integer matrix multiply.int4 16864 Interleaved-64
       {"TensorOpMultiplicand<4,64>",
        new VisualizeLayout<cutlass::layout::TensorOpMultiplicand<4, 64>>},
       // Integer matrix multiply.int4 8832  TN kblock128
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/03_visualize_layout/register_layout.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/examples/03_visualize_layout/register_layout.h`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/03_visualize_layout/visualize_layout.cpp` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/examples/03_visualize_layout/visualize_layout.cpp`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/03_visualize_layout/visualize_layout.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/examples/03_visualize_layout/visualize_layout.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/04_tile_iterator/tile_iterator.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/examples/04_tile_iterator/tile_iterator.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/05_batched_gemm/batched_gemm.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/examples/05_batched_gemm/batched_gemm.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -77,28 +77,28 @@
 ---------------------------------------
 (0,4,0) | (0,4,1) | (1,4,0) | (1,4,1) |
 ---------------------------------------
 (0,5,0) | (0,5,1) | (1,5,0) | (1,5,1) |
 ---------------------------------------
      batch 0      |      batch 1
 , where batch size is 2, M is 6 and K is 2
-The stride (batch_stride_B) between the first element of two batches is lda * k
+The stride (batch_stride_A) between the first element of two batches is lda * k
 
 matrix B can be seen as
 -----------------------------
 (0,0,0) | (0,0,1) | (0,0,2) |
 ----------------------------- batch 0
 (0,1,0) | (0,1,1) | (0,1,2) |
 -------------------------------------
 (1,0,0) | (1,0,1) | (1,0,2) |
 ----------------------------- batch 1
 (1,1,0) | (1,1,1) | (1,1,2) |
 -----------------------------
 , where the batch size is 2, N is 3 and K is 2
-The stride (batch_stride_C) between the first element of two batches is k
+The stride (batch_stride_B) between the first element of two batches is k
 
 
 */
 
 cudaError_t cutlass_array_sgemm(
   int m,
   int n,
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/06_splitK_gemm/splitk_gemm.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/examples/06_splitK_gemm/splitk_gemm.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/07_volta_tensorop_gemm/volta_tensorop_gemm.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/examples/07_volta_tensorop_gemm/volta_tensorop_gemm.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/08_turing_tensorop_gemm/turing_tensorop_gemm.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/examples/08_turing_tensorop_gemm/turing_tensorop_gemm.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/09_turing_tensorop_conv2dfprop/turing_tensorop_conv2dfprop.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/examples/09_turing_tensorop_conv2dfprop/turing_tensorop_conv2dfprop.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/10_planar_complex/planar_complex.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/examples/10_planar_complex/planar_complex.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/11_planar_complex_array/planar_complex_array.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/examples/11_planar_complex_array/planar_complex_array.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/12_gemm_bias_relu/gemm_bias_relu.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/examples/12_gemm_bias_relu/gemm_bias_relu.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/b2b_conv2d_run.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/b2b_conv2d_run.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/b2b_gemm_run.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/b2b_gemm_run.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/b2b_interleaved_conv2d_run.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/b2b_interleaved_conv2d_run.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/b2b_interleaved_gemm_run.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/b2b_interleaved_gemm_run.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/device/b2b_gemm.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/device/b2b_gemm.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/device/b2b_implicit_gemm_convolution.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/device/b2b_implicit_gemm_convolution.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/fused_two_convs_f16_sm75_rf.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/fused_two_convs_f16_sm75_rf.cu`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/fused_two_convs_f16_sm75_shmem.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/fused_two_convs_f16_sm75_shmem.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/fused_two_convs_f16_sm80_rf.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/fused_two_convs_f16_sm80_rf.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/fused_two_convs_f16_sm80_shmem.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/fused_two_convs_f16_sm80_shmem.cu`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/fused_two_convs_s8_sm75_rf.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/fused_two_convs_s8_sm75_rf.cu`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/fused_two_convs_s8_sm75_shmem.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/fused_two_convs_s8_sm75_shmem.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/fused_two_convs_s8_sm80_rf.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/fused_two_convs_s8_sm80_rf.cu`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/fused_two_convs_s8_sm80_shmem.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/fused_two_convs_s8_sm80_shmem.cu`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/fused_two_gemms_f16_sm75_rf.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/fused_two_gemms_f16_sm75_rf.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/fused_two_gemms_f16_sm75_shmem.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/fused_two_gemms_f16_sm75_shmem.cu`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/fused_two_gemms_f16_sm80_rf.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/fused_two_gemms_f16_sm80_rf.cu`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/fused_two_gemms_f16_sm80_shmem.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/fused_two_gemms_f16_sm80_shmem.cu`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/fused_two_gemms_s8_sm75_rf.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/fused_two_gemms_s8_sm75_rf.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/fused_two_gemms_s8_sm75_shmem.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/fused_two_gemms_s8_sm75_shmem.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/fused_two_gemms_s8_sm80_rf.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/fused_two_gemms_s8_sm80_rf.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/fused_two_gemms_s8_sm80_shmem.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/fused_two_gemms_s8_sm80_shmem.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/kernel/b2b_gemm.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/kernel/b2b_gemm.h`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/kernel/b2b_implicit_gemm_convolution.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/kernel/b2b_implicit_gemm_convolution.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/kernel/default_b2b_conv2d_fprop.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/kernel/default_b2b_conv2d_fprop.h`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/kernel/default_b2b_conv2d_fprop_sm75.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/kernel/default_b2b_conv2d_fprop_sm75.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/kernel/default_b2b_conv2d_fprop_sm80.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/kernel/default_b2b_conv2d_fprop_sm80.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/kernel/default_b2b_conv2d_fprop_smem_accumulator_sm75.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/kernel/default_b2b_conv2d_fprop_smem_accumulator_sm75.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/kernel/default_b2b_conv2d_fprop_smem_accumulator_sm80.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/kernel/default_b2b_conv2d_fprop_smem_accumulator_sm80.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/kernel/default_b2b_gemm.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/kernel/default_b2b_gemm.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/kernel/default_b2b_gemm_smem_accumulator.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/kernel/default_b2b_gemm_smem_accumulator.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/reference/device/tensor_scale_bias.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/reference/device/tensor_scale_bias.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/test_run.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/test_run.h`

 * *Files 2% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/threadblock/b2b_implicit_gemm_multistage.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/threadblock/b2b_implicit_gemm_multistage.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/threadblock/b2b_implicit_gemm_multistage_smem_accumulator.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/threadblock/b2b_implicit_gemm_multistage_smem_accumulator.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/threadblock/b2b_implicit_gemm_pipelined.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/threadblock/b2b_implicit_gemm_pipelined.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/threadblock/b2b_implicit_gemm_pipelined_smem_accumulator.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/threadblock/b2b_implicit_gemm_pipelined_smem_accumulator.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/threadblock/b2b_mma_base.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/threadblock/b2b_mma_base.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/threadblock/b2b_mma_base_smem_accumulator.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/threadblock/b2b_mma_base_smem_accumulator.h`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/threadblock/b2b_mma_multistage.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/threadblock/b2b_mma_multistage.h`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -195,39 +195,39 @@
                   "The pipelined structure requires at least two warp-level "
                   "GEMM operations.");
     static_assert(Base::kWarpGemmIterations1 > 1,
                   "The pipelined structure requires at least two warp-level "
                   "GEMM operations.");
 
     /// Number of cp.async instructions to load one stage of operand A
-    static int const TBLDGSTSIterationsA0 =
+    static int const TBLoadIterationsA0 =
         IteratorA0::ThreadMap::Iterations::kCount;
 
     /// Number of cp.async instructions to load one stage of operand B
-    static int const TBLDGSTSIterationsB0 =
+    static int const TBLoadIterationsB0 =
         IteratorB0::ThreadMap::Iterations::kCount;
 
     /// Number of cp.async instructions to load one stage of operand B
-    static int const TBLDGSTSIterationsB1 =
+    static int const TBLoadIterationsB1 =
         IteratorB1::ThreadMap::Iterations::kCount;
 
     /// Number of stages
     static int const kStages = Stages;
 
     /// Number of cp.async instructions to load on group of operand A
     static int const kAccessesPerGroupA0 =
-        (TBLDGSTSIterationsA0 + Base::kWarpGemmIterations0 - 1) / Base::kWarpGemmIterations0;
+        (TBLoadIterationsA0 + Base::kWarpGemmIterations0 - 1) / Base::kWarpGemmIterations0;
 
     /// Number of cp.async instructions to load on group of operand B
     static int const kAccessesPerGroupB0 =
-        (TBLDGSTSIterationsB0 + Base::kWarpGemmIterations0 - 1) / Base::kWarpGemmIterations0;
+        (TBLoadIterationsB0 + Base::kWarpGemmIterations0 - 1) / Base::kWarpGemmIterations0;
 
     /// Number of cp.async instructions to load on group of operand B
     static int const kAccessesPerGroupB1 =
-        (TBLDGSTSIterationsB1 + Base::kWarpGemmIterations1 - 1) / Base::kWarpGemmIterations1;
+        (TBLoadIterationsB1 + Base::kWarpGemmIterations1 - 1) / Base::kWarpGemmIterations1;
   };
 
  private:
 
   using WarpLoadedFragmentA0 = typename Operator0::FragmentA;
   using WarpLoadedFragmentB0 = typename Operator0::FragmentB;
   /// Warp Fragment of operand A1 loaded from accmulator tile
@@ -300,18 +300,18 @@
   CUTLASS_DEVICE
   void copy_tiles_and_advance_0(IteratorA0 &iterator_A0, IteratorB0 &iterator_B0,
                               int group_start_A0 = 0, int group_start_B0 = 0) {
     iterator_A0.set_iteration_index(group_start_A0 *
                                    IteratorA0::kAccessesPerVector);
     this->smem_iterator_A0_.set_iteration_index(group_start_A0);
 
-    // LDGSTS for operand A
+    // Load for operand A
     CUTLASS_PRAGMA_UNROLL
     for (int j = 0; j < Detail::kAccessesPerGroupA0; ++j) {
-      if (group_start_A0 + j < Detail::TBLDGSTSIterationsA0) {
+      if (group_start_A0 + j < Detail::TBLoadIterationsA0) {
         typename IteratorA0::AccessType *dst_ptr =
             reinterpret_cast<typename IteratorA0::AccessType *>(
                 this->smem_iterator_A0_.get());
 
         int const kSrcBytes = sizeof_bits<typename IteratorA0::Element>::value *
                               IteratorA0::ThreadMap::kElementsPerAccess /
                               IteratorA0::kAccessesPerVector / 8;
@@ -330,18 +330,18 @@
       }
     }
 
     iterator_B0.set_iteration_index(group_start_B0 *
                                    IteratorB0::kAccessesPerVector);
     this->smem_iterator_B0_.set_iteration_index(group_start_B0);
 
-    // LDGSTS for operand B
+    // Load for operand B
     CUTLASS_PRAGMA_UNROLL
     for (int j = 0; j < Detail::kAccessesPerGroupB0; ++j) {
-      if (group_start_B0 + j < Detail::TBLDGSTSIterationsB0) {
+      if (group_start_B0 + j < Detail::TBLoadIterationsB0) {
         typename IteratorB0::AccessType *dst_ptr =
             reinterpret_cast<typename IteratorB0::AccessType *>(
                 this->smem_iterator_B0_.get());
 
         int const kSrcBytes = sizeof_bits<typename IteratorB0::Element>::value *
                               IteratorB0::ThreadMap::kElementsPerAccess /
                               IteratorB0::kAccessesPerVector / 8;
@@ -363,18 +363,18 @@
   CUTLASS_DEVICE
   void copy_tiles_and_advance_1(IteratorB1 &iterator_B1,
                               int group_start_B1 = 0) {
     iterator_B1.set_iteration_index(group_start_B1 *
                                    IteratorB1::kAccessesPerVector);
     this->smem_iterator_B1_.set_iteration_index(group_start_B1);
 
-    // LDGSTS for operand B
+    // Load for operand B
     CUTLASS_PRAGMA_UNROLL
     for (int j = 0; j < Detail::kAccessesPerGroupB1; ++j) {
-      if (group_start_B1 + j < Detail::TBLDGSTSIterationsB1) {
+      if (group_start_B1 + j < Detail::TBLoadIterationsB1) {
         typename IteratorB1::AccessType *dst_ptr =
             reinterpret_cast<typename IteratorB1::AccessType *>(
                 this->smem_iterator_B1_.get());
 
         int const kSrcBytes = sizeof_bits<typename IteratorB1::Element>::value *
                               IteratorB1::ThreadMap::kElementsPerAccess /
                               IteratorB1::kAccessesPerVector / 8;
@@ -426,17 +426,17 @@
 
       iterator_A0.clear_mask(gemm_k_iterations_0 == 0);
       iterator_B0.clear_mask(gemm_k_iterations_0 == 0);
 
       iterator_A0.set_iteration_index(0);
       this->smem_iterator_A0_.set_iteration_index(0);
 
-      // LDGSTS for operand A
+      // Load for operand A
       CUTLASS_PRAGMA_UNROLL
-      for (int j = 0; j < Detail::TBLDGSTSIterationsA0; ++j) {
+      for (int j = 0; j < Detail::TBLoadIterationsA0; ++j) {
         typename IteratorA0::AccessType *dst_ptr =
             reinterpret_cast<typename IteratorA0::AccessType *>(
                 this->smem_iterator_A0_.get());
 
         CUTLASS_PRAGMA_UNROLL
         for (int v = 0; v < IteratorA0::kAccessesPerVector; ++v) {
           int const kSrcBytes =
@@ -454,17 +454,17 @@
 
         ++this->smem_iterator_A0_;
       }
 
       iterator_B0.set_iteration_index(0);
       this->smem_iterator_B0_.set_iteration_index(0);
 
-      // LDGSTS for operand B
+      // Load for operand B
       CUTLASS_PRAGMA_UNROLL
-      for (int j = 0; j < Detail::TBLDGSTSIterationsB0; ++j) {
+      for (int j = 0; j < Detail::TBLoadIterationsB0; ++j) {
         typename IteratorB0::AccessType *dst_ptr =
             reinterpret_cast<typename IteratorB0::AccessType *>(
                 this->smem_iterator_B0_.get());
 
         CUTLASS_PRAGMA_UNROLL
         for (int v = 0; v < IteratorB0::kAccessesPerVector; ++v) {
           int const kSrcBytes =
@@ -670,17 +670,17 @@
          ++stage, --gemm_k_iterations_1) {
 
       iterator_B1.clear_mask(gemm_k_iterations_1 == 0);
 
       iterator_B1.set_iteration_index(0);
       this->smem_iterator_B1_.set_iteration_index(0);
 
-      // LDGSTS for operand B
+      // Load for operand B
       CUTLASS_PRAGMA_UNROLL
-      for (int j = 0; j < Detail::TBLDGSTSIterationsB1; ++j) {
+      for (int j = 0; j < Detail::TBLoadIterationsB1; ++j) {
         typename IteratorB1::AccessType *dst_ptr =
             reinterpret_cast<typename IteratorB1::AccessType *>(
                 this->smem_iterator_B1_.get());
 
         CUTLASS_PRAGMA_UNROLL
         for (int v = 0; v < IteratorB1::kAccessesPerVector; ++v) {
           int const kSrcBytes =
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/threadblock/b2b_mma_multistage_smem_accumulator.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/threadblock/b2b_mma_multistage_smem_accumulator.h`

 * *Files 2% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -201,39 +201,39 @@
                   "The pipelined structure requires at least two warp-level "
                   "GEMM operations.");
     static_assert(Base::kWarpGemmIterations1 > 1,
                   "The pipelined structure requires at least two warp-level "
                   "GEMM operations.");
 
     /// Number of cp.async instructions to load one stage of operand A
-    static int const TBLDGSTSIterationsA0 =
+    static int const TBLoadIterationsA0 =
         IteratorA0::ThreadMap::Iterations::kCount;
 
     /// Number of cp.async instructions to load one stage of operand B
-    static int const TBLDGSTSIterationsB0 =
+    static int const TBLoadIterationsB0 =
         IteratorB0::ThreadMap::Iterations::kCount;
 
     /// Number of cp.async instructions to load one stage of operand B
-    static int const TBLDGSTSIterationsB1 =
+    static int const TBLoadIterationsB1 =
         IteratorB1::ThreadMap::Iterations::kCount;
 
     /// Number of stages
     static int const kStages = Stages;
 
     /// Number of cp.async instructions to load on group of operand A
     static int const kAccessesPerGroupA0 =
-        (TBLDGSTSIterationsA0 + Base::kWarpGemmIterations0 - 1) / Base::kWarpGemmIterations0;
+        (TBLoadIterationsA0 + Base::kWarpGemmIterations0 - 1) / Base::kWarpGemmIterations0;
 
     /// Number of cp.async instructions to load on group of operand B
     static int const kAccessesPerGroupB0 =
-        (TBLDGSTSIterationsB0 + Base::kWarpGemmIterations0 - 1) / Base::kWarpGemmIterations0;
+        (TBLoadIterationsB0 + Base::kWarpGemmIterations0 - 1) / Base::kWarpGemmIterations0;
 
     /// Number of cp.async instructions to load on group of operand B
     static int const kAccessesPerGroupB1 =
-        (TBLDGSTSIterationsB1 + Base::kWarpGemmIterations1 - 1) / Base::kWarpGemmIterations1;
+        (TBLoadIterationsB1 + Base::kWarpGemmIterations1 - 1) / Base::kWarpGemmIterations1;
   };
 
  private:
 
   using WarpLoadedFragmentA0 = typename Operator0::FragmentA;
   using WarpLoadedFragmentB0 = typename Operator0::FragmentB;
   using WarpLoadedFragmentA1 = typename Operator1::FragmentA;
@@ -323,18 +323,18 @@
   CUTLASS_DEVICE
   void copy_tiles_and_advance_0(IteratorA0 &iterator_A0, IteratorB0 &iterator_B0,
                               int group_start_A0 = 0, int group_start_B0 = 0) {
     iterator_A0.set_iteration_index(group_start_A0 *
                                    IteratorA0::kAccessesPerVector);
     this->smem_iterator_A0_.set_iteration_index(group_start_A0);
 
-    // LDGSTS for operand A
+    // cp.async for operand A
     CUTLASS_PRAGMA_UNROLL
     for (int j = 0; j < Detail::kAccessesPerGroupA0; ++j) {
-      if (group_start_A0 + j < Detail::TBLDGSTSIterationsA0) {
+      if (group_start_A0 + j < Detail::TBLoadIterationsA0) {
         typename IteratorA0::AccessType *dst_ptr =
             reinterpret_cast<typename IteratorA0::AccessType *>(
                 this->smem_iterator_A0_.get());
 
         int const kSrcBytes = sizeof_bits<typename IteratorA0::Element>::value *
                               IteratorA0::ThreadMap::kElementsPerAccess /
                               IteratorA0::kAccessesPerVector / 8;
@@ -353,18 +353,18 @@
       }
     }
 
     iterator_B0.set_iteration_index(group_start_B0 *
                                    IteratorB0::kAccessesPerVector);
     this->smem_iterator_B0_.set_iteration_index(group_start_B0);
 
-    // LDGSTS for operand B
+    // cp.async for operand B
     CUTLASS_PRAGMA_UNROLL
     for (int j = 0; j < Detail::kAccessesPerGroupB0; ++j) {
-      if (group_start_B0 + j < Detail::TBLDGSTSIterationsB0) {
+      if (group_start_B0 + j < Detail::TBLoadIterationsB0) {
         typename IteratorB0::AccessType *dst_ptr =
             reinterpret_cast<typename IteratorB0::AccessType *>(
                 this->smem_iterator_B0_.get());
 
         int const kSrcBytes = sizeof_bits<typename IteratorB0::Element>::value *
                               IteratorB0::ThreadMap::kElementsPerAccess /
                               IteratorB0::kAccessesPerVector / 8;
@@ -386,18 +386,18 @@
   CUTLASS_DEVICE
   void copy_tiles_and_advance_1(IteratorB1 &iterator_B1,
                               int group_start_B1 = 0) {
     iterator_B1.set_iteration_index(group_start_B1 *
                                    IteratorB1::kAccessesPerVector);
     this->smem_iterator_B1_.set_iteration_index(group_start_B1);
 
-    // LDGSTS for operand B
+    // cp.async for operand B
     CUTLASS_PRAGMA_UNROLL
     for (int j = 0; j < Detail::kAccessesPerGroupB1; ++j) {
-      if (group_start_B1 + j < Detail::TBLDGSTSIterationsB1) {
+      if (group_start_B1 + j < Detail::TBLoadIterationsB1) {
         typename IteratorB1::AccessType *dst_ptr =
             reinterpret_cast<typename IteratorB1::AccessType *>(
                 this->smem_iterator_B1_.get());
 
         int const kSrcBytes = sizeof_bits<typename IteratorB1::Element>::value *
                               IteratorB1::ThreadMap::kElementsPerAccess /
                               IteratorB1::kAccessesPerVector / 8;
@@ -449,17 +449,17 @@
 
       iterator_A0.clear_mask(gemm_k_iterations_0 == 0);
       iterator_B0.clear_mask(gemm_k_iterations_0 == 0);
 
       iterator_A0.set_iteration_index(0);
       this->smem_iterator_A0_.set_iteration_index(0);
 
-      // LDGSTS for operand A
+      // cp.async for operand A
       CUTLASS_PRAGMA_UNROLL
-      for (int j = 0; j < Detail::TBLDGSTSIterationsA0; ++j) {
+      for (int j = 0; j < Detail::TBLoadIterationsA0; ++j) {
         typename IteratorA0::AccessType *dst_ptr =
             reinterpret_cast<typename IteratorA0::AccessType *>(
                 this->smem_iterator_A0_.get());
 
         CUTLASS_PRAGMA_UNROLL
         for (int v = 0; v < IteratorA0::kAccessesPerVector; ++v) {
           int const kSrcBytes =
@@ -477,17 +477,17 @@
 
         ++this->smem_iterator_A0_;
       }
 
       iterator_B0.set_iteration_index(0);
       this->smem_iterator_B0_.set_iteration_index(0);
 
-      // LDGSTS for operand B
+      // cp.async for operand B
       CUTLASS_PRAGMA_UNROLL
-      for (int j = 0; j < Detail::TBLDGSTSIterationsB0; ++j) {
+      for (int j = 0; j < Detail::TBLoadIterationsB0; ++j) {
         typename IteratorB0::AccessType *dst_ptr =
             reinterpret_cast<typename IteratorB0::AccessType *>(
                 this->smem_iterator_B0_.get());
 
         CUTLASS_PRAGMA_UNROLL
         for (int v = 0; v < IteratorB0::kAccessesPerVector; ++v) {
           int const kSrcBytes =
@@ -685,17 +685,17 @@
          ++stage, --gemm_k_iterations_1) {
 
       iterator_B1.clear_mask(gemm_k_iterations_1 == 0);
 
       iterator_B1.set_iteration_index(0);
       this->smem_iterator_B1_.set_iteration_index(0);
 
-      // LDGSTS for operand B
+      // cp.async for operand B
       CUTLASS_PRAGMA_UNROLL
-      for (int j = 0; j < Detail::TBLDGSTSIterationsB1; ++j) {
+      for (int j = 0; j < Detail::TBLoadIterationsB1; ++j) {
         typename IteratorB1::AccessType *dst_ptr =
             reinterpret_cast<typename IteratorB1::AccessType *>(
                 this->smem_iterator_B1_.get());
 
         CUTLASS_PRAGMA_UNROLL
         for (int v = 0; v < IteratorB1::kAccessesPerVector; ++v) {
           int const kSrcBytes =
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/threadblock/b2b_mma_pipelined.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/threadblock/b2b_mma_pipelined.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/threadblock/b2b_mma_pipelined_smem_accumulator.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/threadblock/b2b_mma_pipelined_smem_accumulator.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/threadblock/default_b2b_mma.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/threadblock/default_b2b_mma.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/threadblock/default_b2b_mma_smem_accumulator.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/threadblock/default_b2b_mma_smem_accumulator.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/14_ampere_tf32_tensorop_gemm/ampere_tf32_tensorop_gemm.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/examples/14_ampere_tf32_tensorop_gemm/ampere_tf32_tensorop_gemm.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/15_ampere_sparse_tensorop_gemm/ampere_sparse_tensorop_gemm.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/examples/15_ampere_sparse_tensorop_gemm/ampere_sparse_tensorop_gemm.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/16_ampere_tensorop_conv2dfprop/ampere_tensorop_conv2dfprop.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/examples/16_ampere_tensorop_conv2dfprop/ampere_tensorop_conv2dfprop.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/17_fprop_per_channel_bias/fprop_per_channel_bias.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/examples/17_fprop_per_channel_bias/fprop_per_channel_bias.cu`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/18_ampere_fp64_tensorop_affine2_gemm/ampere_fp64_tensorop_affine2_gemm.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/examples/18_ampere_fp64_tensorop_affine2_gemm/ampere_fp64_tensorop_affine2_gemm.cu`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/19_tensorop_canonical/tensorop_canonical.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/examples/19_tensorop_canonical/tensorop_canonical.cu`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/20_simt_canonical/simt_canonical.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/examples/20_simt_canonical/simt_canonical.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/21_quaternion_gemm/quaternion_gemm.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/examples/21_quaternion_gemm/quaternion_gemm.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/22_quaternion_conv/quaternion_conv.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/examples/22_quaternion_conv/quaternion_conv.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/23_ampere_gemm_operand_reduction_fusion/ampere_gemm_operand_reduction_fusion.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/examples/23_ampere_gemm_operand_reduction_fusion/ampere_gemm_operand_reduction_fusion.cu`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -41,15 +41,15 @@
 */
 
 #include <iostream>
 #include <fstream>
 #include <sstream>
 
 #include "cutlass/cutlass.h"
-#include "cutlass/gemm/device/gemm_universal_adapter.h"
+#include "cutlass/gemm/device/gemm_with_k_reduction.h"
 #include "cutlass/gemm/kernel/default_gemm_with_k_reduction.h"
 #include "cutlass/reduction/device/reduce_split_k.h"
 #include "cutlass/reduction/kernel/reduce_split_k.h"
 #include "cutlass/reduction/thread/reduction_operators.h"
 #include "cutlass/matrix_coord.h"
 
 #include "cutlass/util/command_line.h"
@@ -97,41 +97,49 @@
 
 // Number of pipelines you want to use
 constexpr int NumStages = 4;
 
 // Reduce A or B operand along the K dimension
 constexpr bool ReduceKForA = true;
 
+// Alignment of A operand
+constexpr int AlignmentA = 8;
+
+// Alignment of B operand
+constexpr int AlignmentB = 8;
+
 // This code section describes the epilogue part of the kernel, we use default value
 using EpilogueOp = cutlass::epilogue::thread::LinearCombination<
     ElementOutput,                                        // Data type of output matrix.
     128 / cutlass::sizeof_bits<ElementOutput>::value,     // The number of elements per vectorized.
                                                           // memory access. This becomes the vector width of
                                                           // math instructions in the epilogue too.
     ElementAccumulator,                                   // Data type of accumulator
     ElementComputeEpilogue>;
 
-using GemmKernel = typename cutlass::gemm::kernel::DefaultGemmWithKReduction<
-  ElementInputA, LayoutInputA, cutlass::ComplexTransform::kNone, 8,
-  ElementInputB, LayoutInputB, cutlass::ComplexTransform::kNone, 8,
+using Gemm = typename cutlass::gemm::device::GemmWithKReduction<
+  ElementInputA, LayoutInputA,
+  ElementInputB, LayoutInputB,
   ElementOutput, LayoutOutput,
   ElementAccumulator,
   MMAOp,
   ReduceKForA,
   SmArch,
   ThreadblockShape,
   WarpShape,
   InstructionShape,
   EpilogueOp,
   SwizzleThreadBlock,
   NumStages,
-  cutlass::arch::OpMultiplyAdd
->::GemmKernel;
-
-using Gemm = cutlass::gemm::device::GemmUniversalAdapter<GemmKernel>;
+  AlignmentA,
+  AlignmentB,
+  cutlass::arch::OpMultiplyAdd,
+  cutlass::ComplexTransform::kNone,
+  cutlass::ComplexTransform::kNone
+>;
 
 // Below is the reduction kernel used in the case of parallel split-k
 using ReduceGemmSplitKShape = cutlass::MatrixShape<4, 64>;;
 
 using ReduceOp = cutlass::reduction::thread::ReduceAdd<
     ElementAccumulator,
     ElementOutput,
@@ -364,29 +372,29 @@
 
   cutlass::HostTensor<ElementOutput, LayoutGemmKReduction> tensor_reduction({reduce_vector_length, 1});
   cutlass::HostTensor<ElementOutput, LayoutGemmKReduction> tensor_ref_reduction({reduce_vector_length, 1});
 
   // Fill input and output matrices on host using CUTLASS helper functions
   cutlass::reference::host::TensorFillRandomUniform(
       tensor_a.host_view(),
-      1,
+      1997,
       ElementInputA(2),
       ElementInputA(-2),
       0);  // <- Fill tensor A on host with uniform-distribution random data
 
   cutlass::reference::host::TensorFillRandomUniform(
       tensor_b.host_view(),
-      1,
+      2003,
       ElementInputB(2),
       ElementInputB(-2),
       0);  // <- Fill tensor B on host with uniform-distribution random data
 
   cutlass::reference::host::TensorFillRandomUniform(
       tensor_c.host_view(),
-      1,
+      2017,
       ElementOutput(2),
       ElementOutput(-2),
       0);  // <- Fill matrix C on host with uniform-distribution random data
   cutlass::reference::host::TensorFill(
       tensor_d.host_view());  // <- fill matrix D on host with zeros
   cutlass::reference::host::TensorFill(
       tensor_ref_d.host_view());  // <- fill matrix D for reference on host with zeros
@@ -414,15 +422,15 @@
                      cutlass::gemm::GemmUniversalMode::kGemmSplitKParallel :
                      cutlass::gemm::GemmUniversalMode::kGemm;
 
   int batch_count = options.split_k_slices;
 
   // Create a tuple of gemm kernel arguments. This is later passed as arguments to launch
   // instantiated CUTLASS kernel
-  typename Gemm::Arguments arguments{
+  typename Gemm::Arguments arguments(
     mode,
     options.problem_size,
     batch_count,
     {alpha, beta},
     tensor_a.device_ref().data(),              // <- reference to tensor A on device
     tensor_b.device_ref().data(),              // <- reference to tensor B on device
     tensor_c.device_ref().data(),              // <- reference to matrix C on device
@@ -433,16 +441,15 @@
     options.problem_size.m() * options.problem_size.n(),
     options.problem_size.m() * options.problem_size.n(),
     reduce_vector_length,
     tensor_a.layout().stride(0),
     tensor_b.layout().stride(0),
     tensor_c.layout().stride(0),
     tensor_d.layout().stride(0),
-    tensor_reduction.layout().stride(0)
-  };                    
+    tensor_reduction.layout().stride(0));
 
   // Instantiate CUTLASS kernel depending on templates
   Gemm gemm_op;
 
   // Using the arguments, query for extra workspace required for matrix multiplication computation
   size_t workspace_size = Gemm::get_workspace_size(arguments);
 
@@ -503,23 +510,22 @@
     ElementOutput *workspace_vector_ptr = static_cast<ElementOutput *>(workspace_gemm_ptr) + batch_count * options.problem_size.m() * options.problem_size.n();
     cutlass::TensorRef<ElementOutput, cutlass::layout::RowMajor> workspace_vector_tensorref(workspace_vector_ptr, splitk_vector_layout);
 
     cutlass::TensorRef<ElementOutput, cutlass::layout::RowMajor> tensor_reduction_tensorref(tensor_reduction.device_ref().data(), splitk_vector_layout);
 
     cutlass::TensorRef<ElementOutput, cutlass::layout::RowMajor> tensor_nullptr_tensorref(nullptr, splitk_vector_layout);
 
-    typename ReduceVectorSplitK::Arguments reduce_vector_splitk_arguments{
+    typename ReduceVectorSplitK::Arguments reduce_vector_splitk_arguments(
       cutlass::MatrixCoord(1, reduce_vector_length),
       batch_count,
       size_t(reduce_vector_length),
       workspace_vector_tensorref,
       tensor_reduction_tensorref,
       tensor_nullptr_tensorref,
-      {1.0f, 0.0f} 
-    };
+      {1.0f, 0.0f});
 
     ReduceVectorSplitK reduce_vector_splitk_op;
    
     result.status = reduce_vector_splitk_op.initialize(reduce_vector_splitk_arguments); 
     CUTLASS_CHECK(result.status);
 
     result.status = reduce_vector_splitk_op();
@@ -557,15 +563,15 @@
   
     // Copy output data from CUTLASS and reference kernel to host for comparison
     tensor_d.sync_host();
     tensor_ref_d.sync_host();
   
     tensor_reduction.sync_host();
   
-    // Compute bias + relu in host code
+    // Reduce K in host code
     if (ReduceKForA) {
       for (int m = 0; m < options.problem_size.m(); ++m) {
         for (int k = 0; k < options.problem_size.k(); ++k) {
           tensor_ref_reduction.at({m, 0}) += 
             tensor_a.at(cutlass::MatrixCoord(m, k));
         }
       }
@@ -577,15 +583,15 @@
         }
       }
     }
   
     // Check if output from CUTLASS kernel and reference kernel are equal or not
     bool pass = cutlass::reference::host::TensorEquals(tensor_d.host_view(),
                                                        tensor_ref_d.host_view());
-  
+
     pass &= cutlass::reference::host::TensorEquals(tensor_ref_reduction.host_view(),
                                                    tensor_reduction.host_view());
 
     if (!pass) {
       result.reference_check = cutlass::Status::kErrorInternal;
       std::cout << "ERROR - results miscompared.\n";
     } else {
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/24_gemm_grouped/gemm_grouped.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/examples/24_gemm_grouped/gemm_grouped.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/25_ampere_fprop_mainloop_fusion/ampere_3d_fprop_mainloop_fusion.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/examples/25_ampere_fprop_mainloop_fusion/ampere_3d_fprop_mainloop_fusion.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/25_ampere_fprop_mainloop_fusion/ampere_fprop_mainloop_fusion.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/examples/25_ampere_fprop_mainloop_fusion/ampere_fprop_mainloop_fusion.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/26_ampere_wgrad_mainloop_fusion/ampere_wgrad_mainloop_fusion.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/examples/26_ampere_wgrad_mainloop_fusion/ampere_wgrad_mainloop_fusion.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/27_ampere_3xtf32_fast_accurate_tensorop_gemm/27_ampere_3xtf32_fast_accurate_tensorop_gemm.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/examples/27_ampere_3xtf32_fast_accurate_tensorop_gemm/27_ampere_3xtf32_fast_accurate_tensorop_gemm.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/28_ampere_3xtf32_fast_accurate_tensorop_fprop/ampere_3xtf32_fast_accurate_tensorop_fprop.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/examples/28_ampere_3xtf32_fast_accurate_tensorop_fprop/ampere_3xtf32_fast_accurate_tensorop_fprop.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/29_ampere_3xtf32_fast_accurate_tensorop_complex_gemm/29_ampere_3xtf32_fast_accurate_tensorop_complex_gemm.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/examples/29_ampere_3xtf32_fast_accurate_tensorop_complex_gemm/29_3xtf32_complex_gemm.cu`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -177,15 +177,15 @@
       << "  --beta=<f32>                Epilogue scalar beta\n\n"
       << "  --rand_mode=<string>        gauss / uniform*\n\n"
       << "  --seed=<int>                Random number seed (1*)\n\n"
       << "  --iterations=<int>          Number of profiling iterations to perform.\n\n"
       << "  --benchmark                 If set (true), performance benchmarking on several layers and batch-size.\n\n";
 
     out << "\n\nExamples:\n\n"
-      << "$ ./examples/29_ampere_3xtf32_fast_accurate_tensorop_complex_gemm/29_ampere_3xtf32_fast_accurate_complex_gemm --m=1024 --n=512 \\\n"
+      << "$ ./examples/29_ampere_3xtf32_fast_accurate_tensorop_complex_gemm/29_3xtf32_complex_gemm --m=1024 --n=512 \\\n"
       << "     --alpha=2 --beta=0.707 \n\n";
 
     return out;
   }
 
   /// Compute performance in GFLOP/s
   double gflops(double runtime_s) const {
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/30_wgrad_split_k/30_wgrad_split_k.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/examples/30_wgrad_split_k/30_wgrad_split_k.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -527,25 +527,25 @@
     typename ReductionDevice::Arguments reduction_args(
         cutlass::conv::implicit_gemm_problem_size(kConvolutionalOperator, problem_size).mn(),
         problem_size.split_k_slices,
         cutlass::conv::implicit_gemm_tensor_c_size(kConvolutionalOperator, problem_size),
         // Reduction input
         {
             reinterpret_cast<ElementAccumulator*> (workspace.get()),
-            ReductionStrideIndex(tensor_c.stride()[ImplicitGemm::ImplicitGemmKernel::kTensorCStrideIdx])
+            ReductionStrideIndex(tensor_c.stride()[ImplicitGemm::UnderlyingKernel::kTensorCStrideIdx])
         },
         // Destination
         {
             tensor_d.device_data(),
-            ReductionStrideIndex(tensor_d.stride()[ImplicitGemm::ImplicitGemmKernel::kTensorCStrideIdx])
+            ReductionStrideIndex(tensor_d.stride()[ImplicitGemm::UnderlyingKernel::kTensorCStrideIdx])
         },
         // Source
         {
             tensor_c.device_data(),
-            ReductionStrideIndex(tensor_c.stride()[ImplicitGemm::ImplicitGemmKernel::kTensorCStrideIdx])
+            ReductionStrideIndex(tensor_c.stride()[ImplicitGemm::UnderlyingKernel::kTensorCStrideIdx])
         },
         {options.alpha, options.beta}
     );
 
     status = reduction_op.initialize(reduction_args, nullptr);
     status = reduction_op();
   }
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/31_basic_syrk/basic_syrk.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/examples/31_basic_syrk/basic_syrk.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/32_basic_trmm/basic_trmm.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/examples/32_basic_trmm/basic_trmm.cu`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/33_ampere_3xtf32_tensorop_symm/ampere_3xtf32_tensorop_symm.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/examples/33_ampere_3xtf32_tensorop_symm/ampere_3xtf32_tensorop_symm.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/34_transposed_conv2d/34_transposed_conv2d.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/examples/34_transposed_conv2d/34_transposed_conv2d.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/35_gemm_softmax/gemm_softmax.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/examples/35_gemm_softmax/gemm_softmax.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/35_gemm_softmax/gemm_with_epilogue_visitor.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/examples/35_gemm_softmax/gemm_with_epilogue_visitor.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -363,20 +363,14 @@
     return Status::kSuccess;
   }
 
   static Status can_implement(Arguments const &args) {
     return can_implement(args.problem_size);
   }
 
-  static size_t get_extra_workspace_size(Arguments const &args,
-                                         cutlass::gemm::GemmCoord const &grid_tiled_shape) {
-
-    return 0;
-  }
-
   #define SPLIT_K_ENABLED 1
 
   /// Executes one GEMM
   CUTLASS_DEVICE
   void operator()(Params const &params, SharedStorage &shared_storage) {
 
     // Compute threadblock location
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/35_gemm_softmax/gemm_with_softmax.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/examples/35_gemm_softmax/gemm_with_softmax.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/36_gather_scatter_fusion/gather_scatter_fusion.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/examples/36_gather_scatter_fusion/gather_scatter_fusion.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/37_gemm_layernorm_gemm_fusion/gemm_layernorm.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/examples/37_gemm_layernorm_gemm_fusion/gemm_layernorm.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/37_gemm_layernorm_gemm_fusion/gemm_with_epilogue_visitor.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/examples/37_gemm_layernorm_gemm_fusion/gemm_with_epilogue_visitor.h`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -305,20 +305,14 @@
     return Status::kSuccess;
   }
 
   static Status can_implement(Arguments const &args) {
     return can_implement(args.problem_size);
   }
 
-  static size_t get_extra_workspace_size(Arguments const &args,
-                                         cutlass::gemm::GemmCoord const &grid_tiled_shape) {
-
-    return 0;
-  }
-
   /// Executes one GEMM
   CUTLASS_DEVICE
   void operator()(Params const &params, SharedStorage &shared_storage) {
 
     // Compute threadblock location
     ThreadblockSwizzle threadblock_swizzle;
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/37_gemm_layernorm_gemm_fusion/gemm_with_layernorm.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/examples/37_gemm_layernorm_gemm_fusion/gemm_with_layernorm.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/38_syr2k_grouped/syr2k_grouped.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/examples/38_syr2k_grouped/syr2k_grouped.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/39_gemm_permute/gemm_permute.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/examples/39_gemm_permute/gemm_permute.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -220,15 +220,15 @@
     out << "39_gemm_permute\n\n"
       << " 1) This example firstly profiles the performance of a batched GEMM kernel with BMM whole output"
       << " (including output matrices for each batch) as permuted 4D Tensor."
       << " The BMM tensor output in shape of [B, M, N] is reshaped as [B/D1, D1, M, N] and then permuted with"
       << " permute([0, 2, 1, 3]) to be in shape of [B/D1, M, D1, N].\n\n"
       << " 2) This example also profiles the performance of a normal GEMM kernel with output as permuted 5D Tensor."
       << " The GEMM matrix output in shape of [M, N]  is reshaped as [M/T1, T1, T2, T3, N/T2/T3] and then permuted"
-      << " with permute([2, 0, 3, 1, 4]) to be in shape of [T2, M/T1, T3, T1, N//T2/T3].\n\n"
+      << " with permute([2, 0, 3, 1, 4]) to be in shape of [T2, M/T1, T3, T1, N/T2/T3].\n\n"
       << " Note: D1, T1, T2, T3 are compile-time constants defined in gemm_permute.cu\n\n"
       << "Options:\n\n"
       << "  --help                      If specified, displays this usage statement.\n\n"
       << "  --batch-count=<int>         Sets the number of batches in batched GEMM (batch number for BMM). (default: --batch-count=768)\n"
       << "  --m=<int>                   Sets the M dimension for both batched GEMM and normal GEMM problems. (default: --m=128)\n"
       << "  --n=<int>                   Sets the N dimension for both batched GEMM and normal GEMM problems. (default: --n=192)\n"
       << "  --k=<int>                   Sets the K dimension for both batched GEMM and normal GEMM problems. (default: --k=128)\n"
@@ -686,15 +686,15 @@
       problem.n(),
       problem.n()
     };
 
     // Initialize the GEMM object
     GemmBatched gemm;
 
-    result.status = gemm.initialize(arguments);
+    result.status = gemm.initialize(arguments, nullptr);
 
     if (result.status != cutlass::Status::kSuccess) {
       std::cerr << "Failed to initialize CUTLASS Batched GEMM kernel." << std::endl;
       return result;
     }
 
     // Run the batched GEMM object
@@ -850,15 +850,15 @@
       problem.n(),
       problem.n()
     };
 
     // Initialize the GEMM object
     GemmPermute gemm_normal;
 
-    result.status = gemm_normal.initialize(arguments);
+    result.status = gemm_normal.initialize(arguments, nullptr);
 
     if (result.status != cutlass::Status::kSuccess) {
       std::cerr << "Failed to initialize CUTLASS Batched GEMM kernel." << std::endl;
       return result;
     }
 
     // Run the normal GEMM object
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/41_fused_multi_head_attention/attention_scaling_coefs_updater.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/examples/41_fused_multi_head_attention/attention_scaling_coefs_updater.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/41_fused_multi_head_attention/default_fmha_grouped.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/examples/41_fused_multi_head_attention/default_fmha_grouped.h`

 * *Files 1% similar despite different names*

```diff
@@ -46,16 +46,17 @@
 
 #include "cutlass/complex.h"
 #include "cutlass/layout/matrix.h"
 #include "cutlass/numeric_types.h"
 
 #include "fmha_grouped.h"
 #include "gemm_kernel_utils.h"
-#include "gemm/find_default_mma.h"
-#include "gemm/mma_from_smem.h"
+#include "find_default_mma.h"
+#include "attention_scaling_coefs_updater.h"
+#include "mma_from_smem.h"
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
 namespace cutlass {
 namespace gemm {
 namespace kernel {
 
@@ -149,18 +150,18 @@
         Operator
         >::DefaultMma;
 
     using MmaCore = typename DefaultMma::MmaCore;
     using IteratorA = typename DefaultMma::IteratorA;
     using IteratorB = typename DefaultMma::IteratorB;
     using Mma = typename DefaultMma::ThreadblockMma;
-    using AccumLambdaIterator = typename DefaultMmaAccumLambdaIterator<
+    using ScalingCoefsUpdater = typename DefaultAttentionScalingCoefsUpdater<
         typename Mma::Operator::IteratorC,
         ElementAccumulator,
-        kWarpSize>::Iterator;
+        kWarpSize>::Updater;
 
     static_assert(MmaCore::WarpCount::kCount == kNumWarpsPerBlock, "");
 
     // Epilogue to store to shared-memory in a format that we can use later for
     // the second matmul
     using B2bGemm = typename cutlass::gemm::threadblock::B2bGemm<
         typename Mma::Operator::IteratorC,
@@ -235,16 +236,15 @@
         kStages,
         kSplitKSerial,
         Operator>;
 
     using DefaultMmaFromSmem =
         typename cutlass::gemm::threadblock::DefaultMmaFromSharedMemory<
             typename DefaultGemm::Mma,
-            typename MM0::AccumulatorSharedStorage,
-            false>; // kScaleOperandA
+            typename MM0::AccumulatorSharedStorage>;
 
     using Mma = typename DefaultMmaFromSmem::Mma;
     using IteratorB = typename Mma::IteratorB;
     using WarpCount = typename Mma::WarpCount;
     static_assert(WarpCount::kCount == kNumWarpsPerBlock, "");
 
     using DefaultEpilogue = typename DefaultGemm::Epilogue;
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/41_fused_multi_head_attention/epilogue/epilogue_pipelined.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/examples/41_fused_multi_head_attention/epilogue/epilogue_pipelined.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/41_fused_multi_head_attention/epilogue/epilogue_rescale_output.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/examples/41_fused_multi_head_attention/epilogue/epilogue_rescale_output.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/41_fused_multi_head_attention/epilogue/epilogue_thread_apply_logsumexp.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/examples/41_fused_multi_head_attention/epilogue/epilogue_thread_apply_logsumexp.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/41_fused_multi_head_attention/epilogue_pipelined.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/examples/41_fused_multi_head_attention/epilogue_pipelined.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/41_fused_multi_head_attention/epilogue_rescale_output.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/examples/41_fused_multi_head_attention/epilogue_rescale_output.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/41_fused_multi_head_attention/epilogue_thread_apply_logsumexp.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/examples/41_fused_multi_head_attention/epilogue_thread_apply_logsumexp.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/41_fused_multi_head_attention/find_default_mma.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/examples/41_fused_multi_head_attention/find_default_mma.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/41_fused_multi_head_attention/fmha_grouped.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/examples/41_fused_multi_head_attention/fmha_grouped.h`

 * *Files 10% similar despite different names*

```diff
@@ -44,26 +44,15 @@
 
 #include "cutlass/layout/matrix.h"
 #include "cutlass/trace.h"
 #include "cutlass/gemm/kernel/gemm_transpose_operands.h"
 
 #include "fmha_grouped_problem_visitor.h"
 #include "gemm_kernel_utils.h"
-#include "gemm/mma_accum_lambda_iterator.h"
-#include "epilogue/epilogue_rescale_output.h"
-
-
-namespace {
-  static CUTLASS_DEVICE float atomicMaxFloat(float* addr, float value) {
-  // source: https://stackoverflow.com/a/51549250
-  return (value >= 0)
-      ? __int_as_float(atomicMax((int*)addr, __float_as_int(value)))
-      : __uint_as_float(atomicMin((unsigned int*)addr, __float_as_uint(value)));
-}
-}
+#include "epilogue_rescale_output.h"
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
 namespace cutlass {
 namespace gemm {
 namespace kernel {
 
@@ -135,17 +124,14 @@
   static int const kAlignmentV = 1;
 
   using ThreadblockShape = typename MM0::ThreadblockShape;
 
   static int const kQueriesPerBlock = ThreadblockShape::kM;
   static int const kKeysPerBlock = ThreadblockShape::kN;
 
-  static constexpr bool kSupportsDropout = false;
-  static constexpr bool kSupportsBias = false;
-
   /// Warp count (concept: GemmShape)
   using WarpCount = typename MM1::WarpCount;
   static int const kThreadsPerWarp = 32;
   static int const kThreadCount = kThreadsPerWarp * WarpCount::kCount;
 
   using ProblemVisitor = FMHAGroupedProblemVisitor<
                             ThreadblockShape,
@@ -178,52 +164,48 @@
     ElementOAccum ** ptr_O_accum;
 
     typename LayoutQ::Stride::LongIndex *ldq;
     typename LayoutK::Stride::LongIndex *ldk;
     typename LayoutP::Stride::LongIndex *ldv;
     typename LayoutO::Stride::LongIndex *ldo;
 
-    // Scale
-    ElementAccumulator scale;
-
     // Whether causal masking is to be performed
     bool causal;
 
     // Only used by device-level operator
     GemmCoord *host_problem_sizes;
 
     //
     // Methods
     //
 
     /// Default ctor
     CUTLASS_HOST_DEVICE
-    Arguments():
+    Arguments(): 
       problem_count(0),
-      threadblock_count(0),
+      threadblock_count(0), 
       ptr_Q(nullptr),
       ptr_K(nullptr),
       ptr_P(nullptr),
       ptr_V(nullptr),
       ptr_O(nullptr),
       ptr_O_accum(nullptr),
       ldq(nullptr),
       ldk(nullptr),
       ldv(nullptr),
       ldo(nullptr),
-      scale(0),
       causal(false),
       host_problem_sizes(nullptr)
     {
 
     }
 
     /// Ctor
     CUTLASS_HOST_DEVICE
-    Arguments(
+    Arguments(    
       GemmCoord *problem_sizes0,
       GemmCoord *problem_sizes1,
       int problem_count,
       int threadblock_count,
       ElementQ ** ptr_Q,
       ElementK ** ptr_K,
       ElementP ** ptr_P,
@@ -232,17 +214,16 @@
       ElementOAccum ** ptr_O_accum,
       typename LayoutQ::Stride::LongIndex *ldq,
       typename LayoutK::Stride::LongIndex *ldk,
       typename LayoutP::Stride::LongIndex *ldp,
       typename LayoutV::Stride::LongIndex *ldv,
       typename LayoutO::Stride::LongIndex *ldo,
       bool causal,
-      ElementAccumulator scale,
       GemmCoord *host_problem_sizes=nullptr
-    ):
+    ): 
       problem_sizes0(problem_sizes0),
       problem_sizes1(problem_sizes1),
       problem_count(problem_count),
       threadblock_count(threadblock_count),
       ptr_Q(ptr_Q),
       ptr_K(ptr_K),
       ptr_P(ptr_P),
@@ -250,15 +231,14 @@
       ptr_O(ptr_O),
       ptr_O_accum(kNeedsOutputAccumulatorBuffer ? ptr_O_accum : (accum_t**)ptr_O),
       ldq(ldq),
       ldk(ldk),
       ldv(ldv),
       ldo(ldo),
       causal(causal),
-      scale(scale),
       host_problem_sizes(host_problem_sizes)
     {
 
     }
 
     bool __host__ check_supported() {
       CHECK_ALIGNED_PTR(ptr_Q, kAlignmentQ);
@@ -289,15 +269,14 @@
     ElementOAccum ** ptr_O_accum;
 
     typename LayoutQ::Stride::LongIndex *ldq;
     typename LayoutK::Stride::LongIndex *ldk;
     typename LayoutP::Stride::LongIndex *ldv;
     typename LayoutO::Stride::LongIndex *ldo;
 
-    ElementAccumulator scale;
     bool causal;
 
     //
     // Methods
     //
 
     CUTLASS_HOST_DEVICE
@@ -308,16 +287,15 @@
       ptr_V(nullptr),
       ptr_O(nullptr),
       ptr_O_accum(nullptr),
       ldq(nullptr),
       ldk(nullptr),
       ldv(nullptr),
       ldo(nullptr),
-      causal(false),
-      scale(0)
+      causal(false)
     { }
 
     CUTLASS_HOST_DEVICE
     Params(Arguments const &args,
           void *workspace = nullptr,
           int tile_count = 0):
       problem_visitor(args.problem_sizes0, args.problem_sizes1, args.problem_count, workspace, tile_count),
@@ -328,16 +306,15 @@
       ptr_V(args.ptr_V),
       ptr_O(args.ptr_O),
       ptr_O_accum(kNeedsOutputAccumulatorBuffer ? args.ptr_O_accum : (accum_t**)args.ptr_O),
       ldq(args.ldq),
       ldk(args.ldk),
       ldv(args.ldv),
       ldo(args.ldo),
-      causal(args.causal),
-      scale(args.scale)
+      causal(args.causal)
     { 
 
     }
 
     CUTLASS_HOST_DEVICE
     void update(
       Arguments const &args,
@@ -356,15 +333,14 @@
       ptr_O = args.ptr_O;
       ptr_O_accum = kNeedsOutputAccumulatorBuffer ? args.ptr_O_accum : (accum_t**)args.ptr_O;
       ldq = args.ldq;
       ldk = args.ldk;
       ldv = args.ldv;
       ldo = args.ldo;
       causal = args.causal;
-      scale = args.scale;
     }
   };
 
   // Shared storage - depends on kernel params
   struct ScalingCoefs {
     cutlass::Array<ElementAccumulator, kQueriesPerBlock> m_prime;
     cutlass::Array<ElementAccumulator, kQueriesPerBlock> s_prime;
@@ -484,15 +460,15 @@
   }
 
   /// Executes one GEMM
   CUTLASS_DEVICE
   void operator()(Params const &params, SharedStorage &shared_storage) {
     auto& m_prime = shared_storage.m_prime;
     auto& s_prime = shared_storage.s_prime;
-    [[maybe_unused]] auto& si = shared_storage.after_mm0.si;
+    auto& si = shared_storage.after_mm0.si;
     auto& mi = shared_storage.mi;
 
     ProblemVisitor problem_visitor(
       params.problem_visitor,
       shared_storage.problem_visitor,
       blockIdx.x);
 
@@ -629,18 +605,18 @@
           iteratorC_tile_offset = {
               (warp_id() % MM0::Mma::WarpCount::kM),
               (warp_id() / MM0::Mma::WarpCount::kM)
             };
 
         // Mask out last if causal
         if (params.causal && num_keys - iter_key_start <= kKeysPerBlock) {
-          auto lane_offset = MM0::AccumLambdaIterator::get_lane_offset(
+          auto lane_offset = MM0::ScalingCoefsUpdater::get_lane_offset(
               lane_id(), warp_id(), iteratorC_tile_offset);
           int32_t last_col;
-          MM0::AccumLambdaIterator::iterateRows(
+          MM0::ScalingCoefsUpdater::iterateRows(
               lane_offset,
               [&](int accum_m) {
                 last_col = TileParams::query_start(threadblock_idx) + accum_m - iter_key_start;
               },
               [&](int accum_m, int accum_n, int idx) {
                 if (accum_n > last_col) {
                   accum[idx] =
@@ -651,30 +627,33 @@
         }
         DISPATCH_BOOL(iter_key_start == 0, kIsFirst, ([&] {
                 DISPATCH_BOOL(
                     num_keys - iter_key_start >= kKeysPerBlock,
                     kFullColumns,
                     ([&] {
                       // Update `mi` from accum stored in registers
-                      // Also does accum[i] <- exp(accum[i] - mi)
-                      iterative_softmax<
-                          typename MM0::Mma::Operator::IteratorC,
+                      // Also updates `accum` with accum[i] <-
+                      // exp(accum[i] * scale
+                      // - mi)
+                      MM0::ScalingCoefsUpdater::update<
+                          kQueriesPerBlock,
                           kFullColumns,
-                          kIsFirst>(
+                          kIsFirst,
+                          kKeepOutputInRF>(
                           accum_o,
                           accum,
                           mi,
                           m_prime,
                           s_prime,
                           lane_id(),
                           thread_id(),
                           warp_id(),
                           num_keys - iter_key_start,
                           iteratorC_tile_offset,
-                          kSupportsBias ? 1.0f : params.scale);
+                          1.0f / cutlass::fast_sqrt(float(problem_size0.k())));
                     }));
               }));
 
         // Output results to shared-memory
         int warp_idx_mn_0 = warp_id() %
             (MM0::Mma::Base::WarpCount::kM * MM0::Mma::Base::WarpCount::kN);
         auto output_tile_coords = cutlass::MatrixCoord{
@@ -845,124 +824,14 @@
         epilogue(rescale, dest_iter, accum_o);
       }
 
       // Next tile
       problem_visitor.advance(gridDim.x);
     }
   }
-
-  template <
-      typename WarpIteratorC,
-      bool kFullColumns,
-      bool kIsFirst>
-  CUTLASS_DEVICE static void iterative_softmax(
-      typename WarpIteratorC::Fragment& frag_o, // output so far
-      typename WarpIteratorC::Fragment& frag,
-      cutlass::Array<accum_t, kQueriesPerBlock>& mi,
-      cutlass::Array<accum_t, kQueriesPerBlock>& m_prime,
-      cutlass::Array<accum_t, kQueriesPerBlock>& s_prime,
-      int8_t lane_id,
-      int8_t thread_id,
-      int8_t warp_id,
-      int16_t max_col,
-      typename WarpIteratorC::TensorCoord const& tile_offset,
-      float scaling) {
-    /* Iterates on the accumulator and corresponding position on result matrix
-
-    (1) Update `mi[r]` to the max value of the row `r`
-    (2) In a second iteration do the following:
-        (a) accum   <- exp(accum - mi)
-        (b) m_prime <- exp(m_prime - mi)
-        (c) s_prime <- s_prime * m_prime + sum(accum)
-
-    All of this is done on registers, before we store all of this
-    on shared memory for the next matmul with Value.
-    */
-    using Fragment = typename WarpIteratorC::Fragment;
-    using LambdaIterator = typename DefaultMmaAccumLambdaIterator<
-        WarpIteratorC,
-        accum_t,
-        kThreadsPerWarp>::Iterator;
-    // Convert to `accum_t` (rather than double)
-    constexpr float kLog2e = 1.4426950408889634074; // log_2(e) = M_LOG2E
-    if (!kIsFirst) {
-      if (thread_id < kQueriesPerBlock) {
-        m_prime[thread_id] = mi[thread_id];
-      }
-      __syncthreads();
-    }
-
-    auto lane_offset =
-        LambdaIterator::get_lane_offset(lane_id, warp_id, tile_offset);
-
-    // First update `mi` to the max per-row
-    {
-      accum_t max;
-      LambdaIterator::iterateRows(
-          lane_offset,
-          [&](int accum_m) {
-            max = -cutlass::platform::numeric_limits<accum_t>::infinity();
-          },
-          [&](int accum_m, int accum_n, int idx) {
-            if (kFullColumns || accum_n < max_col) {
-              max = cutlass::fast_max(max, frag[idx]);
-            }
-          },
-          [&](int accum_m) {
-            // Having 4x atomicMax seems faster than reduce within warp
-            // first...
-            atomicMaxFloat(&mi[accum_m], max * scaling);
-          });
-    }
-    frag = cutlass::multiplies<Fragment>()(scaling * kLog2e, frag);
-
-    // Make sure we all share the update values for `mi`
-    __syncthreads();
-
-    if (thread_id < kQueriesPerBlock) {
-      auto m_prime_exp = exp2f(kLog2e * (m_prime[thread_id] - mi[thread_id]));
-      m_prime[thread_id] = m_prime_exp;
-      s_prime[thread_id] *= m_prime_exp;
-    }
-    __syncthreads(); // Update output fragments
-    if (kKeepOutputInRF && !kIsFirst) {
-      accum_t mp;
-      LambdaIterator::iterateRows(
-          lane_offset,
-          [&](int accum_m) { mp = m_prime[accum_m]; },
-          [&](int accum_m, int accum_n, int idx) { frag_o[idx] *= mp; },
-          [&](int accum_m) {});
-      __syncthreads();
-    }
-    // Update accum_m, accum_n, ...
-    {
-      accum_t mi_row, total_row;
-      LambdaIterator::iterateRows(
-          lane_offset,
-          [&](int accum_m) { mi_row = kLog2e * mi[accum_m]; },
-          [&](int accum_m, int accum_n, int idx) {
-            frag[idx] = (kFullColumns || accum_n < max_col)
-                ? exp2f(frag[idx] - mi_row)
-                : accum_t(0.0);
-          },
-          [&](int accum_m) {});
-      LambdaIterator::iterateRows(
-          lane_offset,
-          [&](int accum_m) { total_row = 0.0; },
-          [&](int accum_m, int accum_n, int idx) { total_row += frag[idx]; },
-          [&](int accum_m) {
-            if (LambdaIterator::reduceSameRow(
-                    lane_id, total_row, [](accum_t a, accum_t b) {
-                      return a + b;
-                    })) {
-              atomicAdd(&s_prime[accum_m], total_row);
-            }
-          });
-    }
-  }
 };
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
 } // namespace kernel
 } // namespace gemm
 } // namespace cutlass
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/41_fused_multi_head_attention/fmha_grouped_problem_visitor.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/examples/41_fused_multi_head_attention/fmha_grouped_problem_visitor.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/41_fused_multi_head_attention/fused_multihead_attention_fixed_seqlen.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/examples/41_fused_multi_head_attention/fused_multihead_attention_fixed_seqlen.cu`

 * *Files 4% similar despite different names*

```diff
@@ -500,59 +500,45 @@
     ldq_host.resize(problem_count());
     ldk_host.resize(problem_count());
     ldp_host.resize(problem_count());
     ldv_host.resize(problem_count());
     ldo_host.resize(problem_count());
     seqlen_host.resize(problem_count());
 
-    // Create tensors in BMHK format, where
-    // B = batch_size
-    // M = sequence length
-    // H = num_heads
-    // K = embedding size per head
-    int64_t batch_offset_Q, batch_offset_K, batch_offset_V, batch_offset_O;
-
-    for (int32_t b = 0; b < options.batch_size; ++b) {
-      batch_offset_Q = total_elements_Q;
-      batch_offset_K = total_elements_K;
-      batch_offset_V = total_elements_V;
-      batch_offset_O = total_elements_O;
-      for (int32_t h = 0; h < options.head_number; ++h) {
-        int32_t i = h + b * options.head_number;
-
-        auto problem0 = options.problem_sizes0.at(i);
-        auto problem1 = options.problem_sizes1.at(i);
-
-        ldq_host.at(i) = LayoutQ::packed({problem0.m(), options.head_number * problem0.k()}).stride(0);
-        ldk_host.at(i) = LayoutK::packed({options.head_number * problem0.k(), problem0.n()}).stride(0);
-        ldp_host.at(i) = LayoutP::packed({problem0.m(), problem0.n()}).stride(0);
-        ldv_host.at(i) = LayoutV::packed({problem1.k(), options.head_number * problem1.n()}).stride(0);
-        ldo_host.at(i) = LayoutO::packed({problem1.m(), options.head_number * problem1.n()}).stride(0);
-
-        // m = n for attention problems.
-        seqlen_host.at(i) = problem0.m();
-
-        offset_Q.push_back(batch_offset_Q + h * problem0.k());
-        offset_K.push_back(batch_offset_K + h * problem0.k());
-        offset_P.push_back(total_elements_P);
-        offset_V.push_back(batch_offset_V + h * problem0.k());
-        offset_O.push_back(batch_offset_O + h * problem1.n());
-
-        int64_t elements_Q = problem0.m() * problem0.k();
-        int64_t elements_K = problem0.k() * problem0.n();
-        int64_t elements_P = problem0.m() * problem0.n();
-        int64_t elements_V = problem1.k() * problem1.n();
-        int64_t elements_O = problem1.m() * problem1.n();
-
-        total_elements_Q += elements_Q;
-        total_elements_K += elements_K;
-        total_elements_P += elements_P;
-        total_elements_V += elements_V;
-        total_elements_O += elements_O;
-      }
+    for (int32_t i = 0; i < problem_count(); ++i) {
+
+      auto problem0 = options.problem_sizes0.at(i);
+      auto problem1 = options.problem_sizes1.at(i);
+
+      ldq_host.at(i) = LayoutQ::packed({problem0.m(), problem0.k()}).stride(0);
+      ldk_host.at(i) = LayoutK::packed({problem0.k(), problem0.n()}).stride(0);
+      ldp_host.at(i) = LayoutP::packed({problem0.m(), problem0.n()}).stride(0);
+      ldv_host.at(i) = LayoutV::packed({problem1.k(), problem1.n()}).stride(0);
+      ldo_host.at(i) = LayoutO::packed({problem1.m(), problem1.n()}).stride(0);
+
+      // m = n for attention problems.
+      seqlen_host.at(i) = problem0.m();
+
+      offset_Q.push_back(total_elements_Q);
+      offset_K.push_back(total_elements_K);
+      offset_P.push_back(total_elements_P);
+      offset_V.push_back(total_elements_V);
+      offset_O.push_back(total_elements_O);
+
+      int64_t elements_Q = problem0.m() * problem0.k();
+      int64_t elements_K = problem0.k() * problem0.n();
+      int64_t elements_P = problem0.m() * problem0.n();
+      int64_t elements_V = problem1.k() * problem1.n();
+      int64_t elements_O = problem1.m() * problem1.n();
+
+      total_elements_Q += elements_Q;
+      total_elements_K += elements_K;
+      total_elements_P += elements_P;
+      total_elements_V += elements_V;
+      total_elements_O += elements_O;
     }
 
     problem_sizes_device0.reset(problem_count());
     problem_sizes_device1.reset(problem_count());
     problem_sizes_device0.copy_from_host(options.problem_sizes0.data());
     problem_sizes_device1.copy_from_host(options.problem_sizes1.data());
 
@@ -659,141 +645,138 @@
   }
 
   /// Verifies the result is a GEMM
   bool verify_() {
 
     bool passed = true;
 
-    for (int32_t b = 0; b < options.batch_size; ++b) {
-      int32_t i = b * options.head_number;
-      // Problem size is the same for all heads
-      cutlass::gemm::GemmCoord problem0 = options.problem_sizes0.at(b * options.head_number);
-      cutlass::gemm::GemmCoord problem1 = options.problem_sizes1.at(b * options.head_number);
+    for (int32_t i = 0; i < problem_count(); ++i) {
+      cutlass::gemm::GemmCoord problem0 = options.problem_sizes0.at(i);
+      cutlass::gemm::GemmCoord problem1 = options.problem_sizes1.at(i);
+
+      LayoutQ layout_Q(ldq_host.at(i));
+      LayoutK layout_K(ldk_host.at(i));
+      LayoutP layout_P(ldp_host.at(i));
+      LayoutV layout_V(ldv_host.at(i));
+      LayoutO layout_O(ldo_host.at(i));
 
       MatrixCoord extent_Q{problem0.m(), problem0.k()};
       MatrixCoord extent_K{problem0.k(), problem0.n()};
       MatrixCoord extent_P{problem0.m(), problem0.n()};
       MatrixCoord extent_V{problem1.k(), problem1.n()};
       MatrixCoord extent_O{problem1.m(), problem1.n()};
 
-      LayoutO layout_O(ldo_host.at(i));
-      std::vector<ElementO> matrix_O(layout_O.capacity(extent_O));
-      cutlass::device_memory::copy_to_host(matrix_O.data(),   block_O.get() + offset_O.at(i), matrix_O.size());
-      cutlass::DeviceAllocation<ElementO>    block_Ref_O(layout_O.capacity(extent_O));
+      cutlass::TensorView<ElementQ, LayoutQ> view_Q(block_Q.get() + offset_Q.at(i), layout_Q, extent_Q);
+      cutlass::TensorView<ElementK, LayoutK> view_K(block_K.get() + offset_K.at(i), layout_K, extent_K);
+      cutlass::TensorView<ElementP, LayoutP> view_P(block_P.get() + offset_P.at(i), layout_P, extent_P);
+      cutlass::TensorView<ElementV, LayoutV> view_V(block_V.get() + offset_V.at(i), layout_V, extent_V);
 
-      for (int32_t h = 0; h < options.head_number; ++h) {
-        i = h + b * options.head_number;
+      cutlass::DeviceAllocation<ElementP>    block_Ref(layout_P.capacity(extent_P));
+      cutlass::TensorView<ElementP, LayoutP> view_Ref_device(block_Ref.get(), layout_P, extent_P);
 
-        LayoutQ layout_Q(ldq_host.at(i));
-        LayoutK layout_K(ldk_host.at(i));
-        LayoutP layout_P(ldp_host.at(i));
-        LayoutV layout_V(ldv_host.at(i));
-
-        cutlass::TensorView<ElementQ, LayoutQ> view_Q(block_Q.get() + offset_Q.at(i), layout_Q, extent_Q);
-        cutlass::TensorView<ElementK, LayoutK> view_K(block_K.get() + offset_K.at(i), layout_K, extent_K);
-        cutlass::TensorView<ElementV, LayoutV> view_V(block_V.get() + offset_V.at(i), layout_V, extent_V);
-        cutlass::TensorView<ElementO, LayoutO> view_Ref_O_device(block_Ref_O.get() + offset_O.at(i) - offset_O.at(b * options.head_number), layout_O, extent_O);
-
-        cutlass::DeviceAllocation<ElementP>    block_Ref_P(layout_P.capacity(extent_P));
-        cutlass::TensorView<ElementP, LayoutP> view_Ref_P_device(block_Ref_P.get(), layout_P, extent_P);
-
-        // Reference GEMM
-        cutlass::reference::device::GemmComplex<
-            ElementQ, LayoutQ,
-            ElementK, LayoutK,
-            ElementP, LayoutP, 
-            ElementCompute, ElementAccumulator
-        >(
-          problem0,
-          ElementAccumulator(options.alpha0), 
-          view_Q,
-          Attention::MM0::Mma::kTransformA,
-          view_K,
-          Attention::MM0::Mma::kTransformB,
-          ElementAccumulator(options.beta), 
-          view_Ref_P_device, 
-          view_Ref_P_device, 
-          ElementAccumulator(0)
-        );
-
-        // Compute softmax for P. We need to explicitly compute softmax
-        // over P because softmax is fused to the second GEMM in the
-        // profiled implementation.
-        std::vector<ElementP> matrix_Ref(layout_P.capacity(extent_P));
-        cutlass::device_memory::copy_to_host(matrix_Ref.data(), block_Ref_P.get(), matrix_Ref.size());
-        cutlass::TensorView<ElementP, LayoutP> view_Ref_host(matrix_Ref.data(), layout_P, extent_P);
-        std::vector<ElementNorm> vector_Norm_Ref(problem0.m());
-        std::vector<ElementSum> vector_Sum_Ref(problem0.m());
-
-        int n_dim = options.use_mask ? options.problem_sizes0_real.at(i).n() : problem0.n();
+      cutlass::DeviceAllocation<ElementO>    block_Ref_O(layout_O.capacity(extent_O));
+      cutlass::TensorView<ElementO, LayoutO> view_Ref_O_device(block_Ref_O.get(), layout_O, extent_O);
 
-        // Compute softmax for reference matrix
-        for (int m = 0; m < problem0.m(); m++) {
-          int n_dim_row = n_dim;
-          if (options.causal) {
-            n_dim_row = std::min(m + 1, n_dim);
-          }
-          ElementSoftmaxCompute max = ElementSoftmaxCompute(view_Ref_host.ref().at({m, 0}));
-          for (int n = 1; n < n_dim_row; n++) {
-            max = std::max(max, ElementSoftmaxCompute(view_Ref_host.ref().at({m, n})));
-          }
+      // Reference GEMM
+      cutlass::reference::device::GemmComplex<
+          ElementQ, LayoutQ,
+          ElementK, LayoutK,
+          ElementP, LayoutP, 
+          ElementCompute, ElementAccumulator
+      >(
+        problem0,
+        ElementAccumulator(options.alpha0), 
+        view_Q,
+        Attention::MM0::Mma::kTransformA,
+        view_K,
+        Attention::MM0::Mma::kTransformB,
+        ElementAccumulator(options.beta), 
+        view_P, 
+        view_Ref_device, 
+        ElementAccumulator(0)
+      );
+
+      // Compute softmax for P. We need to explicitly compute softmax
+      // over P because softmax is fused to the second GEMM in the
+      // profiled implementation.
+      std::vector<ElementP> matrix_Ref(layout_P.capacity(extent_P));
+      cutlass::device_memory::copy_to_host(matrix_Ref.data(), block_Ref.get(), matrix_Ref.size());
+      cutlass::TensorView<ElementP, LayoutP> view_Ref_host(matrix_Ref.data(), layout_P, extent_P);
+      std::vector<ElementNorm> vector_Norm_Ref(problem0.m());
+      std::vector<ElementSum> vector_Sum_Ref(problem0.m());
+
+      int n_dim = options.use_mask ? options.problem_sizes0_real.at(i).n() : problem0.n();
+
+      // Compute softmax for referece matrix
+      for (int m = 0; m < problem0.m(); m++) {
+        int n_dim_row = n_dim;
+        if (options.causal) {
+          n_dim_row = std::min(m + 1, n_dim);
+        }
+        ElementSoftmaxCompute max = ElementSoftmaxCompute(view_Ref_host.ref().at({m, 0}));
+        for (int n = 1; n < n_dim_row; n++) {
+           max = std::max(max, ElementSoftmaxCompute(view_Ref_host.ref().at({m, n})));
+        }
 
-          vector_Norm_Ref.at(m) = ElementNorm(max);
+        vector_Norm_Ref.at(m) = ElementNorm(max);
 
-          ElementSoftmaxCompute sum = ElementSoftmaxCompute();
-          for (int n = 0; n < n_dim_row; n++) {
-            sum += std::exp( ElementSoftmaxCompute(view_Ref_host.ref().at({m, n})) - max );
-          }
-          ElementSoftmaxCompute inv_sum = ElementSoftmaxCompute(1.0f / sum);
+        ElementSoftmaxCompute sum = ElementSoftmaxCompute();
+        for (int n = 0; n < n_dim_row; n++) {
+          sum += std::exp( ElementSoftmaxCompute(view_Ref_host.ref().at({m, n})) - max );
+        }
+        ElementSoftmaxCompute inv_sum = ElementSoftmaxCompute(1.0f / sum);
 
-          vector_Sum_Ref.at(m) = ElementSum(inv_sum);
+        vector_Sum_Ref.at(m) = ElementSum(inv_sum);
 
-          for (int n = 0; n < n_dim_row; n++) {
-            view_Ref_host.ref().at({m, n}) = ElementP(
-              std::exp( ElementSoftmaxCompute(view_Ref_host.ref().at({m, n})) - max ) * inv_sum
-            );
-          }
-          // Mask out the rest of the attention matrix
-          for (int n = n_dim_row; n < n_dim; ++n) {
-            view_Ref_host.ref().at({m, n}) = ElementP(0);
-          }
+        for (int n = 0; n < n_dim_row; n++) {
+          view_Ref_host.ref().at({m, n}) = ElementP(
+            std::exp( ElementSoftmaxCompute(view_Ref_host.ref().at({m, n})) - max ) * inv_sum
+          );
+        }
+        // Mask out the rest of the attention matrix
+        for (int n = n_dim_row; n < n_dim; ++n) {
+          view_Ref_host.ref().at({m, n}) = ElementP(0);
         }
+      }
 
-        // when not using mask, problem_real and problem share the same sizes
-        if (options.use_mask) {
-          for (int m = 0; m < problem0.m(); m++) {
-            for (int n = n_dim; n < problem0.n(); n++) {
-              view_Ref_host.ref().at({m, n}) = ElementP(0);
-            }
+      // when not using mask, problem_real and problem share the same sizes
+      if (options.use_mask) {
+        for (int m = 0; m < problem0.m(); m++) {
+          for (int n = n_dim; n < problem0.n(); n++) {
+            view_Ref_host.ref().at({m, n}) = ElementP(0);
           }
         }
+      }
 
-        cutlass::device_memory::copy_to_device(block_Ref_P.get(), matrix_Ref.data(), matrix_Ref.size());
+      cutlass::device_memory::copy_to_device(block_P.get() + offset_P.at(i), matrix_Ref.data(), matrix_Ref.size());
 
-        // Reference GEMM
-        cutlass::reference::device::GemmComplex<
-            ElementP, LayoutP,
-            ElementV, LayoutV,
-            ElementO, LayoutO, 
-            ElementCompute, ElementAccumulator
-        >(
-          problem1,
-          ElementAccumulator(options.alpha1), 
-          view_Ref_P_device,
-          Attention::MM0::Mma::kTransformA,
-          view_V,
-          Attention::MM0::Mma::kTransformB,
-          ElementAccumulator(options.beta), 
-          view_Ref_O_device, 
-          view_Ref_O_device, 
-          ElementAccumulator(0)
-        );
-      }
+      // Reference GEMM
+      cutlass::reference::device::GemmComplex<
+          ElementP, LayoutP,
+          ElementV, LayoutV,
+          ElementO, LayoutO, 
+          ElementCompute, ElementAccumulator
+      >(
+        problem1,
+        ElementAccumulator(options.alpha1), 
+        view_P,
+        Attention::MM0::Mma::kTransformA,
+        view_V,
+        Attention::MM0::Mma::kTransformB,
+        ElementAccumulator(options.beta), 
+        view_Ref_O_device, 
+        view_Ref_O_device, 
+        ElementAccumulator(0)
+      );
 
       // Copy to host memory
+      cutlass::TensorView<ElementP, LayoutP> view_Ref(matrix_Ref.data(), layout_P, extent_P);
+
+      std::vector<ElementO> matrix_O(layout_O.capacity(extent_O));
+      cutlass::device_memory::copy_to_host(matrix_O.data(),   block_O.get() + offset_O.at(i), matrix_O.size());
       std::vector<ElementO> matrix_Ref_O(layout_O.capacity(extent_O));
       cutlass::device_memory::copy_to_host(matrix_Ref_O.data(), block_Ref_O.get(), matrix_Ref_O.size());
 
       // printf("Pb %d: \n    Q=(offset=%d, ldq=%d)\n    K=(offset=%d, ldk=%d)\n    O=(offset=%d, ldo=%d)\n",
       //   int(i), int(offset_Q[i]), int(ldq_host[i]), int(offset_K[i]), int(ldk_host[i]), int(offset_O[i]), int(ldo_host[i]));
   
       bool verified_O = false;
@@ -801,15 +784,15 @@
       if (!verified_O) {
         verified_O = verify_tensor_<ElementO>(matrix_O, matrix_Ref_O);
       }
 
       passed = passed && verified_O;
 
       if (!passed) {
-        std::cerr << "\n***\nError - problem " << i << " (batch " << b << ") failed the QA check\n***\n" << std::endl;
+        std::cerr << "\n***\nError - problem " << i << " failed the QA check\n***\n" << std::endl;
 
         if (!verified_O) {
           std::cout << "Final matrix output is incorrect" << std::endl;
         }
 
         return passed;
       }
@@ -844,37 +827,34 @@
 
       // TODO: support arbitrary seq lengths
       // if (cu_seqlens_q.has_value()) {
       //   p.cu_seqlens_q_ptr = (int32_t*)cu_seqlens_q->data_ptr();
       //   p.cu_seqlens_k_ptr = (int32_t*)cu_seqlens_k->data_ptr();
       // }
 
-      p.scale = options.alpha0;
-
       p.num_heads = options.head_number;
       p.num_batches = options.batch_size;
       p.head_dim = options.head_size;
       p.head_dim_value = options.head_size_v;
       p.num_queries = options.seq_length;
       p.num_keys = options.seq_length_kv;
-      if (options.causal) {
-        p.custom_mask_type = Attention::CausalFromTopLeft;
-      }
+      p.causal = options.causal;
 
-      // All tensors are in BMHK shapes
-      p.q_strideH = options.head_size;
-      p.k_strideH = options.head_size;
-      p.v_strideH = options.head_size_v;
+      // TODO: This might overflow for big tensors
       p.q_strideM = int32_t(ldq_host[0]);
       p.k_strideM = int32_t(ldk_host[0]);
       p.v_strideM = int32_t(ldv_host[0]);
-      p.q_strideB = p.q_strideM * options.seq_length;
-      p.k_strideB = p.k_strideM * options.seq_length_kv;
-      p.v_strideB = p.v_strideM * options.seq_length_kv;
-      p.o_strideM = p.head_dim_value * p.num_heads;
+      p.q_strideH = p.q_strideM * options.seq_length;
+      p.k_strideH = p.k_strideM * options.seq_length_kv;
+      p.v_strideH = p.v_strideM * options.seq_length_kv;
+      p.o_strideH = options.head_size_v * options.seq_length;
+      p.q_strideB = p.q_strideH * options.head_number;
+      p.k_strideB = p.k_strideH * options.head_number;
+      p.v_strideB = p.v_strideH * options.head_number;
+      p.o_strideB = options.head_size_v * options.seq_length * options.head_number;
     }
 
     // launch kernel :)
     constexpr auto kernel_fn = attention_kernel_batched_impl<Attention>;
     int smem_bytes = sizeof(typename Attention::SharedStorage);
     if (smem_bytes > 0xc000) {
       cudaFuncSetAttribute(kernel_fn, cudaFuncAttributeMaxDynamicSharedMemorySize, smem_bytes);
@@ -1004,17 +984,15 @@
 int run_attention(Options& options) {
   using Attention = AttentionKernel<
     cutlass::half_t,      // scalar_t
     cutlass::arch::Sm80,  // ArchTag
     true,                 // Memory is aligned
     kQueriesPerBlock,
     kKeysPerBlock,
-    kSingleValueIteration,
-    false,                // Supports dropout
-    false                 // Supports bias
+    kSingleValueIteration
   >;
 
   //
   // Test and profile
   //
 
   TestbedAttention<Attention> testbed(options);
@@ -1084,15 +1062,15 @@
   if (options.alignment != 1) {
     std::cerr << "--alignment=1 is the only supported value\n";
     return -2;
   }
 
   // Determine kernel configuration based on head size.
   // If head size is less than or equal to 64, each block operates over 64 queries and
-  // 64 keys, and partial results can be stored in the register file.
+  // 64 keys, and parital results can be stored in the register file.
   // If head size is greater than 64, each block operates over 32 queries and 128 keys,
   // and partial results are stored in shared memory.
   if (options.head_size_v > 64) {
     static int const kQueriesPerBlock = 32;
     static int const kKeysPerBlock = 128;
     if (options.head_size_v <= kKeysPerBlock) {
       return run_attention<kQueriesPerBlock, kKeysPerBlock, true>(options);
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/41_fused_multi_head_attention/fused_multihead_attention_variable_seqlen.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/examples/41_fused_multi_head_attention/fused_multihead_attention_variable_seqlen.cu`

 * *Files 0% similar despite different names*

```diff
@@ -917,15 +917,14 @@
       ptr_O_accumulate.get(),
       ldq.get(),
       ldk.get(),
       ldp.get(),
       ldv.get(),
       ldo.get(),
       options.causal,
-      options.alpha0,
       options.problem_sizes1.data()
     );
 
     Attention fmha;
 
     size_t workspace_size = fmha.get_workspace_size(args);
     cutlass::DeviceAllocation<uint8_t> workspace(workspace_size);
@@ -1169,15 +1168,15 @@
   if (options.alignment != 1) {
     std::cerr << "--alignment=1 is the only supported value\n";
     return -2;
   }
 
   // Determine kernel configuration based on head size.
   // If head size is less than or equal to 64, each block operates over 64 queries and
-  // 64 keys, and partial results can be stored in the register file.
+  // 64 keys, and parital results can be stored in the register file.
   // If head size is greater than 64, each block operates over 32 queries and 128 keys,
   // and partial results are stored in shared memory.
   if (options.head_size_v > 64) {
     static int const kQueriesPerBlock = 32;
     static int const kKeysPerBlock = 128;
     if (options.head_size_v <= kKeysPerBlock) {
       return run_attention<kQueriesPerBlock, kKeysPerBlock, true>(options);
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/41_fused_multi_head_attention/gemm/custom_mma.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/examples/41_fused_multi_head_attention/gemm/custom_mma.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/41_fused_multi_head_attention/gemm/custom_mma_base.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/examples/41_fused_multi_head_attention/gemm/custom_mma_base.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/41_fused_multi_head_attention/gemm/custom_mma_multistage.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/examples/41_fused_multi_head_attention/gemm/custom_mma_multistage.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/41_fused_multi_head_attention/gemm/custom_mma_pipelined.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/examples/41_fused_multi_head_attention/gemm/custom_mma_pipelined.h`

 * *Files 0% similar despite different names*

```diff
@@ -306,15 +306,15 @@
     int smem_write_stage_idx = 1;
 
     // Avoid reading out of bounds
     iterator_A.clear_mask(gemm_k_iterations <= 1);
     iterator_B.clear_mask(gemm_k_iterations <= 1);
 
     // Issue loads during the first warp-level matrix multiply-add *AFTER*
-    // issuing shared memory loads (which have the tightest latency requirement).
+    // issuing shared memory loads (which have the tighest latency requirement).
 
     //
     // Mainloop
     //
 
     // Note: The main loop does not support Base::kWarpGemmIterations == 2.
     CUTLASS_GEMM_LOOP
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/41_fused_multi_head_attention/gemm/find_default_mma.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/examples/41_fused_multi_head_attention/gemm/find_default_mma.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/41_fused_multi_head_attention/gemm/mma_accum_lambda_iterator.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/examples/41_fused_multi_head_attention/gemm/mma_accum_lambda_iterator.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/41_fused_multi_head_attention/gemm/mma_from_smem.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/examples/41_fused_multi_head_attention/gemm/mma_from_smem.h`

 * *Files 0% similar despite different names*

```diff
@@ -596,15 +596,15 @@
     int smem_write_stage_idx = 1;
 
     // Avoid reading out of bounds
     iterator_B.set_residual_tile(gemm_k_iterations == 2);
     iterator_B.clear_mask(gemm_k_iterations <= 1);
 
     // Issue loads during the first warp-level matrix multiply-add *AFTER*
-    // issuing shared memory loads (which have the tightest latency requirement).
+    // issuing shared memory loads (which have the tighest latency requirement).
 
     //
     // Mainloop
     //
 
     // Note: The main loop does not support Base::kWarpGemmIterations == 2.
     CUTLASS_GEMM_LOOP
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/41_fused_multi_head_attention/gemm_kernel_utils.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/examples/41_fused_multi_head_attention/gemm_kernel_utils.h`

 * *Files 17% similar despite different names*

```diff
@@ -32,28 +32,28 @@
 #pragma once
 
 #include "cutlass/arch/mma.h"
 
 ////////////////////////////////////////////////////////////////////////////////
 // Some helper functions
 ////////////////////////////////////////////////////////////////////////////////
-#define DISPATCH_TYPES(tensor, func)                                           \
-  {                                                                            \
-    if (query.scalar_type() == at::ScalarType::Float) {                        \
-      using scalar_t = float;                                                  \
-      func();                                                                  \
-    } else if (query.scalar_type() == at::ScalarType::Half) {                  \
-      using scalar_t = cutlass::half_t;                                        \
-      func();                                                                  \
-    } else if (query.scalar_type() == at::ScalarType::BFloat16) {              \
-      using scalar_t = cutlass::bfloat16_t;                                    \
-      func();                                                                  \
-    } else {                                                                   \
-      XFORMERS_CHECK(false, "Only fp32, half & bf16 supported at the moment"); \
-    }                                                                          \
+#define DISPATCH_TYPES(tensor, func)                                        \
+  {                                                                         \
+    if (query.scalar_type() == at::ScalarType::Float) {                     \
+      using scalar_t = float;                                               \
+      func();                                                               \
+    } else if (query.scalar_type() == at::ScalarType::Half) {               \
+      using scalar_t = cutlass::half_t;                                     \
+      func();                                                               \
+    } else if (query.scalar_type() == at::ScalarType::BFloat16) {           \
+      using scalar_t = cutlass::bfloat16_t;                                 \
+      func();                                                               \
+    } else {                                                                \
+      TORCH_CHECK(false, "Only fp32, half & bf16 supported at the moment"); \
+    }                                                                       \
   }
 
 #define DISPATCH_BOOL(BOOL_V, BOOL_NAME, F) \
   {                                         \
     if (BOOL_V) {                           \
       constexpr bool BOOL_NAME = true;      \
       F();                                  \
@@ -73,78 +73,125 @@
     } else if (CC >= 70) {                                                \
       using ArchTag = cutlass::arch::Sm70;                                \
       func();                                                             \
     } else if (CC >= 50) {                                                \
       using ArchTag = cutlass::arch::Sm50;                                \
       func();                                                             \
     } else {                                                              \
-      XFORMERS_CHECK(                                                     \
+      TORCH_CHECK(                                                        \
           false,                                                          \
           "Your device is too old. We require compute capability >= 50"); \
     }                                                                     \
   }
 
-#define CHECK_NOSPARSE_CONTIGUOUS_CUDA(TENSOR)                            \
-  XFORMERS_CHECK(TENSOR.is_cuda(), #TENSOR " must be a CUDA tensor");     \
-  XFORMERS_CHECK(!TENSOR.is_sparse(), #TENSOR " must be a dense tensor"); \
-  XFORMERS_CHECK(TENSOR.is_contiguous());
-
-#define CHECK_NOSPARSE_LASTCONTIGUOUS_CUDA(TENSOR)                        \
-  XFORMERS_CHECK(TENSOR.is_cuda(), #TENSOR " must be a CUDA tensor");     \
-  XFORMERS_CHECK(!TENSOR.is_sparse(), #TENSOR " must be a dense tensor"); \
-  XFORMERS_CHECK(                                                         \
+#define CHECK_NOSPARSE_CONTIGUOUS_CUDA(TENSOR)                         \
+  TORCH_CHECK(TENSOR.is_cuda(), #TENSOR " must be a CUDA tensor");     \
+  TORCH_CHECK(!TENSOR.is_sparse(), #TENSOR " must be a dense tensor"); \
+  TORCH_CHECK(TENSOR.is_contiguous());
+
+#define CHECK_NOSPARSE_LASTCONTIGUOUS_CUDA(TENSOR)                     \
+  TORCH_CHECK(TENSOR.is_cuda(), #TENSOR " must be a CUDA tensor");     \
+  TORCH_CHECK(!TENSOR.is_sparse(), #TENSOR " must be a dense tensor"); \
+  TORCH_CHECK(                                                         \
       TENSOR.stride(-1) == 1, #TENSOR ": last dimension must be contiguous");
 
-#ifdef TORCH_CHECK
+#ifdef HAS_PYTORCH
 #define CHECK_ALIGNED_PTR(PTR, ALIGNMENT) \
-  XFORMERS_CHECK(                         \
-      uint64_t(PTR) % ALIGNMENT == 0, #PTR " is not correctly aligned")
+  TORCH_CHECK(uint64_t(PTR) % ALIGNMENT == 0, #PTR " is not correctly aligned")
 #define XFORMERS_CHECK TORCH_CHECK
 #elif defined(__CUDACC_RTC__)
 #define CHECK_ALIGNED_PTR(PTR, ALIGNMENT)  \
   if (!(uint64_t(PTR) % ALIGNMENT == 0)) { \
     return false;                          \
   }
 #define XFORMERS_CHECK(COND, ERR) \
   if (!(COND)) {                  \
     return false;                 \
   }
 #else
-#include <iostream>
 #define CHECK_ALIGNED_PTR(PTR, ALIGNMENT)            \
   if (!(uint64_t(PTR) % ALIGNMENT == 0)) {           \
     std::cerr << #PTR " is not correctly aligned\n"; \
     return false;                                    \
   }
 #define XFORMERS_CHECK(COND, ERR)   \
   if (!(COND)) {                    \
     std::cerr << #COND " failed\n"; \
     return false;                   \
   }
 #endif
 
-#define ASSIGN_CHECK_OVERFLOW(A, B)                                    \
-  {                                                                    \
-    A = B;                                                             \
-    XFORMERS_CHECK(                                                    \
-        B < std::numeric_limits<decltype(A)>::max(), #B " overflows"); \
+#define ASSIGN_CHECK_OVERFLOW(A, B)                                \
+  {                                                                \
+    A = B;                                                         \
+    TORCH_CHECK(                                                   \
+        B < cutlass::platform::numeric_limits<decltype(A)>::max(), \
+        #B " overflows");                                          \
   }
 
 namespace gemm_kernel_utils {
 
+#ifdef HAS_PYTORCH
+template <typename scalar_t>
+struct TypeTraits;
+
+template <>
+struct TypeTraits<cutlass::half_t> {
+  using scalar_t = cutlass::half_t;
+
+  static constexpr __host__ at::ScalarType atScalarType() {
+    return at::ScalarType::Half;
+  }
+  template <int nDim>
+  static __host__ at::PackedTensorAccessor32<scalar_t, nDim> packed_accessor(
+      at::Tensor const& tensor) {
+    return at::PackedTensorAccessor32<scalar_t, nDim>(
+        (scalar_t*)(tensor.data_ptr()),
+        tensor.sizes().data(),
+        tensor.strides().data());
+  }
+};
+
+template <>
+struct TypeTraits<cutlass::bfloat16_t> {
+  using scalar_t = cutlass::bfloat16_t;
+
+  static constexpr __host__ at::ScalarType atScalarType() {
+    return at::ScalarType::BFloat16;
+  }
+  template <int nDim>
+  static __host__ at::PackedTensorAccessor32<scalar_t, nDim> packed_accessor(
+      at::Tensor const& tensor) {
+    return at::PackedTensorAccessor32<scalar_t, nDim>(
+        (scalar_t*)(tensor.data_ptr()),
+        tensor.sizes().data(),
+        tensor.strides().data());
+  }
+};
+
+template <>
+struct TypeTraits<float> {
+  using scalar_t = float;
+
+  static constexpr __host__ at::ScalarType atScalarType() {
+    return at::ScalarType::Float;
+  }
+  template <int nDim>
+  static __host__ at::PackedTensorAccessor32<scalar_t, nDim> packed_accessor(
+      at::Tensor const& tensor) {
+    return tensor.packed_accessor32<scalar_t, nDim>();
+  }
+};
+#endif
+
 template <typename integer>
 constexpr CUTLASS_HOST_DEVICE integer ceil_div(integer n, integer m) {
   return (n + m - 1) / m;
 }
 
-template <typename integer>
-constexpr CUTLASS_HOST_DEVICE integer align_up(integer n, integer m) {
-  return ((n + m - 1) / m) * m;
-}
-
 ////////////////////////////////////////////////////////////////////////////////
 // Determine the type of GEMM we do (TensorCores or not, Shapes ...)
 // TODO: Maybe we could rely on Cutlass's DefaultGemm templates
 ////////////////////////////////////////////////////////////////////////////////
 
 // Fallback to Simt (FMA on cuda cores) if not in a special case below
 template <typename ArchTag, typename scalar_t_, typename Enable = void>
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/41_fused_multi_head_attention/iterators/epilogue_predicated_tile_iterator.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/examples/41_fused_multi_head_attention/iterators/epilogue_predicated_tile_iterator.h`

 * *Files 0% similar despite different names*

```diff
@@ -307,17 +307,17 @@
 
           CUTLASS_PRAGMA_UNROLL
           for (int column = 0; column < ThreadMap::Iterations::kColumn;
                ++column) {
             // on windows using unsigned long here gives the error
             // error: asm operand type size(4) does not match
             // type/size implied by constraint 'l'
-            uint64_t addr = (uint64_t)((void*)&memory_pointer
-                                           [column * ThreadMap::Delta::kColumn /
-                                            kElementsPerAccess]);
+            uint64_t addr = (uint64_t)(
+                (void*)&memory_pointer
+                    [column * ThreadMap::Delta::kColumn / kElementsPerAccess]);
             asm volatile("prefetch.global.L1 [ %1 ];" : "=l"(addr) : "l"(addr));
           }
 
           if (row + 1 < ThreadMap::Iterations::kRow) {
             if (!ScatterD) {
               byte_pointer += params_.increment_row;
             }
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/41_fused_multi_head_attention/iterators/make_residual_last.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/examples/41_fused_multi_head_attention/iterators/make_residual_last.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/41_fused_multi_head_attention/iterators/predicated_tile_access_iterator_residual_last.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/examples/41_fused_multi_head_attention/iterators/predicated_tile_access_iterator_residual_last.h`

 * *Files 0% similar despite different names*

```diff
@@ -177,15 +177,15 @@
   /// Parameters object with precomputed internal state
   Params const& params_;
 
   /// Internal pointer to first access of tile
   BytePointer pointer_;
 
   /// Below is used when Gather is turned on.  We need to record strided_offset
-  /// and contiguous_offset separated to compute the offset by using
+  /// and contiguous_offset seperated to compute the offset by using
   ///
   /// offset = contiguous_offset + indices[strided_offset]
   ///
 
   /// Gather indices
   int const* indices_;
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/41_fused_multi_head_attention/iterators/predicated_tile_iterator_residual_last.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/examples/41_fused_multi_head_attention/iterators/predicated_tile_iterator_residual_last.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/41_fused_multi_head_attention/iterators/transpose_warp_iterator.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/examples/41_fused_multi_head_attention/iterators/transpose_warp_iterator.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/41_fused_multi_head_attention/iterators/warp_iterator_from_smem.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/examples/41_fused_multi_head_attention/iterators/warp_iterator_from_smem.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/41_fused_multi_head_attention/kernel_forward.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/examples/41_fused_multi_head_attention/kernel_forward.h`

 * *Files 17% similar despite different names*

```diff
@@ -28,31 +28,30 @@
  * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
  *
  **************************************************************************************************/
 
 #pragma once
 
 #ifdef HAS_PYTORCH
-#include <ATen/cuda/CUDAGeneratorImpl.h>
-#include <ATen/cuda/CUDAGraphsUtils.cuh>
+#include <ATen/ATen.h>
+#include <ATen/cuda/CUDAContext.h>
+#include <c10/cuda/CUDAGuard.h>
+#include <torch/library.h>
 #endif
 
-#include <curand_kernel.h>
 #include <cmath>
 #include <vector>
 
 #include "cutlass/bfloat16.h"
-#include "cutlass/fast_math.h"
 #include "cutlass/gemm/gemm.h"
 #include "cutlass/layout/matrix.h"
 #include "cutlass/layout/vector.h"
-#include "cutlass/matrix.h"
 #include "cutlass/numeric_types.h"
-#include "cutlass/tensor_ref.h"
 
+#include "attention_scaling_coefs_updater.h"
 #include "cutlass/epilogue/threadblock/default_epilogue_simt.h"
 #include "cutlass/epilogue/threadblock/default_epilogue_tensor_op.h"
 #include "cutlass/epilogue/threadblock/default_epilogue_volta_tensor_op.h"
 #include "cutlass/gemm/device/default_gemm_configuration.h"
 #include "cutlass/gemm/kernel/default_gemm.h"
 #include "cutlass/gemm/threadblock/default_mma.h"
 #include "cutlass/gemm/threadblock/default_mma_core_simt.h"
@@ -60,77 +59,56 @@
 #include "cutlass/gemm/threadblock/default_mma_core_sm75.h"
 #include "cutlass/gemm/threadblock/default_mma_core_sm80.h"
 #include "cutlass/gemm/threadblock/threadblock_swizzle.h"
 #include "cutlass/matrix_shape.h"
 #include "cutlass/platform/platform.h"
 #include "cutlass/transform/threadblock/predicated_tile_iterator.h"
 #include "debug_utils.h"
-#include "epilogue/epilogue_pipelined.h"
-#include "epilogue/epilogue_rescale_output.h"
-#include "gemm/find_default_mma.h"
-#include "gemm/mma_from_smem.h"
+#include "epilogue_pipelined.h"
+#include "epilogue_rescale_output.h"
+#include "find_default_mma.h"
 #include "gemm_kernel_utils.h"
-#include "transform/tile_smem_loader.h"
+#include "mma_from_smem.h"
 
 #include <inttypes.h>
 
 using namespace gemm_kernel_utils;
 
 namespace {
 template <typename scalar_t, typename Arch>
 constexpr int getWarpsPerSm() {
   return (
       Arch::kMinComputeCapability >= 80 &&
               !cutlass::platform::is_same<scalar_t, float>::value
           ? 16
           : 12);
 }
-static CUTLASS_DEVICE float atomicMaxFloat(float* addr, float value) {
-  // source: https://stackoverflow.com/a/51549250
-  return (value >= 0)
-      ? __int_as_float(atomicMax((int*)addr, __float_as_int(value)))
-      : __uint_as_float(atomicMin((unsigned int*)addr, __float_as_uint(value)));
-}
 } // namespace
 
 template <
     // The datatype of Q/K/V
     typename scalar_t_,
     // Architecture we are targeting (eg `cutlass::arch::Sm80`)
     typename ArchTag,
     // If Q/K/V are correctly aligned in memory and we can run a fast kernel
     bool isAligned_,
     int kQueriesPerBlock,
-    int kKeysPerBlock_,
-    bool kSingleValueIteration_, // = `value.shape[-1] <= kKeysPerBlock`
-    // This is quite slower on V100 for some reason
-    // Set to false if you know at compile-time you will never need dropout
-    bool kSupportsDropout_ = true,
-    bool kSupportsBias_ = true>
+    int kKeysPerBlock,
+    bool kSingleValueIteration // = `value.shape[-1] <= kKeysPerBlock`
+    >
 struct AttentionKernel {
-  enum CustomMaskType {
-    NoCustomMask = 0,
-    CausalFromTopLeft = 1,
-    CausalFromBottomRight = 2,
-    NumCustomMaskTypes,
-  };
-
   using scalar_t = scalar_t_;
   using accum_t = float;
   using lse_scalar_t = float;
   using output_t = scalar_t;
   // Accumulator between 2 iterations
   // Using `accum_t` improves perf on f16 at the cost of
   // numerical errors
   using output_accum_t = accum_t;
-  static constexpr bool kSupportsDropout = kSupportsDropout_;
-  static constexpr bool kSupportsBias = kSupportsBias_;
-  static constexpr int kKeysPerBlock = kKeysPerBlock_;
   static constexpr bool kIsAligned = isAligned_;
-  static constexpr bool kSingleValueIteration = kSingleValueIteration_;
   static constexpr int32_t kAlignLSE = 32; // block size of backward
   static constexpr bool kPreloadV = ArchTag::kMinComputeCapability >= 80 &&
       cutlass::sizeof_bits<scalar_t>::value == 16;
   static constexpr bool kKeepOutputInRF = kSingleValueIteration;
   static constexpr bool kNeedsOutputAccumulatorBuffer = !kKeepOutputInRF &&
       !cutlass::platform::is_same<output_accum_t, output_t>::value;
 
@@ -144,215 +122,139 @@
   static constexpr int kNumThreads = kWarpSize * kNumWarpsPerBlock;
   static constexpr int kMinBlocksPerSm =
       getWarpsPerSm<scalar_t, ArchTag>() / kNumWarpsPerBlock;
 
   struct Params {
     // Input tensors
     scalar_t* query_ptr; // [num_queries, num_heads, head_dim]
-    scalar_t* key_ptr; // [num_keys, num_heads, head_dim]
+    scalar_t* key_ptr;   // [num_keys, num_heads, head_dim]
     scalar_t* value_ptr; // [num_keys, num_heads, head_dim_value]
-    scalar_t* attn_bias_ptr = nullptr; // [num_heads, num_queries, num_keys]
-    int32_t* seqstart_q_ptr = nullptr;
-    int32_t* seqstart_k_ptr = nullptr;
-
-    int32_t* causal_diagonal_ptr = nullptr;
-    int32_t* seqlen_k_ptr = nullptr;
-    uint32_t causal_diagonal_offset = 0;
+    int32_t* cu_seqlens_q_ptr = nullptr;
+    int32_t* cu_seqlens_k_ptr = nullptr;
 
     // Output tensors
     output_t* output_ptr; // [num_queries, num_heads, head_dim_value]
     output_accum_t*
         output_accum_ptr; // [num_queries, num_heads, head_dim_value]
     lse_scalar_t* logsumexp_ptr; // [num_heads, num_queries] - can be null
 
-    // Scale
-    accum_t scale;
-
     // Dimensions/strides
     int32_t head_dim;
     int32_t head_dim_value;
     int32_t num_queries;
     int32_t num_keys;
 
-    uint8_t custom_mask_type = NoCustomMask;
+    bool causal;
 
     int32_t q_strideM;
     int32_t k_strideM;
     int32_t v_strideM;
-    int32_t bias_strideM = 0;
-
-    int32_t o_strideM = 0;
 
     // Everything below is only used in `advance_to_block`
     // and shouldn't use registers
     int32_t q_strideH;
     int32_t k_strideH;
     int32_t v_strideH;
-    int32_t bias_strideH = 0;
-
+    int32_t o_strideH;
     int64_t q_strideB;
     int64_t k_strideB;
     int64_t v_strideB;
-    int32_t bias_strideB = 0;
-
+    int64_t o_strideB;
     int32_t num_batches;
     int32_t num_heads;
 
-    // dropout
-    bool use_dropout;
-    unsigned long long dropout_batch_head_rng_offset;
-    float dropout_prob;
-#ifdef HAS_PYTORCH
-    at::PhiloxCudaState rng_engine_inputs;
-#endif
+    CUTLASS_HOST_DEVICE int32_t o_strideM() const {
+      return head_dim_value;
+    }
 
     // Moves pointers to what we should process
     // Returns "false" if there is no work to do
     CUTLASS_DEVICE bool advance_to_block() {
       auto batch_id = blockIdx.z;
       auto head_id = blockIdx.y;
       auto query_start = blockIdx.x * kQueriesPerBlock;
 
       auto lse_dim = ceil_div((int32_t)num_queries, kAlignLSE) * kAlignLSE;
 
-      if (kSupportsDropout) {
-        dropout_batch_head_rng_offset =
-            batch_id * num_heads * num_queries * num_keys +
-            head_id * num_queries * num_keys;
-      }
-
       int64_t q_start, k_start;
       // Advance to current batch - in case of different sequence lengths
-      if (seqstart_q_ptr != nullptr) {
-        assert(seqstart_k_ptr != nullptr);
-        seqstart_q_ptr += batch_id;
-
-        q_start = seqstart_q_ptr[0];
-        int64_t q_next_start = seqstart_q_ptr[1];
-        int64_t k_end;
-        seqstart_k_ptr += batch_id;
-
-        if (seqlen_k_ptr) {
-          k_start = seqstart_k_ptr[0];
-          k_end = k_start + seqlen_k_ptr[batch_id];
-        } else {
-          k_start = seqstart_k_ptr[0];
-          k_end = seqstart_k_ptr[1];
-        }
-
+      if (cu_seqlens_q_ptr != nullptr) {
+        assert(cu_seqlens_k_ptr != nullptr);
+        cu_seqlens_q_ptr += batch_id;
+        cu_seqlens_k_ptr += batch_id;
+        q_start = cu_seqlens_q_ptr[0];
+        k_start = cu_seqlens_k_ptr[0];
+        int64_t q_next_start = cu_seqlens_q_ptr[1];
+        int64_t k_next_start = cu_seqlens_k_ptr[1];
         num_queries = q_next_start - q_start;
-        num_keys = k_end - k_start;
+        num_keys = k_next_start - k_start;
 
         if (query_start >= num_queries) {
           return false;
         }
       } else {
         query_ptr += batch_id * q_strideB;
         key_ptr += batch_id * k_strideB;
         value_ptr += batch_id * v_strideB;
-        output_ptr += int64_t(batch_id * num_queries) * o_strideM;
+        output_ptr += batch_id * o_strideB;
         if (output_accum_ptr != nullptr) {
-          output_accum_ptr +=
-              int64_t(batch_id * num_queries) * (head_dim_value * num_heads);
+          output_accum_ptr += batch_id * o_strideB;
         }
         q_start = 0;
         k_start = 0;
       }
 
       // Advance to the current batch / head / query_start
       query_ptr += (q_start + query_start) * q_strideM + head_id * q_strideH;
       key_ptr += k_start * k_strideM + head_id * k_strideH;
-
       value_ptr += k_start * v_strideM + head_id * v_strideH;
-      output_ptr +=
-          int64_t(q_start + query_start) * o_strideM + head_id * head_dim_value;
+      output_ptr += int64_t(q_start + query_start) * o_strideM() +
+          head_id * o_strideH;
 
-      if (kSupportsBias && attn_bias_ptr != nullptr) {
-        attn_bias_ptr += (batch_id * bias_strideB) + (head_id * bias_strideH);
-      }
       if (output_accum_ptr != nullptr) {
-        output_accum_ptr +=
-            int64_t(q_start + query_start) * (head_dim_value * num_heads) +
-            head_id * head_dim_value;
+        output_accum_ptr += int64_t(q_start + query_start) * o_strideM() +
+            head_id * o_strideH;
       } else {
         // Accumulate directly in the destination buffer (eg for f32)
         output_accum_ptr = (accum_t*)output_ptr;
       }
-
       if (logsumexp_ptr != nullptr) {
         // lse[batch_id, head_id, query_start]
         logsumexp_ptr +=
             batch_id * lse_dim * num_heads + head_id * lse_dim + query_start;
       }
 
-      // Custom masking
-      if (causal_diagonal_ptr) {
-        causal_diagonal_offset = causal_diagonal_ptr[batch_id];
-      }
-      if (custom_mask_type == CausalFromBottomRight) {
-        causal_diagonal_offset += num_keys - num_queries;
-      }
-      if (custom_mask_type == CausalFromTopLeft ||
-          custom_mask_type == CausalFromBottomRight) {
-        // the bottom row of the current block is query_start + kQueriesPerBlock
-        // the last active key is then query_start + causal_diagonal_offset +
-        // kQueriesPerBlock so num_keys is the min between actual num_keys and
-        // this to avoid extra computations
+      num_queries -= query_start;
+      if (causal) {
         num_keys = cutlass::fast_min(
-            int32_t(query_start + causal_diagonal_offset + kQueriesPerBlock),
-            num_keys);
+            int32_t(query_start + kQueriesPerBlock), num_keys);
       }
-
-      num_queries -= query_start;
       num_batches = 0; // no longer used after
 
-      // If num_queries == 1, and there is only one key head we're wasting
-      // 15/16th of tensor core compute In that case :
-      //  - we only launch kernels for head_id % kQueriesPerBlock == 0
-      //  - we iterate over heads instead of queries (strideM = strideH)
-      if (num_queries == 1 && k_strideH == 0 && v_strideH == 0) {
-        if (head_id % kQueriesPerBlock != 0)
-          return false;
-        q_strideM = q_strideH;
-        num_queries = num_heads;
-        num_heads = 1; // unused but here for intent
-        // remove causal since n_query = 1
-        // otherwise, offset would change with head !
-        custom_mask_type = NoCustomMask;
-        o_strideM = head_dim_value;
-      }
-
       // Make sure the compiler knows these variables are the same on all
       // the threads of the warp.
       query_ptr = warp_uniform(query_ptr);
       key_ptr = warp_uniform(key_ptr);
       value_ptr = warp_uniform(value_ptr);
-      if (kSupportsBias) {
-        attn_bias_ptr = warp_uniform(attn_bias_ptr);
-      }
       output_ptr = warp_uniform(output_ptr);
       output_accum_ptr = warp_uniform(output_accum_ptr);
       logsumexp_ptr = warp_uniform(logsumexp_ptr);
       num_queries = warp_uniform(num_queries);
       num_keys = warp_uniform(num_keys);
-      num_heads = warp_uniform(num_heads);
       head_dim = warp_uniform(head_dim);
       head_dim_value = warp_uniform(head_dim_value);
-      o_strideM = warp_uniform(o_strideM);
-      custom_mask_type = warp_uniform(custom_mask_type);
       return true;
     }
 
     __host__ dim3 getBlocksGrid() const {
       return dim3(
           ceil_div(num_queries, (int32_t)kQueriesPerBlock),
           num_heads,
           num_batches);
     }
-
     __host__ dim3 getThreadsGrid() const {
       return dim3(kWarpSize, kNumWarpsPerBlock, 1);
     }
   };
 
   struct MM0 {
     /*
@@ -399,32 +301,24 @@
                                 // uses too much smem
         typename GemmType::Operator // Operator
         >::DefaultMma;
     using MmaCore = typename DefaultMma::MmaCore;
     using IteratorA = typename DefaultMma::IteratorA;
     using IteratorB = typename DefaultMma::IteratorB;
     using Mma = typename DefaultMma::ThreadblockMma;
-    using AccumLambdaIterator = typename DefaultMmaAccumLambdaIterator<
+    using ScalingCoefsUpdater = typename DefaultAttentionScalingCoefsUpdater<
         typename Mma::Operator::IteratorC,
         accum_t,
-        kWarpSize>::Iterator;
+        kWarpSize>::Updater;
     static_assert(
         MmaCore::WarpCount::kM * MmaCore::WarpCount::kN *
                 MmaCore::WarpCount::kK ==
             kNumWarpsPerBlock,
         "");
 
-    // used for efficient load of bias tile Bij from global to shared memory
-    using BiasLoader = TileSmemLoader<
-        scalar_t,
-        cutlass::MatrixShape<kQueriesPerBlock, kKeysPerBlock>,
-        MmaCore::kThreads,
-        // input restriction: kv_len has to be a multiple of this value
-        128 / cutlass::sizeof_bits<scalar_t>::value>;
-
     // Epilogue to store to shared-memory in a format that we can use later for
     // the second matmul
     using B2bGemm = typename cutlass::gemm::threadblock::B2bGemm<
         typename Mma::Operator::IteratorC,
         typename Mma::Operator,
         scalar_t,
         WarpShape,
@@ -478,16 +372,15 @@
         DefaultConfig::kStages,
         false, // SplitKSerial
         typename GemmType::Operator>;
 
     using DefaultMmaFromSmem =
         typename cutlass::gemm::threadblock::DefaultMmaFromSharedMemory<
             typename DefaultGemm::Mma,
-            typename MM0::AccumulatorSharedStorage,
-            false>; // kScaleOperandA
+            typename MM0::AccumulatorSharedStorage>;
     using Mma = typename DefaultMmaFromSmem::Mma;
     using IteratorB = typename Mma::IteratorB;
     using WarpCount = typename Mma::WarpCount;
     static_assert(
         WarpCount::kM * WarpCount::kN * WarpCount::kK == kNumWarpsPerBlock,
         "");
 
@@ -516,18 +409,15 @@
     cutlass::Array<accum_t, kQueriesPerBlock> s_prime;
     cutlass::Array<accum_t, kQueriesPerBlock> mi;
   };
 
   struct SharedStorageEpilogueAtEnd : ScalingCoefs {
     struct SharedStorageAfterMM0 {
       // Everything here might be overwritten during MM0
-      union {
-        typename MM0::BiasLoader::SmemTile bias;
-        typename MM0::AccumulatorSharedStorage si;
-      };
+      typename MM0::AccumulatorSharedStorage si;
       typename MM1::SharedStorageMM1 mm1;
     };
 
     union {
       typename MM0::Mma::SharedStorage mm0;
       SharedStorageAfterMM0 after_mm0;
       typename MM1::DefaultEpilogue::SharedStorage epilogue;
@@ -538,18 +428,15 @@
       return epilogue;
     }
   };
 
   struct SharedStorageEpilogueInLoop : ScalingCoefs {
     struct SharedStorageAfterMM0 {
       // Everything here might be overwritten during MM0
-      union {
-        typename MM0::BiasLoader::SmemTile bias;
-        typename MM0::AccumulatorSharedStorage si;
-      };
+      typename MM0::AccumulatorSharedStorage si;
       typename MM1::SharedStorageMM1 mm1;
       typename MM1::DefaultEpilogue::SharedStorage epilogue;
     };
 
     union {
       typename MM0::Mma::SharedStorage mm0;
       SharedStorageAfterMM0 after_mm0;
@@ -566,114 +453,74 @@
       SharedStorageEpilogueAtEnd,
       SharedStorageEpilogueInLoop>::type;
 
   static bool __host__ check_supported(Params const& p) {
     CHECK_ALIGNED_PTR(p.query_ptr, kAlignmentQ);
     CHECK_ALIGNED_PTR(p.key_ptr, kAlignmentK);
     CHECK_ALIGNED_PTR(p.value_ptr, kAlignmentV);
-    if (kSupportsBias) {
-      CHECK_ALIGNED_PTR(p.attn_bias_ptr, kAlignmentQ);
-      XFORMERS_CHECK(
-          p.bias_strideB % kAlignmentQ == 0,
-          "attn_bias is not correctly aligned");
-      XFORMERS_CHECK(
-          p.bias_strideH % kAlignmentQ == 0,
-          "attn_bias is not correctly aligned");
-      XFORMERS_CHECK(
-          p.bias_strideM % kAlignmentQ == 0,
-          "attn_bias is not correctly aligned");
-    }
     XFORMERS_CHECK(
         p.q_strideM % kAlignmentQ == 0, "query is not correctly aligned");
     XFORMERS_CHECK(
         p.k_strideM % kAlignmentK == 0, "key is not correctly aligned");
     XFORMERS_CHECK(
         p.v_strideM % kAlignmentV == 0, "value is not correctly aligned");
     XFORMERS_CHECK(
         p.q_strideH % kAlignmentQ == 0, "query is not correctly aligned");
     XFORMERS_CHECK(
         p.k_strideH % kAlignmentK == 0, "key is not correctly aligned");
     XFORMERS_CHECK(
         p.v_strideH % kAlignmentV == 0, "value is not correctly aligned");
-    XFORMERS_CHECK(
-        p.causal_diagonal_ptr == nullptr || p.custom_mask_type != NoCustomMask,
-        "`causal_diagonal_ptr` is only useful when `custom_mask_type` is causal");
-    XFORMERS_CHECK(
-        p.custom_mask_type < NumCustomMaskTypes,
-        "invalid value for `custom_mask_type`");
     return true;
   }
 
   static void CUTLASS_DEVICE attention_kernel(Params& p) {
     // In this block, we will only ever:
     // - read query[query_start:query_end, :]
     // - write to output[query_start:query_end, :]
 
     extern __shared__ char smem_buffer[];
     SharedStorage& shared_storage = *((SharedStorage*)smem_buffer);
     auto& m_prime = shared_storage.m_prime;
     auto& s_prime = shared_storage.s_prime;
+    auto& si = shared_storage.after_mm0.si;
     auto& mi = shared_storage.mi;
-    const uint32_t query_start = blockIdx.x * kQueriesPerBlock;
 
     static_assert(kQueriesPerBlock < kNumWarpsPerBlock * kWarpSize, "");
     if (thread_id() < kQueriesPerBlock) {
       s_prime[thread_id()] = accum_t(0);
       m_prime[thread_id()] =
           -cutlass::platform::numeric_limits<accum_t>::infinity();
       mi[thread_id()] = -cutlass::platform::numeric_limits<accum_t>::infinity();
     }
     typename MM1::Mma::FragmentC accum_o;
     accum_o.clear();
 
     auto createOutputIter = [&](int col) -> typename MM1::OutputTileIterator {
       using OutputTileIterator = typename MM1::OutputTileIterator;
       return OutputTileIterator(
-          typename OutputTileIterator::Params{(int32_t)p.o_strideM},
+          typename OutputTileIterator::Params{(int32_t)p.o_strideM()},
           p.output_ptr,
           typename OutputTileIterator::TensorCoord{
               p.num_queries, p.head_dim_value},
           thread_id(),
           {0, col});
     };
 
     auto createOutputAccumIter = [&](int col) ->
         typename MM1::OutputTileIteratorAccum {
           using OutputTileIteratorAccum = typename MM1::OutputTileIteratorAccum;
           return OutputTileIteratorAccum(
-              typename OutputTileIteratorAccum::Params{
-                  (int32_t)(p.head_dim_value * p.num_heads)},
+              typename OutputTileIteratorAccum::Params{(int32_t)p.o_strideM()},
               p.output_accum_ptr,
               typename OutputTileIteratorAccum::TensorCoord{
                   p.num_queries, p.head_dim_value},
               thread_id(),
               {0, col});
         };
 
-#ifdef HAS_PYTORCH
-    curandStatePhilox4_32_10_t curand_state_init;
-    if (kSupportsDropout && p.use_dropout) {
-      const auto seeds = at::cuda::philox::unpack(p.rng_engine_inputs);
-
-      // each element of the attention matrix P with shape
-      // (batch_sz, n_heads, n_queries, n_keys) is associated with a single
-      // offset in RNG sequence. we initialize the RNG state with offset that
-      // starts at the beginning of a (n_queries, n_keys) matrix for this
-      // block's batch_id and head_id
-      // initializing rng state is very expensive, so we run once per kernel,
-      // rather than once per iteration. each iteration takes a copy of the
-      // initialized RNG state and offsets it as needed.
-      curand_init(
-          std::get<0>(seeds),
-          0,
-          std::get<1>(seeds) + p.dropout_batch_head_rng_offset,
-          &curand_state_init);
-    }
-#endif
-
     // Iterate through keys
     for (int32_t iter_key_start = 0; iter_key_start < p.num_keys;
          iter_key_start += kKeysPerBlock) {
       int32_t problem_size_0_m =
           cutlass::fast_min((int32_t)kQueriesPerBlock, p.num_queries);
       int32_t problem_size_0_n = cutlass::fast_min(
           int32_t(kKeysPerBlock), p.num_keys - iter_key_start);
@@ -758,73 +605,24 @@
       typename MM0::Mma::Operator::IteratorC::TensorCoord
           iteratorC_tile_offset = {
               (tb_tile_offset.m() * MM0::Mma::WarpCount::kM) +
                   (my_warp_id % MM0::Mma::WarpCount::kM),
               (tb_tile_offset.n() * MM0::Mma::WarpCount::kN) +
                   (my_warp_id / MM0::Mma::WarpCount::kM)};
 
-      // multiply by scaling factor
-      if (kSupportsBias) {
-        accum =
-            cutlass::multiplies<typename MM0::Mma::FragmentC>()(p.scale, accum);
-      }
-
-      // apply attention bias if applicable
-      if (kSupportsBias && p.attn_bias_ptr != nullptr) {
-        // load bias tile Bij into shared memory
-        typename MM0::BiasLoader::GmemTileIterator bias_iter(
-            {cutlass::layout::RowMajor(p.bias_strideM)},
-            // attn_bias_pointer points to matrix of size (n_queries, n_keys)
-            // for the relevant batch_id and head_id
-            p.attn_bias_ptr + query_start * p.bias_strideM + iter_key_start,
-            {problem_size_0_m, problem_size_0_n},
-            thread_id());
-        cutlass::TensorRef<scalar_t, cutlass::layout::RowMajor> bias_tensor_ref(
-            shared_storage.after_mm0.bias.data(),
-            cutlass::layout::RowMajor(MM0::ThreadblockShape::kN));
-        typename MM0::BiasLoader::SmemTileIterator smem_tile_iter(
-            bias_tensor_ref, thread_id());
-        MM0::BiasLoader::load(bias_iter, smem_tile_iter);
-
-        // Pij += Bij, Pij is in register fragment and Bij is in shared memory
-        auto lane_offset = MM0::AccumLambdaIterator::get_lane_offset(
-            lane_id(), warp_id(), iteratorC_tile_offset);
-        MM0::AccumLambdaIterator::iterateRows(
-            lane_offset,
-            [&](int accum_m) {},
-            [&](int accum_m, int accum_n, int idx) {
-              if (accum_m < problem_size_0_m && accum_n < problem_size_0_n) {
-                accum[idx] += bias_tensor_ref.at({accum_m, accum_n});
-              }
-            },
-            [&](int accum_m) {});
-      }
-
       // Mask out last if causal
-      // This is only needed if upper-right corner of current query / key block
-      // intersects the mask Coordinates of upper-right corner of current block
-      // is y=query_start x=min(iter_key_start + kKeysPerBlock, num_keys)) The
-      // first masked element is x = y + offset -> query_start + offset There is
-      // intersection (and we need to mask) if min(iter_key_start +
-      // kKeysPerBlock, num_keys)) >= query_start + offset
-      if (p.custom_mask_type &&
-          cutlass::fast_min(iter_key_start + kKeysPerBlock, p.num_keys) >=
-              (query_start + p.causal_diagonal_offset)) {
+      if (p.causal && p.num_keys - iter_key_start <= kKeysPerBlock) {
         auto query_start = blockIdx.x * kQueriesPerBlock;
-        auto lane_offset = MM0::AccumLambdaIterator::get_lane_offset(
+        auto lane_offset = MM0::ScalingCoefsUpdater::get_lane_offset(
             lane_id(), warp_id(), iteratorC_tile_offset);
         int32_t last_col;
-        MM0::AccumLambdaIterator::iterateRows(
+        MM0::ScalingCoefsUpdater::iterateRows(
             lane_offset,
             [&](int accum_m) {
-              // last absolute col is (last absolute query + offset)
-              // last local col is (last absolute query + offset -
-              // iter_key_start)
-              last_col = query_start + accum_m + p.causal_diagonal_offset -
-                  iter_key_start;
+              last_col = query_start + accum_m - iter_key_start;
             },
             [&](int accum_m, int accum_n, int idx) {
               if (accum_n > last_col) {
                 accum[idx] =
                     -cutlass::platform::numeric_limits<accum_t>::infinity();
               }
             },
@@ -832,30 +630,33 @@
       }
       DISPATCH_BOOL(iter_key_start == 0, kIsFirst, ([&] {
                       DISPATCH_BOOL(
                           p.num_keys - iter_key_start >= kKeysPerBlock,
                           kFullColumns,
                           ([&] {
                             // Update `mi` from accum stored in registers
-                            // Also does accum[i] <- exp(accum[i] - mi)
-                            iterative_softmax<
-                                typename MM0::Mma::Operator::IteratorC,
+                            // Also updates `accum` with accum[i] <-
+                            // exp(accum[i] * scale
+                            // - mi)
+                            MM0::ScalingCoefsUpdater::update<
+                                kQueriesPerBlock,
                                 kFullColumns,
-                                kIsFirst>(
+                                kIsFirst,
+                                kKeepOutputInRF>(
                                 accum_o,
                                 accum,
                                 mi,
                                 m_prime,
                                 s_prime,
                                 lane_id(),
                                 thread_id(),
                                 warp_id(),
                                 p.num_keys - iter_key_start,
                                 iteratorC_tile_offset,
-                                kSupportsBias ? 1.0f : p.scale);
+                                1.0f / cutlass::fast_sqrt(float(p.head_dim)));
                           }));
                     }));
 
       // Output results to shared-memory
       int warp_idx_mn_0 = my_warp_id %
           (MM0::Mma::Base::WarpCount::kM * MM0::Mma::Base::WarpCount::kN);
       auto output_tile_coords = cutlass::MatrixCoord{
@@ -863,77 +664,14 @@
           warp_idx_mn_0 / MM0::Mma::Base::WarpCount::kM};
 
       MM0::B2bGemm::accumToSmem(
           shared_storage.after_mm0.si, accum, my_lane_id, output_tile_coords);
 
       __syncthreads();
 
-#ifdef HAS_PYTORCH
-      // apply dropout (if applicable) after we've written Pij to smem.
-      // dropout is applied by multiplying each element of Pij by:
-      // - 0 with probability dropout_p
-      // - 1 / (1 - dropout_p) with probability 1 - dropout_p
-      //
-      // for backward purposes we want to be able to map each element of the
-      // attention matrix to the same random uniform number as the one we used
-      // in forward, without needing to use the same iteration order or having
-      // to store the dropout matrix. its possible to do this in registers but
-      // it ends up being very slow because each thread having noncontiguous
-      // strips of the Pij tile means we have to skip around a lot, and also
-      // have to generate a single random number at a time
-      if (kSupportsDropout && p.use_dropout) {
-        auto si = shared_storage.after_mm0.si.accum_ref();
-        // each thread handles a contiguous sequence of elements from Sij, all
-        // coming from the same row. the reason they have to come from the same
-        // row is that the sampling random numbers from a contiguous random
-        // number sequence is much more efficient than jumping around, and the
-        // linear offset of each element of S (the global matrix) maps to an
-        // offset in a random number sequence. for S, the end of a row and the
-        // beginning of the next have adjacent offsets, but for Sij, this is not
-        // necessarily the case.
-        const int num_threads = blockDim.x * blockDim.y * blockDim.z;
-        const int threads_per_row =
-            cutlass::fast_min(num_threads / problem_size_0_m, problem_size_0_n);
-        const int elts_per_thread = cutlass::round_nearest(
-            cutlass::ceil_div(problem_size_0_n, threads_per_row), 4);
-
-        const int thread_i = thread_id() / threads_per_row;
-        const int thread_start_j =
-            (thread_id() % threads_per_row) * elts_per_thread;
-
-        if (thread_i < problem_size_0_m && thread_start_j < problem_size_0_n) {
-          curandStatePhilox4_32_10_t curand_state = curand_state_init;
-          skipahead(
-              static_cast<unsigned long long>(
-                  (query_start + thread_i) * p.num_keys +
-                  (iter_key_start + thread_start_j)),
-              &curand_state);
-          const float dropout_scale = 1.0 / (1.0 - p.dropout_prob);
-
-          // apply dropout scaling to elements this thread is responsible for,
-          // in chunks of 4
-          for (int sij_start_col_idx = thread_start_j; sij_start_col_idx <
-               cutlass::fast_min(thread_start_j + elts_per_thread,
-                                 problem_size_0_n);
-               sij_start_col_idx += 4) {
-            const float4 rand_uniform_quad = curand_uniform4(&curand_state);
-
-            CUTLASS_PRAGMA_UNROLL
-            for (int quad_idx = 0; quad_idx < 4; ++quad_idx) {
-              si.at({thread_i, sij_start_col_idx + quad_idx}) *=
-                  static_cast<scalar_t>(
-                      dropout_scale *
-                      ((&rand_uniform_quad.x)[quad_idx] > p.dropout_prob));
-            }
-          }
-        }
-        __syncthreads(); // p.use_dropout should have same value kernel-wide
-      }
-#endif
-
       //
       // MATMUL: Attn . V
       // Run the matmul `attn @ V` for a block of attn and V.
       // `attn` is read from shared memory (in `shared_storage_si`)
       // `V` is read from global memory (with iterator_B)
       //
 
@@ -1097,124 +835,14 @@
       } else if (thread_id() < lse_dim) {
         p.logsumexp_ptr[thread_id()] =
             cutlass::platform::numeric_limits<accum_t>::infinity();
       }
     }
   }
 
-  template <
-      typename WarpIteratorC,
-      bool kFullColumns,
-      bool kIsFirst>
-  CUTLASS_DEVICE static void iterative_softmax(
-      typename WarpIteratorC::Fragment& frag_o, // output so far
-      typename WarpIteratorC::Fragment& frag,
-      cutlass::Array<accum_t, kQueriesPerBlock>& mi,
-      cutlass::Array<accum_t, kQueriesPerBlock>& m_prime,
-      cutlass::Array<accum_t, kQueriesPerBlock>& s_prime,
-      int8_t lane_id,
-      int8_t thread_id,
-      int8_t warp_id,
-      int16_t max_col,
-      typename WarpIteratorC::TensorCoord const& tile_offset,
-      float scaling) {
-    /* Iterates on the accumulator and corresponding position on result matrix
-
-    (1) Update `mi[r]` to the max value of the row `r`
-    (2) In a second iteration do the following:
-        (a) accum   <- exp(accum - mi)
-        (b) m_prime <- exp(m_prime - mi)
-        (c) s_prime <- s_prime * m_prime + sum(accum)
-
-    All of this is done on registers, before we store all of this
-    on shared memory for the next matmul with Value.
-    */
-    using Fragment = typename WarpIteratorC::Fragment;
-    using LambdaIterator = typename DefaultMmaAccumLambdaIterator<
-        WarpIteratorC,
-        accum_t,
-        kWarpSize>::Iterator;
-    // Convert to `accum_t` (rather than double)
-    constexpr float kLog2e = 1.4426950408889634074; // log_2(e) = M_LOG2E
-    if (!kIsFirst) {
-      if (thread_id < kQueriesPerBlock) {
-        m_prime[thread_id] = mi[thread_id];
-      }
-      __syncthreads();
-    }
-
-    auto lane_offset =
-        LambdaIterator::get_lane_offset(lane_id, warp_id, tile_offset);
-
-    // First update `mi` to the max per-row
-    {
-      accum_t max;
-      LambdaIterator::iterateRows(
-          lane_offset,
-          [&](int accum_m) {
-            max = -cutlass::platform::numeric_limits<accum_t>::infinity();
-          },
-          [&](int accum_m, int accum_n, int idx) {
-            if (kFullColumns || accum_n < max_col) {
-              max = cutlass::fast_max(max, frag[idx]);
-            }
-          },
-          [&](int accum_m) {
-            // Having 4x atomicMax seems faster than reduce within warp
-            // first...
-            atomicMaxFloat(&mi[accum_m], max * scaling);
-          });
-    }
-    frag = cutlass::multiplies<Fragment>()(scaling * kLog2e, frag);
-
-    // Make sure we all share the update values for `mi`
-    __syncthreads();
-
-    if (thread_id < kQueriesPerBlock) {
-      auto m_prime_exp = exp2f(kLog2e * (m_prime[thread_id] - mi[thread_id]));
-      m_prime[thread_id] = m_prime_exp;
-      s_prime[thread_id] *= m_prime_exp;
-    }
-    __syncthreads(); // Update output fragments
-    if (kKeepOutputInRF && !kIsFirst) {
-      accum_t mp;
-      LambdaIterator::iterateRows(
-          lane_offset,
-          [&](int accum_m) { mp = m_prime[accum_m]; },
-          [&](int accum_m, int accum_n, int idx) { frag_o[idx] *= mp; },
-          [&](int accum_m) {});
-      __syncthreads();
-    }
-    // Update accum_m, accum_n, ...
-    {
-      accum_t mi_row, total_row;
-      LambdaIterator::iterateRows(
-          lane_offset,
-          [&](int accum_m) { mi_row = kLog2e * mi[accum_m]; },
-          [&](int accum_m, int accum_n, int idx) {
-            frag[idx] = (kFullColumns || accum_n < max_col)
-                ? exp2f(frag[idx] - mi_row)
-                : accum_t(0.0);
-          },
-          [&](int accum_m) {});
-      LambdaIterator::iterateRows(
-          lane_offset,
-          [&](int accum_m) { total_row = 0.0; },
-          [&](int accum_m, int accum_n, int idx) { total_row += frag[idx]; },
-          [&](int accum_m) {
-            if (LambdaIterator::reduceSameRow(
-                    lane_id, total_row, [](accum_t a, accum_t b) {
-                      return a + b;
-                    })) {
-              atomicAdd(&s_prime[accum_m], total_row);
-            }
-          });
-    }
-  }
-
   static CUTLASS_DEVICE int8_t lane_id() {
     return threadIdx.x;
   }
   static CUTLASS_DEVICE int8_t warp_id() {
     return threadIdx.y;
   }
   static CUTLASS_DEVICE int16_t thread_id() {
@@ -1230,7 +858,93 @@
   }
   AK::attention_kernel(p);
 }
 
 template <typename AK>
 __global__ void __launch_bounds__(AK::kNumThreads, AK::kMinBlocksPerSm)
     attention_kernel_batched(typename AK::Params params);
+
+#define _ATTENTION_KERNEL_FORWARD_BEGIN(...)                                  \
+  template <>                                                                 \
+  __global__ void __launch_bounds__(                                          \
+      __VA_ARGS__::kNumThreads, __VA_ARGS__::kMinBlocksPerSm)                 \
+      attention_kernel_batched<__VA_ARGS__>(typename __VA_ARGS__::Params p) { \
+    using Kernel = __VA_ARGS__;
+#define _ATTENTION_KERNEL_FORWARD_END() }
+
+#ifdef __CUDA_ARCH__
+#define __CUDA_ARCH_OR_ZERO__ __CUDA_ARCH__
+#else
+#define __CUDA_ARCH_OR_ZERO__ 0
+#endif
+
+#define INSTANTIATE_ATTENTION_KERNEL_FORWARD(              \
+    ARCH,                                                  \
+    SCALAR_T,                                              \
+    IS_ALIGNED,                                            \
+    QUERIES_PER_BLOCK,                                     \
+    KEYS_PER_BLOCK,                                        \
+    SINGLE_VALUE_ITER)                                     \
+  _ATTENTION_KERNEL_FORWARD_BEGIN(AttentionKernel<         \
+                                  SCALAR_T,                \
+                                  cutlass::arch::Sm##ARCH, \
+                                  IS_ALIGNED,              \
+                                  QUERIES_PER_BLOCK,       \
+                                  KEYS_PER_BLOCK,          \
+                                  SINGLE_VALUE_ITER>)      \
+  if (!p.advance_to_block()) {                             \
+    return;                                                \
+  }                                                        \
+  Kernel::attention_kernel(p);                             \
+  _ATTENTION_KERNEL_FORWARD_END();
+
+#define INSTANTIATE_ATTENTION_KERNEL_FORWARD_DISABLED(              \
+    ARCH,                                                           \
+    SCALAR_T,                                                       \
+    IS_ALIGNED,                                                     \
+    QUERIES_PER_BLOCK,                                              \
+    KEYS_PER_BLOCK,                                                 \
+    SINGLE_VALUE_ITER)                                              \
+  _ATTENTION_KERNEL_FORWARD_BEGIN(AttentionKernel<                  \
+                                  SCALAR_T,                         \
+                                  cutlass::arch::Sm##ARCH,          \
+                                  IS_ALIGNED,                       \
+                                  QUERIES_PER_BLOCK,                \
+                                  KEYS_PER_BLOCK,                   \
+                                  SINGLE_VALUE_ITER>)               \
+  printf(                                                           \
+      "FATAL: this function is for sm%d, but was built for sm%d\n", \
+      int(ARCH),                                                    \
+      int(__CUDA_ARCH_OR_ZERO__));                                  \
+  _ATTENTION_KERNEL_FORWARD_END();
+
+// All kernels are disabled by default
+#define INSTANTIATE_ATTENTION_KERNEL_FORWARD_SM50(...) \
+  INSTANTIATE_ATTENTION_KERNEL_FORWARD_DISABLED(50, __VA_ARGS__)
+#define INSTANTIATE_ATTENTION_KERNEL_FORWARD_SM70(...) \
+  INSTANTIATE_ATTENTION_KERNEL_FORWARD_DISABLED(70, __VA_ARGS__)
+#define INSTANTIATE_ATTENTION_KERNEL_FORWARD_SM75(...) \
+  INSTANTIATE_ATTENTION_KERNEL_FORWARD_DISABLED(75, __VA_ARGS__)
+#define INSTANTIATE_ATTENTION_KERNEL_FORWARD_SM80(...) \
+  INSTANTIATE_ATTENTION_KERNEL_FORWARD_DISABLED(80, __VA_ARGS__)
+
+// Enable the right one based on __CUDA_ARCH__
+#ifndef __CUDA_ARCH__
+#elif __CUDA_ARCH__ < 500
+#error "Need cuda arch at least 5.0"
+#elif __CUDA_ARCH__ < 700
+#undef INSTANTIATE_ATTENTION_KERNEL_FORWARD_SM50
+#define INSTANTIATE_ATTENTION_KERNEL_FORWARD_SM50(...) \
+  INSTANTIATE_ATTENTION_KERNEL_FORWARD(50, __VA_ARGS__)
+#elif __CUDA_ARCH__ < 750
+#undef INSTANTIATE_ATTENTION_KERNEL_FORWARD_SM70
+#define INSTANTIATE_ATTENTION_KERNEL_FORWARD_SM70(...) \
+  INSTANTIATE_ATTENTION_KERNEL_FORWARD(70, __VA_ARGS__)
+#elif __CUDA_ARCH__ < 800
+#undef INSTANTIATE_ATTENTION_KERNEL_FORWARD_SM75
+#define INSTANTIATE_ATTENTION_KERNEL_FORWARD_SM75(...) \
+  INSTANTIATE_ATTENTION_KERNEL_FORWARD(75, __VA_ARGS__)
+#elif __CUDA_ARCH__ >= 800
+#undef INSTANTIATE_ATTENTION_KERNEL_FORWARD_SM80
+#define INSTANTIATE_ATTENTION_KERNEL_FORWARD_SM80(...) \
+  INSTANTIATE_ATTENTION_KERNEL_FORWARD(80, __VA_ARGS__)
+#endif
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/41_fused_multi_head_attention/mma_from_smem.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/examples/41_fused_multi_head_attention/mma_from_smem.h`

 * *Files 0% similar despite different names*

```diff
@@ -380,15 +380,15 @@
         {Base::kWarpGemmIterations * warp_idx_k, warp_idx_n});
   }
 
   // For API compatibility with MmaMultistageFromSharedMemory
   // but not supported as it worsens perf: older gpus < sm80 don't
   // support async tranfers and have to waste registers
   CUTLASS_DEVICE
-  void set_prologue_done(bool value) {}
+  bool set_prologue_done(bool value) {}
   CUTLASS_DEVICE
   static void prologue(
       typename Base::SharedStorage& shared_storage,
       IteratorB iterator_B1,
       int thread_idx,
       int problem_size_0_n) {}
 
@@ -691,15 +691,15 @@
     warp_tile_iterator_A1_.add_tile_offset(
         {warp_idx_m_1, Base::kWarpGemmIterations1 * warp_idx_k_1});
     this->warp_tile_iterator_B_.add_tile_offset(
         {Base::kWarpGemmIterations1 * warp_idx_k_1, warp_idx_n_1});
   }
 
   CUTLASS_DEVICE
-  void set_prologue_done(bool value) {
+  bool set_prologue_done(bool value) {
     prologue_done_ = value;
   }
 
   CUTLASS_DEVICE
   static void prologue(
       typename Base::SharedStorage& shared_storage,
       IteratorB iterator_B1,
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/41_fused_multi_head_attention/transform/tile_smem_loader.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/examples/41_fused_multi_head_attention/transform/tile_smem_loader.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/41_multi_head_attention/fused_multihead_attention.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/examples/41_multi_head_attention/fused_multihead_attention.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/41_multi_head_attention/gemm_attention.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/examples/41_multi_head_attention/gemm_attention.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/41_multi_head_attention/gemm_grouped_with_softmax_visitor.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/examples/41_multi_head_attention/gemm_grouped_with_softmax_visitor.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/42_ampere_tensorop_group_conv/ampere_tensorop_group_conv.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/examples/42_ampere_tensorop_group_conv/ampere_tensorop_group_conv.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/43_ell_block_sparse_gemm/ell_block_sparse_gemm.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/examples/43_ell_block_sparse_gemm/ell_block_sparse_gemm.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/44_multi_gemm_ir_and_codegen/fixed_impl/epilogue/threadblock/default_bias_act_epilogue_tensor_op.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/examples/44_multi_gemm_ir_and_codegen/fixed_impl/epilogue/threadblock/default_bias_act_epilogue_tensor_op.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/44_multi_gemm_ir_and_codegen/fixed_impl/epilogue/threadblock/default_thread_map_tensor_op_for_fused_bias.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/examples/44_multi_gemm_ir_and_codegen/fixed_impl/epilogue/threadblock/default_thread_map_tensor_op_for_fused_bias.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/44_multi_gemm_ir_and_codegen/fixed_impl/epilogue/threadblock/fused_bias_act_epilogue.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/examples/44_multi_gemm_ir_and_codegen/fixed_impl/epilogue/threadblock/fused_bias_act_epilogue.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/44_multi_gemm_ir_and_codegen/fixed_impl/epilogue/threadblock/output_tile_thread_map_for_fused_bias.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/examples/44_multi_gemm_ir_and_codegen/fixed_impl/epilogue/threadblock/output_tile_thread_map_for_fused_bias.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/44_multi_gemm_ir_and_codegen/fixed_impl/epilogue/warp/fused_bias_act_fragment_iterator_tensor_op.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/examples/44_multi_gemm_ir_and_codegen/fixed_impl/epilogue/warp/fused_bias_act_fragment_iterator_tensor_op.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/44_multi_gemm_ir_and_codegen/fixed_impl/gemm/warp/mma_tensor_op_fragment_iterator_without_output_op.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/examples/44_multi_gemm_ir_and_codegen/fixed_impl/gemm/warp/mma_tensor_op_fragment_iterator_without_output_op.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/44_multi_gemm_ir_and_codegen/leaky_bias.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/examples/44_multi_gemm_ir_and_codegen/leaky_bias.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/44_multi_gemm_ir_and_codegen/utils.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/examples/44_multi_gemm_ir_and_codegen/utils.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/45_dual_gemm/device/dual_gemm.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/examples/45_dual_gemm/device/dual_gemm.h`

 * *Files 6% similar despite different names*

```diff
@@ -48,15 +48,14 @@
 
 #include "cutlass/gemm/device/default_gemm_configuration.h"
 #include "cutlass/gemm/threadblock/default_mma.h"
 #include "cutlass/epilogue/thread/linear_combination_relu.h"
 #include "cutlass/epilogue/threadblock/default_epilogue_tensor_op.h"
 
 #include "../kernel/dual_gemm.h"
-#include "../dual_gemm_common.h"
 
 ////////////////////////////////////////////////////////////////////////////////
 
 namespace cutlass {
 namespace gemm {
 namespace device {
 
@@ -65,18 +64,16 @@
 template <
     /// Element type for A matrix operand
     typename ElementA_,
     /// Layout type for A matrix operand
     typename LayoutA_,
     /// Element type for B matrix operand
     typename ElementB_,
-    /// Layout type for B0 matrix operand
-    typename LayoutB0_,
-    /// Layout type for B1 matrix operand
-    typename LayoutB1_,
+    /// Layout type for B matrix operand
+    typename LayoutB_,
     /// Element type for C and D matrix operands
     typename ElementC_,
     /// Layout type for C and D matrix operands
     typename LayoutC_,
     /// Element type for internal accumulation
     typename ElementAccumulator_,
     /// Operator class tag
@@ -118,18 +115,16 @@
 class DualGemm {
  public:
 
   using ElementA = ElementA_;
   using LayoutA = LayoutA_;
   using TensorRefA = TensorRef<ElementA const, LayoutA>;
   using ElementB = ElementB_;
-  using LayoutB0 = LayoutB0_;
-  using LayoutB1 = LayoutB1_;
-  using TensorRefB0 = TensorRef<ElementB const, LayoutB0>;
-  using TensorRefB1 = TensorRef<ElementB const, LayoutB1>;
+  using LayoutB = LayoutB_;
+  using TensorRefB = TensorRef<ElementB const, LayoutB>;
   using ElementC = ElementC_;
   using LayoutC = LayoutC_;
   using TensorRefC = TensorRef<ElementC const, LayoutC>;
   using TensorRefD = TensorRef<ElementC, LayoutC>;
   using ElementAccumulator = ElementAccumulator_;
   using OperatorClass = OperatorClass_;
   using ArchTag = ArchTag_;
@@ -152,52 +147,44 @@
   static ComplexTransform const kTransformB = ComplexTransform::kNone;
 
   using LayoutScaleBias = layout::RowMajor;
   /// Define the kernel
   /// Define the threadblock-scoped matrix multiply-accumulate
   static_assert(ArchTag::kMinComputeCapability >= 80, "Only multistage is implemented");
   static_assert(kStages >= 3, "Only multistage is implemented");
-  using Mma0 = typename cutlass::gemm::threadblock::DefaultMma<
-      ElementA, LayoutA, kAlignmentA, ElementB, LayoutB0, kAlignmentB,
-      ElementAccumulator, layout::RowMajor, arch::OpClassTensorOp, ArchTag,
-      ThreadblockShape, WarpShape, 
-      InstructionShape, Stages, Operator>::ThreadblockMma;
-  using Mma1 = typename cutlass::gemm::threadblock::DefaultMma<
-      ElementA, LayoutA, kAlignmentA, ElementB, LayoutB1, kAlignmentB,
+  using Mma = typename cutlass::gemm::threadblock::DefaultMma<
+      ElementA, LayoutA, kAlignmentA, ElementB, LayoutB, kAlignmentB,
       ElementAccumulator, layout::RowMajor, arch::OpClassTensorOp, ArchTag,
       ThreadblockShape, WarpShape, 
       InstructionShape, Stages, Operator>::ThreadblockMma;
   using DualMma = threadblock::DualMmaMultistage<
-    typename Mma0::Shape,
-    typename Mma0::IteratorA,
-    typename Mma0::SmemIteratorA,
-    Mma0::kCacheOpA,
-    typename Mma0::IteratorB,
-    typename Mma0::SmemIteratorB,
-    Mma0::kCacheOpB,
-    typename Mma1::IteratorB,
-    typename Mma1::SmemIteratorB,
-    typename Mma0::ElementC,
-    typename Mma0::LayoutC,
-    typename Mma0::Policy,
-    typename Mma1::Policy,
-    Mma0::kStages,
+    typename Mma::Shape,
+    typename Mma::IteratorA,
+    typename Mma::SmemIteratorA,
+    Mma::kCacheOpA,
+    typename Mma::IteratorB,
+    typename Mma::SmemIteratorB,
+    Mma::kCacheOpB,
+    typename Mma::ElementC,
+    typename Mma::LayoutC,
+    typename Mma::Policy,
+    Mma::kStages,
     SharedMemoryClearOption::kNone
   >;
 
   static const int kPartitionsK = ThreadblockShape::kK / WarpShape::kK;
 
   /// Define the epilogue
   using Epilogue0 =
       typename cutlass::epilogue::threadblock::DefaultEpilogueTensorOp<
-          ThreadblockShape, typename DualMma::Operator0, kPartitionsK, EpilogueOutputOp0,
+          ThreadblockShape, typename DualMma::Operator, kPartitionsK, EpilogueOutputOp0,
           EpilogueOutputOp0::kCount>::Epilogue;
   using Epilogue1 =
       typename cutlass::epilogue::threadblock::DefaultEpilogueTensorOp<
-          ThreadblockShape, typename DualMma::Operator1, kPartitionsK, EpilogueOutputOp1,
+          ThreadblockShape, typename DualMma::Operator, kPartitionsK, EpilogueOutputOp1,
           EpilogueOutputOp1::kCount>::Epilogue;
 
   /// Define the kernel-level GEMM operator.
   using DualGemmKernel = kernel::DualGemm<
     DualMma,
     Epilogue0, Epilogue1, EpilogueOutputOp2,
     ThreadblockSwizzle, kSplitKSerial,
@@ -206,93 +193,71 @@
   /// Argument structure
   struct Arguments {
 
     //
     // Data members
     //
 
-    DualGemmMode mode;
     GemmCoord problem_size;
     TensorRef<ElementA const, LayoutA> ref_A0;
-    TensorRef<ElementB const, LayoutB0> ref_B0;
+    TensorRef<ElementB const, LayoutB> ref_B0;
     TensorRef<ElementC const, LayoutC> ref_C0;
     TensorRef<ElementC, LayoutC> ref_D0;
-    TensorRef<ElementB const, LayoutB1> ref_B1;
+    TensorRef<ElementB const, LayoutB> ref_B1;
     TensorRef<ElementC const, LayoutC> ref_C1;
     TensorRef<ElementC, LayoutC> ref_D1;
     TensorRef<ElementC, LayoutC> ref_D2;
     typename EpilogueOutputOp0::Params epilogue0;
     typename EpilogueOutputOp1::Params epilogue1;
     typename EpilogueOutputOp2::Params epilogue2;
     int split_k_slices;
 
-    int batch_count;
-    int64_t batch_stride_A;
-    int64_t batch_stride_B0;
-    int64_t batch_stride_B1;
-    int64_t batch_stride_C;
-    int64_t batch_stride_D;
-
     //
     // Methods
     //
 
     /// Default ctor
     CUTLASS_HOST_DEVICE
     Arguments(): problem_size(0, 0, 0), split_k_slices(1) {
 
     }
 
     /// Constructs an Arguments structure 
     CUTLASS_HOST_DEVICE
     Arguments(
-      DualGemmMode mode,
       GemmCoord problem_size_,
       TensorRef<ElementA const, LayoutA> ref_A0_,
-      TensorRef<ElementB const, LayoutB0> ref_B0_,
+      TensorRef<ElementB const, LayoutB> ref_B0_,
       TensorRef<ElementC const, LayoutC> ref_C0_,
       TensorRef<ElementC, LayoutC> ref_D0_,
-      TensorRef<ElementB const, LayoutB1> ref_B1_,
+      TensorRef<ElementB const, LayoutB> ref_B1_,
       TensorRef<ElementC const, LayoutC> ref_C1_,
       TensorRef<ElementC, LayoutC> ref_D1_,
       TensorRef<ElementC, LayoutC> ref_D2_,
       typename EpilogueOutputOp0::Params epilogue0_ =
         typename EpilogueOutputOp0::Params(),
       typename EpilogueOutputOp1::Params epilogue1_ =
         typename EpilogueOutputOp1::Params(),
       typename EpilogueOutputOp2::Params epilogue2_ =
         typename EpilogueOutputOp2::Params(),
-      int split_k_slices_ = 1,
-      int batch_count = 1,
-      int64_t batch_stride_A = 0,
-      int64_t batch_stride_B0 = 0,
-      int64_t batch_stride_B1 = 0,
-      int64_t batch_stride_C = 0,
-      int64_t batch_stride_D = 0
+      int split_k_slices_ = 1
     ):
-      mode(mode),
       problem_size(problem_size_),
       ref_A0(ref_A0_),
       ref_B0(ref_B0_),
       ref_C0(ref_C0_),
       ref_D0(ref_D0_),
       ref_B1(ref_B1_),
       ref_C1(ref_C1_),
       ref_D1(ref_D1_),
       ref_D2(ref_D2_),
       epilogue0(epilogue0_),
       epilogue1(epilogue1_),
       epilogue2(epilogue2_),
-      split_k_slices(split_k_slices_),
-      batch_count(batch_count),
-      batch_stride_A(batch_stride_A),
-      batch_stride_B0(batch_stride_B0),
-      batch_stride_B1(batch_stride_B1),
-      batch_stride_C(batch_stride_C),
-      batch_stride_D(batch_stride_D) {
+      split_k_slices(split_k_slices_) {
 
     }
   };
 
 private:
 
   /// Kernel parameters object
@@ -302,17 +267,14 @@
 
   /// Constructs the GEMM.
   DualGemm() = default;
 
   /// Determines whether the GEMM can execute the given problem.
   static Status can_implement(Arguments const &args) {
 
-    if (args.mode == DualGemmMode::kBatched && kSplitKSerial) {
-      return Status::kErrorInvalidProblem;
-    }
     if (!kSplitKSerial && args.split_k_slices > 1) {
       return Status::kErrorInvalidProblem;
     }
     if (kStoreD0 != (args.ref_D0.data() != nullptr)) {
       return Status::kErrorInternal;
     }
     if (kStoreD1 != (args.ref_D1.data() != nullptr)) {
@@ -338,23 +300,25 @@
     return Status::kSuccess;
   }
 
   /// Gets the workspace size
   static size_t get_workspace_size(Arguments const &args) {
 
     size_t bytes = 0;
+      
+    // Determine grid shape
+    ThreadblockSwizzle threadblock_swizzle;
+
+    cutlass::gemm::GemmCoord tiled_shape = threadblock_swizzle.get_tiled_shape(
+      args.problem_size, 
+      {ThreadblockShape::kM, ThreadblockShape::kN, ThreadblockShape::kK},
+      args.split_k_slices);
 
     if (kSplitKSerial && args.split_k_slices > 1) {
-      // Determine grid shape
-      ThreadblockSwizzle threadblock_swizzle;
 
-      cutlass::gemm::GemmCoord tiled_shape = threadblock_swizzle.get_tiled_shape(
-        args.problem_size, 
-        {ThreadblockShape::kM, ThreadblockShape::kN, ThreadblockShape::kK},
-        args.split_k_slices);
 
       bytes += sizeof(int) * size_t(tiled_shape.m()) * size_t(tiled_shape.n());
     }
 
     return bytes;
   }
 
@@ -363,15 +327,15 @@
 
     // Determine grid shape
     ThreadblockSwizzle threadblock_swizzle;
 
     cutlass::gemm::GemmCoord grid_shape = threadblock_swizzle.get_tiled_shape(
       args.problem_size, 
       {ThreadblockShape::kM, ThreadblockShape::kN, ThreadblockShape::kK},
-      args.mode == DualGemmMode::kBatched ? args.batch_count : args.split_k_slices);
+      args.split_k_slices);
 
     if (kSplitKSerial) {
       if (args.split_k_slices > 1) {
         if (!workspace) {
           return Status::kErrorWorkspaceNull;
         }
 
@@ -389,34 +353,28 @@
       if (args.split_k_slices > 1) {
         return Status::kErrorInvalidProblem;
       }
     }
 
     // Initialize the Params structure
     params_ = typename DualGemmKernel::Params{
-      args.mode,
       args.problem_size,
       grid_shape,
       args.ref_A0.non_const_ref(),
       args.ref_B0.non_const_ref(),
       args.ref_C0.non_const_ref(),
       args.ref_D0,
       args.ref_B1.non_const_ref(),
       args.ref_C1.non_const_ref(),
       args.ref_D1,
       args.ref_D2,
       args.epilogue0,
       args.epilogue1,
       args.epilogue2,
       reinterpret_cast<int *>(workspace),
-      args.batch_stride_A,
-      args.batch_stride_B0,
-      args.batch_stride_B1,
-      args.batch_stride_C,
-      args.batch_stride_D,
     };
 
     return Status::kSuccess;
   }
 
   /// Lightweight update given a subset of arguments
   Status update(Arguments const &args, void *workspace = nullptr) {
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/45_dual_gemm/dual_gemm.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_fprop_implicit_gemm_f16nhwc_f16nhwc_f16nhwc_tensor_op_f16_sm80.cu`

 * *Files 25% similar despite different names*

```diff
@@ -24,437 +24,327 @@
  * DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR
  * SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER
  * CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,
  * OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
  * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
  *
  **************************************************************************************************/
-
 /*! \file
-    \brief CUTLASS Dual-GEMM Example.
-
-    Fused kernel that outputs `D0` and `D1`.
-    We assume that B0/B1 have the same shape/layout
-
-```
-D0 = epilogue0(X @ B0, C0)
-D1 = epilogue1(X @ B1, C1)
-D2 = element_wise(D0, D1)
-```
-    D0 and D1 will be optionally stored in gmem (`kStoreD0` / `kStoreD1`)
+    \brief Tests for device-wide Implicit GEMM interface
 */
 
-#include <iostream>
-
+#include "../../common/cutlass_unit_test.h"
 #include "cutlass/cutlass.h"
-#include "cutlass/gemm/device/gemm.h"
 
-#include "cutlass/util/host_tensor.h"
-#include "cutlass/util/tensor_view_io.h"
-#include "cutlass/util/reference/host/tensor_fill.h"
-#include "cutlass/util/reference/host/tensor_copy.h"
-#include "cutlass/util/reference/host/tensor_compare.h"
-#include "cutlass/util/reference/host/gemm.h"
-
-#include "device/dual_gemm.h"
-#include "thread/left_silu_and_mul.h"
-#include "dual_gemm_run.h"
-#include "test_run.h"
 
+#include "cutlass/conv/kernel/default_conv2d_fprop.h"
+#include "cutlass/conv/device/implicit_gemm_convolution.h"
 
-////////////////////////////////////////////////////////////////////////////////
+#include "conv2d_testbed.h"
 
-cutlass::gemm::GemmCoord problem_size(4096, 4096, 8192);
-cutlass::gemm::GemmCoord batch_problem_size(321, 256, 512);
+#if defined(CUTLASS_ARCH_MMA_SM80_SUPPORTED)
 
-constexpr int kStages = 3;
-constexpr bool kSplitKSerial = false;
-constexpr bool kUseBias = true;
-constexpr int kBatchCount = 37;
-
-
-#if 0
-using ElementOperandA = cutlass::bfloat16_t;
-using ElementOperandB = cutlass::bfloat16_t;
-using ElementOutput = cutlass::bfloat16_t;
-using ElementAccumulator = float;
-using ElementCompute = float;
-#else
-using ElementOperandA = cutlass::half_t;
-using ElementOperandB = cutlass::half_t;
-using ElementOutput = cutlass::half_t;
-using ElementAccumulator = cutlass::half_t;
-using ElementCompute = cutlass::half_t;
-#endif
-
-constexpr auto kScaleType = kUseBias ? cutlass::epilogue::thread::ScaleType::NoBetaScaling : (
-  // No bias
-  kSplitKSerial ? cutlass::epilogue::thread::ScaleType::Default : cutlass::epilogue::thread::ScaleType::Nothing
-);
-using EpilogueOutputOp0 = cutlass::epilogue::thread::LinearCombination<
-  ElementOutput,
-  128 / cutlass::sizeof_bits<ElementOutput>::value,
-  ElementAccumulator,
-  ElementCompute,
-  kScaleType
->;
-using EpilogueOutputOp1 = cutlass::epilogue::thread::LinearCombination<
-  ElementOutput,
-  128 / cutlass::sizeof_bits<ElementOutput>::value,
-  ElementAccumulator,
-  ElementCompute,
-  kScaleType
->;
-using EpilogueOutputOp2 = cutlass::epilogue::thread::LeftSiLUAndMul<
-  ElementOutput,
-  128 / cutlass::sizeof_bits<ElementOutput>::value,
-  ElementOutput,
-  ElementCompute
->;
-
-const ElementCompute alpha0 = ElementCompute(1);
-const ElementCompute beta0 = ElementCompute(kUseBias ? 1 : 0);
-const ElementCompute alpha1 = ElementCompute(1);
-const ElementCompute beta1 = ElementCompute(kUseBias ? 1 : 0);
-
-bool run_nonfused_gemm_f16_sm80() {
-  using ThreadblockShape = cutlass::gemm::GemmShape<128, 128, 32>;
-  using WarpShape = cutlass::gemm::GemmShape<64, 64, 32>;
-  using InstructionShape = cutlass::gemm::GemmShape<16, 8, 16>;
-
-  using Gemm0 = cutlass::gemm::device::Gemm<
-    ElementOperandA,
-    cutlass::layout::RowMajor,
-    ElementOperandB,
-    cutlass::layout::ColumnMajor,
-    ElementOutput,
-    cutlass::layout::RowMajor,
-    ElementAccumulator,
-    cutlass::arch::OpClassTensorOp,
-    cutlass::arch::Sm80,
-    ThreadblockShape,
-    WarpShape,
-    InstructionShape,
-    EpilogueOutputOp0,
-    cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<1>,
-    kStages,
-    8,
-    8,
-    kSplitKSerial
-  >;
-  using Gemm1 = cutlass::gemm::device::Gemm<
-    ElementOperandA,
-    cutlass::layout::RowMajor,
-    ElementOperandB,
-    cutlass::layout::ColumnMajor,
-    ElementOutput,
-    cutlass::layout::RowMajor,
+////////////////////////////////////////////////////////////////////////////////
+
+TEST(SM80_Device_Conv2d_Fprop_Analytic_ImplicitGemm_f16nhwc_f16nhwc_f16nhwc_tensor_op_f16,
+  128x128_64x3_64x64x64) {
+ 
+  /// Conv operation element types for the Gemm equivalent (ImplicitGemm)
+  using ElementA           = cutlass::half_t;
+  using ElementB           = cutlass::half_t;
+  using ElementC           = cutlass::half_t;
+  using ElementAccumulator = cutlass::half_t;
+  using ElementCompute     = cutlass::half_t;
+
+  /// Device-level Conv2d instance
+  using Conv2dFpropKernel = typename cutlass::conv::kernel::DefaultConv2dFprop<
+    ElementA, cutlass::layout::TensorNHWC,
+    ElementB, cutlass::layout::TensorNHWC,
+    ElementC, cutlass::layout::TensorNHWC,
     ElementAccumulator,
     cutlass::arch::OpClassTensorOp,
     cutlass::arch::Sm80,
-    ThreadblockShape,
-    WarpShape,
-    InstructionShape,
-    EpilogueOutputOp1,
-    cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<1>,
-    kStages,
-    8,
-    8,
-    kSplitKSerial
-  >;
-
-  NonFusedDualGemmRun<Gemm0, Gemm1> nonFusedGemm;
-
-  std::cout << "Running Non-fused GEMMs FP16 TN GEMMs...\n";
-
-  bool pass = nonFusedGemm.run(
-    problem_size, 
-    alpha0, 
-    beta0, 
-    alpha1, 
-    beta1,
-    true  /* is_profiling */
-  );
-
-  if(pass)
-    std::cout << "Pass\n";
-  else
-    std::cout << "Fail\n";
-
-  return pass;
+    cutlass::gemm::GemmShape<128, 128, 64>,
+    cutlass::gemm::GemmShape<64, 64, 64>,
+    cutlass::gemm::GemmShape<16, 8, 16>,
+    cutlass::epilogue::thread::LinearCombination<
+      ElementC,
+      128 / cutlass::sizeof_bits<ElementC>::value,
+      ElementAccumulator,
+      ElementCompute
+    >,
+    cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>,
+    3,
+    cutlass::arch::OpMultiplyAdd,
+    cutlass::conv::IteratorAlgorithm::kAnalytic
+  >::Kernel;
+
+  using Conv2dFprop = cutlass::conv::device::ImplicitGemmConvolution<Conv2dFpropKernel>;
+  
+  /// Run all unit test sizes with device-level Conv2d instance
+  EXPECT_TRUE(test::conv::device::TestAllConv2d<Conv2dFprop>());
 }
 
-template <typename T>
-struct LeftSiLUAndMul {
-  struct Params{};
-  CUTLASS_HOST_DEVICE LeftSiLUAndMul(Params p) {}
-
-  CUTLASS_HOST_DEVICE void set_k_partition(int, int) {}
-
-  CUTLASS_HOST_DEVICE T operator() (
-    T const &lhs, 
-    T const &rhs) const {
-    cutlass::epilogue::thread::SiLu<T> silu;
-    cutlass::multiplies<T> mul;
-    auto silu_lhs = silu(lhs);
-    return mul(silu_lhs, rhs);
-  }
-
-  template <int kCount>
-  CUTLASS_HOST_DEVICE cutlass::Array<T, kCount> operator() (
-    cutlass::Array<T, kCount> const &lhs, 
-    cutlass::Array<T, kCount> const &rhs) const {
-    cutlass::epilogue::thread::SiLu<T> silu;
-    cutlass::multiplies<T> mul;
-    auto silu_lhs = silu(lhs);
-    return mul(silu_lhs, rhs);
-  }
-};
-
-bool run_fused_gemm_f16_sm80_shmem() {
-  using ThreadblockShape = cutlass::gemm::GemmShape<128, 64, 32>;
-  using WarpShape = cutlass::gemm::GemmShape<64, 32, 32>;
-  using InstructionShape = cutlass::gemm::GemmShape<16, 8, 16>;
-
-  // Optionally, we might not need intermediate GEMM outputs
-  constexpr bool kStoreD0 = true;
-  constexpr bool kStoreD1 = true;
-
-  using DualGemm = cutlass::gemm::device::DualGemm<
-    ElementOperandA,
-    cutlass::layout::RowMajor,
-    ElementOperandB,
-    cutlass::layout::ColumnMajor,
-    cutlass::layout::ColumnMajor,
-    ElementOutput,
-    cutlass::layout::RowMajor,
+////////////////////////////////////////////////////////////////////////////////
+
+TEST(SM80_Device_Conv2d_Fprop_Optimized_ImplicitGemm_f16nhwc_f16nhwc_f16nhwc_tensor_op_f16,
+  128x128_64x3_64x64x64) {
+ 
+  /// Conv operation element types for the Gemm equivalent (ImplicitGemm)
+  using ElementA           = cutlass::half_t;
+  using ElementB           = cutlass::half_t;
+  using ElementC           = cutlass::half_t;
+  using ElementAccumulator = cutlass::half_t;
+  using ElementCompute     = cutlass::half_t;
+
+  /// Device-level Conv2d instance
+  using Conv2dFpropKernel = typename cutlass::conv::kernel::DefaultConv2dFprop<
+    ElementA, cutlass::layout::TensorNHWC,
+    ElementB, cutlass::layout::TensorNHWC,
+    ElementC, cutlass::layout::TensorNHWC,
     ElementAccumulator,
     cutlass::arch::OpClassTensorOp,
     cutlass::arch::Sm80,
-    ThreadblockShape,
-    WarpShape,
-    InstructionShape,
-    EpilogueOutputOp0,
-    EpilogueOutputOp1,
-    EpilogueOutputOp2,
-    cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<1>,
-    kStages,
-    kStoreD0,
-    kStoreD1,
-    kSplitKSerial
-  >;
-
-  DualFusedGemmRun<DualGemm> fusedGemm;
-
-  std::cout << "Running Fused FP16 TN GEMMs + Epilogue2...\n";
-
-  bool passed = fusedGemm.run(
-    problem_size, 
-    alpha0, 
-    beta0, 
-    alpha1, 
-    beta1
-  );
-
-  if(passed)
-    std::cout << "Pass\n";
-  else
-    std::cout << "Fail\n";
-
-  return passed;
+    cutlass::gemm::GemmShape<128, 128, 64>,
+    cutlass::gemm::GemmShape<64, 64, 64>,
+    cutlass::gemm::GemmShape<16, 8, 16>,
+    cutlass::epilogue::thread::LinearCombination<
+      ElementC,
+      128 / cutlass::sizeof_bits<ElementC>::value,
+      ElementAccumulator,
+      ElementCompute
+    >,
+    cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>,
+    3,
+    cutlass::arch::OpMultiplyAdd,
+    cutlass::conv::IteratorAlgorithm::kOptimized
+  >::Kernel;
+
+  using Conv2dFprop = cutlass::conv::device::ImplicitGemmConvolution<Conv2dFpropKernel>;
+  
+  /// Run all unit test sizes with device-level Conv2d instance
+  EXPECT_TRUE(test::conv::device::TestAllConv2d<Conv2dFprop>());
 }
 
-bool run_batched_fused_gemm_f16_sm80_shmem() {
-  using ThreadblockShape = cutlass::gemm::GemmShape<128, 64, 32>;
-  using WarpShape = cutlass::gemm::GemmShape<64, 32, 32>;
-  using InstructionShape = cutlass::gemm::GemmShape<16, 8, 16>;
-
-  // Optionally, we might not need intermediate GEMM outputs
-  constexpr bool kStoreD0 = true;
-  constexpr bool kStoreD1 = true;
-
-  using DualGemm = cutlass::gemm::device::DualGemm<
-    ElementOperandA,
-    cutlass::layout::RowMajor,
-    ElementOperandB,
-    cutlass::layout::ColumnMajor,
-    cutlass::layout::ColumnMajor,
-    ElementOutput,
-    cutlass::layout::RowMajor,
-    ElementAccumulator,
-    cutlass::arch::OpClassTensorOp,
-    cutlass::arch::Sm80,
-    ThreadblockShape,
-    WarpShape,
-    InstructionShape,
-    EpilogueOutputOp0,
-    EpilogueOutputOp1,
-    EpilogueOutputOp2,
-    cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<1>,
-    kStages,
-    kStoreD0,
-    kStoreD1,
-    kSplitKSerial
-  >;
-
-  DualFusedGemmRun<DualGemm> fusedGemm;
-
-  std::cout << "Running Batched Fused FP16 TN GEMMs + Epilogue2...\n";
-
-  bool passed = fusedGemm.run(
-    batch_problem_size, 
-    alpha0, 
-    beta0, 
-    alpha1, 
-    beta1, 
-    kBatchCount,
-    false,  /* broadcast_b1 */
-    false   /* is_profiling */
-  );
-
-  if(passed)
-    std::cout << "Pass\n";
-  else
-    std::cout << "Fail\n";
+////////////////////////////////////////////////////////////////////////////////
 
-  return passed;
-}
+TEST(SM80_Device_Conv2d_Fprop_Analytic_ImplicitGemm_f16nhwc_f16nhwc_f16nhwc_tensor_op_f16_align2,
+  128x128_64x3_64x64x64) {
 
-bool run_broadcast_fused_gemm_f16_sm80_shmem() {
-  using ThreadblockShape = cutlass::gemm::GemmShape<128, 64, 32>;
-  using WarpShape = cutlass::gemm::GemmShape<64, 32, 32>;
-  using InstructionShape = cutlass::gemm::GemmShape<16, 8, 16>;
-
-  // Optionally, we might not need intermediate GEMM outputs
-  constexpr bool kStoreD0 = true;
-  constexpr bool kStoreD1 = true;
-
-  using DualGemm = cutlass::gemm::device::DualGemm<
-    ElementOperandA,
-    cutlass::layout::RowMajor,
-    ElementOperandB,
-    // different LayoutB0 and B1
-    cutlass::layout::RowMajor,
-    cutlass::layout::ColumnMajor,
-    ElementOutput,
-    cutlass::layout::RowMajor,
+  /// Conv operation element types for the Gemm equivalent (ImplicitGemm)
+  using ElementA           = cutlass::half_t;
+  using ElementB           = cutlass::half_t;
+  using ElementC           = cutlass::half_t;
+  using ElementAccumulator = cutlass::half_t;
+  using ElementCompute     = cutlass::half_t;
+
+  /// Device-level Conv2d instance
+  using Conv2dFpropKernel = typename cutlass::conv::kernel::DefaultConv2dFprop<
+    ElementA, cutlass::layout::TensorNHWC,
+    ElementB, cutlass::layout::TensorNHWC,
+    ElementC, cutlass::layout::TensorNHWC,
     ElementAccumulator,
     cutlass::arch::OpClassTensorOp,
     cutlass::arch::Sm80,
-    ThreadblockShape,
-    WarpShape,
-    InstructionShape,
-    EpilogueOutputOp0,
-    EpilogueOutputOp1,
-    EpilogueOutputOp2,
-    cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<1>,
-    kStages,
-    kStoreD0,
-    kStoreD1,
-    kSplitKSerial
-  >;
-
-  DualFusedGemmRun<DualGemm> fusedGemm;
-
-  std::cout << "Running Broadcast Fused FP16 TN GEMMs + Epilogue2...\n";
-
-  bool passed = fusedGemm.run(
-    problem_size, 
-    alpha0, 
-    beta0, 
-    alpha1, 
-    beta1, 
-    1,     /* batch_count */
-    true,  /* broadcast_b1 */
-    true   /* is_profiling */
-  );
-
-  if(passed)
-    std::cout << "Pass\n";
-  else
-    std::cout << "Fail\n";
+    cutlass::gemm::GemmShape<128, 128, 64>,
+    cutlass::gemm::GemmShape<64, 64, 64>,
+    cutlass::gemm::GemmShape<16, 8, 16>,
+    cutlass::epilogue::thread::LinearCombination<
+      ElementC,
+      8,
+      ElementAccumulator,
+      ElementCompute
+    >,
+    cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>,
+    3,
+    cutlass::arch::OpMultiplyAdd,
+    cutlass::conv::IteratorAlgorithm::kAnalytic,
+    cutlass::conv::StrideSupport::kStrided,
+    2,
+    2
+  >::Kernel;
+
+  using Conv2dFprop = cutlass::conv::device::ImplicitGemmConvolution<Conv2dFpropKernel>;
+
+  test::conv::device::Conv2dProblemVector problem_size_list;
+
+  // run specific problem size in the unit test first
+  problem_size_list.push_back(cutlass::conv::Conv2dProblemSize(
+    {1, 4, 4, 12},     // input size (NHWC)
+    {8, 3, 3, 12},     // filter size (KRSC)
+    {0, 0, 0, 0},      // padding (pad_h, _, pad_w, _)
+    {3, 3},            // stride (stride_h, stride_w)
+    {1, 1}             // dilation (dilation_h, dilation_w)
+  ));
+
+  // run specific problem size in the unit test first
+  problem_size_list.push_back(cutlass::conv::Conv2dProblemSize(
+    {1, 4, 4, 14},     // input size (NHWC)
+    {8, 3, 3, 14},     // filter size (KRSC)
+    {0, 0, 0, 0},      // padding (pad_h, _, pad_w, _)
+    {3, 3},            // stride (stride_h, stride_w)
+    {1, 1}             // dilation (dilation_h, dilation_w)
+  ));
+
+  // run specific problem size in the unit test first
+  problem_size_list.push_back(cutlass::conv::Conv2dProblemSize(
+    {1, 23, 56, 98},     // input size (NHWC)
+    {128, 3, 3, 98},     // filter size (KRSC)
+    {4, 0, 5, 0},      // padding (pad_h, _, pad_w, _)
+    {3, 3},            // stride (stride_h, stride_w)
+    {1, 1}             // dilation (dilation_h, dilation_w)
+  ));
 
-  return passed;
+
+  /// Run all unit test sizes with device-level Conv2d instance
+  EXPECT_TRUE(test::conv::device::TestAllConv2d<Conv2dFprop>(problem_size_list));
 }
 
-bool run_batched_broadcast_fused_gemm_f16_sm80_shmem() {
-  using ThreadblockShape = cutlass::gemm::GemmShape<128, 64, 32>;
-  using WarpShape = cutlass::gemm::GemmShape<64, 32, 32>;
-  using InstructionShape = cutlass::gemm::GemmShape<16, 8, 16>;
-
-  // Optionally, we might not need intermediate GEMM outputs
-  constexpr bool kStoreD0 = true;
-  constexpr bool kStoreD1 = true;
-
-  using DualGemm = cutlass::gemm::device::DualGemm<
-    ElementOperandA,
-    cutlass::layout::RowMajor,
-    ElementOperandB,
-    // different LayoutB0 and B1
-    cutlass::layout::RowMajor,
-    cutlass::layout::ColumnMajor,
-    ElementOutput,
-    cutlass::layout::RowMajor,
+////////////////////////////////////////////////////////////////////////////////
+
+TEST(SM80_Device_Conv2d_Fprop_Optimized_ImplicitGemm_f16nhwc_f16nhwc_f16nhwc_tensor_op_f16_align2,
+  128x128_64x3_64x64x64) {
+
+  /// Conv operation element types for the Gemm equivalent (ImplicitGemm)
+  using ElementA           = cutlass::half_t;
+  using ElementB           = cutlass::half_t;
+  using ElementC           = cutlass::half_t;
+  using ElementAccumulator = cutlass::half_t;
+  using ElementCompute     = cutlass::half_t;
+
+  /// Device-level Conv2d instance
+  using Conv2dFpropKernel = typename cutlass::conv::kernel::DefaultConv2dFprop<
+    ElementA, cutlass::layout::TensorNHWC,
+    ElementB, cutlass::layout::TensorNHWC,
+    ElementC, cutlass::layout::TensorNHWC,
     ElementAccumulator,
     cutlass::arch::OpClassTensorOp,
     cutlass::arch::Sm80,
-    ThreadblockShape,
-    WarpShape,
-    InstructionShape,
-    EpilogueOutputOp0,
-    EpilogueOutputOp1,
-    EpilogueOutputOp2,
-    cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<1>,
-    kStages,
-    kStoreD0,
-    kStoreD1,
-    kSplitKSerial
-  >;
-
-  DualFusedGemmRun<DualGemm> fusedGemm;
-
-  std::cout << "Running Batch Broadcast Fused FP16 TN GEMMs + Epilogue2...\n";
-
-  bool passed = fusedGemm.run(
-    batch_problem_size, 
-    alpha0, 
-    beta0, 
-    alpha1, 
-    beta1, 
-    kBatchCount,
-    true,  /* broadcast_b1 */
-    false  /* is_profiling */
-  );
-
-  if(passed)
-    std::cout << "Pass\n";
-  else
-    std::cout << "Fail\n";
+    cutlass::gemm::GemmShape<128, 128, 64>,
+    cutlass::gemm::GemmShape<64, 64, 64>,
+    cutlass::gemm::GemmShape<16, 8, 16>,
+    cutlass::epilogue::thread::LinearCombination<
+      ElementC,
+      8,
+      ElementAccumulator,
+      ElementCompute
+    >,
+    cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>,
+    3,
+    cutlass::arch::OpMultiplyAdd,
+    cutlass::conv::IteratorAlgorithm::kOptimized,
+    cutlass::conv::StrideSupport::kStrided,
+    2,
+    2
+  >::Kernel;
+
+  using Conv2dFprop = cutlass::conv::device::ImplicitGemmConvolution<Conv2dFpropKernel>;
+
+  test::conv::device::Conv2dProblemVector problem_size_list;
+
+  // run specific problem size in the unit test first
+  problem_size_list.push_back(cutlass::conv::Conv2dProblemSize(
+    {1, 4, 4, 12},     // input size (NHWC)
+    {8, 3, 3, 12},     // filter size (KRSC)
+    {0, 0, 0, 0},      // padding (pad_h, _, pad_w, _)
+    {3, 3},            // stride (stride_h, stride_w)
+    {1, 1}             // dilation (dilation_h, dilation_w)
+  ));
+
+  // run specific problem size in the unit test first
+  problem_size_list.push_back(cutlass::conv::Conv2dProblemSize(
+    {1, 4, 4, 14},     // input size (NHWC)
+    {8, 3, 3, 14},     // filter size (KRSC)
+    {0, 0, 0, 0},      // padding (pad_h, _, pad_w, _)
+    {3, 3},            // stride (stride_h, stride_w)
+    {1, 1}             // dilation (dilation_h, dilation_w)
+  ));
+
+  // run specific problem size in the unit test first
+  problem_size_list.push_back(cutlass::conv::Conv2dProblemSize(
+    {1, 23, 56, 98},     // input size (NHWC)
+    {128, 3, 3, 98},     // filter size (KRSC)
+    {4, 0, 5, 0},      // padding (pad_h, _, pad_w, _)
+    {3, 3},            // stride (stride_h, stride_w)
+    {1, 1}             // dilation (dilation_h, dilation_w)
+  ));
 
-  return passed;
+
+  /// Run all unit test sizes with device-level Conv2d instance
+  EXPECT_TRUE(test::conv::device::TestAllConv2d<Conv2dFprop>(problem_size_list));
 }
 
-int main() {
+////////////////////////////////////////////////////////////////////////////////
 
-  std::vector<bool (*)()>funcs = {
-    &run_nonfused_gemm_f16_sm80,
-    &run_fused_gemm_f16_sm80_shmem,
-    &run_batched_fused_gemm_f16_sm80_shmem,
-    &run_broadcast_fused_gemm_f16_sm80_shmem,
-    &run_batched_broadcast_fused_gemm_f16_sm80_shmem
-  };
-
-  std::string test_name = (
-    "dual-gemm f16 bias=" + 
-    std::to_string(kUseBias) + 
-    " split_k_serial=" + 
-    std::to_string(kSplitKSerial) +
-    " batch_count=" + 
-    std::to_string(kBatchCount)
-  );
+TEST(SM80_Device_Conv2d_Fprop_Optimized_ImplicitGemm_f16nhwc_f16nhwc_f16nhwc_tensor_op_f16_align4,
+  128x128_64x3_64x64x64) {
 
-  return testRun(80, funcs, test_name);
+  /// Conv operation element types for the Gemm equivalent (ImplicitGemm)
+  using ElementA           = cutlass::half_t;
+  using ElementB           = cutlass::half_t;
+  using ElementC           = cutlass::half_t;
+  using ElementAccumulator = cutlass::half_t;
+  using ElementCompute     = cutlass::half_t;
+
+  /// Device-level Conv2d instance
+  using Conv2dFpropKernel = typename cutlass::conv::kernel::DefaultConv2dFprop<
+    ElementA, cutlass::layout::TensorNHWC,
+    ElementB, cutlass::layout::TensorNHWC,
+    ElementC, cutlass::layout::TensorNHWC,
+    ElementAccumulator,
+    cutlass::arch::OpClassTensorOp,
+    cutlass::arch::Sm80,
+    cutlass::gemm::GemmShape<128, 128, 64>,
+    cutlass::gemm::GemmShape<64, 64, 64>,
+    cutlass::gemm::GemmShape<16, 8, 16>,
+    cutlass::epilogue::thread::LinearCombination<
+      ElementC,
+      8,
+      ElementAccumulator,
+      ElementCompute
+    >,
+    cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>,
+    3,
+    cutlass::arch::OpMultiplyAdd,
+    cutlass::conv::IteratorAlgorithm::kOptimized,
+    cutlass::conv::StrideSupport::kStrided,
+    4,
+    4
+  >::Kernel;
+
+  using Conv2dFprop = cutlass::conv::device::ImplicitGemmConvolution<Conv2dFpropKernel>;
+
+  test::conv::device::Conv2dProblemVector problem_size_list;
+
+  // run specific problem size in the unit test first
+  problem_size_list.push_back(cutlass::conv::Conv2dProblemSize(
+    {1, 4, 4, 12},     // input size (NHWC)
+    {8, 3, 3, 12},     // filter size (KRSC)
+    {0, 0, 0, 0},      // padding (pad_h, _, pad_w, _)
+    {3, 3},            // stride (stride_h, stride_w)
+    {1, 1}             // dilation (dilation_h, dilation_w)
+  ));
+
+  // run specific problem size in the unit test first
+  problem_size_list.push_back(cutlass::conv::Conv2dProblemSize(
+    {1, 4, 4, 28},     // input size (NHWC)
+    {8, 3, 3, 28},     // filter size (KRSC)
+    {0, 0, 0, 0},      // padding (pad_h, _, pad_w, _)
+    {3, 3},            // stride (stride_h, stride_w)
+    {1, 1}             // dilation (dilation_h, dilation_w)
+  ));
+
+  // run specific problem size in the unit test first
+  problem_size_list.push_back(cutlass::conv::Conv2dProblemSize(
+    {1, 23, 56, 100},     // input size (NHWC)
+    {128, 3, 3, 100},     // filter size (KRSC)
+    {4, 0, 5, 0},      // padding (pad_h, _, pad_w, _)
+    {3, 3},            // stride (stride_h, stride_w)
+    {1, 1}             // dilation (dilation_h, dilation_w)
+  ));
+  
+  /// Run all unit test sizes with device-level Conv2d instance
+  EXPECT_TRUE(test::conv::device::TestAllConv2d<Conv2dFprop>(problem_size_list));
 }
 
-
-
 ////////////////////////////////////////////////////////////////////////////////
+#endif  // CUTLASS_ARCH_MMA_SM80_SUPPORTED
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/45_dual_gemm/dual_gemm_common.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/examples/45_dual_gemm/dual_gemm_common.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/45_dual_gemm/dual_gemm_run.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/examples/45_dual_gemm/dual_gemm_run.h`

 * *Files 19% similar despite different names*

```diff
@@ -29,30 +29,25 @@
  *
  **************************************************************************************************/
 #pragma once
 
 #include <iostream>
 #include <fstream>
 #include <sstream>
-#include <type_traits>
 
 #include "cutlass/util/host_tensor.h"
 #include "cutlass/util/tensor_view_io.h"
 #include "cutlass/util/distribution.h"
 #include "cutlass/util/reference/host/tensor_fill.h"
 #include "cutlass/util/reference/host/tensor_copy.h"
 #include "cutlass/util/reference/host/tensor_compare.h"
 #include "cutlass/util/reference/host/tensor_norm.h"
 #include "cutlass/util/reference/device/gemm.h"
 #include "cutlass/util/reference/device/tensor_relu.h"
 
-#include "cutlass/gemm/gemm.h"
-#include "cutlass/gemm/device/gemm_universal.h"
-
-#include "dual_gemm_common.h"
 #include "helper.h"
 
 #define CHECK_GT(val1, val2) \
     if((val1) <= (val2)) \
         std::cerr << __FILE__ << " " << __LINE__ << ": CHECK_GT failed\n";
 #define CHECK_TRUE(val) \
     if(!(val)) \
@@ -206,15 +201,14 @@
   /// Executes one test
   bool run(
     cutlass::gemm::GemmCoord problem_size,
     ElementCompute alpha0 = ElementCompute(1),
     ElementCompute beta0 = ElementCompute(0),
     ElementCompute alpha1 = ElementCompute(1),
     ElementCompute beta1 = ElementCompute(0),
-    bool is_profiling = true,
     bool relu = false,
     int warm_ups = 1,
     int runs = 100) {
     
     //
     // Allocate the GEMM workspace
     //
@@ -337,50 +331,49 @@
 
     for(int i = 0; i < warm_ups; i++) {
         status = gemm_op_0();
         CUTLASS_CHECK(status);
         status = gemm_op_1();
         CUTLASS_CHECK(status);
     }
+#ifdef IS_PROFILING
+    return true;
+#endif
+    //
+    // Run the GEMM
+    //
+    cudaEvent_t start, stop1, stop2;
+    cudaEventCreate(&start);
+    cudaEventCreate(&stop1);
+    cudaEventCreate(&stop2);
 
-    if (is_profiling) {
-      //
-      // Profile the GEMM
-      //
-
-      cudaEvent_t start, stop1, stop2;
-      cudaEventCreate(&start);
-      cudaEventCreate(&stop1);
-      cudaEventCreate(&stop2);
-
-      cudaEventRecord(start);
-
-      for(int i = 0; i < runs; i++) {
-          status = gemm_op_0();
-      
-          CUTLASS_CHECK(status);
-      }
-      cudaEventRecord(stop1);
-      for(int i = 0; i < runs; i++) {
-          status = gemm_op_1();
-      
-          CUTLASS_CHECK(status);
-      }
-
-      cudaEventRecord(stop2);
-      cudaDeviceSynchronize();
-      float gemm0Time, gemm1Time, totalTime;
-      cudaEventElapsedTime(&gemm0Time, start, stop1);
-      cudaEventElapsedTime(&gemm1Time, stop1, stop2);
-      cudaEventElapsedTime(&totalTime, start, stop2);
-      std::cout << "gemm 0 time " << gemm0Time / (float)runs << " ms\n";
-      std::cout << "gemm 1 time " << gemm1Time / (float)runs << " ms\n";
-      std::cout << "Non-fusion GEMM only time " << totalTime / (float)runs << " ms\n";
+    cudaEventRecord(start);
+
+    for(int i = 0; i < runs; i++) {
+        status = gemm_op_0();
+    
+        CUTLASS_CHECK(status);
+    }
+    cudaEventRecord(stop1);
+    for(int i = 0; i < runs; i++) {
+        status = gemm_op_1();
+    
+        CUTLASS_CHECK(status);
     }
 
+    cudaEventRecord(stop2);
+    cudaDeviceSynchronize();
+    float gemm0Time, gemm1Time, totalTime;
+    cudaEventElapsedTime(&gemm0Time, start, stop1);
+    cudaEventElapsedTime(&gemm1Time, stop1, stop2);
+    cudaEventElapsedTime(&totalTime, start, stop2);
+    std::cout << "gemm 0 time " << gemm0Time / (float)runs << " ms\n";
+    std::cout << "gemm 1 time " << gemm1Time / (float)runs << " ms\n";
+    std::cout << "Non-fusion GEMM only time " << totalTime / (float)runs << " ms\n";
+
     tensor_D0.sync_host();
     tensor_D1.sync_host();
 
     //
     // Verify
     //
     cutlass::reference::device::Gemm<
@@ -546,112 +539,73 @@
   /// Executes one test
   bool run(
     cutlass::gemm::GemmCoord problem_size,
     ElementCompute alpha0 = ElementCompute(1),
     ElementCompute beta0 = ElementCompute(1),
     ElementCompute alpha1 = ElementCompute(1),
     ElementCompute beta1 = ElementCompute(1),
-    int batch_count = 1,
-    bool broadcast_b1 = false,
-    bool is_profiling = true,
     bool relu = false,
     int warm_ups = 1,
     int runs = 100) {
     
     //
     // Allocate the GEMM workspace
     //
 
     cutlass::HostTensor<
       typename DualGemm::ElementA,
-      typename DualGemm::LayoutA> tensor_A0(
-        std::is_same<typename DualGemm::LayoutA, cutlass::layout::RowMajor>::value ? 
-          cutlass::MatrixCoord(batch_count * problem_size.m(), problem_size.k()) : 
-          cutlass::MatrixCoord(problem_size.m(), batch_count * problem_size.k()));
+      typename DualGemm::LayoutA> tensor_A0(problem_size.mk());
 
     cutlass::HostTensor<
       typename DualGemm::ElementB,
-      typename DualGemm::LayoutB0> tensor_B0(
-        std::is_same<typename DualGemm::LayoutB0, cutlass::layout::RowMajor>::value ? 
-          cutlass::MatrixCoord(batch_count * problem_size.k(), problem_size.n()) : 
-          cutlass::MatrixCoord(problem_size.k(), batch_count * problem_size.n()));
+      typename DualGemm::LayoutB> tensor_B0(problem_size.kn());
 
     cutlass::HostTensor<
       typename DualGemm::ElementC,
-      typename DualGemm::LayoutC> tensor_C0(
-        std::is_same<typename DualGemm::LayoutC, cutlass::layout::RowMajor>::value ? 
-          cutlass::MatrixCoord(batch_count * problem_size.m(), problem_size.n()) : 
-          cutlass::MatrixCoord(problem_size.m(), batch_count * problem_size.n()));
+      typename DualGemm::LayoutC> tensor_C0(problem_size.mn());
 
     cutlass::HostTensor<
       typename DualGemm::ElementC,
-      typename DualGemm::LayoutScaleBias> tensor_Bias0({batch_count, problem_size.n()});
+      typename DualGemm::LayoutScaleBias> tensor_Bias0({1, problem_size.n()});
 
     cutlass::HostTensor<
       typename DualGemm::ElementC,
-      typename DualGemm::LayoutC> tensor_D0(
-        std::is_same<typename DualGemm::LayoutC, cutlass::layout::RowMajor>::value ? 
-          cutlass::MatrixCoord(batch_count * problem_size.m(), problem_size.n()) : 
-          cutlass::MatrixCoord(problem_size.m(), batch_count * problem_size.n()));
+      typename DualGemm::LayoutC> tensor_D0(problem_size.mn());
 
     cutlass::HostTensor<
       typename DualGemm::ElementC,
-      typename DualGemm::LayoutC> reference_D0(
-        std::is_same<typename DualGemm::LayoutC, cutlass::layout::RowMajor>::value ? 
-          cutlass::MatrixCoord(batch_count * problem_size.m(), problem_size.n()) : 
-          cutlass::MatrixCoord(problem_size.m(), batch_count * problem_size.n()));
+      typename DualGemm::LayoutC> reference_D0(problem_size.mn());
 
     cutlass::HostTensor<
       typename DualGemm::ElementB,
-      typename DualGemm::LayoutB1> tensor_B1(
-        std::is_same<typename DualGemm::LayoutB1, cutlass::layout::RowMajor>::value ? 
-          cutlass::MatrixCoord(batch_count * problem_size.k(), problem_size.n()) : 
-          cutlass::MatrixCoord(problem_size.k(), batch_count * problem_size.n()));
-    if (broadcast_b1) {
-      tensor_B1.resize({problem_size.k(), batch_count});
-    }
+      typename DualGemm::LayoutB> tensor_B1(problem_size.kn());
 
     cutlass::HostTensor<
       typename DualGemm::ElementC,
-      typename DualGemm::LayoutC> tensor_C1(
-        std::is_same<typename DualGemm::LayoutC, cutlass::layout::RowMajor>::value ? 
-          cutlass::MatrixCoord(batch_count * problem_size.m(), problem_size.n()) : 
-          cutlass::MatrixCoord(problem_size.m(), batch_count * problem_size.n()));
+      typename DualGemm::LayoutC> tensor_C1(problem_size.mn());
 
     cutlass::HostTensor<
       typename DualGemm::ElementC,
-      typename DualGemm::LayoutScaleBias> tensor_Bias1({batch_count, problem_size.n()});
+      typename DualGemm::LayoutScaleBias> tensor_Bias1({1, problem_size.n()});
 
     cutlass::HostTensor<
       typename DualGemm::ElementC,
-      typename DualGemm::LayoutC> tensor_D1(
-        std::is_same<typename DualGemm::LayoutC, cutlass::layout::RowMajor>::value ? 
-          cutlass::MatrixCoord(batch_count * problem_size.m(), problem_size.n()) : 
-          cutlass::MatrixCoord(problem_size.m(), batch_count * problem_size.n()));
+      typename DualGemm::LayoutC> tensor_D1(problem_size.mn());
 
     cutlass::HostTensor<
       typename DualGemm::ElementC,
-      typename DualGemm::LayoutC> tensor_D2(
-        std::is_same<typename DualGemm::LayoutC, cutlass::layout::RowMajor>::value ? 
-          cutlass::MatrixCoord(batch_count * problem_size.m(), problem_size.n()) : 
-          cutlass::MatrixCoord(problem_size.m(), batch_count * problem_size.n()));
+      typename DualGemm::LayoutC> tensor_D2(problem_size.mn());
 
     cutlass::HostTensor<
       typename DualGemm::ElementC,
-      typename DualGemm::LayoutC> reference_D1(
-        std::is_same<typename DualGemm::LayoutC, cutlass::layout::RowMajor>::value ? 
-          cutlass::MatrixCoord(batch_count * problem_size.m(), problem_size.n()) : 
-          cutlass::MatrixCoord(problem_size.m(), batch_count * problem_size.n()));
+      typename DualGemm::LayoutC> reference_D1(problem_size.mn());
 
     cutlass::HostTensor<
       typename DualGemm::ElementC,
-      typename DualGemm::LayoutC> reference_D2(
-        std::is_same<typename DualGemm::LayoutC, cutlass::layout::RowMajor>::value ? 
-          cutlass::MatrixCoord(batch_count * problem_size.m(), problem_size.n()) : 
-          cutlass::MatrixCoord(problem_size.m(), batch_count * problem_size.n()));
+      typename DualGemm::LayoutC> reference_D2(problem_size.mn());
 
     CHECK_TRUE(initialize_tensor(tensor_A0.host_view(), init_A, seed + 2019));
     CHECK_TRUE(initialize_tensor(tensor_B0.host_view(), init_B, seed + 2118));
     CHECK_TRUE(initialize_tensor(tensor_C0.host_view(), init_C, seed + 2017));
     CHECK_TRUE(initialize_tensor(tensor_Bias0.host_view(), init_Bias, seed + 2011));
     CHECK_TRUE(initialize_tensor(tensor_B1.host_view(), init_B, seed + 2113));
     CHECK_TRUE(initialize_tensor(tensor_C1.host_view(), init_C, seed + 2015));
@@ -681,71 +635,42 @@
     tensor_D1.sync_device();
     tensor_D2.sync_device();
     reference_D0.sync_device();
     reference_D1.sync_device();
     reference_D2.sync_device();
 
     //
-    // Batch strides (irrelevant when batch_count == 1)
-    //
-
-    int64_t batch_stride_A = problem_size.m() * problem_size.k();
-    int64_t batch_stride_B0 = problem_size.k() * problem_size.n();
-    int64_t batch_stride_B1 = problem_size.k() * problem_size.n();
-    if (broadcast_b1) {
-      // B1 is a (column) vector
-      batch_stride_B1 = problem_size.k();
-    }
-    int64_t batch_stride_Bias = problem_size.n();
-    int64_t batch_stride_D = problem_size.m() * problem_size.n();
-
-    //
     // Initialize the GEMM operator
     //
 
     int split_k_slices = DualGemm::kSplitKSerial ? 2 : 1;
     typename cutlass::TensorRef<typename DualGemm::ElementC, typename DualGemm::LayoutC> nullptr_ref{};
     decltype(nullptr_ref) ref_B0, ref_B1;
     if (beta0 != ElementCompute(0)) {
       ref_B0 = {tensor_Bias0.device_data(), typename DualGemm::LayoutC::Stride(0)};
     }
     if (beta1 != ElementCompute(0)) {
       ref_B1 = {tensor_Bias1.device_data(), typename DualGemm::LayoutC::Stride(0)};
     }
     typename DualGemm::Arguments arguments{
-      (batch_count > 1 ? 
-        cutlass::gemm::DualGemmMode::kBatched : 
-        cutlass::gemm::DualGemmMode::kGemm),
       problem_size,
       tensor_A0.device_ref(),
       tensor_B0.device_ref(),
       ref_B0,
       DualGemm::kStoreD0 ? tensor_D0.device_ref() : nullptr_ref,
-      (broadcast_b1 ? 
-        typename DualGemm::TensorRefB1(tensor_B1.device_data(), 0) : 
-        tensor_B1.device_ref()),
+      tensor_B1.device_ref(),
       ref_B1,
       DualGemm::kStoreD1 ? tensor_D1.device_ref() : nullptr_ref,
       tensor_D2.device_ref(),
       {alpha0, beta0},
       {alpha1, beta1},
       {},
-      split_k_slices,
-      batch_count,
-      batch_stride_A,
-      batch_stride_B0,
-      batch_stride_B1,
-      batch_stride_Bias,
-      batch_stride_D,
+      split_k_slices
     };
 
-    //
-    // Run the GEMM
-    //
-
     DualGemm b2b_gemm_op;
 
     cutlass::device_memory::allocation<uint8_t> workspace(b2b_gemm_op.get_workspace_size(arguments));
   
     cutlass::Status status = b2b_gemm_op.can_implement(arguments);
 
     CUTLASS_CHECK(status);
@@ -755,120 +680,86 @@
     CUTLASS_CHECK(status);
 
     for(int i = 0; i < warm_ups; i++) {
         status = b2b_gemm_op();
         CUTLASS_CHECK(status);
     }
 
-    if (is_profiling) {
-      //
-      // Profile the GEMM
-      //
-
-      cudaEvent_t start, stop;
-      cudaEventCreate(&start);
-      cudaEventCreate(&stop);
-
-      cudaEventRecord(start);
-
-      for(int i = 0; i < runs; i++) {
-          status = b2b_gemm_op();
-          CUTLASS_CHECK(status);
-      }
-
-      cudaEventRecord(stop);
-      cudaDeviceSynchronize();
-      float gemmTime;
-      cudaEventElapsedTime(&gemmTime, start, stop);
-      std::cout << "Fusion time " << gemmTime / (float)runs << " ms\n";
+#ifdef IS_PROFILING
+    return true;
+#endif
+    //
+    // Run the GEMM
+    //
+
+    cudaEvent_t start, stop;
+    cudaEventCreate(&start);
+    cudaEventCreate(&stop);
+
+    cudaEventRecord(start);
+
+    for(int i = 0; i < runs; i++) {
+        status = b2b_gemm_op();
+
+        CUTLASS_CHECK(status);
     }
 
+    cudaEventRecord(stop);
+    cudaDeviceSynchronize();
+    float gemmTime;
+    cudaEventElapsedTime(&gemmTime, start, stop);
+    std::cout << "Fusion time " << gemmTime / (float)runs << " ms\n";
+
     tensor_D0.sync_host();
     tensor_D1.sync_host();
     tensor_D2.sync_host();
 
     //
     // Verify
     //
 
-    using GemmUniversal0 = cutlass::gemm::device::GemmUniversal<
-      typename DualGemm::ElementA, typename DualGemm::LayoutA,
-      typename DualGemm::ElementB, typename DualGemm::LayoutB0,
-      typename DualGemm::ElementC, typename DualGemm::LayoutC, 
-      ElementAccumulator
-    >;
-
-    GemmUniversal0 reference_gemm0;
-
-    typename GemmUniversal0::Arguments args0 {
-      (batch_count > 1 ? 
-        cutlass::gemm::GemmUniversalMode::kBatched : 
-        cutlass::gemm::GemmUniversalMode::kGemm),
-      problem_size,
-      batch_count,
-      {alpha0, beta0},
-      tensor_A0.device_data(),
-      tensor_B0.device_data(),
-      tensor_Bias0.device_data(),
-      reference_D0.device_data(),
-      batch_stride_A,
-      batch_stride_B0,
-      batch_stride_Bias,
-      batch_stride_D,
-      tensor_A0.stride(0),
-      tensor_B0.stride(0),
-      0,  // zero stride for the bias vector
-      reference_D0.stride(0),
-    };
+    cutlass::reference::device::Gemm<
+        typename DualGemm::ElementA, typename DualGemm::LayoutA,
+        typename DualGemm::ElementB, typename DualGemm::LayoutB,
+        typename DualGemm::ElementC, typename DualGemm::LayoutC, 
+        ElementAccumulator, ElementAccumulator>
+        reference_gemm_0;
 
-    status = reference_gemm0.can_implement(args0);
-    CUTLASS_CHECK(status);
-    status = reference_gemm0(args0);
-    CUTLASS_CHECK(status);
+    cutlass::reference::device::Gemm<
+        typename DualGemm::ElementA, typename DualGemm::LayoutA,
+        typename DualGemm::ElementB, typename DualGemm::LayoutB,
+        typename DualGemm::ElementC, typename DualGemm::LayoutC, ElementCompute,
+        ElementAccumulator, typename DualGemm::Operator>
+        reference_gemm_1;
 
-    using GemmUniversal1 = cutlass::gemm::device::GemmUniversal<
-      typename DualGemm::ElementA, typename DualGemm::LayoutA,
-      typename DualGemm::ElementB, typename DualGemm::LayoutB1,
-      typename DualGemm::ElementC, typename DualGemm::LayoutC, 
-      ElementAccumulator
-    >;
-
-    GemmUniversal1 reference_gemm1;
-
-    typename GemmUniversal1::Arguments args1 {
-      (batch_count > 1 ? 
-        cutlass::gemm::GemmUniversalMode::kBatched : 
-        cutlass::gemm::GemmUniversalMode::kGemm),
+    reference_gemm_0(
       problem_size,
-      batch_count,
-      {alpha1, beta1},
-      tensor_A0.device_data(),
-      tensor_B1.device_data(),
-      tensor_Bias1.device_data(),
-      reference_D1.device_data(),
-      batch_stride_A,
-      batch_stride_B1,
-      batch_stride_Bias,
-      batch_stride_D,
-      tensor_A0.stride(0),
-      (broadcast_b1 ? 0 : tensor_B1.stride(0)),
-      0,  // zero stride for the bias vector
-      reference_D1.stride(0),
-    };
-
-    status = reference_gemm1.can_implement(args1);
-    CUTLASS_CHECK(status);
-    status = reference_gemm1(args1);
-    CUTLASS_CHECK(status);
-
+      alpha0,
+      tensor_A0.device_ref(), 
+      tensor_B0.device_ref(), 
+      beta0,
+      {tensor_Bias0.device_data(), typename DualGemm::LayoutC::Stride(0)},
+      reference_D0.device_ref()
+    );
     if(relu) {
        cutlass::reference::device::TensorReLu(reference_D0.device_view()); 
-       cutlass::reference::device::TensorReLu(reference_D1.device_view()); 
     }
 
+    reference_gemm_1(
+      problem_size,
+      alpha1, 
+      tensor_A0.device_ref(), 
+      tensor_B1.device_ref(), 
+      beta1,
+      {tensor_Bias1.device_data(), typename DualGemm::LayoutC::Stride(0)},
+      reference_D1.device_ref()
+    );
+    if(relu) {
+       cutlass::reference::device::TensorReLu(reference_D1.device_view()); 
+    }
     TensorEpilogueForEach<EpilogueOutputOp2>(reference_D0.device_view(), reference_D1.device_view(), reference_D2.device_view());
     cudaDeviceSynchronize();
     reference_D0.sync_host();
     reference_D1.sync_host();
     reference_D2.sync_host();
 
     CHECK_GT(cutlass::reference::host::TensorNorm(reference_D0.host_view()), 0);
@@ -898,14 +789,15 @@
       reference_D2.host_view(), 
       tensor_D2.host_view());
     CHECK_TRUE(passed_out2);
 
     bool passed = passed_out0 && passed_out1 && passed_out2;
     if (!passed)
     {
+
       std::stringstream fname;
 
       fname << "error_DualGemm_device_fused.txt";
       std::cerr << "Dumping results in " << fname.str() << "\n";
 
       std::ofstream file(fname.str());
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/45_dual_gemm/kernel/dual_gemm.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/examples/45_dual_gemm/kernel/dual_gemm.h`

 * *Files 6% similar despite different names*

```diff
@@ -38,15 +38,14 @@
 
 #include "cutlass/gemm/gemm.h"
 #include "cutlass/matrix_coord.h"
 #include "cutlass/semaphore.h"
 
 #include "../threadblock/dual_mma_multistage.h"
 #include "../threadblock/dual_epilogue.h"
-#include "../dual_gemm_common.h"
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
 namespace cutlass {
 namespace gemm {
 namespace kernel {
 
@@ -89,100 +88,82 @@
       typename Epilogue0::Padding,
       kStoreD0,
       kStoreD1,
       Epilogue0::kFragmentsPerIteration,
       true // IterationsUnroll
   >;
 
-  using ElementA = typename DualMma::IteratorA::Element;
-  using ElementB = typename DualMma::IteratorB0::Element;
-  using ElementC = typename DualEpilogue::OutputTileIterator::Element;
-
   static bool const kSplitKSerial = SplitKSerial;
   static_assert(!kSplitKSerial || (kStoreD0 && kStoreD1),
     "Split-K serial requires buffers for D0/D1 for reduction");
 
   /// Warp count (concept: GemmShape)
   using WarpCount0 = typename DualMma::WarpCount;
   static int const kThreadCount = 32 * WarpCount0::kCount;
 
   /// Parameters structure
   struct Params {
-    DualGemmMode mode;
     cutlass::gemm::GemmCoord problem_size;
     cutlass::gemm::GemmCoord grid_tiled_shape;
     int swizzle_log_tile;
 
     // Mma0
     typename DualMma::IteratorA::Params params_A0;
     typename DualMma::IteratorA::TensorRef ref_A0;
-    typename DualMma::IteratorB0::Params params_B0;
-    typename DualMma::IteratorB0::TensorRef ref_B0;
+    typename DualMma::IteratorB::Params params_B0;
+    typename DualMma::IteratorB::TensorRef ref_B0;
     typename Epilogue0::OutputTileIterator::Params params_C0;
     typename Epilogue0::OutputTileIterator::TensorRef ref_C0;
     typename Epilogue0::OutputTileIterator::Params params_D0;
     typename Epilogue0::OutputTileIterator::TensorRef ref_D0;
     typename OutputOp0::Params output_op_0;
 
     // Mma1
-    typename DualMma::IteratorB1::Params params_B1;
-    typename DualMma::IteratorB1::TensorRef ref_B1;
+    typename DualMma::IteratorB::Params params_B1;
+    typename DualMma::IteratorB::TensorRef ref_B1;
     typename Epilogue1::OutputTileIterator::Params params_C1;
     typename Epilogue1::OutputTileIterator::TensorRef ref_C1;
     typename Epilogue1::OutputTileIterator::Params params_D1;
     typename Epilogue1::OutputTileIterator::TensorRef ref_D1;
     typename OutputOp1::Params output_op_1;
 
     typename Epilogue1::OutputTileIterator::Params params_D2;
     typename Epilogue1::OutputTileIterator::TensorRef ref_D2;
     typename OutputOp2::Params output_op_2;
 
     int *semaphore;
     int gemm_k_size;
 
-    int64_t batch_stride_A;
-    int64_t batch_stride_B0;
-    int64_t batch_stride_B1;
-    int64_t batch_stride_C;
-    int64_t batch_stride_D;
-
     //
     // Methods
     //
 
     CUTLASS_HOST_DEVICE
     Params(): swizzle_log_tile(0), semaphore(0), gemm_k_size(0) { }
 
     CUTLASS_HOST_DEVICE
     Params(
-      DualGemmMode mode,
       cutlass::gemm::GemmCoord const & problem_size,
       cutlass::gemm::GemmCoord const & grid_tiled_shape,
       // Mma0: D0 = A @ B0 + C0
       typename DualMma::IteratorA::TensorRef ref_A0,
-      typename DualMma::IteratorB0::TensorRef ref_B0,
+      typename DualMma::IteratorB::TensorRef ref_B0,
       typename Epilogue0::OutputTileIterator::TensorRef ref_C0,
       typename Epilogue0::OutputTileIterator::TensorRef ref_D0,
       // Mma1: D1 = A @ B1 + C1
-      typename DualMma::IteratorB1::TensorRef ref_B1,
+      typename DualMma::IteratorB::TensorRef ref_B1,
       typename Epilogue1::OutputTileIterator::TensorRef ref_C1,
       typename Epilogue1::OutputTileIterator::TensorRef ref_D1,
 
       typename Epilogue1::OutputTileIterator::TensorRef ref_D2,
       typename OutputOp0::Params output_op_0 = typename OutputOp0::Params(),
       typename OutputOp1::Params output_op_1 = typename OutputOp1::Params(),
       typename OutputOp2::Params output_op_2 = typename OutputOp2::Params(),
-      int *workspace = nullptr,
-      int64_t batch_stride_A = 1,
-      int64_t batch_stride_B0 = 1,
-      int64_t batch_stride_B1 = 1,
-      int64_t batch_stride_C = 1,
-      int64_t batch_stride_D = 1
+      int *workspace = nullptr
     ):
-      mode(mode),
       problem_size(problem_size),
       grid_tiled_shape(grid_tiled_shape),
       swizzle_log_tile(ThreadblockSwizzle().get_log_tile(grid_tiled_shape)),
       // Mma0
       params_A0(ref_A0.layout()),
       ref_A0(ref_A0),
       params_B0(ref_B0.layout()),
@@ -198,26 +179,21 @@
       ref_C1(ref_C1),
       params_D1(ref_D1.layout()),
       ref_D1(ref_D1),
       params_D2(ref_D2.layout()),
       ref_D2(ref_D2),
       output_op_0(output_op_0),
       output_op_1(output_op_1),
-      output_op_2(output_op_2),
-      batch_stride_A(batch_stride_A),
-      batch_stride_B0(batch_stride_B0),
-      batch_stride_B1(batch_stride_B1),
-      batch_stride_C(batch_stride_C),
-      batch_stride_D(batch_stride_D) {
+      output_op_2(output_op_2) {
 
       int total_gemm_k_iterations = (problem_size.k() + DualMma::Shape::kK - 1) / DualMma::Shape::kK;
       int gemm_k_iterations = (total_gemm_k_iterations + grid_tiled_shape.k() - 1) / grid_tiled_shape.k();
       gemm_k_size = gemm_k_iterations * DualMma::Shape::kK;
 
-      semaphore = workspace;
+    semaphore = workspace;
     }
   };
 
   /// Shared memory storage structure
   union SharedStorage {
     typename DualMma::SharedStorage main_loop;
     typename DualEpilogue::SharedStorage epilogue;
@@ -230,24 +206,24 @@
   CUTLASS_HOST_DEVICE
   DualGemm() { }
 
   /// Determines whether kernel satisfies alignment
     static Status can_implement(
       cutlass::gemm::GemmCoord const & problem_size,
       typename DualMma::IteratorA::TensorRef ref_A0,
-      typename DualMma::IteratorB0::TensorRef ref_B0,
+      typename DualMma::IteratorB::TensorRef ref_B0,
       typename Epilogue0::OutputTileIterator::TensorRef ref_C0,
       typename Epilogue0::OutputTileIterator::TensorRef ref_D0,
-      typename DualMma::IteratorB1::TensorRef ref_B1,
+      typename DualMma::IteratorB::TensorRef ref_B1,
       typename Epilogue1::OutputTileIterator::TensorRef ref_C1,
       typename Epilogue1::OutputTileIterator::TensorRef ref_D1,
       typename Epilogue1::OutputTileIterator::TensorRef ref_D2) {
 
     static int const kAlignmentA = DualMma::IteratorA::AccessType::kElements;
-    static int const kAlignmentB = DualMma::IteratorB0::AccessType::kElements;
+    static int const kAlignmentB = DualMma::IteratorB::AccessType::kElements;
     static int const kAlignmentC = Epilogue0::OutputTileIterator::kElementsPerAccess;
 
     if (!TensorRef_aligned(ref_A0, kAlignmentA)) {
       return Status::kErrorMisalignedOperand;
     }
 
     if (!TensorRef_aligned(ref_B0, kAlignmentB)) {
@@ -293,74 +269,60 @@
     // Early exit if CTA is out of range
     if (params.grid_tiled_shape.m() <= threadblock_tile_offset.m() ||
       params.grid_tiled_shape.n() <= threadblock_tile_offset.n()) {
 
       return;
     }
 
-    int offset_k = 0;
-    int problem_size_k = params.problem_size.k();
-
-    ElementA *ptr_A0 = static_cast<ElementA *>(params.ref_A0.data()); 
-    ElementB *ptr_B0 = static_cast<ElementB *>(params.ref_B0.data());
-    ElementB *ptr_B1 = static_cast<ElementB *>(params.ref_B1.data());
-
-    //
-    // Fetch pointers based on mode.
-    //
-    if (params.mode == DualGemmMode::kGemm) {
-      if (threadblock_tile_offset.k() + 1 < params.grid_tiled_shape.k()) {
-        problem_size_k = (threadblock_tile_offset.k() + 1) * params.gemm_k_size; 
-      }
-
-      offset_k = threadblock_tile_offset.k() * params.gemm_k_size;
-    }
-    else if (params.mode == DualGemmMode::kBatched) {
-      ptr_A0 += threadblock_tile_offset.k() * params.batch_stride_A;
-      ptr_B0 += threadblock_tile_offset.k() * params.batch_stride_B0;
-      ptr_B1 += threadblock_tile_offset.k() * params.batch_stride_B1;
-    }
-
     // Compute initial location in logical coordinates
     cutlass::MatrixCoord tb_offset_A0{
       threadblock_tile_offset.m() * DualMma::Shape::kM,
-      offset_k,
+      threadblock_tile_offset.k() * params.gemm_k_size,
     };
 
     cutlass::MatrixCoord tb_offset_B0{
-      offset_k,
+      threadblock_tile_offset.k() * params.gemm_k_size,
       threadblock_tile_offset.n() * DualMma::Shape::kN
     };
 
     cutlass::MatrixCoord tb_offset_B1{
-      offset_k,
+      threadblock_tile_offset.k() * params.gemm_k_size,
       threadblock_tile_offset.n() * DualMma::Shape::kN
     };
 
+    // Problem size is a function of threadblock index in the K dimension
+    int problem_size_k =
+      (params.problem_size.k() < (threadblock_tile_offset.k() + 1) * params.gemm_k_size) ?
+       params.problem_size.k() :
+       (threadblock_tile_offset.k() + 1) * params.gemm_k_size;
+
+    // Compute threadblock-scoped matrix multiply-add
+    int gemm_k_iterations = (problem_size_k - tb_offset_A0.column() + DualMma::Shape::kK - 1) / DualMma::Shape::kK;
+
     // Compute position within threadblock
     int thread_idx = threadIdx.x;
 
     // Construct iterators to A and B operands
     typename DualMma::IteratorA iterator_A0(
       params.params_A0,
-      ptr_A0,
+      params.ref_A0.data(),
       {params.problem_size.m(), problem_size_k},
       thread_idx,
       tb_offset_A0);
 
-    typename DualMma::IteratorB0 iterator_B0(
+    typename DualMma::IteratorB iterator_B0(
       params.params_B0,
-      ptr_B0,
+      params.ref_B0.data(),
       {problem_size_k, params.problem_size.n()},
       thread_idx,
       tb_offset_B0);
 
-    typename DualMma::IteratorB1 iterator_B1(
+    typename DualMma::IteratorB iterator_B1(
       params.params_B1,
-      ptr_B1,
+      params.ref_B1.data(),
       {problem_size_k, params.problem_size.n()},
       thread_idx,
       tb_offset_B1);
 
 
     // Broadcast the warp_id computed by lane 0 to ensure dependent code
     // is compiled as warp-uniform.
@@ -374,17 +336,14 @@
 
     // Construct thread-scoped matrix multiply
     typename DualMma::FragmentC accum0;
     typename DualMma::FragmentC accum1;
     accum0.clear();
     accum1.clear();
 
-    // Compute threadblock-scoped matrix multiply-add
-    int gemm_k_iterations = (problem_size_k - offset_k + DualMma::Shape::kK - 1) / DualMma::Shape::kK;
-
     DualMma mma(shared_storage.main_loop, thread_idx, warp_idx, lane_idx);
     if (!kSplitKSerial || gemm_k_iterations > 0) {
       // Compute threadblock-scoped matrix multiply-add
       mma(gemm_k_iterations,
         accum0, accum1,
         iterator_A0, iterator_B0, iterator_B1,
         accum0, accum1);
@@ -409,77 +368,62 @@
     MatrixCoord threadblock_offset(
       threadblock_tile_offset.m() * DualMma::Shape::kM,
       threadblock_tile_offset.n() * DualMma::Shape::kN
     );
 
     int block_idx = threadblock_tile_offset.m() + threadblock_tile_offset.n() * params.grid_tiled_shape.m();
 
-    ElementC *ptr_C0 = static_cast<ElementC *>(params.ref_C0.data()); 
-    ElementC *ptr_C1 = static_cast<ElementC *>(params.ref_C1.data()); 
-    ElementC *ptr_D0 = static_cast<ElementC *>(params.ref_D0.data()); 
-    ElementC *ptr_D1 = static_cast<ElementC *>(params.ref_D1.data()); 
-    ElementC *ptr_D2 = static_cast<ElementC *>(params.ref_D2.data()); 
-
     // Construct the semaphore.
     Semaphore semaphore(params.semaphore + block_idx, thread_idx);
 
-    if (params.mode == DualGemmMode::kGemm) {
-      // If performing a reduction via split-K, fetch the initial synchronization
-      if (kSplitKSerial && params.grid_tiled_shape.k() > 1) {
-        
-        // Fetch the synchronization lock initially but do not block.
-        semaphore.fetch();
-
-        // Indicate which position in a serial reduction the output operator is currently updating
-        output_op_0.set_k_partition(threadblock_tile_offset.k(), params.grid_tiled_shape.k());
-        output_op_1.set_k_partition(threadblock_tile_offset.k(), params.grid_tiled_shape.k());
-      }
-    }
-    else if (params.mode == DualGemmMode::kBatched) {
-      ptr_C0 += threadblock_tile_offset.k() * params.batch_stride_C;
-      ptr_C1 += threadblock_tile_offset.k() * params.batch_stride_C;
-      ptr_D0 += threadblock_tile_offset.k() * params.batch_stride_D;
-      ptr_D1 += threadblock_tile_offset.k() * params.batch_stride_D;
-      ptr_D2 += threadblock_tile_offset.k() * params.batch_stride_D;
+    // If performing a reduction via split-K, fetch the initial synchronization
+    if (kSplitKSerial && params.grid_tiled_shape.k() > 1) {
+      
+      // Fetch the synchronization lock initially but do not block.
+      semaphore.fetch();
+
+      // Indicate which position in a serial reduction the output operator is currently updating
+      output_op_0.set_k_partition(threadblock_tile_offset.k(), params.grid_tiled_shape.k());
+      output_op_1.set_k_partition(threadblock_tile_offset.k(), params.grid_tiled_shape.k());
     }
 
     // Tile iterator loading from source tensor.
     typename Epilogue0::OutputTileIterator iterator_C0(
       params.params_C0,
-      ptr_C0,
+      params.ref_C0.data(),
       params.problem_size.mn(),
       thread_idx,
       threadblock_offset
     );
     typename Epilogue1::OutputTileIterator iterator_C1(
       params.params_C1,
-      ptr_C1,
+      params.ref_C1.data(),
       params.problem_size.mn(),
       thread_idx,
       threadblock_offset
     );
 
     // Tile iterator writing to destination tensor.
     typename Epilogue0::OutputTileIterator iterator_D0(
       params.params_D0,
-      ptr_D0,
+      params.ref_D0.data(),
       params.problem_size.mn(),
       thread_idx,
       threadblock_offset
     );
     typename Epilogue1::OutputTileIterator iterator_D1(
       params.params_D1,
-      ptr_D1,
+      params.ref_D1.data(),
       params.problem_size.mn(),
       thread_idx,
       threadblock_offset
     );
     typename Epilogue1::OutputTileIterator iterator_D2(
       params.params_D2,
-      ptr_D2,
+      params.ref_D2.data(),
       params.problem_size.mn(),
       thread_idx,
       threadblock_offset
     );
 
     DualEpilogue epilogue(
       shared_storage.epilogue,
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/45_dual_gemm/test_run.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/examples/45_dual_gemm/test_run.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/45_dual_gemm/thread/left_silu_and_mul.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/examples/45_dual_gemm/thread/left_silu_and_mul.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/45_dual_gemm/threadblock/dual_epilogue.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/examples/45_dual_gemm/threadblock/dual_epilogue.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/45_dual_gemm/threadblock/dual_mma_base.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/mma_planar_complex_base.h`

 * *Files 14% similar despite different names*

```diff
@@ -38,191 +38,167 @@
 #include "cutlass/arch/memory.h"
 #include "cutlass/array.h"
 #include "cutlass/cutlass.h"
 #include "cutlass/gemm/gemm.h"
 #include "cutlass/matrix_shape.h"
 #include "cutlass/numeric_types.h"
 
-#include "cutlass/gemm/threadblock/mma_base.h"
-
 ////////////////////////////////////////////////////////////////////////////////
 
 namespace cutlass {
 namespace gemm {
 namespace threadblock {
 
 ////////////////////////////////////////////////////////////////////////////////
 
 /// Structure to compute the matrix product targeting CUDA cores and SIMT math
 /// instructions.
 template <
     /// Size of the Gemm problem - concept: gemm::GemmShape<>
     typename Shape_,
     /// Policy describing tuning details (concept: MmaPolicy)
-    typename Policy0_,
-    /// B1-specific version of the policy (concept: MmaPolicy)
-    typename Policy1_,
+    typename Policy_,
     /// Number of stages,
     int Stages,
     /// Used for partial specialization
     typename Enable = bool>
-class DualMmaBase {
+class MmaPlanarComplexBase {
  public:
   ///< Size of the Gemm problem - concept: gemm::GemmShape<>
   using Shape = Shape_;
 
   ///< Policy describing tuning details
-  using Policy0 = Policy0_;
-  using Policy1 = Policy1_;
+  using Policy = Policy_;
 
   //
   // Dependent types
   //
 
   /// Warp-level Mma
-  using Operator0 = typename Policy0::Operator;
-  using Operator1 = typename Policy1::Operator;
+  using Operator = typename Policy::Operator;
 
   /// Shape describing the overall GEMM computed from shared memory
   /// by each warp.
-  using WarpGemm = typename Policy0::Operator::Shape;
+  using WarpGemm = typename Policy::Operator::Shape;
 
   /// Shape describing the number of warps filling the CTA
   using WarpCount = GemmShape<Shape::kM / WarpGemm::kM,
                               Shape::kN / WarpGemm::kN,
                               Shape::kK / WarpGemm::kK>;
 
   /// Number of warp-level GEMM oeprations
   static int const kWarpGemmIterations =
-      (WarpGemm::kK / Operator0::Policy::MmaShape::kK);
+      (WarpGemm::kK / Operator::Policy::MmaShape::kK);
 
   /// Number of stages
   static int const kStages = Stages;
 
   /// Tensor reference to the A operand
-  using TensorRefA = TensorRef<typename Operator0::ElementA, typename Operator0::LayoutA>;
+  using TensorRefA = TensorRef<typename Operator::ElementA, typename Operator::LayoutA>;
 
   /// Tensor reference to the B operand
-  using TensorRefB0 = TensorRef<typename Operator0::ElementB, typename Operator0::LayoutB>;
-  using TensorRefB1 = TensorRef<typename Operator1::ElementB, typename Operator1::LayoutB>;
-
-  static_assert(kWarpGemmIterations > 1,
-                "The pipelined structure requires at least two warp-level "
-                "GEMM operations.");
-
-  static_assert((kWarpGemmIterations % 2) == 0,
-                "Inner loop iteration must be an even number.");
+  using TensorRefB = TensorRef<typename Operator::ElementB, typename Operator::LayoutB>;
 
   //
   // Nested structs
   //
 
   /// Shared storage object needed by threadblock-scoped GEMM
   class SharedStorage {
    public:
     //
     // Type definitions
     //
 
     /// Shape of the A matrix operand in shared memory
-    using ShapeA = MatrixShape<Shape::kM + Policy0::SmemPaddingA::kRow,
+    using ShapeA = MatrixShape<Shape::kM + Policy::SmemPaddingA::kRow,
                                Shape::kK * kStages +
-                                   Policy0::SmemPaddingA::kColumn>;
+                                   Policy::SmemPaddingA::kColumn>;
+
+    /// Stride to the imaginary part of the A operand
+    static int const kImaginaryStrideA = ShapeA::kCount;
 
     /// Shape of the B matrix operand in shared memory
-    using ShapeB0 =
-        MatrixShape<Shape::kK * kStages + Policy0::SmemPaddingB::kRow,
-                    Shape::kN + Policy0::SmemPaddingB::kColumn>;
-    using ShapeB1 =
-        MatrixShape<Shape::kK * kStages + Policy1::SmemPaddingB::kRow,
-                    Shape::kN + Policy1::SmemPaddingB::kColumn>;
+    using ShapeB =
+        MatrixShape<Shape::kK * kStages + Policy::SmemPaddingB::kRow,
+                    Shape::kN + Policy::SmemPaddingB::kColumn>;
+
+    /// Stride to the imaginary part of the A operand
+    static int const kImaginaryStrideB = ShapeB::kCount;
 
    public:
     //
     // Data members
     //
 
     /// Buffer for A operand
-    AlignedBuffer<typename Operator0::ElementA, ShapeA::kCount> operand_A;
+    AlignedBuffer<typename Operator::ElementA, ShapeA::kCount + kImaginaryStrideA> operand_A;
 
     /// Buffer for B operand
-    AlignedBuffer<typename Operator0::ElementB, ShapeB0::kCount> operand_B0;
-    AlignedBuffer<typename Operator1::ElementB, ShapeB1::kCount> operand_B1;
+    AlignedBuffer<typename Operator::ElementB, ShapeB::kCount + kImaginaryStrideB> operand_B;
 
    public:
 
     //
     // Methods
     //
 
     /// Returns a layout object for the A matrix
     CUTLASS_DEVICE
-    static typename Operator0::LayoutA LayoutA() {
-      return Operator0::LayoutA::packed({ShapeA::kRow, ShapeA::kColumn});
+    static typename Operator::LayoutA LayoutA() {
+      return Operator::LayoutA::packed({ShapeA::kRow, ShapeA::kColumn});
     }
 
     /// Returns a layout object for the B matrix
     CUTLASS_HOST_DEVICE
-    static typename Operator0::LayoutB LayoutB0() {
-      return Operator0::LayoutB::packed({ShapeB0::kRow, ShapeB0::kColumn});
-    }
-
-    /// Returns a layout object for the B matrix
-    CUTLASS_HOST_DEVICE
-    static typename Operator1::LayoutB LayoutB1() {
-      return Operator1::LayoutB::packed({ShapeB1::kRow, ShapeB1::kColumn});
+    static typename Operator::LayoutB LayoutB() {
+      return Operator::LayoutB::packed({ShapeB::kRow, ShapeB::kColumn});
     }
 
     /// Returns a TensorRef to the A operand
     CUTLASS_HOST_DEVICE
     TensorRefA operand_A_ref() {
       return TensorRefA{operand_A.data(), LayoutA()};
     }
 
     /// Returns a TensorRef to the B operand
     CUTLASS_HOST_DEVICE
-    TensorRefB0 operand_B0_ref() {
-      return TensorRefB0{operand_B0.data(), LayoutB0()};
-    }
-    CUTLASS_HOST_DEVICE
-    TensorRefB1 operand_B1_ref() {
-      return TensorRefB1{operand_B1.data(), LayoutB1()};
+    TensorRefB operand_B_ref() {
+      return TensorRefB{operand_B.data(), LayoutB()};
     }
   };
 
  protected:
 
   //
   // Data members
   //
 
   /// Iterator to load a warp-scoped tile of A operand from shared memory
-  typename Operator0::IteratorA warp_tile_iterator_A_;
+  typename Operator::IteratorA warp_tile_iterator_A_;
 
   /// Iterator to load a warp-scoped tile of B operand from shared memory
-  typename Operator0::IteratorB warp_tile_iterator_B0_;
-  typename Operator1::IteratorB warp_tile_iterator_B1_;
+  typename Operator::IteratorB warp_tile_iterator_B_;
 
 public:
 
   /// Construct from tensor references
   CUTLASS_DEVICE
-  DualMmaBase(
+  MmaPlanarComplexBase(
       ///< Shared storage needed for internal use by threadblock-scoped GEMM
       SharedStorage &shared_storage,
       ///< ID within the threadblock
       int thread_idx,
       ///< ID of warp
       int warp_idx,
       ///< ID of each thread within a warp
       int lane_idx
     ):
       warp_tile_iterator_A_(shared_storage.operand_A_ref(), lane_idx),
-      warp_tile_iterator_B0_(shared_storage.operand_B0_ref(), lane_idx),
-      warp_tile_iterator_B1_(shared_storage.operand_B1_ref(), lane_idx) {
+      warp_tile_iterator_B_(shared_storage.operand_B_ref(), lane_idx) {
 
   }
 };
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
 }  // namespace threadblock
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/45_dual_gemm/threadblock/dual_mma_multistage.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/examples/45_dual_gemm/threadblock/dual_mma_multistage.h`

 * *Files 3% similar despite different names*

```diff
@@ -63,103 +63,88 @@
     //  MaskedTileIterator)
     typename IteratorA_,
     /// Iterates over tiles of A operand in shared memory
     /// (concept: WriteableTileIterator | RandomAccessTileIterator)
     typename SmemIteratorA_,
     /// Cache operation for operand A
     cutlass::arch::CacheOperation::Kind CacheOpA,
-    /// Iterates over tiles of B0 operand in global memory
+    /// Iterates over tiles of B operand in global memory
     //  (concept: ReadableTileIterator | ForwardTileIterator |
     //  MaskedTileIterator)
-    typename IteratorB0_,
-    /// Iterates over tiles of B0 operand in shared memory
+    typename IteratorB_,
+    /// Iterates over tiles of B operand in shared memory
     /// (concept: WriteableTileIterator | RandomAccessTileIterator)
-    typename SmemIteratorB0_,
+    typename SmemIteratorB_,
     /// Cache operation for operand B
     cutlass::arch::CacheOperation::Kind CacheOpB,
-    /// Iterates over tiles of B1 operand in global memory
-    //  (concept: ReadableTileIterator | ForwardTileIterator |
-    //  MaskedTileIterator)
-    typename IteratorB1_,
-    /// Iterates over tiles of B1 operand in shared memory
-    /// (concept: WriteableTileIterator | RandomAccessTileIterator)
-    typename SmemIteratorB1_,
     /// Data type of accumulator matrix
     typename ElementC_,
     /// Data type of accumulator matrix
     typename LayoutC_,
     /// Policy describing tuning details (concept: MmaPolicy)
-    typename Policy0_,
-    /// B1-specific version of the policy (concept: MmaPolicy)
-    typename Policy1_,
+    typename Policy_,
     /// Number of stages,
     int Stages,
     /// Use zfill or predicate for out-of-bound cp.async
     SharedMemoryClearOption SharedMemoryClear = SharedMemoryClearOption::kNone,
     /// Used for partial specialization
     typename Enable = bool>
 class DualMmaMultistage : 
-  public DualMmaBase<Shape_, Policy0_, Policy1_, Stages> {
+  public DualMmaBase<Shape_, Policy_, Stages> {
 public:
   ///< Base class
-  using Base = DualMmaBase<Shape_, Policy0_, Policy1_, Stages>;
+  using Base = DualMmaBase<Shape_, Policy_, Stages>;
   ///< Size of the Gemm problem - concept: gemm::GemmShape<>
   using Shape = Shape_;
   ///< Iterates over tiles of A operand in global memory
   using IteratorA = IteratorA_;
-  ///< Iterates over tiles of B0 operand in global memory
-  using IteratorB0 = IteratorB0_;
-  ///< Iterates over tiles of B1 operand in global memory
-  using IteratorB1 = IteratorB1_;
+  ///< Iterates over tiles of B operand in global memory
+  using IteratorB = IteratorB_;
   ///< Data type of accumulator matrix
   using ElementC = ElementC_;
   ///< Layout of accumulator matrix
   using LayoutC = LayoutC_;
   ///< Policy describing tuning details
-  using Policy0 = Policy0_;
-  using Policy1 = Policy1_;
+  using Policy = Policy_;
 
   using SmemIteratorA = SmemIteratorA_;
-  using SmemIteratorB0 = SmemIteratorB0_;
-  using SmemIteratorB1 = SmemIteratorB1_;
+  using SmemIteratorB = SmemIteratorB_;
 
   static cutlass::arch::CacheOperation::Kind const kCacheOpA = CacheOpA;
   static cutlass::arch::CacheOperation::Kind const kCacheOpB = CacheOpB;
 
   //
   // Dependent types
   //
 
   /// Fragment of accumulator tile
-  using FragmentC = typename Policy0::Operator::FragmentC;
+  using FragmentC = typename Policy::Operator::FragmentC;
 
   /// Warp-level Mma
-  using Operator0 = typename Policy0::Operator;
-  using Operator1 = typename Policy1::Operator;
+  using Operator = typename Policy::Operator;
 
   /// Minimum architecture is Sm80 to support cp.async
   using ArchTag = arch::Sm80;
   
   /// Complex transform on A operand
-  static ComplexTransform const kTransformA = Operator0::kTransformA;
+  static ComplexTransform const kTransformA = Operator::kTransformA;
 
   /// Complex transform on B operand
-  static ComplexTransform const kTransformB0 = Operator0::kTransformB;
-  static ComplexTransform const kTransformB1 = Operator1::kTransformB;
+  static ComplexTransform const kTransformB = Operator::kTransformB;
 
   /// Internal structure exposed for introspection.
   struct Detail {
 
     /// Number of cp.async instructions to load one stage of operand A
     static int const AsyncCopyIterationsPerStageA =
         IteratorA::ThreadMap::Iterations::kCount;
 
     /// Number of cp.async instructions to load one stage of operand B
     static int const AsyncCopyIterationsPerStageB =
-        IteratorB0::ThreadMap::Iterations::kCount;
+        IteratorB::ThreadMap::Iterations::kCount;
 
     /// Number of stages
     static int const kStages = Stages;
 
     /// Number of cp.async instructions to load on group of operand A
     static int const kAccessesPerGroupA =
         (AsyncCopyIterationsPerStageA + Base::kWarpGemmIterations - 1) / Base::kWarpGemmIterations;
@@ -167,33 +152,31 @@
     /// Number of cp.async instructions to load on group of operand B
     static int const kAccessesPerGroupB =
         (AsyncCopyIterationsPerStageB + Base::kWarpGemmIterations - 1) / Base::kWarpGemmIterations;
   };
 
  private:
 
-  using WarpLoadedFragmentA = typename Operator0::FragmentA;
-  using WarpLoadedFragmentB0 = typename Operator0::FragmentB;
-  using WarpLoadedFragmentB1 = typename Operator1::FragmentB;
-  using WarpTransformedFragmentA = typename Operator0::TransformedFragmentA;
-  using WarpTransformedFragmentB0 = typename Operator0::TransformedFragmentB;
-  using WarpTransformedFragmentB1 = typename Operator1::TransformedFragmentB;
+  using WarpLoadedFragmentA = typename Operator::FragmentA;
+  using WarpLoadedFragmentB = typename Operator::FragmentB;
+  using WarpTransformedFragmentA = typename Operator::TransformedFragmentA;
+  using WarpTransformedFragmentB = typename Operator::TransformedFragmentB;
 
  private:
 
   //
   // Data members
   //
 
   /// Iterator to write threadblock-scoped tile of A operand to shared memory
   SmemIteratorA smem_iterator_A_;
 
   /// Iterator to write threadblock-scoped tile of B operand to shared memory
-  SmemIteratorB0 smem_iterator_B0_;
-  SmemIteratorB1 smem_iterator_B1_;
+  SmemIteratorB smem_iterator_B0_;
+  SmemIteratorB smem_iterator_B1_;
 
 public:
 
   /// Construct from tensor references
   CUTLASS_DEVICE
   DualMmaMultistage(
       ///< Shared storage needed for internal use by threadblock-scoped GEMM
@@ -228,15 +211,15 @@
     this->warp_tile_iterator_B0_.add_tile_offset(
         {Base::kWarpGemmIterations * warp_idx_k, warp_idx_n});
     this->warp_tile_iterator_B1_.add_tile_offset(
         {Base::kWarpGemmIterations * warp_idx_k, warp_idx_n});
   }
 
   CUTLASS_DEVICE
-  void copy_tiles_and_advance(IteratorA &iterator_A, IteratorB0 &iterator_B0, IteratorB1 &iterator_B1,
+  void copy_tiles_and_advance(IteratorA &iterator_A, IteratorB &iterator_B0, IteratorB &iterator_B1,
                               int group_start_A = 0, int group_start_B = 0) {
     iterator_A.set_iteration_index(group_start_A *
                                    IteratorA::kAccessesPerVector);
     this->smem_iterator_A_.set_iteration_index(group_start_A);
 
     // Async Copy for operand A
     CUTLASS_PRAGMA_UNROLL
@@ -266,34 +249,34 @@
         }
 
         ++this->smem_iterator_A_;
       }
     }
 
     iterator_B0.set_iteration_index(group_start_B *
-                                   IteratorB0::kAccessesPerVector);
+                                   IteratorB::kAccessesPerVector);
     iterator_B1.set_iteration_index(group_start_B *
-                                   IteratorB1::kAccessesPerVector);
+                                   IteratorB::kAccessesPerVector);
     this->smem_iterator_B0_.set_iteration_index(group_start_B);
     this->smem_iterator_B1_.set_iteration_index(group_start_B);
 
     // Async Copy for operand B0
     CUTLASS_PRAGMA_UNROLL
     for (int j = 0; j < Detail::kAccessesPerGroupB; ++j) {
       if (group_start_B + j < Detail::AsyncCopyIterationsPerStageB) {
-        typename IteratorB0::AccessType *dst_ptr =
-            reinterpret_cast<typename IteratorB0::AccessType *>(
+        typename IteratorB::AccessType *dst_ptr =
+            reinterpret_cast<typename IteratorB::AccessType *>(
                 this->smem_iterator_B0_.get());
 
-        int const kSrcBytes = sizeof_bits<typename IteratorB0::Element>::value *
-                              IteratorB0::ThreadMap::kElementsPerAccess /
-                              IteratorB0::kAccessesPerVector / 8;
+        int const kSrcBytes = sizeof_bits<typename IteratorB::Element>::value *
+                              IteratorB::ThreadMap::kElementsPerAccess /
+                              IteratorB::kAccessesPerVector / 8;
 
         CUTLASS_PRAGMA_UNROLL
-        for (int v = 0; v < IteratorB0::kAccessesPerVector; ++v) {
+        for (int v = 0; v < IteratorB::kAccessesPerVector; ++v) {
           auto gmem_ptr = iterator_B0.get();
 
           if (SharedMemoryClear == SharedMemoryClearOption::kZfill) {
             cutlass::arch::cp_async_zfill<kSrcBytes, kCacheOpB>(
                 dst_ptr + v, gmem_ptr, iterator_B0.valid());
           } else {
             cutlass::arch::cp_async<kSrcBytes, kCacheOpB>(
@@ -305,24 +288,24 @@
         ++this->smem_iterator_B0_;
       }
     }
     // Async Copy for operand B1
     CUTLASS_PRAGMA_UNROLL
     for (int j = 0; j < Detail::kAccessesPerGroupB; ++j) {
       if (group_start_B + j < Detail::AsyncCopyIterationsPerStageB) {
-        typename IteratorB1::AccessType *dst_ptr =
-            reinterpret_cast<typename IteratorB1::AccessType *>(
+        typename IteratorB::AccessType *dst_ptr =
+            reinterpret_cast<typename IteratorB::AccessType *>(
                 this->smem_iterator_B1_.get());
 
-        int const kSrcBytes = sizeof_bits<typename IteratorB1::Element>::value *
-                              IteratorB1::ThreadMap::kElementsPerAccess /
-                              IteratorB1::kAccessesPerVector / 8;
+        int const kSrcBytes = sizeof_bits<typename IteratorB::Element>::value *
+                              IteratorB::ThreadMap::kElementsPerAccess /
+                              IteratorB::kAccessesPerVector / 8;
 
         CUTLASS_PRAGMA_UNROLL
-        for (int v = 0; v < IteratorB1::kAccessesPerVector; ++v) {
+        for (int v = 0; v < IteratorB::kAccessesPerVector; ++v) {
           auto gmem_ptr = iterator_B1.get();
 
           if (SharedMemoryClear == SharedMemoryClearOption::kZfill) {
             cutlass::arch::cp_async_zfill<kSrcBytes, kCacheOpB>(
                 dst_ptr + v, gmem_ptr, iterator_B1.valid());
           } else {
             cutlass::arch::cp_async<kSrcBytes, kCacheOpB>(
@@ -343,16 +326,16 @@
       int gemm_k_iterations,
       ///< destination accumulator tile
       FragmentC &accum0,
       FragmentC &accum1,
       ///< iterator over A operand in global memory
       IteratorA iterator_A,
       ///< iterator over B operand in global memory
-      IteratorB0 iterator_B0,
-      IteratorB1 iterator_B1,
+      IteratorB iterator_B0,
+      IteratorB iterator_B1,
       ///< initial value of accumulator
       FragmentC const &src_accum0,
       FragmentC const &src_accum1
     ) {
 
     //
     // Prologue
@@ -399,46 +382,46 @@
       iterator_B1.set_iteration_index(0);
       this->smem_iterator_B0_.set_iteration_index(0);
       this->smem_iterator_B1_.set_iteration_index(0);
 
       // Async Copy for operand B0
       CUTLASS_PRAGMA_UNROLL
       for (int j = 0; j < Detail::AsyncCopyIterationsPerStageB; ++j) {
-        typename IteratorB0::AccessType *dst_ptr =
-            reinterpret_cast<typename IteratorB0::AccessType *>(
+        typename IteratorB::AccessType *dst_ptr =
+            reinterpret_cast<typename IteratorB::AccessType *>(
                 this->smem_iterator_B0_.get());
 
         CUTLASS_PRAGMA_UNROLL
-        for (int v = 0; v < IteratorB0::kAccessesPerVector; ++v) {
+        for (int v = 0; v < IteratorB::kAccessesPerVector; ++v) {
           int const kSrcBytes =
-              sizeof_bits<typename IteratorB0::Element>::value *
-              IteratorB0::ThreadMap::kElementsPerAccess /
-              IteratorB0::kAccessesPerVector / 8;
+              sizeof_bits<typename IteratorB::Element>::value *
+              IteratorB::ThreadMap::kElementsPerAccess /
+              IteratorB::kAccessesPerVector / 8;
 
           cutlass::arch::cp_async_zfill<kSrcBytes, kCacheOpB>(
               dst_ptr + v, iterator_B0.get(), iterator_B0.valid());
 
           ++iterator_B0;
         }
 
         ++this->smem_iterator_B0_;
       }
       // Async Copy for operand B1
       CUTLASS_PRAGMA_UNROLL
       for (int j = 0; j < Detail::AsyncCopyIterationsPerStageB; ++j) {
-        typename IteratorB1::AccessType *dst_ptr =
-            reinterpret_cast<typename IteratorB1::AccessType *>(
+        typename IteratorB::AccessType *dst_ptr =
+            reinterpret_cast<typename IteratorB::AccessType *>(
                 this->smem_iterator_B1_.get());
 
         CUTLASS_PRAGMA_UNROLL
-        for (int v = 0; v < IteratorB1::kAccessesPerVector; ++v) {
+        for (int v = 0; v < IteratorB::kAccessesPerVector; ++v) {
           int const kSrcBytes =
-              sizeof_bits<typename IteratorB1::Element>::value *
-              IteratorB1::ThreadMap::kElementsPerAccess /
-              IteratorB1::kAccessesPerVector / 8;
+              sizeof_bits<typename IteratorB::Element>::value *
+              IteratorB::ThreadMap::kElementsPerAccess /
+              IteratorB::kAccessesPerVector / 8;
 
           cutlass::arch::cp_async_zfill<kSrcBytes, kCacheOpB>(
               dst_ptr + v, iterator_B1.get(), iterator_B1.valid());
 
           ++iterator_B1;
         }
 
@@ -486,43 +469,43 @@
                 last_smem_iterator_A.get());
 
         *dst_ptr = zero_A;
 
         ++last_smem_iterator_A;
       }
 
-      typename IteratorB0::AccessType zero_B;
+      typename IteratorB::AccessType zero_B;
       zero_B.clear();
 
       /// Iterator to write threadblock-scoped tile of B0 operand to shared memory
-      SmemIteratorB0 last_smem_iterator_B0(this->smem_iterator_B0_);
+      SmemIteratorB last_smem_iterator_B0(this->smem_iterator_B0_);
       last_smem_iterator_B0.set_iteration_index(0);
 
-      // Async Copy for operand B0
+      // Async Copy for operand B
       CUTLASS_PRAGMA_UNROLL
       for (int j = 0; j < Detail::AsyncCopyIterationsPerStageB; ++j) {
-        typename IteratorB0::AccessType *dst_ptr =
-            reinterpret_cast<typename IteratorB0::AccessType *>(
+
+        typename IteratorB::AccessType *dst_ptr =
+            reinterpret_cast<typename IteratorB::AccessType *>(
                 last_smem_iterator_B0.get());
 
         *dst_ptr = zero_B;
 
         ++last_smem_iterator_B0;
       }
-
       /// Iterator to write threadblock-scoped tile of B1 operand to shared memory
-      SmemIteratorB1 last_smem_iterator_B1(this->smem_iterator_B1_);
+      SmemIteratorB last_smem_iterator_B1(this->smem_iterator_B1_);
       last_smem_iterator_B1.set_iteration_index(0);
 
-      // Async Copy for operand B1
+      // Async Copy for operand B
       CUTLASS_PRAGMA_UNROLL
       for (int j = 0; j < Detail::AsyncCopyIterationsPerStageB; ++j) {
 
-        typename IteratorB1::AccessType *dst_ptr =
-            reinterpret_cast<typename IteratorB1::AccessType *>(
+        typename IteratorB::AccessType *dst_ptr =
+            reinterpret_cast<typename IteratorB::AccessType *>(
                 last_smem_iterator_B1.get());
 
         *dst_ptr = zero_B;
 
         ++last_smem_iterator_B1;
       }
     }
@@ -530,22 +513,21 @@
     // Waits until stages up to the previous (kStages-2)th stage have committed.
     cutlass::arch::cp_async_wait<Base::kStages - 2>();
     __syncthreads();
 
     // Pair of fragments used to overlap shared memory loads and math
     // instructions
     WarpLoadedFragmentA warp_loaded_frag_A[2];
-    WarpLoadedFragmentB0 warp_loaded_frag_B0[2];
-    WarpLoadedFragmentB1 warp_loaded_frag_B1[2];
+    WarpLoadedFragmentB warp_loaded_frag_B0[2];
+    WarpLoadedFragmentB warp_loaded_frag_B1[2];
     WarpTransformedFragmentA warp_transformed_frag_A[2];
-    WarpTransformedFragmentB0 warp_transformed_frag_B0[2];
-    WarpTransformedFragmentB1 warp_transformed_frag_B1[2];
+    WarpTransformedFragmentB warp_transformed_frag_B0[2];
+    WarpTransformedFragmentB warp_transformed_frag_B1[2];
 
-    Operator0 warp_mma0;
-    Operator1 warp_mma1;
+    Operator warp_mma;
 
     this->warp_tile_iterator_A_.set_kgroup_index(0);
     this->warp_tile_iterator_B0_.set_kgroup_index(0);
     this->warp_tile_iterator_B1_.set_kgroup_index(0);
 
     this->warp_tile_iterator_A_.load(warp_loaded_frag_A[0]);
     this->warp_tile_iterator_B0_.load(warp_loaded_frag_B0[0]);
@@ -558,29 +540,29 @@
     iterator_A.clear_mask(gemm_k_iterations == 0);
     iterator_B0.clear_mask(gemm_k_iterations == 0);
     iterator_B1.clear_mask(gemm_k_iterations == 0);
 
     int smem_write_stage_idx = Base::kStages - 1;
     int smem_read_stage_idx = 0;
 
-    warp_mma0.transform(warp_transformed_frag_A[0], warp_transformed_frag_B0[0],
-                        warp_loaded_frag_A[0], warp_loaded_frag_B0[0]);
-    warp_mma1.transform(warp_transformed_frag_A[0], warp_transformed_frag_B1[0],
-                        warp_loaded_frag_A[0], warp_loaded_frag_B1[0]);
+    warp_mma.transform(warp_transformed_frag_A[0], warp_transformed_frag_B0[0],
+                       warp_loaded_frag_A[0], warp_loaded_frag_B0[0]);
+    warp_mma.transform(warp_transformed_frag_A[0], warp_transformed_frag_B1[0],
+                       warp_loaded_frag_A[0], warp_loaded_frag_B1[0]);
 
     // tf32x3 kernels use staging accumulation. warp_mma uses a temporary
     // accumulator and this temporary accumulator is added to the final
     // accumulator once in every mainloop iteration.
     plus<FragmentC> plus_accum;
 
     FragmentC tmp_accum0, tmp_accum1;
 
-    if (platform::is_same<typename Operator0::MathOperator,
+    if (platform::is_same<typename Operator::MathOperator,
                           arch::OpMultiplyAddFastF32>::value
-      || platform::is_same<typename Operator0::MathOperator,
+      || platform::is_same<typename Operator::MathOperator,
                            arch::OpMultiplyAddComplexFastF32>::value) {
 
       tmp_accum0.clear();
       tmp_accum1.clear();
     }
 
     //
@@ -611,56 +593,56 @@
         this->warp_tile_iterator_B1_.load(warp_loaded_frag_B1[(warp_mma_k + 1) % 2]);
 
         ++this->warp_tile_iterator_A_;
         ++this->warp_tile_iterator_B0_;
         ++this->warp_tile_iterator_B1_;
 
         if (warp_mma_k > 0) {
-          warp_mma0.transform(warp_transformed_frag_A[warp_mma_k % 2],
-                              warp_transformed_frag_B0[warp_mma_k % 2],
-                              warp_loaded_frag_A[warp_mma_k % 2],
-                              warp_loaded_frag_B0[warp_mma_k % 2]);
-          warp_mma1.transform(warp_transformed_frag_A[warp_mma_k % 2],
-                              warp_transformed_frag_B1[warp_mma_k % 2],
-                              warp_loaded_frag_A[warp_mma_k % 2],
-                              warp_loaded_frag_B1[warp_mma_k % 2]);
+          warp_mma.transform(warp_transformed_frag_A[warp_mma_k % 2],
+                             warp_transformed_frag_B0[warp_mma_k % 2],
+                             warp_loaded_frag_A[warp_mma_k % 2],
+                             warp_loaded_frag_B0[warp_mma_k % 2]);
+          warp_mma.transform(warp_transformed_frag_A[warp_mma_k % 2],
+                             warp_transformed_frag_B1[warp_mma_k % 2],
+                             warp_loaded_frag_A[warp_mma_k % 2],
+                             warp_loaded_frag_B1[warp_mma_k % 2]);
         }
 
-        if (platform::is_same<typename Operator0::MathOperator,
+        if (platform::is_same<typename Operator::MathOperator,
                               arch::OpMultiplyAddFastF32>::value
-          || platform::is_same<typename Operator0::MathOperator,
+          || platform::is_same<typename Operator::MathOperator,
                                arch::OpMultiplyAddComplexFastF32>::value) {
 
-          warp_mma0(
+          warp_mma(
             tmp_accum0,
             warp_transformed_frag_A[warp_mma_k % 2],
             warp_transformed_frag_B0[warp_mma_k % 2], 
             tmp_accum0
           );
-          warp_mma1(
+          warp_mma(
             tmp_accum1,
             warp_transformed_frag_A[warp_mma_k % 2],
             warp_transformed_frag_B1[warp_mma_k % 2], 
             tmp_accum1
           );
 
           if (warp_mma_k == 0) {
             accum0 = plus_accum(accum0, tmp_accum0);
             accum1 = plus_accum(accum1, tmp_accum1);
             tmp_accum0.clear();
             tmp_accum1.clear();
           }
         } else {
-          warp_mma0(
+          warp_mma(
             accum0,
             warp_transformed_frag_A[warp_mma_k % 2],
             warp_transformed_frag_B0[warp_mma_k % 2],
             accum0
           );
-          warp_mma1(
+          warp_mma(
             accum1,
             warp_transformed_frag_A[warp_mma_k % 2],
             warp_transformed_frag_B1[warp_mma_k % 2],
             accum1
           );
         }
 
@@ -710,22 +692,22 @@
             smem_write_stage_idx = 0;
           } else {
             ++smem_write_stage_idx;
           }
 
           if (smem_read_stage_idx == (Base::kStages - 1)) {
             this->warp_tile_iterator_A_.add_tile_offset(
-                {0, -Base::kStages * Policy0::kPartitionsK *
+                {0, -Base::kStages * Policy::kPartitionsK *
                         Base::kWarpGemmIterations});
             this->warp_tile_iterator_B0_.add_tile_offset(
-                {-Base::kStages * Policy0::kPartitionsK *
+                {-Base::kStages * Policy::kPartitionsK *
                      Base::kWarpGemmIterations,
                  0});
             this->warp_tile_iterator_B1_.add_tile_offset(
-                {-Base::kStages * Policy1::kPartitionsK *
+                {-Base::kStages * Policy::kPartitionsK *
                      Base::kWarpGemmIterations,
                  0});
             smem_read_stage_idx = 0;
           } else {
             ++smem_read_stage_idx;
           }
 
@@ -734,30 +716,30 @@
           iterator_B0.clear_mask(gemm_k_iterations == 0);
           iterator_B1.clear_mask(gemm_k_iterations == 0);
         }
 
         // Do any conversions feeding the first stage at the end of the loop so
         // we can start right away on mma instructions
         if (warp_mma_k + 1 == Base::kWarpGemmIterations) {
-          warp_mma0.transform(warp_transformed_frag_A[(warp_mma_k + 1) % 2],
-                              warp_transformed_frag_B0[(warp_mma_k + 1) % 2],
-                              warp_loaded_frag_A[(warp_mma_k + 1) % 2],
-                              warp_loaded_frag_B0[(warp_mma_k + 1) % 2]);
-          warp_mma1.transform(warp_transformed_frag_A[(warp_mma_k + 1) % 2],
-                              warp_transformed_frag_B1[(warp_mma_k + 1) % 2],
-                              warp_loaded_frag_A[(warp_mma_k + 1) % 2],
-                              warp_loaded_frag_B1[(warp_mma_k + 1) % 2]);
+          warp_mma.transform(warp_transformed_frag_A[(warp_mma_k + 1) % 2],
+                             warp_transformed_frag_B0[(warp_mma_k + 1) % 2],
+                             warp_loaded_frag_A[(warp_mma_k + 1) % 2],
+                             warp_loaded_frag_B0[(warp_mma_k + 1) % 2]);
+          warp_mma.transform(warp_transformed_frag_A[(warp_mma_k + 1) % 2],
+                             warp_transformed_frag_B1[(warp_mma_k + 1) % 2],
+                             warp_loaded_frag_A[(warp_mma_k + 1) % 2],
+                             warp_loaded_frag_B1[(warp_mma_k + 1) % 2]);
         }
       }
 
     }
 
-    if (platform::is_same<typename Operator0::MathOperator,
+    if (platform::is_same<typename Operator::MathOperator,
                           arch::OpMultiplyAddFastF32>::value
-      || platform::is_same<typename Operator0::MathOperator,
+      || platform::is_same<typename Operator::MathOperator,
                            arch::OpMultiplyAddComplexFastF32>::value) {
       accum0 = plus_accum(accum0, tmp_accum0); 
       accum1 = plus_accum(accum1, tmp_accum1); 
     }
  
     if (SharedMemoryClear == SharedMemoryClearOption::kZfill) {
       // commit and drain all pending and predicated cp.async pnz from the GEMM mainloop
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/46_depthwise_simt_conv2dfprop/depthwise_simt_conv2dfprop.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/examples/46_depthwise_simt_conv2dfprop/depthwise_simt_conv2dfprop.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/47_ampere_gemm_universal_streamk/ampere_gemm_universal_streamk.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/examples/47_ampere_gemm_universal_streamk/ampere_gemm_universal_streamk.cu`

 * *Files 0% similar despite different names*

```diff
@@ -30,15 +30,15 @@
  **************************************************************************************************/
 
 /***************************************************************************************************
  Example contrasting the Stream-K parallel decomposition for GEMM threadblocks versus the
  "classic data-parallel" and "Split-K" decompositions.
 
  For more details regarding the Stream-K method, see "Stream-K: Work-centric Parallel Decomposition
- for Dense Matrix-Matrix Multiplication on the GPU" (https://arxiv.org/abs/2301.03598)
+ for Dense Matrix-Matrix Multiplication on the GPU" (https://arxiv.org/abs/2301.03598) 
 
  Requires NVIDIA Ampere or newer device (SM80+).
 
  - To lock persistence mode, power (400W), clocks (1005MHz) for evaluation (assumes device 0 and A100)
 
      cutlass$ sudo nvidia-smi -pm 1 -i 0
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/48_hopper_warp_specialized_gemm/48_hopper_warp_specialized_gemm.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/examples/48_hopper_warp_specialized_gemm/48_hopper_warp_specialized_gemm.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/49_hopper_gemm_schedules_with_collective_builder/49_hopper_gemm_schedules_with_collective_builder.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/examples/49_hopper_gemm_schedules_with_collective_builder/49_hopper_gemm_schedules_with_collective_builder.cu`

 * *Files 1% similar despite different names*

```diff
@@ -29,52 +29,52 @@
  *
  **************************************************************************************************/
 
 /*! \file
     \brief Hopper GEMM example leveraging collective operation builders.
 
     This example showcases the use of CUTLASS's CollectiveBuilder to easily construct performant kernels
-    targeting the NVIDIA Hopper architecture.
+    targetting the NVIDIA Hopper architecture.
 
     Background and motivation
     -------------------------
     CUTLASS kernels are highly parameterizable via template parameters. To ease the selection of template
     parameters, CUTLASS 2 leveraged DefaultGemmConfigurations. Given a small set of parameters, such as
     the data types of operands and the compute capability of the GPU, DefaultGemmConfigurations defined sensible
     defaults for the many other parameters to the kernel (e.g., warp shape, stage count).
 
     However, DefaultGemmConfigurations leave multiple opportunities for improvement, which are addressed
     in CUTLASS 3:
       (1) DefaultGemmConfigurations do not allow one to use a more-performant set of parameters without
-          specifying every parameter. For example, the DefaultGemmConfigurations for GEMMs targeting
+          specifying every parameter. For example, the DefaultGemmConfigurations for GEMMs targetting
           Ampere specify that three pipeline stages should be used regardless of the sizes of operands.
           If one wished to increase this value, one would also need to specify all other template parameters.
           This leaves a gap between a high-level ease-of-use interface and a lower-level detailed interface.
       (2) A new DefaultGemmConfiguration was required for each combination of operand types, GPU architecture,
           and operation type (e.g., Tensor Core or SIMT). This led to increased code size to cover each unique
           configuration and a lack of extensibility from one DefaultGemmConfiguration to another.
 
     Alongside these opportunities for improvement, the Hopper architecture offers new features that increase
     the number of valid configurations of a kernel. In addition to the many template parameters already available
-    in CUTLASS 2 kernels, CUTLASS 3 kernels targeting Hopper also have various scheduling modes to select from that control:
+    in CUTLASS 2 kernels, CUTLASS 3 kernels targetting Hopper also have various scheduling modes to select from that control:
       (1) how data is to be loaded (e.g., using the Hopper TMA feature or Ampere cp.async)
       (2) how work is to be divided among warps in a thread block (e.g., whether to use "warp specialization")
       (3) whether persistent thread blocks should be used
     This increased configuration space further motivates rethinking DefaultGemmConfigurations.
 
     Introduction to the CollectiveBuilder
     -------------------------------------
     CUTLASS 3 introduces the CollectiveBuilder to further ease the process of selecting template parameters
-    for kernels targeting Hopper. Similar to the DefaultGemmConfigurations used in CUTLASS 2, the CollectiveBuilder
+    for kernels targetting Hopper. Similar to the DefaultGemmConfigurations used in CUTLASS 2, the CollectiveBuilder
     takes in a small set of template parameters (e.g., the data types of operands A and B). It then automatically
     determines the data loading strategy to use depending on whether the Hopper TMA feature can be used with the provided
     parameters. If one does not indicate a particular scheduling policy or stage count to use (by using `Auto` template
     parameters), the CollectiveBuilder will also automatically select these.
 
-    Unlike DefaultGemmConfigurations a partial specialization of the CollectiveBuilder is not needed for many
+    Unlike DefaultGemmConfigurations a parital specialization of the CollectiveBuilder is not needed for many
     configurations of operand types. Instead the CollectiveBuilder "builds" a configuration based on generic
     properties of the specified operands, layouts, and other parameters. For example, when the stage count
     is set to `Auto`, the CollectiveBuilder may automatically calculate the maximum number of stages that
     will fit in shared memory given the types of operands and the thread block shape, rather than simply using
     a single default value.
 
     Note that one does not need to use the CollectiveBuilder to declare CUTLASS 3 kernels; one can still provide
@@ -86,15 +86,15 @@
     a guarantee. Furthermore, the behavior of the CollectiveBuilder when `Auto` parameters are provided is subject
     to change in future CUTLASS releases -- do not rely on `Auto` if you require a specific scheduling policy and/or
     stage count to be used.
 
     Details of this example
     -----------------------
     This example walks through the use of the CollectiveBuilder with various schedules and stage counts specified.
-    This example also illustrates how CUTLASS 3 GEMMs targeting Hopper automatically support batched GEMMs by simply
+    This example also illustrates how CUTLASS 3 GEMMs targetting Hopper automatically support batched GEMMs by simply
     extending the problem size with an additional tensor rank.
 
     Example usage:
       $ ./examples/49_hopper_gemm_schedules_with_collective_builder/49_hopper_gemm_schedules_with_collective_builder \
             --m=2048 --n=2048 --k=2048 --l=2
 */
 
@@ -158,15 +158,15 @@
   }
 
   /// Prints the usage statement.
   std::ostream & print_usage(std::ostream &out) const {
 
     out << "49_hopper_gemm_schedules_with_collective_builder\n\n"
       << "  This example showcases the use of CUTLASS's collective operation builders to easily construct\n"
-      << "  performant kernels targeting NVIDIA's Hopper architecture.\n\n"
+      << "  performant kernels targetting NVIDIA's Hopper architecture.\n\n"
       << "Options:\n\n"
       << "  --help                      If specified, displays this usage statement\n\n"
       << "  --m=<int>                   Sets the M extent of the GEMM\n"
       << "  --n=<int>                   Sets the N extent of the GEMM\n"
       << "  --k=<int>                   Sets the K extent of the GEMM\n"
       << "  --l=<int>                   Sets the L extent (batch count) of the GEMM\n"
       << "  --alpha=<f32>               Epilogue scalar alpha\n"
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/50_hopper_gemm_with_epilogue_swizzle/50_hopper_gemm_with_epilogue_swizzle.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/examples/50_hopper_gemm_with_epilogue_swizzle/50_hopper_gemm_with_epilogue_swizzle.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/60_cutlass_import/main.cpp` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/examples/60_cutlass_import/main.cpp`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/cute/tutorial/sgemm_nt_1.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/examples/cute/tutorial/sgemm_nt_1.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/aligned_buffer.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/aligned_buffer.h`

 * *Files 2% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -76,17 +76,17 @@
 
   typedef T value_type;
   typedef size_t size_type;
   typedef ptrdiff_t difference_type;
   typedef value_type *pointer;
   typedef value_type const * const_pointer;
 
-  using ArrayType = Array<T, N>;
-  using reference = typename ArrayType::reference;
-  using const_reference = typename ArrayType::const_reference;
+  using Array = Array<T, N>;
+  using reference = typename Array::reference;
+  using const_reference = typename Array::const_reference;
 
 public:
 
   CUTLASS_HOST_DEVICE
   pointer data() {
     return reinterpret_cast<pointer>(storage); 
   }
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/arch/arch.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/arch/arch.h`

 * *Files 8% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -30,14 +30,16 @@
  **************************************************************************************************/
 /*! \file
     \brief Defines tags for architecture-specific configurations.
 */
 
 #pragma once
 
+#include "cutlass/cutlass.h"
+
 ////////////////////////////////////////////////////////////////////////////////////////////////////
 
 namespace cutlass {
 namespace arch {
 
 #if defined(__NVCC__) || (defined(__clang__) && defined(__CUDA__))
 
@@ -81,14 +83,18 @@
 struct Sm80 {
   static int const kMinComputeCapability = 80; 
 };
 struct Sm86 {
   static int const kMinComputeCapability = 86;
 };
 
+struct Sm90 {
+  static int const kMinComputeCapability = 90; 
+};
+
 /// Triggers a breakpoint on the device
 CUTLASS_DEVICE
 void device_breakpoint() {
 #if defined(__CUDA_ARCH__)
   asm volatile ("  brkpt;\n");
 #endif
 }
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/arch/barrier.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/arch/barrier.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/arch/cache_operation.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/arch/cache_operation.h`

 * *Files 9% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/arch/memory.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/arch/memory.h`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -447,15 +447,15 @@
 }
 
 /// st.shared - 128b
 template <>
 CUTLASS_DEVICE
 void shared_store<16>(uint32_t ptr, void const *src) {
   uint4 const *dst_u128 = reinterpret_cast<uint4 const *>(src);
-  asm volatile("ld.shared.v4.u32 [%0], {%1, %2, %3, %4};\n"
+  asm volatile("st.shared.v4.u32 [%0], {%1, %2, %3, %4};\n"
     : :
       "r"(ptr),
       "r"(dst_u128->x),
       "r"(dst_u128->y),
       "r"(dst_u128->z),
       "r"(dst_u128->w)
     );
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/arch/memory_sm75.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/arch/memory_sm75.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/arch/memory_sm80.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/arch/memory_sm80.h`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -32,14 +32,15 @@
 /*! \file
     \brief Architecture-specific operators on memory added for SM80
 */
 
 #pragma once
 
 #include "cutlass/cutlass.h"
+#include "cutlass/complex.h"
 #include "cutlass/arch/memory.h"
 #include "cutlass/arch/memory_sm75.h"
 #include "cutlass/arch/cache_operation.h"
 
 #if defined(__CUDA_ARCH__) && (__CUDA_ARCH__ >= 800)
   #define CUDA_CP_ASYNC_ACTIVATED 1
 #else
@@ -49,51 +50,51 @@
 namespace cutlass {
 namespace arch {
 
 ////////////////////////////////////////////////////////////////////////////////////////////////////
 
 /// Initiates an asynchronous copy from global memory to shared memory.
 ///
-/// LDGSTS
+/// cp.async
 ///
 template <
     /// Size of the access in bytes
     int SizeInBytes,
     /// Cache operation
     CacheOperation::Kind cache_op = CacheOperation::Always>
 struct cp_async;
 
 /// Initiates an asynchronous copy from global memory to shared memory. Rather than predicate
 /// the entire transfer, zeros are written to SMEM if the guard predicate is false.
 ///
-/// LDGSTS
+/// cp.async
 ///
 template <
     /// Size of the access in bytes
     int SizeInBytes,
     /// Cache operation
     CacheOperation::Kind cache_op = CacheOperation::Always>
 struct cp_async_zfill;
 
 /// Initiates an asynchronous copy from global memory to shared memory. Rather than predicate
 /// the entire transfer, nans (0x7eff) are written to SMEM if the guard predicate is false.
 ///
-/// LDGSTS
+/// cp.async
 ///
 template <
     /// Size of the access in bytes
     int SizeInBytes,
     /// Cache operation
     CacheOperation::Kind cache_op = CacheOperation::Always>
 struct cp_async_nan;
 
 /// Either 0 or 1 are written to SMEM based on input element type
 /// Used for diagonal elements of triangular matrix of BLAS3 functions
 ///
-/// STS
+/// st.shared
 ///
 template <
    /// Type of Element
    typename Element,
    /// If the data is for a Hermitian matrix diagonal
    bool IsHermitianData = false>
 struct cp_async_diag;
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/arch/mma.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/arch/mma.h`

 * *Files 4% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -45,69 +45,69 @@
 
 namespace cutlass {
 namespace arch {
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
 /// Tag indicating the operation implied by MMA.
-struct OpMultiplyAdd;
+struct OpMultiplyAdd {};
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
 /// Tag indicating the result is saturated to MAX_FLOAT|MIN_FLOAT or MAX_INT|MIN_INT
-struct OpMultiplyAddSaturate;
+struct OpMultiplyAddSaturate {};
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
 /// Tag indicating the input is converted to a narrower type (BF16)
-struct OpMultiplyAddFastBF16;
+struct OpMultiplyAddFastBF16 {};
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
 /// Tag indicating the input is converted to a narrower type (F16)
-struct OpMultiplyAddFastF16;
+struct OpMultiplyAddFastF16 {};
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
 /// Tag indicating the input is converted to 2 (big and small) TF32 components
 //  Perform 3xTF32 or 4xTF32 for every F32 output element
-struct OpMultiplyAddFastF32;
+struct OpMultiplyAddFastF32 {};
 
 /// Tag indicating the input is converted to 2 (big and small) TF32 components
 //  Perform 3xTF32 or 4xTF32 for every complex<F32> output element
-struct OpMultiplyAddComplexFastF32;
+struct OpMultiplyAddComplexFastF32 {};
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
 /// Tag indicating the complex multiply-add operation
-struct OpMultiplyAddComplex;
+struct OpMultiplyAddComplex {};
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
 /// Tag indicating the gaussian complex multiply-add operation
-struct OpMultiplyAddGaussianComplex;
+struct OpMultiplyAddGaussianComplex {};
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
 /// Tag indicating the inner product is defined by (XOR, POPC)
-struct OpXorPopc;
+struct OpXorPopc {};
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
 /// Tag classifying math operators as thread-level operations.
-struct OpClassSimt;
+struct OpClassSimt {};
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
 /// Tag classifing operators as Tensor Core operations.
-struct OpClassTensorOp;
+struct OpClassTensorOp {};
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 /// Tag classifing operators as WMMA Tensor Core operations
-struct OpClassWmmaTensorOp;
+struct OpClassWmmaTensorOp {};
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
 /// Matrix multiply-add operation
 template <
   /// Size of the matrix product (concept: GemmShape)
   typename Shape_,
@@ -219,8 +219,10 @@
 #include "cutlass/arch/mma_sm50.h"
 #include "cutlass/arch/mma_sm60.h"
 #include "cutlass/arch/mma_sm61.h"
 #include "cutlass/arch/mma_sm70.h"
 #include "cutlass/arch/mma_sm75.h"
 #include "cutlass/arch/mma_sm80.h"
 #include "cutlass/arch/mma_sparse_sm80.h"
+#include "cutlass/arch/mma_sm90.h"
+
 /////////////////////////////////////////////////////////////////////////////////////////////////
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/arch/mma_sm50.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/arch/mma_sm50.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/arch/mma_sm60.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/arch/mma_sm60.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/arch/mma_sm61.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/arch/mma_sm61.h`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/arch/mma_sm70.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/arch/mma_sm70.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/arch/mma_sm75.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/arch/mma_sm75.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -1243,15 +1243,16 @@
     FragmentC &d,
     FragmentA const &a,
     FragmentB const &b,
     FragmentC const &c
   ) const {
 
 #if defined(CUTLASS_ARCH_MMA_SM75_ENABLED)
-#if defined(CUTLASS_ARCH_WMMA_ENABLED)
+
+#if (__CUDA_ARCH__ >= 900) || (defined(CUTLASS_ARCH_WMMA_ENABLED))
   using WmmaFragmentA = nvcuda::wmma::fragment<
           nvcuda::wmma::matrix_a,
           Shape::kM,
           Shape::kN,
           Shape::kK,
           nvcuda::wmma::experimental::precision::b1,
           nvcuda::wmma::row_major>;
@@ -1275,32 +1276,26 @@
   WmmaFragmentB const & B = reinterpret_cast<WmmaFragmentB const &>(b);
 
   WmmaFragmentC const & C = reinterpret_cast<WmmaFragmentC const &>(c);
   WmmaFragmentC & D = reinterpret_cast<WmmaFragmentC &>(d);
 
   nvcuda::wmma::bmma_sync(D, A, B, C, nvcuda::wmma::experimental::bmmaBitOpXOR, 
                                           nvcuda::wmma::experimental::bmmaAccumulateOpPOPC);
+
 #else
 
   CUTLASS_UNUSED(a);
   CUTLASS_UNUSED(b);
   CUTLASS_UNUSED(c);
   CUTLASS_UNUSED(d);
   assert(0); // WMMA must be supported to issue binary matrix multiply-accumulate instructions.
 
 #endif // defined(CUTLASS_ARCH_WMMA_ENABLED)
 
-#else
-    CUTLASS_UNUSED(a);
-    CUTLASS_UNUSED(b);
-    CUTLASS_UNUSED(c);
-    CUTLASS_UNUSED(d);
-    assert(0);
 #endif
-
   }
 };
 
 ////////////////////////////////////////////////////////////////////////////////
 
 } // namespace arch
 } // namespace cutlass
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/arch/mma_sm80.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/arch/mma_sm80.h`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -2152,30 +2152,31 @@
 #if defined(CUTLASS_ARCH_MMA_SM80_ENABLED)
 
     uint32_t const *A = reinterpret_cast<uint32_t const *>(&a);
     uint32_t const *B = reinterpret_cast<uint32_t const *>(&b);
 
     int const *C = reinterpret_cast<int const *>(&c);
     int *D = reinterpret_cast<int *>(&d);
+
     asm volatile(
         "mma.sync.aligned.m16n8k256.row.col.s32.b1.b1.s32.xor.popc {%0,%1,%2,%3}, "
         "{%4,%5,%6,%7}, "
         "{%8,%9}, {%10,%11,%12,%13};\n"
         : "=r"(D[0]), "=r"(D[1]), "=r"(D[2]), "=r"(D[3])
         : "r"(A[0]), "r"(A[1]), "r"(A[2]), "r"(A[3]), "r"(B[0]), "r"(B[1]),
           "r"(C[0]), "r"(C[1]), "r"(C[2]), "r"(C[3]));
 
 #else
-    
+
     CUTLASS_UNUSED(a);
     CUTLASS_UNUSED(b);
     CUTLASS_UNUSED(c);
     CUTLASS_UNUSED(d);
     assert(0);
-    
+
 #endif // defined(CUTLASS_ARCH_MMA_SM80_ENABLED)
   }
 };
 
 ////////////////////////////////////////////////////////////////////////////////
 
 } // namespace arch
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/arch/mma_sparse_sm80.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/arch/mma_sparse_sm80.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/arch/reg_reconfig.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/arch/reg_reconfig.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/arch/simd.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/arch/simd.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/arch/simd_sm60.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/arch/simd_sm60.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/arch/simd_sm61.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/arch/simd_sm61.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/arch/wmma.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/arch/wmma.h`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/arch/wmma_sm70.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/arch/wmma_sm70.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/arch/wmma_sm72.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/arch/wmma_sm72.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/arch/wmma_sm75.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/arch/wmma_sm75.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/array.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/array_subbyte.h`

 * *Files 20% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -30,401 +30,462 @@
  **************************************************************************************************/
 /*! \file
     \brief Statically sized array of elements that accommodates all CUTLASS-supported numeric types
            and is safe to use in a union.
 */
 
 #pragma once
+
 #include "cutlass/cutlass.h"
-#include "cutlass/numeric_types.h"
+#include "cutlass/array.h"
+#include "cutlass/platform/platform.h"
 
 namespace cutlass {
 
 ////////////////////////////////////////////////////////////////////////////////////////////////////
 
 /// Statically sized array for any data type
 template <
   typename T,
-  int N,
-  bool RegisterSized = sizeof_bits<T>::value >= 32
->
-class Array;
-
-////////////////////////////////////////////////////////////////////////////////////////////////////
-
-/// Defines the size of an Array<> in bits
-template <typename T, int N, bool RegisterSized>
-struct sizeof_bits<Array<T, N, RegisterSized> > {
-  static int const value =
-    int(sizeof(typename Array<T, N, RegisterSized>::Storage)) * 8 * int(Array<T, N, RegisterSized>::kStorageElements);
-};
-
-////////////////////////////////////////////////////////////////////////////////////////////////////
-
-/// Returns true if the argument is a power of 2
-CUTLASS_HOST_DEVICE
-constexpr bool ispow2(unsigned x) {
-  return x && (!(x & (x - 1)));
-}
-
-////////////////////////////////////////////////////////////////////////////////////////////////////
-
-/// Returns the largest power of two not greater than the argument.
-CUTLASS_HOST_DEVICE
-constexpr unsigned floor_pow_2(unsigned x) {
-  return (x == 0 || ispow2(x)) ? x : ((floor_pow_2(x >> 1)) << 1);
-}
-
-////////////////////////////////////////////////////////////////////////////////////////////////////
-
-/// Statically sized array for any data type
-template <
-  typename T,
   int N
 >
-class Array<T, N, true> {
+class Array<T, N, false> {
 public:
 
+  static int const kSizeBits = sizeof_bits<T>::value * N;
+
   /// Storage type
-  using Storage = T;
+  using Storage = typename platform::conditional<
+    ((kSizeBits % 32) != 0),
+    typename platform::conditional<
+      ((kSizeBits % 16) != 0),
+      uint8_t,
+      uint16_t
+    >::type,
+    uint32_t
+  >::type;
 
   /// Element type
   using Element = T;
 
+  /// Number of logical elements per stored object
+  static int const kElementsPerStoredItem = int(sizeof(Storage) * 8) / sizeof_bits<T>::value;
+
   /// Number of storage elements
-  //static std::size_t const kStorageElements = N;
-  static size_t const kStorageElements = N;
+  static size_t const kStorageElements = N / kElementsPerStoredItem;
 
   /// Number of logical elements
   static size_t const kElements = N;
 
+  /// Bitmask for covering one item
+  static Storage const kMask = ((Storage(1) << sizeof_bits<T>::value) - 1);
+
   //
-  // C++ standard members
+  // C++ standard members with pointer types removed
   //
 
   typedef T value_type;
   typedef size_t size_type;
   typedef ptrdiff_t difference_type;
-  typedef value_type &reference;
-  typedef value_type const & const_reference;
   typedef value_type *pointer;
-  typedef value_type const * const_pointer;
+  typedef value_type const *const_pointer;
 
   //
-  // Iterators
+  // References
   //
 
-  /// Bidirectional iterator over elements
-  class iterator {
+  /// Reference object inserts or extracts sub-byte items
+  class reference {
+    /// Pointer to storage element
+    Storage *ptr_;
 
-    /// Pointer to object
-    T *ptr_;
+    /// Index into elements packed into Storage object
+    int idx_;
 
   public:
 
+    /// Default ctor
     CUTLASS_HOST_DEVICE
-    iterator(): ptr_(nullptr) { }
+    reference(): ptr_(nullptr), idx_(0) { }
 
+    /// Ctor
     CUTLASS_HOST_DEVICE
-    iterator(T *_ptr): ptr_(_ptr) { }
+    reference(Storage *ptr, int idx = 0): ptr_(ptr), idx_(idx) { }
 
+    /// Assignment
     CUTLASS_HOST_DEVICE
-    iterator &operator++() {
-      ++ptr_;
-      return *this;
-    }
+    reference &operator=(T x) {
+      Storage item = (reinterpret_cast<Storage const &>(x) & kMask);
+
+      Storage kUpdateMask = Storage(~(kMask << (idx_ * sizeof_bits<T>::value)));
+      *ptr_ = Storage(((*ptr_ & kUpdateMask) | (item << idx_ * sizeof_bits<T>::value)));
 
-    CUTLASS_HOST_DEVICE
-    iterator &operator--() {
-      --ptr_;
       return *this;
     }
 
     CUTLASS_HOST_DEVICE
-    iterator operator++(int) {
-      iterator ret(*this);
-      ++ptr_;
-      return ret;
+    T get() const {
+      Storage item = Storage((*ptr_ >> (idx_ * sizeof_bits<T>::value)) & kMask);
+      return reinterpret_cast<T const &>(item);
     }
 
+    /// Extract
     CUTLASS_HOST_DEVICE
-    iterator operator--(int) {
-      iterator ret(*this);
-      --ptr_;
-      return ret;
+    operator T() const {
+      return get();
     }
 
+    /// Explicit cast to int
     CUTLASS_HOST_DEVICE
-    T &operator*() const {
-      return *ptr_;
+    explicit operator int() const {
+      return int(get());
     }
 
+    /// Explicit cast to float
     CUTLASS_HOST_DEVICE
-    bool operator==(iterator const &other) const {
-      return ptr_ == other.ptr_;
-    }
-
-    CUTLASS_HOST_DEVICE
-    bool operator!=(iterator const &other) const {
-      return ptr_ != other.ptr_;
+    explicit operator float() const {
+      return float(get());
     }
   };
 
-  /// Bidirectional constant iterator over elements
-  class const_iterator {
+  /// Reference object extracts sub-byte items
+  class const_reference {
 
-    /// Pointer to object
-    const T *ptr_;
-
-  public:
+    /// Pointer to storage element
+    Storage const *ptr_;
 
-    CUTLASS_HOST_DEVICE
-    const_iterator(): ptr_(nullptr) { }
+    /// Index into elements packed into Storage object
+    int idx_;
 
-    CUTLASS_HOST_DEVICE
-    const_iterator(T const *_ptr): ptr_(_ptr) { }
+  public:
 
+    /// Default ctor
     CUTLASS_HOST_DEVICE
-    const_iterator &operator++() {
-      ++ptr_;
-      return *this;
-    }
+    const_reference(): ptr_(nullptr), idx_(0) { }
 
+    /// Ctor
     CUTLASS_HOST_DEVICE
-    const_iterator &operator--() {
-      --ptr_;
-      return *this;
-    }
+    const_reference(Storage const *ptr, int idx = 0): ptr_(ptr), idx_(idx) { }
 
     CUTLASS_HOST_DEVICE
-    const_iterator operator++(int) {
-      const_iterator ret(*this);
-      ++ptr_;
-      return ret;
+    const T get() const {
+      Storage item = (*ptr_ >> (idx_ * sizeof_bits<T>::value)) & kMask;
+      return reinterpret_cast<T const &>(item);
     }
 
+    /// Extract
     CUTLASS_HOST_DEVICE
-    const_iterator operator--(int) {
-      const_iterator ret(*this);
-      --ptr_;
-      return ret;
+    operator T() const {
+      Storage item = Storage(Storage(*ptr_ >> Storage(idx_ * sizeof_bits<T>::value)) & kMask);
+      return reinterpret_cast<T const &>(item);
     }
 
+    /// Explicit cast to int
     CUTLASS_HOST_DEVICE
-    T const &operator*() const {
-      return *ptr_;
+    explicit operator int() const {
+      return int(get());
     }
 
+    /// Explicit cast to float
     CUTLASS_HOST_DEVICE
-    bool operator==(const_iterator const &other) const {
-      return ptr_ == other.ptr_;
-    }
-
-    CUTLASS_HOST_DEVICE
-    bool operator!=(const_iterator const &other) const {
-      return ptr_ != other.ptr_;
+    explicit operator float() const {
+      return float(get());
     }
   };
 
+  //
+  // Iterators
+  //
+
   /// Bidirectional iterator over elements
-  class reverse_iterator {
+  class iterator {
 
-    /// Pointer to object
-    T *ptr_;
+    /// Pointer to storage element
+    Storage *ptr_;
+
+    /// Index into elements packed into Storage object
+    int idx_;
 
   public:
 
     CUTLASS_HOST_DEVICE
-    reverse_iterator(): ptr_(nullptr) { }
+    iterator(): ptr_(nullptr), idx_(0) { }
 
     CUTLASS_HOST_DEVICE
-    reverse_iterator(T *_ptr): ptr_(_ptr) { }
+    iterator(Storage *ptr, int idx = 0): ptr_(ptr), idx_(idx) { }
 
     CUTLASS_HOST_DEVICE
-    reverse_iterator &operator++() {
-      --ptr_;
+    iterator &operator++() {
+      ++idx_;
+      if (idx_ == kElementsPerStoredItem) {
+        ++ptr_;
+        idx_ = 0;
+      }
       return *this;
     }
 
     CUTLASS_HOST_DEVICE
-    reverse_iterator &operator--() {
-      ++ptr_;
+    iterator &operator--() {
+      if (!idx_) {
+        --ptr_;
+        idx_ = kElementsPerStoredItem - 1;
+      }
+      else {
+        --idx_;
+      }
       return *this;
     }
 
     CUTLASS_HOST_DEVICE
-    reverse_iterator operator++(int) {
+    iterator operator++(int) {
       iterator ret(*this);
-      --ptr_;
+      ++idx_;
+      if (idx_ == kElementsPerStoredItem) {
+        ++ptr_;
+        idx_ = 0;
+      }
       return ret;
     }
 
     CUTLASS_HOST_DEVICE
-    reverse_iterator operator--(int) {
+    iterator operator--(int) {
       iterator ret(*this);
-      ++ptr_;
+      if (!idx_) {
+        --ptr_;
+        idx_ = kElementsPerStoredItem - 1;
+      }
+      else {
+        --idx_;
+      }
       return ret;
     }
 
     CUTLASS_HOST_DEVICE
-    T &operator*() const {
-      return *(ptr_ - 1);
+    reference operator*() const {
+      return reference(ptr_, idx_);
     }
 
     CUTLASS_HOST_DEVICE
-    bool operator==(reverse_iterator const &other) const {
-      return ptr_ == other.ptr_;
+    bool operator==(iterator const &other) const {
+      return ptr_ == other.ptr_ && idx_ == other.idx_;
     }
 
     CUTLASS_HOST_DEVICE
-    bool operator!=(reverse_iterator const &other) const {
-      return ptr_ != other.ptr_;
+    bool operator!=(iterator const &other) const {
+      return !(*this == other);
     }
   };
 
   /// Bidirectional constant iterator over elements
-  class const_reverse_iterator {
+  class const_iterator {
+
+    /// Pointer to storage element
+    Storage const *ptr_;
 
-    /// Pointer to object
-    T const *ptr_;
+    /// Index into elements packed into Storage object
+    int idx_;
 
   public:
 
     CUTLASS_HOST_DEVICE
-    const_reverse_iterator(): ptr_(nullptr) { }
+    const_iterator(): ptr_(nullptr), idx_(0) { }
 
     CUTLASS_HOST_DEVICE
-    const_reverse_iterator(T const *_ptr): ptr_(_ptr) { }
+    const_iterator(Storage const *ptr, int idx = 0): ptr_(ptr), idx_(idx) { }
 
     CUTLASS_HOST_DEVICE
-    const_reverse_iterator &operator++() {
-      --ptr_;
+    iterator &operator++() {
+      ++idx_;
+      if (idx_ == kElementsPerStoredItem) {
+        ++ptr_;
+        idx_ = 0;
+      }
       return *this;
     }
 
     CUTLASS_HOST_DEVICE
-    const_reverse_iterator &operator--() {
-      ++ptr_;
+    iterator &operator--() {
+      if (!idx_) {
+        --ptr_;
+        idx_ = kElementsPerStoredItem - 1;
+      }
+      else {
+        --idx_;
+      }
       return *this;
     }
 
     CUTLASS_HOST_DEVICE
-    const_reverse_iterator operator++(int) {
-      const_reverse_iterator ret(*this);
-      --ptr_;
+    iterator operator++(int) {
+      iterator ret(*this);
+      ++idx_;
+      if (idx_ == kElementsPerStoredItem) {
+        ++ptr_;
+        idx_ = 0;
+      }
       return ret;
     }
 
     CUTLASS_HOST_DEVICE
-    const_reverse_iterator operator--(int) {
-      const_reverse_iterator ret(*this);
-      ++ptr_;
+    iterator operator--(int) {
+      iterator ret(*this);
+      if (!idx_) {
+        --ptr_;
+        idx_ = kElementsPerStoredItem - 1;
+      }
+      else {
+        --idx_;
+      }
       return ret;
     }
 
     CUTLASS_HOST_DEVICE
-    T const &operator*() const {
-      return *(ptr_ - 1);
+    const_reference operator*() const {
+      return const_reference(ptr_, idx_);
     }
 
     CUTLASS_HOST_DEVICE
-    bool operator==(const_iterator const &other) const {
-      return ptr_ == other.ptr_;
+    bool operator==(iterator const &other) const {
+      return ptr_ == other.ptr_ && idx_ == other.idx_;
     }
 
     CUTLASS_HOST_DEVICE
-    bool operator!=(const_iterator const &other) const {
-      return ptr_ != other.ptr_;
+    bool operator!=(iterator const &other) const {
+      return !(*this == other);
     }
   };
 
+  /// Bidirectional iterator over elements
+  class reverse_iterator {
+
+    /// Pointer to storage element
+    Storage *ptr_;
+
+    /// Index into elements packed into Storage object
+    int idx_;
+
+  public:
+
+    CUTLASS_HOST_DEVICE
+    reverse_iterator(): ptr_(nullptr), idx_(0) { }
+
+    CUTLASS_HOST_DEVICE
+    reverse_iterator(Storage *ptr, int idx = 0): ptr_(ptr), idx_(idx) { }
+
+    // TODO
+  };
+
+  /// Bidirectional constant iterator over elements
+  class const_reverse_iterator {
+
+    /// Pointer to storage element
+    Storage const *ptr_;
+
+    /// Index into elements packed into Storage object
+    int idx_;
+
+  public:
+
+    CUTLASS_HOST_DEVICE
+    const_reverse_iterator(): ptr_(nullptr), idx_(0) { }
+
+    CUTLASS_HOST_DEVICE
+    const_reverse_iterator(Storage const *ptr, int idx = 0): ptr_(ptr), idx_(idx) { }
+
+    // TODO
+  };
+
 private:
 
   /// Internal storage
-  Storage storage[kElements];
+  Storage storage[kStorageElements];
 
 public:
 
   #if 0
   CUTLASS_HOST_DEVICE
   Array() { }
 
   CUTLASS_HOST_DEVICE
   Array(Array const &x) {
     CUTLASS_PRAGMA_UNROLL
-    for (int i = 0; i < kElements; ++i) {
+    for (int i = 0; i < int(kStorageElements); ++i) {
       storage[i] = x.storage[i];
     }
   }
   #endif
 
   /// Efficient clear method
   CUTLASS_HOST_DEVICE
   void clear() {
-    fill(T(0));
+
+    CUTLASS_PRAGMA_UNROLL
+    for (int i = 0; i < int(kStorageElements); ++i) {
+      storage[i] = Storage(0);
+    }
   }
 
   CUTLASS_HOST_DEVICE
   reference at(size_type pos) {
-    return reinterpret_cast<reference>(storage[pos]);
+    return reference(storage + pos / kElementsPerStoredItem, pos % kElementsPerStoredItem);
   }
 
   CUTLASS_HOST_DEVICE
   const_reference at(size_type pos) const {
-    return reinterpret_cast<const_reference>(storage[pos]);
+    return const_reference(storage + pos / kElementsPerStoredItem, pos % kElementsPerStoredItem);
   }
 
   CUTLASS_HOST_DEVICE
   reference operator[](size_type pos) {
-    return reinterpret_cast<reference>(storage[pos]);
+    return at(pos);
   }
 
   CUTLASS_HOST_DEVICE
   const_reference operator[](size_type pos) const {
-    return reinterpret_cast<const_reference>(storage[pos]);
+    return at(pos);
   }
 
   CUTLASS_HOST_DEVICE
   reference front() {
-    return reinterpret_cast<reference>(storage[0]);
+    return at(0);
   }
 
   CUTLASS_HOST_DEVICE
   const_reference front() const {
-    return reinterpret_cast<const_reference>(storage[0]);
+    return at(0);
   }
 
   CUTLASS_HOST_DEVICE
   reference back() {
-    return reinterpret_cast<reference>(storage[kStorageElements - 1]);
+    return reference(storage + kStorageElements - 1, kElementsPerStoredItem - 1);
   }
 
   CUTLASS_HOST_DEVICE
   const_reference back() const {
-    return reinterpret_cast<const_reference>(storage[kStorageElements - 1]);
+    return const_reference(storage + kStorageElements - 1, kElementsPerStoredItem - 1);
   }
 
   CUTLASS_HOST_DEVICE
   pointer data() {
     return reinterpret_cast<pointer>(storage);
   }
 
   CUTLASS_HOST_DEVICE
   const_pointer data() const {
     return reinterpret_cast<const_pointer>(storage);
   }
   
   CUTLASS_HOST_DEVICE
-  pointer raw_data() {
-    return reinterpret_cast<pointer>(storage);
+  Storage * raw_data() {
+    return storage;
   }
 
   CUTLASS_HOST_DEVICE
-  const_pointer raw_data() const {
-    return reinterpret_cast<const_pointer>(storage);
+  Storage const * raw_data() const {
+    return storage;
   }
 
 
   CUTLASS_HOST_DEVICE
   constexpr bool empty() const {
     return !kElements;
   }
@@ -437,17 +498,24 @@
   CUTLASS_HOST_DEVICE
   constexpr size_type max_size() const {
     return kElements;
   }
 
   CUTLASS_HOST_DEVICE
   void fill(T const &value) {
+
+    CUTLASS_PRAGMA_UNROLL
+    for (int i = 0; i < kElementsPerStoredItem; ++i) {
+      reference ref(storage, i);
+      ref = value;
+    }
+
     CUTLASS_PRAGMA_UNROLL
-    for (int i = 0; i < kElements; ++i) {
-      storage[i] = static_cast<Storage>(value);
+    for (int i = 1; i < kStorageElements; ++i) {
+      storage[i] = storage[0];
     }
   }
 
   CUTLASS_HOST_DEVICE
   iterator begin() {
     return iterator(storage);
   }
@@ -455,115 +523,46 @@
   CUTLASS_HOST_DEVICE
   const_iterator cbegin() const {
     return const_iterator(storage);
   }
 
   CUTLASS_HOST_DEVICE
   iterator end() {
-    return iterator(reinterpret_cast<pointer>(storage + kStorageElements));
+    return iterator(storage + kStorageElements);
   }
 
   CUTLASS_HOST_DEVICE
   const_iterator cend() const {
-    return const_iterator(reinterpret_cast<const_pointer>(storage + kStorageElements));
+    return const_iterator(storage + kStorageElements);
   }
 
   CUTLASS_HOST_DEVICE
   reverse_iterator rbegin() {
-    return reverse_iterator(reinterpret_cast<pointer>(storage + kStorageElements));
+    return reverse_iterator(storage + kStorageElements);
   }
 
   CUTLASS_HOST_DEVICE
   const_reverse_iterator crbegin() const {
-    return const_reverse_iterator(reinterpret_cast<const_pointer>(storage + kStorageElements));
+    return const_reverse_iterator(storage + kStorageElements);
   }
 
   CUTLASS_HOST_DEVICE
   reverse_iterator rend() {
-    return reverse_iterator(reinterpret_cast<pointer>(storage));
+    return reverse_iterator(storage);
   }
 
   CUTLASS_HOST_DEVICE
   const_reverse_iterator crend() const {
-    return const_reverse_iterator(reinterpret_cast<const_pointer>(storage));
+    return const_reverse_iterator(storage);
   }
 
   //
   // Comparison operators
   //
 
 };
 
 ////////////////////////////////////////////////////////////////////////////////////////////////////
 
-template <typename Element>
-CUTLASS_HOST_DEVICE
-Array<Element, 1> make_Array(Element x) {
-  Array<Element, 1> m;
-  m[0] = x;
-  return m;
-}
-
-template <typename Element>
-CUTLASS_HOST_DEVICE
-Array<Element, 2> make_Array(Element x, Element y) {
-  Array<Element, 2> m;
-  m[0] = x;
-  m[1] = y;
-  return m;
-}
-
-template <typename Element>
-CUTLASS_HOST_DEVICE
-Array<Element, 3> make_Array(Element x, Element y, Element z) {
-  Array<Element, 3> m;
-  m[0] = x;
-  m[1] = y;
-  m[2] = z;
-  return m;
-}
-
-template <typename Element>
-CUTLASS_HOST_DEVICE
-Array<Element, 4> make_Array(Element x, Element y, Element z, Element w) {
-  Array<Element, 4> m;
-  m[0] = x;
-  m[1] = y;
-  m[2] = z;
-  m[3] = w;
-  return m;
-}
-
-////////////////////////////////////////////////////////////////////////////////////////////////////
-
 } // namespace cutlass
 
 ////////////////////////////////////////////////////////////////////////////////////////////////////
-
-#include "cutlass/array_subbyte.h"
-
-////////////////////////////////////////////////////////////////////////////////////////////////////
-
-namespace cutlass {
-
-////////////////////////////////////////////////////////////////////////////////////////////////////
-
-/// Aligned array type
-template <
-  /// Element type
-  typename T,
-  /// Number of elements in the array
-  int N,
-  /// Alignment requirement in bytes
-  int Alignment = sizeof_bits<T>::value * N / 8
->
-class alignas(Alignment) AlignedArray: public Array<T, N> {
-public:
-
-};
-
-////////////////////////////////////////////////////////////////////////////////////////////////////
-
-} // namespace cutlass
-
-////////////////////////////////////////////////////////////////////////////////////////////////////
-
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/array_planar_complex.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/array_planar_complex.h`

 * *Files 4% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/array_subbyte.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/conv2d_tile_iterator.h`

 * *Files 26% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -25,544 +25,313 @@
  * SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER
  * CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,
  * OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
  * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
  *
  **************************************************************************************************/
 /*! \file
-    \brief Statically sized array of elements that accommodates all CUTLASS-supported numeric types
-           and is safe to use in a union.
+    \brief Template wraps the tile access iterator concept to load whole tiles from tensors in
+      memory used for implicit GEMM convolution.
 */
 
 #pragma once
 
 #include "cutlass/cutlass.h"
 #include "cutlass/array.h"
-#include "cutlass/platform/platform.h"
+#include "cutlass/coord.h"
+#include "cutlass/matrix_shape.h"
+#include "cutlass/tensor_ref.h"
+#include "cutlass/tensor_view.h"
+#include "cutlass/layout/pitch_linear.h"
+#include "cutlass/layout/tensor.h"
+#include "cutlass/layout/matrix.h"
+#include "cutlass/conv/convolution.h"
+#include "cutlass/conv/conv2d_problem_size.h"
+
+/////////////////////////////////////////////////////////////////////////////////////////////////
 
 namespace cutlass {
+namespace conv {
+namespace threadblock {
 
-////////////////////////////////////////////////////////////////////////////////////////////////////
+/////////////////////////////////////////////////////////////////////////////////////////////////
 
-/// Statically sized array for any data type
-template <
-  typename T,
-  int N
->
-class Array<T, N, false> {
+template <typename TileAccessIterator_>
+class TileIterator {
 public:
+  using TileAccessIterator = TileAccessIterator_;
 
-  static int const kSizeBits = sizeof_bits<T>::value * N;
-
-  /// Storage type
-  using Storage = typename platform::conditional<
-    ((kSizeBits % 32) != 0),
-    typename platform::conditional<
-      ((kSizeBits % 16) != 0),
-      uint8_t,
-      uint16_t
-    >::type,
-    uint32_t
-  >::type;
-
-  /// Element type
-  using Element = T;
-
-  /// Number of logical elements per stored object
-  static int const kElementsPerStoredItem = int(sizeof(Storage) * 8) / sizeof_bits<T>::value;
-
-  /// Number of storage elements
-  static size_t const kStorageElements = N / kElementsPerStoredItem;
-
-  /// Number of logical elements
-  static size_t const kElements = N;
-
-  /// Bitmask for covering one item
-  static Storage const kMask = ((Storage(1) << sizeof_bits<T>::value) - 1);
-
-  //
-  // C++ standard members with pointer types removed
-  //
-
-  typedef T value_type;
-  typedef size_t size_type;
-  typedef ptrdiff_t difference_type;
-  typedef value_type *pointer;
-  typedef value_type const *const_pointer;
-
-  //
-  // References
-  //
-
-  /// Reference object inserts or extracts sub-byte items
-  class reference {
-    /// Pointer to storage element
-    Storage *ptr_;
-
-    /// Index into elements packed into Storage object
-    int idx_;
-
-  public:
-
-    /// Default ctor
-    CUTLASS_HOST_DEVICE
-    reference(): ptr_(nullptr), idx_(0) { }
-
-    /// Ctor
-    CUTLASS_HOST_DEVICE
-    reference(Storage *ptr, int idx = 0): ptr_(ptr), idx_(idx) { }
-
-    /// Assignment
-    CUTLASS_HOST_DEVICE
-    reference &operator=(T x) {
-      Storage item = (reinterpret_cast<Storage const &>(x) & kMask);
-
-      Storage kUpdateMask = Storage(~(kMask << (idx_ * sizeof_bits<T>::value)));
-      *ptr_ = Storage(((*ptr_ & kUpdateMask) | (item << idx_ * sizeof_bits<T>::value)));
-
-      return *this;
-    }
-
-    CUTLASS_HOST_DEVICE
-    T get() const {
-      Storage item = Storage((*ptr_ >> (idx_ * sizeof_bits<T>::value)) & kMask);
-      return reinterpret_cast<T const &>(item);
-    }
-
-    /// Extract
-    CUTLASS_HOST_DEVICE
-    operator T() const {
-      return get();
-    }
-
-    /// Explicit cast to int
-    CUTLASS_HOST_DEVICE
-    explicit operator int() const {
-      return int(get());
-    }
-
-    /// Explicit cast to float
-    CUTLASS_HOST_DEVICE
-    explicit operator float() const {
-      return float(get());
-    }
-  };
-
-  /// Reference object extracts sub-byte items
-  class const_reference {
-
-    /// Pointer to storage element
-    Storage const *ptr_;
-
-    /// Index into elements packed into Storage object
-    int idx_;
-
-  public:
-
-    /// Default ctor
-    CUTLASS_HOST_DEVICE
-    const_reference(): ptr_(nullptr), idx_(0) { }
-
-    /// Ctor
-    CUTLASS_HOST_DEVICE
-    const_reference(Storage const *ptr, int idx = 0): ptr_(ptr), idx_(idx) { }
-
-    CUTLASS_HOST_DEVICE
-    const T get() const {
-      Storage item = (*ptr_ >> (idx_ * sizeof_bits<T>::value)) & kMask;
-      return reinterpret_cast<T const &>(item);
-    }
-
-    /// Extract
-    CUTLASS_HOST_DEVICE
-    operator T() const {
-      Storage item = Storage(Storage(*ptr_ >> Storage(idx_ * sizeof_bits<T>::value)) & kMask);
-      return reinterpret_cast<T const &>(item);
-    }
-
-    /// Explicit cast to int
-    CUTLASS_HOST_DEVICE
-    explicit operator int() const {
-      return int(get());
-    }
-
-    /// Explicit cast to float
-    CUTLASS_HOST_DEVICE
-    explicit operator float() const {
-      return float(get());
-    }
-  };
-
-  //
-  // Iterators
-  //
-
-  /// Bidirectional iterator over elements
-  class iterator {
-
-    /// Pointer to storage element
-    Storage *ptr_;
-
-    /// Index into elements packed into Storage object
-    int idx_;
-
-  public:
-
-    CUTLASS_HOST_DEVICE
-    iterator(): ptr_(nullptr), idx_(0) { }
-
-    CUTLASS_HOST_DEVICE
-    iterator(Storage *ptr, int idx = 0): ptr_(ptr), idx_(idx) { }
-
-    CUTLASS_HOST_DEVICE
-    iterator &operator++() {
-      ++idx_;
-      if (idx_ == kElementsPerStoredItem) {
-        ++ptr_;
-        idx_ = 0;
-      }
-      return *this;
-    }
-
-    CUTLASS_HOST_DEVICE
-    iterator &operator--() {
-      if (!idx_) {
-        --ptr_;
-        idx_ = kElementsPerStoredItem - 1;
-      }
-      else {
-        --idx_;
-      }
-      return *this;
-    }
-
-    CUTLASS_HOST_DEVICE
-    iterator operator++(int) {
-      iterator ret(*this);
-      ++idx_;
-      if (idx_ == kElementsPerStoredItem) {
-        ++ptr_;
-        idx_ = 0;
-      }
-      return ret;
-    }
-
-    CUTLASS_HOST_DEVICE
-    iterator operator--(int) {
-      iterator ret(*this);
-      if (!idx_) {
-        --ptr_;
-        idx_ = kElementsPerStoredItem - 1;
-      }
-      else {
-        --idx_;
-      }
-      return ret;
-    }
-
-    CUTLASS_HOST_DEVICE
-    reference operator*() const {
-      return reference(ptr_, idx_);
-    }
-
-    CUTLASS_HOST_DEVICE
-    bool operator==(iterator const &other) const {
-      return ptr_ == other.ptr_ && idx_ == other.idx_;
-    }
-
-    CUTLASS_HOST_DEVICE
-    bool operator!=(iterator const &other) const {
-      return !(*this == other);
-    }
-  };
-
-  /// Bidirectional constant iterator over elements
-  class const_iterator {
-
-    /// Pointer to storage element
-    Storage const *ptr_;
-
-    /// Index into elements packed into Storage object
-    int idx_;
-
-  public:
-
-    CUTLASS_HOST_DEVICE
-    const_iterator(): ptr_(nullptr), idx_(0) { }
-
-    CUTLASS_HOST_DEVICE
-    const_iterator(Storage const *ptr, int idx = 0): ptr_(ptr), idx_(idx) { }
-
-    CUTLASS_HOST_DEVICE
-    iterator &operator++() {
-      ++idx_;
-      if (idx_ == kElementsPerStoredItem) {
-        ++ptr_;
-        idx_ = 0;
-      }
-      return *this;
-    }
-
-    CUTLASS_HOST_DEVICE
-    iterator &operator--() {
-      if (!idx_) {
-        --ptr_;
-        idx_ = kElementsPerStoredItem - 1;
-      }
-      else {
-        --idx_;
-      }
-      return *this;
-    }
-
-    CUTLASS_HOST_DEVICE
-    iterator operator++(int) {
-      iterator ret(*this);
-      ++idx_;
-      if (idx_ == kElementsPerStoredItem) {
-        ++ptr_;
-        idx_ = 0;
-      }
-      return ret;
-    }
-
-    CUTLASS_HOST_DEVICE
-    iterator operator--(int) {
-      iterator ret(*this);
-      if (!idx_) {
-        --ptr_;
-        idx_ = kElementsPerStoredItem - 1;
-      }
-      else {
-        --idx_;
-      }
-      return ret;
-    }
-
-    CUTLASS_HOST_DEVICE
-    const_reference operator*() const {
-      return const_reference(ptr_, idx_);
-    }
-
-    CUTLASS_HOST_DEVICE
-    bool operator==(iterator const &other) const {
-      return ptr_ == other.ptr_ && idx_ == other.idx_;
-    }
-
-    CUTLASS_HOST_DEVICE
-    bool operator!=(iterator const &other) const {
-      return !(*this == other);
-    }
-  };
-
-  /// Bidirectional iterator over elements
-  class reverse_iterator {
-
-    /// Pointer to storage element
-    Storage *ptr_;
-
-    /// Index into elements packed into Storage object
-    int idx_;
-
-  public:
-
-    CUTLASS_HOST_DEVICE
-    reverse_iterator(): ptr_(nullptr), idx_(0) { }
-
-    CUTLASS_HOST_DEVICE
-    reverse_iterator(Storage *ptr, int idx = 0): ptr_(ptr), idx_(idx) { }
-
-    // TODO
-  };
-
-  /// Bidirectional constant iterator over elements
-  class const_reverse_iterator {
-
-    /// Pointer to storage element
-    Storage const *ptr_;
-
-    /// Index into elements packed into Storage object
-    int idx_;
-
-  public:
-
-    CUTLASS_HOST_DEVICE
-    const_reverse_iterator(): ptr_(nullptr), idx_(0) { }
-
-    CUTLASS_HOST_DEVICE
-    const_reverse_iterator(Storage const *ptr, int idx = 0): ptr_(ptr), idx_(idx) { }
-
-    // TODO
-  };
+  using Shape = typename TileAccessIterator::Shape;
+  using Element = typename TileAccessIterator::Element;
+  using Layout = typename TileAccessIterator::Layout;
+  using TensorCoord = typename Layout::TensorCoord;
+  using ThreadMap = typename TileAccessIterator::ThreadMap;
+  using AccessType = typename TileAccessIterator::AccessType;
+  using TensorRef = typename TileAccessIterator::TensorRef;
+  using Index = typename TileAccessIterator::Index;
+  using LongIndex = typename TileAccessIterator::LongIndex;
+  static IteratorAlgorithm const kIteratorAlgorithm = TileAccessIterator::kIteratorAlgorithm;
+  static StrideSupport const kStrideSupport = TileAccessIterator::kStrideSupport;
+  using Params = typename TileAccessIterator::Params;
+  static int const kConvDim = TileAccessIterator::kConvDim;
+  using ConvProblemSize = typename TileAccessIterator::ConvProblemSize;
+  static int const kAccessesPerVector = TileAccessIterator::kAccessesPerVector;
+
+  /// Fragment object to be loaded or stored
+  using Fragment = cutlass::Array<
+    Element, 
+    ThreadMap::Iterations::kCount * ThreadMap::kElementsPerAccess>;
 
 private:
 
-  /// Internal storage
-  Storage storage[kStorageElements];
+  /// Internal state
+  TileAccessIterator tile_access_iterator_;
 
 public:
 
-  #if 0
+  /// Constructor
   CUTLASS_HOST_DEVICE
-  Array() { }
+  TileIterator(
+    Params const &params,
+    ConvProblemSize const &problem_size,
+    Element const *ptr,
+    int thread_idx,
+    MatrixCoord const &threadblock_offset = MatrixCoord()
+  ):
+    tile_access_iterator_(params, problem_size, ptr, thread_idx, threadblock_offset) { }
 
   CUTLASS_HOST_DEVICE
-  Array(Array const &x) {
-    CUTLASS_PRAGMA_UNROLL
-    for (int i = 0; i < int(kStorageElements); ++i) {
-      storage[i] = x.storage[i];
-    }
+  static Params getParams(ConvProblemSize const &problem_size, Layout const &layout) {
+    return TileAccessIterator::getParams(problem_size, layout);
   }
-  #endif
 
-  /// Efficient clear method
+  /// Overrides the internal iteration index
   CUTLASS_HOST_DEVICE
-  void clear() {
-
-    CUTLASS_PRAGMA_UNROLL
-    for (int i = 0; i < int(kStorageElements); ++i) {
-      storage[i] = Storage(0);
-    }
+  void set_iteration_index(Index index) {
+    tile_access_iterator_.set_iteration_index(index);
   }
 
+  /// Adds a pointer offset in units of Element
   CUTLASS_HOST_DEVICE
-  reference at(size_type pos) {
-    return reference(storage + pos / kElementsPerStoredItem, pos % kElementsPerStoredItem);
+  void add_pointer_offset(LongIndex pointer_offset) {
+    tile_access_iterator_.add_pointer_offset(pointer_offset);
   }
 
+  /// Advances to the next tile in memory.
   CUTLASS_HOST_DEVICE
-  const_reference at(size_type pos) const {
-    return const_reference(storage + pos / kElementsPerStoredItem, pos % kElementsPerStoredItem);
+  TileIterator &operator++() {
+    tile_access_iterator_.advance();
+    return *this;
   }
 
+  /// Advances to the next tile in memory.
   CUTLASS_HOST_DEVICE
-  reference operator[](size_type pos) {
-    return at(pos);
+  TileIterator operator++(int) {
+    TileIterator self(*this);
+    operator++();
+    return self;
   }
 
-  CUTLASS_HOST_DEVICE
-  const_reference operator[](size_type pos) const {
-    return at(pos);
-  }
+  /// Loads a fragment from memory
+  CUTLASS_DEVICE
+  void load_with_pointer_offset(Fragment &frag, Index pointer_offset) {
 
-  CUTLASS_HOST_DEVICE
-  reference front() {
-    return at(0);
-  }
+    frag.clear();
+    AccessType *frag_ptr = reinterpret_cast<AccessType *>(&frag);
 
-  CUTLASS_HOST_DEVICE
-  const_reference front() const {
-    return at(0);
+    CUTLASS_PRAGMA_UNROLL
+    for (int s = 0; s < ThreadMap::Iterations::kStrided; ++s) {
+      CUTLASS_PRAGMA_UNROLL
+      for (int c = 0; c < ThreadMap::Iterations::kContiguous; ++c) {
+        CUTLASS_PRAGMA_UNROLL
+        for (int v = 0; v < kAccessesPerVector; ++v) {
+
+          int idx = v + kAccessesPerVector * (c + s * ThreadMap::Iterations::kContiguous);
+
+          cutlass::arch::global_load<
+            AccessType,
+            sizeof(AccessType)
+          >(
+            frag_ptr[idx],
+            tile_access_iterator_.get() + pointer_offset,
+            tile_access_iterator_.valid()
+          );
+  
+          ++tile_access_iterator_;
+        }
+      }
+    }
   }
 
-  CUTLASS_HOST_DEVICE
-  reference back() {
-    return reference(storage + kStorageElements - 1, kElementsPerStoredItem - 1);
+  /// Loads a fragment from memory
+  CUTLASS_DEVICE
+  void load(Fragment &frag) {
+    tile_access_iterator_.set_iteration_index(0);
+    load_with_pointer_offset(frag, 0);
   }
 
-  CUTLASS_HOST_DEVICE
-  const_reference back() const {
-    return const_reference(storage + kStorageElements - 1, kElementsPerStoredItem - 1);
+  CUTLASS_DEVICE
+  void advance() {
+    tile_access_iterator_.advance();
   }
 
+  /// Determines whether the Implicit GEMM can execute the given problem.
   CUTLASS_HOST_DEVICE
-  pointer data() {
-    return reinterpret_cast<pointer>(storage);
+  static Status can_implement(ConvProblemSize const &problem_size) {
+
+    // dispatch to iterator implementation
+    return TileAccessIterator::can_implement(problem_size);
   }
+};
+
+/////////////////////////////////////////////////////////////////////////////////////////////////
+// Strided Dgrad Tile Iterator
+template <typename TileAccessIterator_>
+class TileIteratorStridedDgrad {
+public:
+  using TileAccessIterator = TileAccessIterator_;
+
+  using Shape = typename TileAccessIterator::Shape;
+  using Element = typename TileAccessIterator::Element;
+  using Layout = typename TileAccessIterator::Layout;
+  using TensorCoord = typename Layout::TensorCoord;
+  using ThreadMap = typename TileAccessIterator::ThreadMap;
+  using AccessType = typename TileAccessIterator::AccessType;
+  using TensorRef = typename TileAccessIterator::TensorRef;
+  using Index = typename TileAccessIterator::Index;
+  using LongIndex = typename TileAccessIterator::LongIndex;
+  static IteratorAlgorithm const kIteratorAlgorithm = TileAccessIterator::kIteratorAlgorithm;
+  static StrideSupport const kStrideSupport = TileAccessIterator::kStrideSupport;
+  using Params = typename TileAccessIterator::Params;
+  static int const kConvDim = TileAccessIterator::kConvDim;
+  using ConvProblemSize = typename TileAccessIterator::ConvProblemSize;
+
+  /// Fragment object to be loaded or stored
+  using Fragment = cutlass::Array<
+    Element, 
+    ThreadMap::Iterations::kCount * ThreadMap::kElementsPerAccess>;
+
+private:
+
+  /// Internal state
+  TileAccessIterator tile_access_iterator_;
 
+public:
+
+  /// Constructor (output gradient (Dy) OperandA ctor)
   CUTLASS_HOST_DEVICE
-  const_pointer data() const {
-    return reinterpret_cast<const_pointer>(storage);
-  }
-  
+  TileIteratorStridedDgrad(
+    Params const &params,
+    ConvProblemSize const &problem_size,
+    Element const *ptr,
+    int thread_idx,
+    FastDivmod const &stride_h_divmod, FastDivmod const &stride_w_divmod,
+    int start_r, int start_s,
+    MatrixCoord const &threadblock_offset = MatrixCoord()
+  ):
+    tile_access_iterator_(
+      params, 
+      problem_size, 
+      ptr, 
+      thread_idx, 
+      stride_h_divmod, stride_w_divmod, 
+      start_r, start_s, 
+      threadblock_offset) { }
+
+  /// Constructor (filter (w) OperandB ctor)
   CUTLASS_HOST_DEVICE
-  Storage * raw_data() {
-    return storage;
-  }
+  TileIteratorStridedDgrad(
+    Params const &params,
+    ConvProblemSize const &problem_size,
+    Element const *ptr,
+    int thread_idx,
+    int start_r, int start_s,
+    MatrixCoord const &threadblock_offset = MatrixCoord()
+  ):
+    tile_access_iterator_(params, 
+      problem_size, 
+      ptr, 
+      thread_idx, 
+      start_r, start_s, 
+      threadblock_offset) { }
 
   CUTLASS_HOST_DEVICE
-  Storage const * raw_data() const {
-    return storage;
+  static Params getParams(ConvProblemSize const &problem_size, Layout const &layout) {
+    return TileAccessIterator::getParams(problem_size, layout);
   }
 
 
+  /// Adds a pointer offset in units of Element
   CUTLASS_HOST_DEVICE
-  constexpr bool empty() const {
-    return !kElements;
+  void add_pointer_offset(LongIndex pointer_offset) {
+    tile_access_iterator_.add_pointer_offset(pointer_offset);
   }
 
+  /// Advances to the next tile in memory.
   CUTLASS_HOST_DEVICE
-  constexpr size_type size() const {
-    return kElements;
+  TileIteratorStridedDgrad &operator++() {
+    tile_access_iterator_.advance();
+    return *this;
   }
 
+  /// Advances to the next tile in memory.
   CUTLASS_HOST_DEVICE
-  constexpr size_type max_size() const {
-    return kElements;
+  TileIteratorStridedDgrad operator++(int) {
+    TileIteratorStridedDgrad self(*this);
+    operator++();
+    return self;
   }
 
-  CUTLASS_HOST_DEVICE
-  void fill(T const &value) {
+  /// Loads a fragment from memory
+  CUTLASS_DEVICE
+  void load_with_pointer_offset(Fragment &frag, Index pointer_offset) {
 
-    CUTLASS_PRAGMA_UNROLL
-    for (int i = 0; i < kElementsPerStoredItem; ++i) {
-      reference ref(storage, i);
-      ref = value;
-    }
+    frag.clear();
+    AccessType *frag_ptr = reinterpret_cast<AccessType *>(&frag);
 
     CUTLASS_PRAGMA_UNROLL
-    for (int i = 1; i < kStorageElements; ++i) {
-      storage[i] = storage[0];
-    }
-  }
+    for (int s = 0; s < ThreadMap::Iterations::kStrided; ++s) {
+      CUTLASS_PRAGMA_UNROLL
+      for (int c = 0; c < ThreadMap::Iterations::kContiguous; ++c) {
 
-  CUTLASS_HOST_DEVICE
-  iterator begin() {
-    return iterator(storage);
-  }
+        cutlass::arch::global_load<
+          AccessType,
+          sizeof(AccessType)
+        >(
+          frag_ptr[c + s * ThreadMap::Iterations::kContiguous],
+          tile_access_iterator_.get() + pointer_offset,
+          tile_access_iterator_.valid()
+        );
 
-  CUTLASS_HOST_DEVICE
-  const_iterator cbegin() const {
-    return const_iterator(storage);
-  }
-
-  CUTLASS_HOST_DEVICE
-  iterator end() {
-    return iterator(storage + kStorageElements);
-  }
-
-  CUTLASS_HOST_DEVICE
-  const_iterator cend() const {
-    return const_iterator(storage + kStorageElements);
+        ++tile_access_iterator_;
+      }
+    }
   }
 
-  CUTLASS_HOST_DEVICE
-  reverse_iterator rbegin() {
-    return reverse_iterator(storage + kStorageElements);
+  /// Loads a fragment from memory
+  CUTLASS_DEVICE
+  void load(Fragment &frag) {
+    tile_access_iterator_.set_iteration_index(0);
+    load_with_pointer_offset(frag, 0);
   }
 
-  CUTLASS_HOST_DEVICE
-  const_reverse_iterator crbegin() const {
-    return const_reverse_iterator(storage + kStorageElements);
+  CUTLASS_DEVICE
+  void advance() {
+    tile_access_iterator_.advance();
   }
 
+  /// Determines whether the Implicit GEMM can execute the given problem.
   CUTLASS_HOST_DEVICE
-  reverse_iterator rend() {
-    return reverse_iterator(storage);
-  }
+  static Status can_implement(ConvProblemSize const &problem_size) {
 
-  CUTLASS_HOST_DEVICE
-  const_reverse_iterator crend() const {
-    return const_reverse_iterator(storage);
+    // dispatch to iterator implementation
+    return TileAccessIterator::can_implement(problem_size);
   }
-
-  //
-  // Comparison operators
-  //
-
 };
+/////////////////////////////////////////////////////////////////////////////////////////////////
 
-////////////////////////////////////////////////////////////////////////////////////////////////////
-
+} // namespace threadblock
+} // namespace conv
 } // namespace cutlass
 
-////////////////////////////////////////////////////////////////////////////////////////////////////
+/////////////////////////////////////////////////////////////////////////////////////////////////
+
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/barrier.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/barrier.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/bfloat16.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/bfloat16.h`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -31,15 +31,17 @@
 /*!
     \file
     \brief Defines a proxy class for storing non-standard 16-bit floating point values with
           8 bits of exponent and 7 bit of mantissa.
 */
 #pragma once
 
-#if !defined(__CUDACC_RTC__)
+#if defined(__CUDACC_RTC__)
+#include "cutlass/floating_point_nvrtc.h"
+#else
 #include <cmath>
 #include <limits>
 #include <cstdint>
 #include <cstring>
 #endif
 
 #include "cutlass/cutlass.h"
@@ -67,16 +69,15 @@
   static bfloat16_t bitcast(uint16_t x) {
     bfloat16_t h;
     h.storage = x;
     return h;
   }
 
   /// Default constructor
-  CUTLASS_HOST_DEVICE
-  bfloat16_t() : storage(0) { }
+  bfloat16_t() = default;
 
   /// Floating-point conversion - round toward nearest
   CUTLASS_HOST_DEVICE
   explicit bfloat16_t(float x) {
 
     #if defined(__CUDA_ARCH__) && (__CUDA_ARCH__ >= 800) && (__CUDACC_VER_MAJOR__ >= 11)
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/blas3.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/blas3.h`

 * *Files 2% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -36,14 +36,15 @@
 */
 
 #pragma once
 
 #include "cutlass/cutlass.h"
 #include "cutlass/array.h"
 #include "cutlass/coord.h"
+#include "cutlass/complex.h"
 #include "cutlass/functional.h"
 #include "cutlass/numeric_types.h"
 
 ////////////////////////////////////////////////////////////////////////////////////////////////////
 
 namespace cutlass {
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/block_striped.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/block_striped.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/complex.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/complex.h`

 * *Files 25% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -28,37 +28,42 @@
  * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
  *
  **************************************************************************************************/
 #pragma once
 
 #include <cuComplex.h>
 
+#include <cuda_fp16.h>
+
 #if defined(__CUDACC_RTC__)
 #include <cuda/std/cstdint>
 #else
 #include <cstdint>
 #endif
 
 #include "cutlass/cutlass.h"
+#include "cutlass/functional.h"
 #include "cutlass/half.h"
 #include "cutlass/real.h"
 
 #include "cutlass/bfloat16.h"
 #include "cutlass/tfloat32.h"
 
 #include "cutlass/fast_math.h"
 
 #if !defined(__CUDACC_RTC__)
 #include <iosfwd>
 #endif
 
 namespace cutlass {
 
-//////////////////////////////////////////////////////////////////////////////////////////////////
 
+
+
+/////////////////////////////////////////////////////////////////////////////////////////////////
 /// Enumeraed type describing a transformation on a complex value.
 enum class ComplexTransform {
   kNone,
   kConjugate
 };
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
@@ -143,23 +148,26 @@
 
  public:
 
 //
 // Methods
 //
 
-/// Constructor
+  /// Default constructor
+  complex() = default;
+
+  /// Constructor
   CUTLASS_HOST_DEVICE
-  complex(T r = T(0)) : _real(r), _imag(T(0)) {}
+  complex(T r) : _real(r), _imag(T(0)) {}
 
-/// Constructor
+  /// Constructor
   CUTLASS_HOST_DEVICE
   complex(T r, T i) : _real(r), _imag(i) {}
-  //
-/// Constructor
+
+  /// Constructor
   template<typename A>
   CUTLASS_HOST_DEVICE
   complex(complex<A> const &z) : _real(static_cast<T>(z.real())), _imag(static_cast<T>(z.imag())) {}
 
 
   #if !defined(__CUDACC_RTC__)
   /// Conversion from cuFloatComplex
@@ -193,14 +201,32 @@
 
   /// Addition
     template <typename A>
   CUTLASS_HOST_DEVICE complex<T> operator+(complex<A> const &rhs) const {
     return complex<T>(this->real() + rhs.real(), this->imag() + rhs.imag());
   }
 
+  /// Reduction into memory address.  Components may update out of order.
+  template <typename OtherT>
+  CUTLASS_DEVICE void red(complex<OtherT> *ptr) const {
+    static_assert(platform::is_same<T, OtherT>::value, "Component type must match");
+    cutlass::red<T> reduce;
+    reduce(&ptr->_real, _real);
+    reduce(&ptr->_imag, _imag);
+  }
+
+  /// Reduction into memory address.  Components may update out of order.  (Half specialization)
+  CUTLASS_DEVICE void red(complex<half_t> *ptr) const {
+    static_assert(platform::is_same<T, half_t>::value, "Component type must match");
+    half2 *h2_ptr = reinterpret_cast<half2*>(ptr);
+    half2 h2_data = reinterpret_cast<half2&>(*this);
+    cutlass::red<half2> reduce;
+    reduce(h2_ptr, h2_data);
+  }
+
   /// Subtraction
     template <typename A>
   CUTLASS_HOST_DEVICE complex<T> operator-(complex<A> const &rhs) const {
     return complex<T>(this->real() - rhs.real(), this->imag() - rhs.imag());
   }
 
   /// Multiplication
@@ -502,21 +528,22 @@
   return true; 
 }
 
 //////////////////////////////////////////////////////////////////////////////////////////////////
 
 /// Partial specialization for complex-valued type.
 template <typename T>
-struct RealType< complex<T> > {
+struct RealType< complex<T> >
+{
   using Type = T;
 
   /// Number of elements
   static int const kExtent = 2;
 
-CUTLASS_HOST_DEVICE
+  CUTLASS_HOST_DEVICE
   static complex<T> from_real(double x) {
     return complex<T>(static_cast<T>(x));
   }
 };
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
@@ -546,12 +573,133 @@
 };
 
 template <typename T>
 struct is_complex<complex<T>> {
   static bool const value = true;
 };
 
+
+/////////////////////////////////////////////////////////////////////////////////////////////////
+// functional.h numeric specializations
+/////////////////////////////////////////////////////////////////////////////////////////////////
+
+/// Squares with optional conversion
+template <typename T, typename Output>
+struct magnitude_squared<complex<T>, Output> {
+  CUTLASS_HOST_DEVICE
+  Output operator()(complex<T> lhs) const {
+    multiplies<Output> mul_op;
+
+    Output y_r = Output(lhs.real());
+    Output y_i = Output(lhs.imag());
+
+    return mul_op(y_r, y_r) + mul_op(y_i, y_i);
+  }
+};
+
+/// Fused multiply-add
+template <typename T>
+struct multiply_add<complex<T>, complex<T>, complex<T>> {
+  CUTLASS_HOST_DEVICE
+  complex<T> operator()(
+    complex<T> const &a,
+    complex<T> const &b,
+    complex<T> const &c) const {
+
+    T real = c.real();
+    T imag = c.imag();
+
+    real += a.real() * b.real();
+    real += -a.imag() * b.imag();
+    imag += a.real() * b.imag();
+    imag += a.imag () * b.real();
+
+    return complex<T>{
+      real,
+      imag
+    };
+  }
+};
+
+/// Fused multiply-add
+template <typename T>
+struct multiply_add<complex<T>, T, complex<T>> {
+  CUTLASS_HOST_DEVICE
+  complex<T> operator()(
+    complex<T> const &a,
+    T const &b,
+    complex<T> const &c) const {
+
+    T real = c.real();
+    T imag = c.imag();
+
+    real += a.real() * b;
+    imag += a.imag () * b;
+
+    return complex<T>{
+      real,
+      imag
+    };
+  }
+};
+
+/// Fused multiply-add
+template <typename T>
+struct multiply_add<T, complex<T>, complex<T>> {
+  CUTLASS_HOST_DEVICE
+  complex<T> operator()(
+    T const &a,
+    complex<T> const &b,
+    complex<T> const &c) const {
+
+    T real = c.real();
+    T imag = c.imag();
+
+    real += a * b.real();
+    imag += a * b.imag();
+
+    return complex<T>{
+      real,
+      imag
+    };
+  }
+};
+
+/// Conjugate
+template <typename T>
+struct conjugate<complex<T>>  {
+  CUTLASS_HOST_DEVICE
+  complex<T> operator()(complex<T> const &a) const {
+    return conj(a);
+  }
+};
+
+/// Computes the square of a difference with optional conversion
+template <typename T, typename Output>
+struct magnitude_squared_difference<complex<T>, Output> {
+  CUTLASS_HOST_DEVICE
+  Output operator()(complex<T> lhs, complex<T> rhs) const {
+    multiplies<Output> mul_op;
+
+    Output y_r = Output(lhs.real()) - Output(rhs.real());
+    Output y_i = Output(lhs.imag()) - Output(rhs.imag());
+
+    return mul_op(y_r, y_r) + mul_op(y_i, y_i);
+  }
+};
+
+/// Reduces value into the data pointed to by ptr (complex<T> specialization)
+template <typename T>
+struct red<complex<T>> {
+  CUTLASS_DEVICE
+  void operator()(complex<T> *ptr, const complex<T> &data)
+  {
+    data.red(ptr);
+  }
+};
+
+
 //////////////////////////////////////////////////////////////////////////////////////////////////
 
 }  // namespace cutlass
 
 //////////////////////////////////////////////////////////////////////////////////////////////////
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/constants.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/constants.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/conv/conv2d_problem_size.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/conv/conv2d_problem_size.h`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -217,20 +217,20 @@
     return tmp; 
   }
 
   /// Equality operator (ignores mode and split_k_slice)
   CUTLASS_HOST_DEVICE
   bool operator==(Conv2dProblemSize const &conv) const {
     return (
-      (N == conv.N) && (W == conv.H) && (W == conv.W) && (C == conv.C) &&
+      (N == conv.N) && (H == conv.H) && (W == conv.W) && (C == conv.C) &&
       (K == conv.K) && (R == conv.R) && (S == conv.S) &&
       (P == conv.P) && (Q == conv.Q) &&
       (pad_h == conv.pad_h) && (pad_w == conv.pad_w) &&
       (stride_h == conv.stride_h) && (stride_w == conv.stride_w) &&
-      (dilation_h == conv.dilation_h) && (dilation_h == conv.dilation_h)
+      (dilation_h == conv.dilation_h) && (dilation_w == conv.dilation_w)
     );  
   }
 
   /// Inequality operator
   CUTLASS_HOST_DEVICE
   bool operator!=(Conv2dProblemSize const &rhs) const {
     return !(*this == rhs);
@@ -243,15 +243,15 @@
     return cutlass::Tensor4DCoord ({N, H, W, C});
   }
 
   /// Returns filter extent as Tensor4DCoord
   CUTLASS_HOST_DEVICE
   cutlass::Tensor4DCoord filter_extent() const {
 
-    return cutlass::Tensor4DCoord ({K, R, S, C});
+    return cutlass::Tensor4DCoord ({K, R, S, C / groups});
   }
 
   /// Returns output extent as Tensor4DCoord
   CUTLASS_HOST_DEVICE
   cutlass::Tensor4DCoord output_extent() const {
 
     return cutlass::Tensor4DCoord ({N, P, Q, K});
@@ -274,15 +274,15 @@
   /// Returns output size in number of elements
   CUTLASS_HOST_DEVICE
   int64_t output_size() const {
 
     return (N * P * Q * K);
   }
   
-  /// Returns output extent as Tensor4DCoord
+  /// Returns padding as Tensor4DCoord
   CUTLASS_HOST_DEVICE
   cutlass::Tensor4DCoord padding() const {
 
     return cutlass::Tensor4DCoord ({pad_h, pad_h, pad_w, pad_w});
   }
 
   /// Returns stride as MatrixCoord
@@ -332,15 +332,15 @@
   Conv2dProblemSize const &problem_size) {
   // Compute problem size
   switch (conv_operator) {
   case Operator::kFprop:
     return gemm::GemmCoord(
       problem_size.N * problem_size.P * problem_size.Q,
       problem_size.K,
-      problem_size.R * problem_size.S * problem_size.C
+      problem_size.R * problem_size.S * problem_size.C / problem_size.groups
     );
   case Operator::kDgrad:
     return gemm::GemmCoord(
       problem_size.N * problem_size.H * problem_size.W,
       problem_size.C,
       problem_size.R * problem_size.S * problem_size.K
     );
@@ -447,22 +447,53 @@
             }
           }
           break;
 
         default:
           break;
       }
+    } else if (algorithm == IteratorAlgorithm::kOptimized) {
+      // Current optimized iterator only support GroupMode::kSingleGroup
+      if (group_mode == GroupMode::kSingleGroup) {
+        switch (conv_operator) {
+          case Operator::kFprop:
+            iterations = problem_size.R * problem_size.S * ((channels_per_group + threadblock_K - 1) / threadblock_K);
+            break;
+
+          default:
+            break;
+        }
+      }
     }
 
   }
 
   return iterations;
 }
 
 
+template <int N = 1, int Output_P = 1, int Output_Q = 1>
+CUTLASS_HOST_DEVICE
+int depthwise_gemm_k_iterations(
+  Operator conv_operator, 
+  int threadblock_K, 
+  Conv2dProblemSize const &problem_size,
+  IteratorAlgorithm algorithm = IteratorAlgorithm::kAnalytic,
+  GroupMode group_mode = GroupMode::kNone,
+  int threadblock_N = 0) {
+
+    int n =  problem_size.N;
+    int p = (problem_size.P + Output_P - 1) /  Output_P;
+    int q = (problem_size.Q + Output_Q - 1) /  Output_Q;
+
+    int iterations = (n * p * q + problem_size.split_k_slices - 1) / problem_size.split_k_slices;
+    return iterations;
+}
+
+
 CUTLASS_HOST_DEVICE
 int implicit_gemm_k_iterations_per_channel(
     Operator conv_operator,
     int threadblock_K,
     Conv2dProblemSize const &problem_size,
     IteratorAlgorithm algorithm = IteratorAlgorithm::kAnalytic) {
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/conv/conv3d_problem_size.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/conv/conv3d_problem_size.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -201,16 +201,16 @@
   CUTLASS_HOST_DEVICE
   bool operator==(Conv3dProblemSize const &conv) const {
     return (
       (N == conv.N) && (D == conv.D) && (H == conv.H) && (W == conv.W) && (C == conv.C) &&
       (K == conv.K) && (T == conv.T) && (R == conv.R) && (S == conv.S) &&
       (Z == conv.Z) &&(P == conv.P) && (Q == conv.Q) &&
       (pad_d == conv.pad_d) && (pad_h == conv.pad_h) && (pad_w == conv.pad_w) &&
-      (stride_d == conv.stride_d) && (stride_h == conv.stride_h) && (stride_w == conv.stride_h) &&
-      (dilation_d == conv.dilation_d) && (dilation_h == conv.dilation_h) && (dilation_h == conv.dilation_h)
+      (stride_d == conv.stride_d) && (stride_h == conv.stride_h) && (stride_w == conv.stride_w) &&
+      (dilation_d == conv.dilation_d) && (dilation_h == conv.dilation_h) && (dilation_w == conv.dilation_w)
     );  
   }
 
   /// Inequality operator
   CUTLASS_HOST_DEVICE
   bool operator!=(Conv3dProblemSize const &rhs) const {
     return !(*this == rhs);
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/conv/convolution.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/conv/convolution.h`

 * *Files 8% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -96,22 +96,24 @@
 };
 
 /// Selects among several implementation variants trading off performance with simplicity
 enum class IteratorAlgorithm { 
   kAnalytic,      ///< functionally correct in all cases but lower performance
   kOptimized,     ///< optimized for R <= 32, S <= 32 and unity-stride dgrad
   kFixedChannels, ///< Analytic algorithm optimized for fixed channel count (C == AccessSize)
-  kFewChannels    ///< Analytic algorithm optimized for few channels (C divisible by AccessSize)
+  kFewChannels,   ///< Analytic algorithm optimized for few channels (C divisible by AccessSize)
+  kFixedStrideDilation ///< Optimized for fixed stride and dilation
 };
 
 /// Distinguishes among partial specializations that accelerate certain problems where convolution
 /// stride is unit.
 enum class StrideSupport {
   kStrided,       ///< arbitrary convolution stride
-  kUnity          ///< unit convolution stride
+  kUnity,         ///< unit convolution stride
+  kFixed          ///< fixed convolution stride
 };
 
 /// Identifies split-K mode
 enum class SplitKMode { 
   kNone, 
   kSerial, 
   kParallel
@@ -121,13 +123,45 @@
 enum class GroupMode {
   kNone,
   kSingleGroup,   ///< One CTA calculates one group or less
   kMultipleGroup, ///< One CTA calculates multiple groups
   kDepthwise      ///< One CTA calculates cta_n groups (problem_size.C == problem_size.K == problem_size.groups)
 };
 
+/////////////////////////////////////////////////////////////////////////////////////////////////
+
+/// Shape of a tensor
+template <
+  int N = 1,
+  int H = 1,
+  int W = 1,
+  int C = 1
+>
+struct TensorNHWCShape {
+  static int const kN = N;
+  static int const kH = H;
+  static int const kW = W;
+  static int const kC = C;
+
+  static int const kHW = H * W;
+  static int const kNHW = N * kHW;
+  static int const kNHWC = N * H * W * C;
+
+  static int const kCount = kNHWC;
+
+  //
+  // Static member functions
+  //
+
+  /// Returns a Coord object
+  CUTLASS_HOST_DEVICE
+  static Coord<4> toCoord() {
+    return make_Coord(kN, kH, kW, kC);
+  }
+};
+
 ////////////////////////////////////////////////////////////////////////////////////////////////////
 
 } // namespace conv
 } // namespace cutlass
 
 ////////////////////////////////////////////////////////////////////////////////////////////////////
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/conv/device/direct_convolution.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/conv/device/direct_convolution.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/conv/device/implicit_gemm_convolution.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/conv/device/implicit_gemm_convolution.h`

 * *Files 14% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -48,70 +48,70 @@
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
 template<typename ImplicitGemmKernel_>
 class ImplicitGemmConvolution {
 public:
 
-  using ImplicitGemmKernel = ImplicitGemmKernel_;
+  using UnderlyingKernel = ImplicitGemmKernel_;
 
-  using ElementA = typename ImplicitGemmKernel::ElementA;
-  using LayoutA = typename ImplicitGemmKernel::LayoutA;
-  using ElementB = typename ImplicitGemmKernel::ElementB;
-  using LayoutB = typename ImplicitGemmKernel::LayoutB;
-  using ElementC = typename ImplicitGemmKernel::ElementC;
-  using LayoutC = typename ImplicitGemmKernel::LayoutC;
-  using ElementAccumulator = typename ImplicitGemmKernel::ElementAccumulator;
-  using ElementCompute = typename ImplicitGemmKernel::ElementCompute;
-  using OperatorClass = typename ImplicitGemmKernel::OperatorClass;
-  using ArchTag = typename ImplicitGemmKernel::ArchTag;
-  using ThreadblockShape = typename ImplicitGemmKernel::ThreadblockShape;
-  using WarpShape = typename ImplicitGemmKernel::WarpShape;
-  using InstructionShape = typename ImplicitGemmKernel::InstructionShape;
-  using ThreadblockSwizzle = typename ImplicitGemmKernel::ThreadblockSwizzle;
-  using EpilogueOutputOp = typename ImplicitGemmKernel::EpilogueOutputOp;
-  static int const kStages = ImplicitGemmKernel::kStages;
-  static int const kConvDim = ImplicitGemmKernel::kConvDim;
-  using WarpMmaOperator = typename ImplicitGemmKernel::WarpMmaOperator;
-  using ArchMmaOperator = typename ImplicitGemmKernel::ArchMmaOperator;
-  using MathOperator = typename ImplicitGemmKernel::MathOperator; 
-
-  static cutlass::conv::Operator const kConvolutionalOperator = ImplicitGemmKernel::kConvolutionalOperator;
-  static cutlass::conv::IteratorAlgorithm const kIteratorAlgorithm = ImplicitGemmKernel::kIteratorAlgorithm;
-  static cutlass::conv::StrideSupport const kStrideSupport = ImplicitGemmKernel::kStrideSupport;
-  static cutlass::conv::GroupMode const kGroupMode = ImplicitGemmKernel::kGroupMode;
+  using ElementA = typename UnderlyingKernel::ElementA;
+  using LayoutA = typename UnderlyingKernel::LayoutA;
+  using ElementB = typename UnderlyingKernel::ElementB;
+  using LayoutB = typename UnderlyingKernel::LayoutB;
+  using ElementC = typename UnderlyingKernel::ElementC;
+  using LayoutC = typename UnderlyingKernel::LayoutC;
+  using ElementAccumulator = typename UnderlyingKernel::ElementAccumulator;
+  using ElementCompute = typename UnderlyingKernel::ElementCompute;
+  using OperatorClass = typename UnderlyingKernel::OperatorClass;
+  using ArchTag = typename UnderlyingKernel::ArchTag;
+  using ThreadblockShape = typename UnderlyingKernel::ThreadblockShape;
+  using WarpShape = typename UnderlyingKernel::WarpShape;
+  using InstructionShape = typename UnderlyingKernel::InstructionShape;
+  using ThreadblockSwizzle = typename UnderlyingKernel::ThreadblockSwizzle;
+  using EpilogueOutputOp = typename UnderlyingKernel::EpilogueOutputOp;
+  static int const kStages = UnderlyingKernel::kStages;
+  static int const kConvDim = UnderlyingKernel::kConvDim;
+  using WarpMmaOperator = typename UnderlyingKernel::WarpMmaOperator;
+  using ArchMmaOperator = typename UnderlyingKernel::ArchMmaOperator;
+  using MathOperator = typename UnderlyingKernel::MathOperator; 
+
+  static cutlass::conv::Operator const kConvolutionalOperator = UnderlyingKernel::kConvolutionalOperator;
+  static cutlass::conv::IteratorAlgorithm const kIteratorAlgorithm = UnderlyingKernel::kIteratorAlgorithm;
+  static cutlass::conv::StrideSupport const kStrideSupport = UnderlyingKernel::kStrideSupport;
+  static cutlass::conv::GroupMode const kGroupMode = UnderlyingKernel::kGroupMode;
 
   static int const kWarpCount = 
     (ThreadblockShape::kM / WarpShape::kM) * 
     (ThreadblockShape::kN / WarpShape::kN) *
     (ThreadblockShape::kK / WarpShape::kK);
 
   /// Argument structure
-  using Arguments = typename ImplicitGemmKernel::Arguments;
+  using Arguments = typename UnderlyingKernel::Arguments;
 
 private:
 
   /// Kernel parameters object
-  typename ImplicitGemmKernel::Params params_;
+  typename UnderlyingKernel::Params params_;
 
 public:
 
   /// Constructs Implicit GEMM
   ImplicitGemmConvolution() { }
 
   /// Determines whether the Implicit GEMM can execute the given problem.
   static Status can_implement(Arguments const &args) {
 
     // dispatch to iterators
-    Status status = ImplicitGemmKernel::Mma::IteratorA::can_implement(args.problem_size);
+    Status status = UnderlyingKernel::Mma::IteratorA::can_implement(args.problem_size);
     if (Status::kSuccess != status) {
       return status;
     }
 
-    status = ImplicitGemmKernel::Mma::IteratorB::can_implement(args.problem_size);
+    status = UnderlyingKernel::Mma::IteratorB::can_implement(args.problem_size);
     if (Status::kSuccess != status) {
       return status;
     }
 
     // check group conv constraint
     if (args.problem_size.groups != 1) {
       if (kGroupMode == conv::GroupMode::kNone) {
@@ -134,17 +134,23 @@
       if (kGroupMode == conv::GroupMode::kSingleGroup && k_per_group % ThreadblockShape::kN) {
         return Status::kErrorInvalidProblem;
       }
       // ThreadblockShape::kN should be divisible by k_per_group, one CTA calculate multiple groups
       if (kGroupMode == conv::GroupMode::kMultipleGroup && ThreadblockShape::kN % k_per_group) {
         return Status::kErrorInvalidProblem;
       }
+
+      // current optimized iterator algo only supports SingleGroup mode
+      if (kIteratorAlgorithm == IteratorAlgorithm::kOptimized &&
+        kGroupMode != conv::GroupMode::kSingleGroup) {
+        return Status::kErrorInvalidProblem;
+      }
     }
 
-    static int const kAlignmentC = ImplicitGemmKernel::Epilogue::OutputTileIterator::kElementsPerAccess;
+    static int const kAlignmentC = UnderlyingKernel::Epilogue::OutputTileIterator::kElementsPerAccess;
     if (kConvolutionalOperator == conv::Operator::kFprop) {
       if (args.problem_size.K % kAlignmentC)
         return Status::kErrorMisalignedOperand;
     } else if (kConvolutionalOperator == conv::Operator::kDgrad) {
        if (args.problem_size.C % kAlignmentC)
         return Status::kErrorMisalignedOperand;
     } else if (kConvolutionalOperator == conv::Operator::kWgrad) {
@@ -152,23 +158,14 @@
         return Status::kErrorMisalignedOperand;
     }
 
     // check for unsupported problem sizes for strided dgrad implementation
     if (kConvolutionalOperator == conv::Operator::kDgrad && 
       kStrideSupport == conv::StrideSupport::kStrided) {
 
-      // Unity stride (1x1) is supported by strided dgrad but disabled for performance 
-      // reasons. For unity stride, use strided dgrad optimized unity stride specialization.
-      // Note that unit tests strided dgrad for unity stride to make sure that strided 
-      // dgrad implemetnation is functionaly sound. 
-      // Strided dgrad implementation also support mixed strides, i.e., (1x2) and (2x1)
-      if(args.problem_size.stride_h == 1 && args.problem_size.stride_w == 1) {
-        return Status::kErrorNotSupported;
-      }
-
       // split-k (serial or parallel) is not supported for strided dgrad
       if(args.problem_size.split_k_slices > 1) {
         return Status::kErrorNotSupported;
       }
       
       // dilation > {1x1} is not supported for strided dgrad
       if(args.problem_size.dilation_h > 1 || args.problem_size.dilation_w > 1) {
@@ -245,23 +242,23 @@
 
       if (status != cudaSuccess) {
         return Status::kErrorInternal;
       }
     }
 
     // initialize the params structure from the arguments
-    params_ = typename ImplicitGemmKernel::Params(
+    params_ = typename UnderlyingKernel::Params(
     	args,
     	static_cast<int *>(workspace)
     );
     
-    int smem_size = int(sizeof(typename ImplicitGemmKernel::SharedStorage));
+    int smem_size = int(sizeof(typename UnderlyingKernel::SharedStorage));
 
     if (smem_size >= (48 << 10)) {
-      cudaError_t result = cudaFuncSetAttribute(cutlass::Kernel<ImplicitGemmKernel>,
+      cudaError_t result = cudaFuncSetAttribute(cutlass::Kernel<UnderlyingKernel>,
                                     cudaFuncAttributeMaxDynamicSharedMemorySize,
                                     smem_size);
 
       if (result != cudaSuccess) {
         return Status::kErrorInternal;
       }
     }
@@ -288,17 +285,17 @@
 
 
     ThreadblockSwizzle threadblock_swizzle;
 
     dim3 grid = threadblock_swizzle.get_grid_shape(params_.grid_tiled_shape);
     dim3 block(32 * kWarpCount, 1, 1);
 
-    int smem_size = int(sizeof(typename ImplicitGemmKernel::SharedStorage));
+    int smem_size = int(sizeof(typename UnderlyingKernel::SharedStorage));
 
-    cutlass::Kernel<ImplicitGemmKernel><<<grid, block, smem_size, stream>>>(params_);
+    cutlass::Kernel<UnderlyingKernel><<<grid, block, smem_size, stream>>>(params_);
 
     cudaError_t result = cudaGetLastError();
 
     return result == cudaSuccess ? Status::kSuccess : Status::kErrorInternal;
   }
 
   /// Runs the kernel using initialized state.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/conv/device/implicit_gemm_convolution_fusion.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/conv/device/implicit_gemm_convolution_fusion.h`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/conv/kernel/default_conv2d.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/conv/kernel/default_conv2d.h`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/conv/kernel/default_conv2d_dgrad.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/conv/kernel/default_conv2d_dgrad.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/conv/kernel/default_conv2d_fprop.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/conv/kernel/default_conv2d_fprop.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/conv/kernel/default_conv2d_fprop_fusion.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/conv/kernel/default_conv2d_fprop_fusion.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/conv/kernel/default_conv2d_fprop_with_broadcast.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/conv/kernel/default_conv2d_fprop_with_broadcast.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/conv/kernel/default_conv2d_fprop_with_reduction.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/conv/kernel/default_conv2d_fprop_with_reduction.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/conv/kernel/default_conv2d_group_fprop.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/default_mma_softmax_mainloop_fusion.h`

 * *Files 23% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -24,199 +24,137 @@
  * DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR
  * SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER
  * CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,
  * OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
  * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
  *
  **************************************************************************************************/
-
 /*! \file
-    \brief 
-    Default kernel-level implicit GEMM convolution definitions combine threadblock-scoped 
-      matrix multiply-add with the appropriate threadblock-scoped epilogue.  
+    \brief Template for a pipelined softmax-GEMM kernel.
 */
 
 #pragma once
 
 #include "cutlass/cutlass.h"
-#include "cutlass/conv/kernel/default_conv2d.h"
+#include "cutlass/numeric_types.h"
+#include "cutlass/arch/arch.h"
 
-#include "cutlass/conv/threadblock/conv2d_fprop_activation_tile_access_iterator_analytic.h"
-#include "cutlass/conv/threadblock/conv2d_fprop_activation_tile_access_iterator_optimized.h"
-#include "cutlass/conv/threadblock/conv2d_fprop_activation_tile_access_iterator_fixed_channels.h"
-#include "cutlass/conv/threadblock/conv2d_fprop_activation_tile_access_iterator_few_channels.h"
-
-#include "cutlass/conv/threadblock/conv2d_fprop_filter_tile_access_iterator_analytic.h"
-#include "cutlass/conv/threadblock/conv2d_fprop_filter_tile_access_iterator_optimized.h"
-#include "cutlass/conv/threadblock/conv2d_fprop_filter_tile_access_iterator_fixed_channels.h"
-#include "cutlass/conv/threadblock/conv2d_fprop_filter_tile_access_iterator_few_channels.h"
+#include "cutlass/layout/matrix.h"
+#include "cutlass/gemm/threadblock/default_mma_core.h"
+#include "cutlass/gemm/threadblock/mma_softmax_mainloop_fusion_multistage.h"
+#include "cutlass/transform/threadblock/predicated_scale_bias_vector_iterator.h"
+#include "cutlass/transform/threadblock/predicated_scale_bias_vector_access_iterator.h"
+#include "cutlass/transform/threadblock/regular_scale_bias_vector_access_iterator.h"
+#include "cutlass/gemm/warp/scale_bias_tile_iterator.h"
+#include "cutlass/transform/threadblock/predicated_tile_iterator.h"
 
-/////////////////////////////////////////////////////////////////////////////////////////////////
+////////////////////////////////////////////////////////////////////////////////
 
 namespace cutlass {
-namespace conv {
-namespace kernel {
+namespace gemm {
+namespace threadblock {
 
-/////////////////////////////////////////////////////////////////////////////////////////////////
-/// Defines a kernel for Conv2dGroupFpro
-template <
-  typename ElementA,
-  typename LayoutA,
-  typename ElementB,
-  typename LayoutB,
-  typename ElementC,
-  typename LayoutC,
-  typename ElementAccumulator,
-  typename OperatorClass,
-  typename ArchTag,
-  typename ThreadblockShape,
-  typename WarpShape,
-  typename InstructionShape,
-  typename EpilogueOutputOp,
-  typename ThreadblockSwizzle,
-  int Stages,
-  typename MathOperatorTag,
-  conv::GroupMode GroupMode,
-  conv::IteratorAlgorithm IteratorAlgorithm = IteratorAlgorithm::kOptimized,
-  conv::StrideSupport StrideSupport = StrideSupport::kStrided,
-  /// Access granularity of A matrix in units of elements
-  int AlignmentA = 128 / cutlass::sizeof_bits<ElementA>::value,
-  /// Access granularity of B matrix in units of elements
-  int AlignmentB = 128 / cutlass::sizeof_bits<ElementB>::value
-> struct DefaultConv2dGroupFprop;
-
-/////////////////////////////////////////////////////////////////////////////////////////////////
-//                         OpClassTensorOp convolutions 
-/////////////////////////////////////////////////////////////////////////////////////////////////
+////////////////////////////////////////////////////////////////////////////////
 
-/// Defines a kernel for Conv2dGroupFprop specialization for Analytic IteratorAlgorithm and multistage 
-/// pipeline.
 template <
-  typename ElementA,
-  typename LayoutA,
-  typename ElementB,
-  typename LayoutB,
-  typename ElementC,
-  typename LayoutC,
-  typename ElementAccumulator,
-  typename ArchTag,
-  typename ThreadblockShape,
-  typename WarpShape,
-  typename InstructionShape,
-  typename EpilogueOutputOp,
-  typename ThreadblockSwizzle,
-  int Stages,
-  typename MathOperatorTag,
-  conv::GroupMode GroupMode,
-  conv::StrideSupport StrideSupport, 
-  int AlignmentA,
-  int AlignmentB
->
-struct DefaultConv2dGroupFprop <
-  ElementA,
-  LayoutA,
-  ElementB,
-  LayoutB,
-  ElementC,
-  LayoutC,
-  ElementAccumulator,
-  arch::OpClassTensorOp,
-  ArchTag,
-  ThreadblockShape,
-  WarpShape,
-  InstructionShape,
-  EpilogueOutputOp,
-  ThreadblockSwizzle,
-  Stages,
-  MathOperatorTag,
-  GroupMode,
-  IteratorAlgorithm::kAnalytic,
-  StrideSupport,
-  AlignmentA,
-  AlignmentB
-> {
+    /// Element type for A matrix operand
+    typename ElementA,
+    /// Layout type for A matrix operand
+    typename LayoutA,
+    /// Access granularity of A matrix in units of elements
+    int kAlignmentA,
+    /// Element type for B matrix operand
+    typename ElementB,
+    /// Layout type for B matrix operand
+    typename LayoutB,
+    /// Access granularity of B matrix in units of elements
+    int kAlignmentB,
+    /// Element type for Scale/Bias vectors
+    typename ElementScaleBias,
+    /// Layout type for Scale/Bias vectors
+    typename LayoutScaleBias,
+    /// Element type for internal accumulation
+    typename ElementAccumulator,
+    /// Layout type for C and D matrix operands
+    typename LayoutC,
+    /// Operator class tag
+    typename OperatorClass,
+    /// Tag indicating architecture to tune for
+    typename ArchTag,
+    /// Threadblock-level tile size (concept: GemmShape)
+    typename ThreadblockShape,
+    /// Warp-level tile size (concept: GemmShape)
+    typename WarpShape,
+    /// Instruction-level tile size (concept: GemmShape)
+    typename InstructionShape,
+    /// Number of stages used in the pipelined mainloop
+    int Stages,
+    /// Whether problem has been transformed. This determines to which operand
+    /// the softmax is applied.
+    bool InternalTranspose,
+    /// Operation perfomed by GEMM
+    typename Operator,
+    /// Store the accumulators in row major or column major.  Row major is used
+    /// when output layout is interleaved.
+    bool AccumulatorsInRowMajor = false,
+    /// Use zfill or predicate for SM80 out-of-bound cp.async 
+    SharedMemoryClearOption SharedMemoryClear = SharedMemoryClearOption::kNone
+    >
+struct DefaultMmaSoftmaxMainloopFusion {
+
+  static cutlass::arch::CacheOperation::Kind const CacheOpA =
+      ((sizeof_bits<ElementA>::value * kAlignmentA) == 128)
+          ? cutlass::arch::CacheOperation::Global
+          : cutlass::arch::CacheOperation::Always;
+
+  static cutlass::arch::CacheOperation::Kind const CacheOpB =
+      ((sizeof_bits<ElementB>::value * kAlignmentB) == 128)
+          ? cutlass::arch::CacheOperation::Global
+          : cutlass::arch::CacheOperation::Always;
+
+  static cutlass::arch::CacheOperation::Kind const CacheOpGammaBeta = CacheOpA;
 
-  // Define the core components from GEMM
+  // Define the MmaCore components
   using MmaCore = typename cutlass::gemm::threadblock::DefaultMmaCore<
-      ThreadblockShape, WarpShape, InstructionShape, ElementA, layout::RowMajor,
-      ElementB, layout::ColumnMajor, ElementAccumulator, layout::RowMajor, arch::OpClassTensorOp,
-      Stages, MathOperatorTag>;
+      ThreadblockShape, WarpShape, InstructionShape, ElementA, LayoutA,
+      ElementB, LayoutB, ElementAccumulator, layout::RowMajor, arch::OpClassTensorOp,
+      Stages, Operator, false, CacheOpA, CacheOpB>;
 
   // Define iterators over tiles from the A operand
   using ThreadMapA = typename MmaCore::IteratorThreadMapA;
-  using AccessTypeA = cutlass::AlignedArray<ElementA, AlignmentA>;
+  using AccessTypeA = cutlass::Array<ElementA, kAlignmentA>;
   using IteratorA =
-    cutlass::conv::threadblock::Conv2dFpropActivationTileAccessIteratorAnalytic<
-      cutlass::MatrixShape<ThreadblockShape::kM, ThreadblockShape::kK>,
-      ElementA, LayoutA,
-      ThreadMapA,
-      AccessTypeA,
-      GroupMode
-    >;
-
-  using SmemIteratorA = typename MmaCore::SmemIteratorA;
+      cutlass::transform::threadblock::PredicatedTileAccessIterator<
+          cutlass::MatrixShape<ThreadblockShape::kM, ThreadblockShape::kK>,
+          ElementA, LayoutA, 1, ThreadMapA, AccessTypeA>;
 
   // Define iterators over tiles from the B operand
   using ThreadMapB = typename MmaCore::IteratorThreadMapB;
-  using AccessTypeB = cutlass::AlignedArray<ElementB, AlignmentB>;
+  using AccessTypeB = cutlass::Array<ElementB, kAlignmentB>;
   using IteratorB =
-    cutlass::conv::threadblock::Conv2dFpropFilterTileAccessIteratorAnalytic<
-      cutlass::MatrixShape<ThreadblockShape::kK, ThreadblockShape::kN>,
-      ElementB, LayoutB,
-      ThreadMapB,
-      AccessTypeB,
-      GroupMode
-    >;
-  
-  using SmemIteratorB = typename MmaCore::SmemIteratorB;
-
-  // Warp-level GEMM components
-  using WarpMmaTensorOp = typename MmaCore::MmaTensorOp;
-  using MmaPolicy = typename MmaCore::MmaPolicy;
-
-  static cutlass::arch::CacheOperation::Kind const CacheOpB =
-      ((sizeof_bits<ElementB>::value * AlignmentB) == 128)
-          ? cutlass::arch::CacheOperation::Global
-          : cutlass::arch::CacheOperation::Always;
-
-  // Define the Mma
-  using Mma = threadblock::ImplicitGemmMultistage<
-    ThreadblockShape,
-    IteratorA,
-    SmemIteratorA,
-    arch::CacheOperation::Always,
-    IteratorB,
-    SmemIteratorB,
-    CacheOpB,
-    MmaPolicy,
-    Stages 
-  >;
-
-  static const int kPartitionsK = ThreadblockShape::kK / WarpShape::kK;
-
-  // Define the epilogue
-  using Epilogue = typename epilogue::threadblock::DefaultEpilogueTensorOp<
-    ThreadblockShape,
-    WarpMmaTensorOp,
-    kPartitionsK,
-    EpilogueOutputOp,
-    EpilogueOutputOp::kCount
-  >::Epilogue;
-
-  // Define the kernel
-  using Kernel = cutlass::conv::kernel::ImplicitGemmConvolution<
-    Mma,
-    Epilogue,
-    ThreadblockSwizzle,
-    conv::Operator::kFprop,
-    Conv2dProblemSize,
-    GroupMode
-  >;
+      cutlass::transform::threadblock::PredicatedTileAccessIterator<
+          cutlass::MatrixShape<ThreadblockShape::kK, ThreadblockShape::kN>,
+          ElementB, LayoutB, 0, ThreadMapB, AccessTypeB>;
+
+  /// Define iterators over tiles from scale/bias vectors
+  using IteratorNormSum =
+      cutlass::transform::threadblock::PredicatedScaleBiasVectorIterator<
+          cutlass::MatrixShape<1, WarpShape::kN>,
+          ElementScaleBias,
+          LayoutScaleBias>;
+
+  // Define the threadblock-scoped multistage matrix multiply
+  using ThreadblockMma = cutlass::gemm::threadblock::MmaSoftmaxMainloopFusionMultistage<
+      typename MmaCore::Shape, IteratorA, typename MmaCore::SmemIteratorA,
+      MmaCore::kCacheOpA, IteratorB, typename MmaCore::SmemIteratorB,
+      MmaCore::kCacheOpB, IteratorNormSum,
+      ElementAccumulator, layout::RowMajor,
+      typename MmaCore::MmaPolicy, Stages, InternalTranspose, SharedMemoryClear>;
 };
 
-/////////////////////////////////////////////////////////////////////////////////////////////////
+////////////////////////////////////////////////////////////////////////////////
 
-} // namespace kernel
-} // namespace conv
-} // namespace cutlass
+} // namespace threadblock
+} // namespace gemm
+} // namespace cutlass 
 
-/////////////////////////////////////////////////////////////////////////////////////////////////
+////////////////////////////////////////////////////////////////////////////////
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/conv/kernel/default_conv2d_wgrad.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/conv/kernel/default_conv2d_wgrad.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/conv/kernel/default_conv2d_wgrad_fusion.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/conv/kernel/default_conv2d_wgrad_fusion.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/conv/kernel/default_conv3d_dgrad.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/conv/kernel/default_conv3d_dgrad.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/conv/kernel/default_conv3d_fprop.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/conv/kernel/default_conv3d_fprop.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/conv/kernel/default_conv3d_fprop_fusion.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/conv/kernel/default_conv3d_fprop_fusion.h`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/conv/kernel/default_conv3d_wgrad.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/conv/kernel/default_conv3d_wgrad.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/conv/kernel/default_depthwise_fprop.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/default_mma_layernorm_mainloop_fusion.h`

 * *Files 17% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -24,195 +24,155 @@
  * DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR
  * SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER
  * CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,
  * OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
  * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
  *
  **************************************************************************************************/
-
 /*! \file
-    \brief 
-    Default kernel-level Depthwise implicit GEMM convolution definitions combine threadblock-scoped 
-      matrix multiply-add with the appropriate threadblock-scoped epilogue.  
+    \brief Template for a pipelined GEMM kernel. Does not compute batching or support split-K.
 */
 
 #pragma once
 
 #include "cutlass/cutlass.h"
-#include "cutlass/conv/kernel/default_conv2d.h"
-
-#include "cutlass/conv/threadblock/depthwise_mma_core_with_lane_access_size.h"
-
-#include "cutlass/conv/threadblock/conv2d_fprop_activation_tile_access_iterator_analytic.h"
+#include "cutlass/numeric_types.h"
+#include "cutlass/arch/arch.h"
 
-#include "cutlass/conv/threadblock/conv2d_fprop_filter_tile_access_iterator_analytic.h"
-#include "cutlass/conv/threadblock/depthwise_fprop_pipelined.h"
+#include "cutlass/layout/matrix.h"
+#include "cutlass/gemm/threadblock/default_mma_core.h"
+#include "cutlass/gemm/threadblock/mma_layernorm_mainloop_fusion_multistage.h"
+#include "cutlass/transform/threadblock/predicated_scale_bias_vector_iterator.h"
+#include "cutlass/transform/threadblock/predicated_scale_bias_vector_access_iterator.h"
+#include "cutlass/transform/threadblock/regular_scale_bias_vector_access_iterator.h"
+#include "cutlass/gemm/warp/scale_bias_tile_iterator.h"
+#include "cutlass/transform/threadblock/predicated_tile_iterator.h"
 
-/////////////////////////////////////////////////////////////////////////////////////////////////
+////////////////////////////////////////////////////////////////////////////////
 
 namespace cutlass {
-namespace conv {
-namespace kernel {
+namespace gemm {
+namespace threadblock {
 
-/////////////////////////////////////////////////////////////////////////////////////////////////
-/// Defines a kernel for Conv2dFprop
-template <
-  typename ElementA,
-  typename LayoutA,
-  typename ElementB,
-  typename LayoutB,
-  typename ElementC,
-  typename LayoutC,
-  typename ElementAccumulator,
-  typename OperatorClass,
-  typename ArchTag,
-  typename ThreadblockShape,
-  typename WarpShape,
-  typename InstructionShape,
-  typename EpilogueOutputOp,
-  typename ThreadblockSwizzle,
-  int Stages,
-  typename MathOperatorTag,
-  conv::IteratorAlgorithm IteratorAlgorithm = IteratorAlgorithm::kAnalytic,
-  conv::StrideSupport StrideSupport = StrideSupport::kStrided,
-  /// Access granularity of A matrix in units of elements
-  int AlignmentA = 128 / cutlass::sizeof_bits<ElementA>::value,
-  /// Access granularity of B matrix in units of elements
-  int AlignmentB = cutlass::sizeof_bits<ElementB>::value / cutlass::sizeof_bits<ElementB>::value
-> struct DefaultDepthwiseFprop;
-
-/////////////////////////////////////////////////////////////////////////////////////////////////
-//                            OpClassSimt convolutions
-/////////////////////////////////////////////////////////////////////////////////////////////////
+////////////////////////////////////////////////////////////////////////////////
 
-/// Defines a kernel for Depthwise specialization for Analytic IteratorAlgorithm, 
-/// 2 stage pipeline, and FFMA-based mainloop for SM50
 template <
-  typename ElementA,
-  typename LayoutA,
-  typename ElementB,
-  typename LayoutB,
-  typename ElementC,
-  typename LayoutC,
-  typename ElementAccumulator,
-  typename ArchTag,
-  typename ThreadblockShape,
-  typename WarpShape,
-  typename InstructionShape,
-  typename EpilogueOutputOp,
-  typename ThreadblockSwizzle,
-  typename MathOperatorTag,
-  conv::StrideSupport StrideSupport,
-  int AlignmentA,
-  int AlignmentB
->
-struct DefaultDepthwiseFprop <
-  ElementA,
-  LayoutA,
-  ElementB,
-  LayoutB,
-  ElementC,
-  LayoutC,
-  ElementAccumulator,
-  arch::OpClassSimt,
-  ArchTag,
-  ThreadblockShape,
-  WarpShape,
-  InstructionShape,
-  EpilogueOutputOp,
-  ThreadblockSwizzle,
-  2,
-  MathOperatorTag, //   cutlass::arch::OpMultiplyAdd
-  IteratorAlgorithm::kAnalytic,
-  StrideSupport,
-  AlignmentA,
-  AlignmentB
-> {
-
-  // Define the core components from GEMM
-  using MmaCore = typename cutlass::conv::threadblock::DepthwiseMmaCoreWithLaneAccessSize<
-      ThreadblockShape,
-      WarpShape,
-      InstructionShape,
-      ElementA,
-      layout::RowMajor,
-      ElementB,
-      layout::ColumnMajor,
-      ElementAccumulator,
-      layout::RowMajor,
-      arch::OpClassSimt,
-      128,
-      sizeof_bits<ElementB>::value,
-      2,
-      MathOperatorTag>;
+    /// Element type for A matrix operand
+    typename ElementA,
+    /// Layout type for A matrix operand
+    typename LayoutA,
+    /// Access granularity of A matrix in units of elements
+    int kAlignmentA,
+    /// Element type for B matrix operand
+    typename ElementB,
+    /// Layout type for B matrix operand
+    typename LayoutB,
+    /// Access granularity of B matrix in units of elements
+    int kAlignmentB,
+    /// Element type for Scale/Bias vectors
+    typename ElementScaleBias,
+    /// Layout type for Scale/Bias vectors
+    typename LayoutScaleBias,
+    /// Element type for internal accumulation
+    typename ElementAccumulator,
+    /// Layout type for C and D matrix operands
+    typename LayoutC,
+    /// Operator class tag
+    typename OperatorClass,
+    /// Tag indicating architecture to tune for
+    typename ArchTag,
+    /// Threadblock-level tile size (concept: GemmShape)
+    typename ThreadblockShape,
+    /// Warp-level tile size (concept: GemmShape)
+    typename WarpShape,
+    /// Instruction-level tile size (concept: GemmShape)
+    typename InstructionShape,
+    /// Number of stages used in the pipelined mainloop
+    int Stages,
+    /// Operation perfomed by GEMM
+    typename Operator,
+    /// Store the accumulators in row major or column major.  Row major is used
+    /// when output layout is interleaved.
+    bool AccumulatorsInRowMajor = false,
+    /// Use zfill or predicate for SM80 out-of-bound cp.async 
+    SharedMemoryClearOption SharedMemoryClear = SharedMemoryClearOption::kNone
+    >
+struct DefaultMmaLayernormMainloopFusion {
+
+  static cutlass::arch::CacheOperation::Kind const CacheOpA =
+      ((sizeof_bits<ElementA>::value * kAlignmentA) == 128)
+          ? cutlass::arch::CacheOperation::Global
+          : cutlass::arch::CacheOperation::Always;
+
+  static cutlass::arch::CacheOperation::Kind const CacheOpB =
+      ((sizeof_bits<ElementB>::value * kAlignmentB) == 128)
+          ? cutlass::arch::CacheOperation::Global
+          : cutlass::arch::CacheOperation::Always;
+
+  static cutlass::arch::CacheOperation::Kind const CacheOpGammaBeta = CacheOpA;
+
+  // Define the MmaCore components
+  using MmaCore = typename cutlass::gemm::threadblock::DefaultMmaCore<
+      ThreadblockShape, WarpShape, InstructionShape, ElementA, LayoutA,
+      ElementB, LayoutB, ElementAccumulator, layout::RowMajor, arch::OpClassTensorOp,
+      Stages, Operator, false, CacheOpA, CacheOpB>;
 
   // Define iterators over tiles from the A operand
   using ThreadMapA = typename MmaCore::IteratorThreadMapA;
+  using AccessTypeA = cutlass::Array<ElementA, kAlignmentA>;
   using IteratorA =
-    cutlass::conv::threadblock::TileIterator<
-      cutlass::conv::threadblock::Conv2dFpropActivationTileAccessIteratorAnalytic<
-        cutlass::MatrixShape<ThreadblockShape::kM, ThreadblockShape::kK>,
-        ElementA, LayoutA,
-        ThreadMapA
-      >
-    >;
-
-  using SmemIteratorA = typename MmaCore::SmemIteratorA;
+      cutlass::transform::threadblock::PredicatedTileAccessIterator<
+          cutlass::MatrixShape<ThreadblockShape::kM, ThreadblockShape::kK>,
+          ElementA, LayoutA, 1, ThreadMapA, AccessTypeA>;
 
   // Define iterators over tiles from the B operand
   using ThreadMapB = typename MmaCore::IteratorThreadMapB;
-  using AccessTypeB = cutlass::AlignedArray<ElementB, AlignmentB>;
+  using AccessTypeB = cutlass::Array<ElementB, kAlignmentB>;
   using IteratorB =
-    cutlass::conv::threadblock::TileIterator<
-      cutlass::conv::threadblock::Conv2dFpropFilterTileAccessIteratorAnalytic<
-        cutlass::MatrixShape<ThreadblockShape::kK, ThreadblockShape::kN>,
-        ElementB, LayoutB,
-        ThreadMapB,
-        AccessTypeB,
-        cutlass::conv::GroupMode::kDepthwise
-      >
-    >;
-  
-  using SmemIteratorB = typename MmaCore::SmemIteratorB;
-
-  // Warp-level GEMM components
-  using WarpMmaSimtOp = typename MmaCore::MmaWarpSimt;
-  using MmaPolicy = typename MmaCore::MmaPolicy;
-
-  // Define the Mma
-  using Mma = threadblock::DepthwiseFpropPipelined<
-    ThreadblockShape,
-    IteratorA,
-    SmemIteratorA,
-    IteratorB,
-    SmemIteratorB,
-    ElementC,
-    LayoutC,
-    MmaPolicy
-  >;
-
-  // Define the epilogue
-  using Epilogue = typename epilogue::threadblock::DefaultEpilogueSimt<
-    ThreadblockShape,
-    WarpMmaSimtOp,
-    EpilogueOutputOp,
-    EpilogueOutputOp::kCount
-  >::Epilogue;
-
-  // Define the kernel
-  using Kernel = cutlass::conv::kernel::ImplicitGemmConvolution<
-    Mma,
-    Epilogue,
-    ThreadblockSwizzle,
-    conv::Operator::kFprop,
-    Conv2dProblemSize,
-    cutlass::conv::GroupMode::kDepthwise
-  >;
+      cutlass::transform::threadblock::PredicatedTileAccessIterator<
+          cutlass::MatrixShape<ThreadblockShape::kK, ThreadblockShape::kN>,
+          ElementB, LayoutB, 0, ThreadMapB, AccessTypeB>;
+
+  /// Define iterators over tiles from scale/bias vectors
+  using IteratorVarMean =
+      cutlass::transform::threadblock::PredicatedScaleBiasVectorIterator<
+          cutlass::MatrixShape<1, WarpShape::kN>,
+          ElementScaleBias,
+          LayoutScaleBias>;
+
+  /// Define iterators over tiles from scale/bias vectors
+  using IteratorGammaBeta =
+      cutlass::transform::threadblock::PredicatedScaleBiasVectorAccessIterator<
+          cutlass::MatrixShape<1, ThreadblockShape::kK>, ElementScaleBias,
+          LayoutScaleBias>;
+
+  using SmemIteratorGammaBeta =
+      cutlass::transform::threadblock::RegularScaleBiasVectorAccessIterator<
+          cutlass::MatrixShape<1, ThreadblockShape::kK>, ElementScaleBias,
+          LayoutScaleBias>;
+
+  static int const kThreadCount = 32;
+
+  // Warp-level iterators to load scale and bias vectors
+  using WarpIteratorGammaBeta = cutlass::gemm::warp::ScaleBiasTileIterator<
+      MatrixShape<WarpShape::kM, WarpShape::kK>, ElementScaleBias,
+      LayoutScaleBias, MatrixShape<InstructionShape::kM, InstructionShape::kK>,
+      typename MmaCore::MmaTensorOp::IteratorA::Base::Policy, kThreadCount,
+      MmaCore::WarpCount::kK>;
+
+  // Define the threadblock-scoped multistage matrix multiply
+  using ThreadblockMma = cutlass::gemm::threadblock::MmaLayernormMainloopFusionMultistage<
+      typename MmaCore::Shape, IteratorA, typename MmaCore::SmemIteratorA,
+      MmaCore::kCacheOpA, IteratorB, typename MmaCore::SmemIteratorB,
+      MmaCore::kCacheOpB, IteratorVarMean, IteratorGammaBeta, SmemIteratorGammaBeta,
+      CacheOpGammaBeta,
+      ElementAccumulator, layout::RowMajor,
+      typename MmaCore::MmaPolicy, WarpIteratorGammaBeta, Stages, SharedMemoryClear>;
 };
 
-/////////////////////////////////////////////////////////////////////////////////////////////////
+////////////////////////////////////////////////////////////////////////////////
 
-} // namespace kernel
-} // namespace conv
-} // namespace cutlass
+} // namespace threadblock
+} // namespace gemm
+} // namespace cutlass 
 
-/////////////////////////////////////////////////////////////////////////////////////////////////
+////////////////////////////////////////////////////////////////////////////////
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/conv/kernel/direct_convolution.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/conv/kernel/direct_convolution.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/conv/kernel/implicit_gemm_convolution.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/conv/kernel/implicit_gemm_convolution.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/conv/kernel/implicit_gemm_convolution_fusion.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/conv/kernel/implicit_gemm_convolution_fusion.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/conv/kernel/implicit_gemm_convolution_strided_dgrad.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/conv/kernel/implicit_gemm_convolution_strided_dgrad.h`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -385,24 +385,23 @@
     // Epilogue
     //
 
     EpilogueOutputOp output_op(params.output_op);
 
     // Construct the semaphore.
     int block_idx = threadblock_tile_idx.m() + threadblock_tile_idx.n() * params.grid_tiled_shape.m();
-
     Semaphore semaphore(params.semaphore + block_idx, thread_idx);
-    
+
     // Compute logical position within grid
     threadblock_tile_idx =
         threadblock_swizzle.get_tile_offset(params.grid_tiled_shape);
 
     // If performing a reduction via split-K, fetch the initial synchronization
     if (params.split_k_mode == SplitKMode::kSerial && params.grid_tiled_shape.k() > 1) {
-        
+
       // Fetch the synchronization lock initially but do not block.
       semaphore.fetch();
 
       // Indicate which position in a serial reduction the output operator is currently updating
       output_op.set_k_partition(threadblock_tile_idx.k(), params.grid_tiled_shape.k());
     }
 
@@ -417,74 +416,75 @@
       params.ptr_D,
       ConvOutputIteratorParameter::extent(params.problem_size),
       thread_idx,
       params.stride_h_divmod, params.stride_w_divmod,
       start_r, start_s,
       threadblock_offset
     );
-    
-    // Tile iterator reading from source accumulator tensor
-    typename Epilogue::OutputTileIterator iterator_C(
-      params.iterator_C,
-      params.ptr_C,
-      ConvOutputIteratorParameter::extent(params.problem_size),
-      thread_idx,
-      params.stride_h_divmod, params.stride_w_divmod,
-      start_r, start_s,
-      threadblock_offset
-    );
-
 
     // Construct the epilogue
     Epilogue epilogue(
-      shared_storage.epilogue, 
-      thread_idx, 
-      warp_idx, 
+      shared_storage.epilogue,
+      thread_idx,
+      warp_idx,
       lane_idx);
 
-    // Wait on the semaphore - this latency may have been covered by iterator construction
-    if (params.split_k_mode == SplitKMode::kSerial && params.grid_tiled_shape.k() > 1) {
-        
-      // For subsequent threadblocks, the source matrix is held in the 'D' tensor.
-      if (threadblock_tile_idx.k()) {
-        iterator_C = iterator_D;
-      }
+    if (output_op.is_source_needed())
+    {
+      // Tile iterator reading from source accumulator tensor
+      typename Epilogue::OutputTileIterator iterator_C(
+        params.iterator_C,
+        params.ptr_C,
+        ConvOutputIteratorParameter::extent(params.problem_size),
+        thread_idx,
+        params.stride_h_divmod, params.stride_w_divmod,
+        start_r, start_s,
+        threadblock_offset);
+
+      // Wait on the semaphore - this latency may have been covered by iterator construction
+      if (params.split_k_mode == SplitKMode::kSerial && params.grid_tiled_shape.k() > 1) {
+
+        // For subsequent threadblocks, the source matrix is held in the 'D' tensor.
+        if (threadblock_tile_idx.k()) {
+          iterator_C = iterator_D;
+        }
 
-      semaphore.wait(threadblock_tile_idx.k());
+        semaphore.wait(threadblock_tile_idx.k());
+      }
 
+      // Run epilogue with addend source iterator
+      epilogue(output_op, iterator_D, accumulators, iterator_C);
     }
-    // Each split-k-slice writes to a unique tensor location
-    else if (params.split_k_mode == SplitKMode::kParallel) {
-      iterator_D.add_pointer_offset(threadblock_tile_idx.k() * 
-        cutlass::conv::implicit_gemm_tensor_c_size(ConvOperator, params.problem_size));
+    else
+    {
+      // Run epilogue without addend source iterator
+      epilogue(output_op, iterator_D, accumulators);
     }
 
-    // Run efficient epilogue
-    epilogue(output_op, iterator_D, accumulators, iterator_C);
-  
     //
     // Release the semaphore
     //
 
-    if (params.split_k_mode == SplitKMode::kSerial && params.grid_tiled_shape.k() > 1) { 
+    if (params.split_k_mode == SplitKMode::kSerial && params.grid_tiled_shape.k() > 1) {
 
       int lock = 0;
       if (params.grid_tiled_shape.k() == threadblock_tile_idx.k() + 1) {
 
         // The final threadblock resets the semaphore for subsequent grids.
         lock = 0;
       }
       else {
         // Otherwise, the semaphore is incremented
         lock = threadblock_tile_idx.k() + 1;
       }
-      
+
       semaphore.release(lock);
     }
-  } 
+
+  }
 };
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
 } // namespace kernel
 } // namespace conv
 } // namespace cutlass
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/conv/kernel/implicit_gemm_convolution_with_fused_epilogue.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/conv/kernel/implicit_gemm_convolution_with_fused_epilogue.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/conv/thread/depthwise_mma.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/conv/thread/depthwise_mma.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/conv2d_dgrad_filter_tile_access_iterator_analytic.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/conv2d_dgrad_filter_tile_access_iterator_analytic.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/conv2d_dgrad_filter_tile_access_iterator_optimized.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/conv2d_dgrad_filter_tile_access_iterator_optimized.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/conv2d_dgrad_output_gradient_tile_access_iterator_analytic.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/conv2d_dgrad_output_gradient_tile_access_iterator_analytic.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/conv2d_dgrad_output_gradient_tile_access_iterator_optimized.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/conv2d_dgrad_output_gradient_tile_access_iterator_optimized.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/conv2d_fprop_activation_tile_access_iterator_analytic.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/conv2d_fprop_activation_tile_access_iterator_analytic.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/conv2d_fprop_activation_tile_access_iterator_few_channels.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/conv2d_fprop_activation_tile_access_iterator_few_channels.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/conv2d_fprop_activation_tile_access_iterator_fixed_channels.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/conv2d_fprop_activation_tile_access_iterator_fixed_channels.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/conv2d_fprop_activation_tile_access_iterator_optimized.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/conv2d_fprop_activation_tile_access_iterator_optimized.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/conv2d_fprop_filter_tile_access_iterator_analytic.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/conv2d_fprop_filter_tile_access_iterator_analytic.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/conv2d_fprop_filter_tile_access_iterator_few_channels.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/conv2d_fprop_filter_tile_access_iterator_few_channels.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/conv2d_fprop_filter_tile_access_iterator_fixed_channels.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/conv2d_fprop_filter_tile_access_iterator_fixed_channels.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/conv2d_fprop_filter_tile_access_iterator_optimized.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/conv2d_fprop_filter_tile_access_iterator_optimized.h`

 * *Files 3% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -141,14 +141,15 @@
   LongIndex iteration_strided_;
   LongIndex iteration_vector_;
   char const *pointer_;
 
   uint32_t predicates_[kAccessesPerVector];
   int filter_rs_;
   int filter_c_;
+  int channels_per_group_;
 
   //
   // Assertions
   //
 
   // We map predicates into bits packed in this uint32_t container
   static_assert(ThreadMap::Iterations::kStrided < sizeof(predicates_) * 8,
@@ -171,28 +172,29 @@
     filter_rs_(0),
     filter_c_(0) {
 
     layout::PitchLinearCoord thread_coord = ThreadMap::initial_offset(thread_idx);
 
     filter_c_ = threadblock_offset.row() + thread_coord.contiguous();
     Index column = threadblock_offset.column() + thread_coord.strided();
+    channels_per_group_ = problem_size_.C / problem_size_.groups;
 
     CUTLASS_PRAGMA_UNROLL
     for (int s = 0; s < ThreadMap::Iterations::kStrided; ++s) {
       uint32_t pred = ((column + s * ThreadMap::Delta::kStrided < problem_size_.K) ? 1u : 0);
 
       CUTLASS_PRAGMA_UNROLL
       for (int v_idx = 0; v_idx < kAccessesPerVector; ++v_idx) {
         predicates_[v_idx] |= (pred << s);
       }
     }
 
     CUTLASS_PRAGMA_UNROLL
     for (int v_idx = 0; v_idx < kAccessesPerVector; ++v_idx) {
-      clear_mask(v_idx, filter_c_ + v_idx * AccessType::kElements >= problem_size_.C);
+      clear_mask(v_idx, filter_c_ + v_idx * AccessType::kElements >= channels_per_group_);
     }
 
     pointer_ += (
       params_.layout({filter_c_, column}) 
     ) * sizeof_bits<Element>::value / 8;
 
     set_iteration_index(0);
@@ -225,15 +227,15 @@
       filter_rs_ = 0;
       next = params_.inc_next_c;
       filter_c_ += params_.filter_c_delta;
     }
  
     CUTLASS_PRAGMA_UNROLL
     for (int v_idx = 0; v_idx < kAccessesPerVector; ++v_idx) {
-      clear_mask(v_idx, filter_c_ + v_idx * AccessType::kElements >= problem_size_.C);
+      clear_mask(v_idx, filter_c_ + v_idx * AccessType::kElements >= channels_per_group_);
     }
       
     pointer_ += next;
   }
 
   /// Clears the predicates
   CUTLASS_HOST_DEVICE
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/conv2d_params.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/conv2d_params.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/conv2d_tile_iterator.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/transform/threadblock/predicated_vector_access_iterator.h`

 * *Files 21% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -24,314 +24,394 @@
  * DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR
  * SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER
  * CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,
  * OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
  * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
  *
  **************************************************************************************************/
+
 /*! \file
-    \brief Template wraps the tile access iterator concept to load whole tiles from tensors in
-      memory used for implicit GEMM convolution.
+    \brief Templates implementing computing the addresses of loading small
+    vectors from the global memory.
 */
 
 #pragma once
 
 #include "cutlass/cutlass.h"
 #include "cutlass/array.h"
 #include "cutlass/coord.h"
-#include "cutlass/matrix_shape.h"
-#include "cutlass/tensor_ref.h"
-#include "cutlass/tensor_view.h"
 #include "cutlass/layout/pitch_linear.h"
-#include "cutlass/layout/tensor.h"
 #include "cutlass/layout/matrix.h"
-#include "cutlass/conv/convolution.h"
-#include "cutlass/conv/conv2d_problem_size.h"
+#include "cutlass/layout/tensor.h"
+#include "cutlass/matrix_coord.h"
+#include "cutlass/matrix_shape.h"
+#include "cutlass/tensor_ref.h"
 
-/////////////////////////////////////////////////////////////////////////////////////////////////
+////////////////////////////////////////////////////////////////////////////////
 
 namespace cutlass {
-namespace conv {
+namespace transform {
 namespace threadblock {
 
-/////////////////////////////////////////////////////////////////////////////////////////////////
+////////////////////////////////////////////////////////////////////////////////
+
+/// PredicatedVectorAccessIterator
+///
+template <
+    /// Shape of the vector accessed by the entire threadblock
+    typename Shape,
+    /// Shape of the vector accessed by the warp
+    typename WarpShape,
+    /// Type of Element
+    typename Element,
+    /// Layout of the vector
+    typename Layout,
+    /// Number of elements for each access
+    int ElementsPerAccess,
+    /// Support residual tile
+    bool EnableResidualAccess = false
+>
+class PredicatedVectorAccessIterator;
+
+////////////////////////////////////////////////////////////////////////////////
+
+/// Vector access iterator specialized for vectors, e.g. scale and bias
+/// Thread arrangements are for TensorOps
+///
+template <
+  typename Shape_, 
+  typename WarpShape_, 
+  typename Element_, 
+  int ElementsPerAccess, 
+  bool EnableResidualAccess
+>
+class PredicatedVectorAccessIterator <
+  Shape_,
+  WarpShape_,
+  Element_,
+  layout::PitchLinear,
+  ElementsPerAccess,
+  EnableResidualAccess
+> {
+  public:
+
+  using Shape = Shape_;
+  using WarpShape = WarpShape_;
+  using Element = Element_;
+  using Layout = layout::PitchLinear;
+
+  using Index = typename Layout::Index;
+  using LongIndex = typename Layout::LongIndex;
 
-template <typename TileAccessIterator_>
-class TileIterator {
-public:
-  using TileAccessIterator = TileAccessIterator_;
-
-  using Shape = typename TileAccessIterator::Shape;
-  using Element = typename TileAccessIterator::Element;
-  using Layout = typename TileAccessIterator::Layout;
+  using TensorRef = TensorRef<Element, Layout>;
+  using TensorView = TensorView<Element, Layout>;
   using TensorCoord = typename Layout::TensorCoord;
-  using ThreadMap = typename TileAccessIterator::ThreadMap;
-  using AccessType = typename TileAccessIterator::AccessType;
-  using TensorRef = typename TileAccessIterator::TensorRef;
-  using Index = typename TileAccessIterator::Index;
-  using LongIndex = typename TileAccessIterator::LongIndex;
-  static IteratorAlgorithm const kIteratorAlgorithm = TileAccessIterator::kIteratorAlgorithm;
-  static StrideSupport const kStrideSupport = TileAccessIterator::kStrideSupport;
-  using Params = typename TileAccessIterator::Params;
-  static int const kConvDim = TileAccessIterator::kConvDim;
-  using ConvProblemSize = typename TileAccessIterator::ConvProblemSize;
-  static int const kAccessesPerVector = TileAccessIterator::kAccessesPerVector;
-
-  /// Fragment object to be loaded or stored
-  using Fragment = cutlass::Array<
-    Element, 
-    ThreadMap::Iterations::kCount * ThreadMap::kElementsPerAccess>;
-
-private:
-
-  /// Internal state
-  TileAccessIterator tile_access_iterator_;
-
-public:
-
-  /// Constructor
-  CUTLASS_HOST_DEVICE
-  TileIterator(
-    Params const &params,
-    ConvProblemSize const &problem_size,
-    Element const *ptr,
-    int thread_idx,
-    MatrixCoord const &threadblock_offset = MatrixCoord()
-  ):
-    tile_access_iterator_(params, problem_size, ptr, thread_idx, threadblock_offset) { }
 
-  CUTLASS_HOST_DEVICE
-  static Params getParams(ConvProblemSize const &problem_size, Layout const &layout) {
-    return TileAccessIterator::getParams(problem_size, layout);
-  }
+  using ConstPointer = const Element *;
+  using NonConstPointer = typename platform::remove_const<Element>::type *;
 
-  /// Overrides the internal iteration index
-  CUTLASS_HOST_DEVICE
-  void set_iteration_index(Index index) {
-    tile_access_iterator_.set_iteration_index(index);
+//  static int const kElementsPerAccess = 128 / sizeof_bits<Element>::value;
+  static int const kElementsPerAccess = ElementsPerAccess;
+  static int const kThreads = 32;
+  static int const kRowsPerIteration = 8;
+  static int const kThreadsPerRow = kThreads / kRowsPerIteration;
+  static int const kThreadsPerRowMask = 0x3;
+  static int const kIterations = WarpShape::kContiguous / (kThreadsPerRow * kElementsPerAccess); 
+  static int const kWarpCountStrided = Shape::kStrided / WarpShape::kStrided;
+
+  using AccessType = AlignedArray<Element, kElementsPerAccess>;
+
+ private:
+  /// Internal pointer type permits fast address arithmetic
+  using BytePointer = char *;
+
+ private:
+  //
+  // Data members
+  //
+
+  /// Internal pointer to first access of tile
+  BytePointer pointer_;
+
+  /// Extent of tensor
+  TensorCoord extent_;
+
+  /// pointer offset of each thread
+  TensorCoord thread_offset_;
+
+  /// iteration index
+  LongIndex iteration_;
+
+  /// residual access
+  bool is_residual_;
+
+  /// residual offset of each thread
+  TensorCoord residual_offset_;
+
+ public:
+  /// Constructs a vector access iterator
+  CUTLASS_HOST_DEVICE
+  PredicatedVectorAccessIterator(
+    /// Pointer to the start of the vector
+    ConstPointer pointer,
+    /// Extent of vector
+    TensorCoord extent,
+    /// ID of each participating thread
+    int thread_id,
+    /// ID of each participating warp
+    int warp_id,
+    /// Initial offset of threadblock
+    TensorCoord const &threadblock_offset)
+    : pointer_(reinterpret_cast<BytePointer>(
+                       const_cast<NonConstPointer>(pointer))),
+      extent_(extent),
+      is_residual_(false) {
+
+
+    int warp_offset = (warp_id / kWarpCountStrided) * WarpShape::kContiguous;
+
+    // Per-thread offset in logical coordinates of tensor
+
+    thread_offset_ = threadblock_offset + TensorCoord(warp_offset, 0) +
+        TensorCoord((thread_id & kThreadsPerRowMask) * kElementsPerAccess, 0);
+
+    set_iteration_index(0);
+
+    if(EnableResidualAccess) {
+      // compute residual offset
+      typename TensorCoord::Index residual_size = extent_.contiguous() % WarpShape::kContiguous;
+      if (residual_size) {
+        is_residual_ = true;
+        residual_offset_ = make_Coord(residual_size, 0);
+      }
+    }
   }
 
-  /// Adds a pointer offset in units of Element
+  /// Construct a PredicatedVectorAccessIterator with zero threadblock offset
   CUTLASS_HOST_DEVICE
-  void add_pointer_offset(LongIndex pointer_offset) {
-    tile_access_iterator_.add_pointer_offset(pointer_offset);
-  }
+  PredicatedVectorAccessIterator(
+    /// Pointer to start of vector
+    ConstPointer pointer,
+    /// Extent of vector
+    TensorCoord extent,
+    ///< ID of each participating thread
+    int thread_id,
+    /// ID of each participating warp
+    int warp_id)
+    : PredicatedVectorAccessIterator(pointer, extent, thread_id, warp_id,
+                                     make_Coord(0, 0)) {}
 
-  /// Advances to the next tile in memory.
-  CUTLASS_HOST_DEVICE
-  TileIterator &operator++() {
-    tile_access_iterator_.advance();
-    return *this;
-  }
 
-  /// Advances to the next tile in memory.
+  /// Overrides the internal iteration index
   CUTLASS_HOST_DEVICE
-  TileIterator operator++(int) {
-    TileIterator self(*this);
-    operator++();
-    return self;
+  void set_iteration_index(int index) {
+    iteration_ = index;
   }
 
-  /// Loads a fragment from memory
+  /// Advances an iterator along logical dimensions of matrix in units of whole tiles
   CUTLASS_DEVICE
-  void load_with_pointer_offset(Fragment &frag, Index pointer_offset) {
+  void add_tile_offset(
+      TensorCoord const &tile_offset) {
+
+    thread_offset_ =
+        thread_offset_ +
+        TensorCoord(WarpShape::kContiguous * tile_offset.contiguous(), 0);
+  }
 
-    frag.clear();
-    AccessType *frag_ptr = reinterpret_cast<AccessType *>(&frag);
+  /// Returns a pointer
+  CUTLASS_HOST_DEVICE
+  AccessType *get() const {
 
-    CUTLASS_PRAGMA_UNROLL
-    for (int s = 0; s < ThreadMap::Iterations::kStrided; ++s) {
-      CUTLASS_PRAGMA_UNROLL
-      for (int c = 0; c < ThreadMap::Iterations::kContiguous; ++c) {
-        CUTLASS_PRAGMA_UNROLL
-        for (int v = 0; v < kAccessesPerVector; ++v) {
-
-          int idx = v + kAccessesPerVector * (c + s * ThreadMap::Iterations::kContiguous);
-
-          cutlass::arch::global_load<
-            AccessType,
-            sizeof(AccessType)
-          >(
-            frag_ptr[idx],
-            tile_access_iterator_.get() + pointer_offset,
-            tile_access_iterator_.valid()
-          );
-  
-          ++tile_access_iterator_;
-        }
-      }
-    }
+    return reinterpret_cast<AccessType *>(
+        pointer_ +
+        ((thread_offset_.contiguous() + iteration_ * kThreadsPerRow * kElementsPerAccess) 
+        * sizeof_bits<Element>::value / 8));
   }
 
-  /// Loads a fragment from memory
-  CUTLASS_DEVICE
-  void load(Fragment &frag) {
-    tile_access_iterator_.set_iteration_index(0);
-    load_with_pointer_offset(frag, 0);
+  /// Increment and return an instance to self.
+  CUTLASS_HOST_DEVICE
+  PredicatedVectorAccessIterator &operator++() {
+    ++iteration_;
+    if(iteration_ >= kIterations)
+      iteration_ = 0; 
+
+    return *this;
   }
 
-  CUTLASS_DEVICE
+  /// Increment and return an instance to self.
+  CUTLASS_HOST_DEVICE
   void advance() {
-    tile_access_iterator_.advance();
+    if(EnableResidualAccess && is_residual_) {
+      is_residual_ = false;
+      thread_offset_ += residual_offset_; 
+    }
+    else
+      add_tile_offset(TensorCoord(1, 0));
   }
 
-  /// Determines whether the Implicit GEMM can execute the given problem.
+  /// Increment and return an instance to self.
   CUTLASS_HOST_DEVICE
-  static Status can_implement(ConvProblemSize const &problem_size) {
+  PredicatedVectorAccessIterator operator++(int) {
+    PredicatedVectorAccessIterator self(*this);
+    operator++();
+    return self;
+  }
 
-    // dispatch to iterator implementation
-    return TileAccessIterator::can_implement(problem_size);
+  /// Returns whether access is valid or not
+  CUTLASS_HOST_DEVICE
+  bool valid() {
+    return ((thread_offset_.contiguous() + 
+              iteration_ * kThreadsPerRow * kElementsPerAccess) < extent_.contiguous());
   }
 };
 
-/////////////////////////////////////////////////////////////////////////////////////////////////
-// Strided Dgrad Tile Iterator
-template <typename TileAccessIterator_>
-class TileIteratorStridedDgrad {
-public:
-  using TileAccessIterator = TileAccessIterator_;
-
-  using Shape = typename TileAccessIterator::Shape;
-  using Element = typename TileAccessIterator::Element;
-  using Layout = typename TileAccessIterator::Layout;
+////////////////////////////////////////////////////////////////////////////////
+
+/// Specialization of PredicatedVectorAccessIterator for row-major data.
+///
+template <
+  typename Shape_,
+  typename WarpShape_,
+  typename Element_,
+  int ElementsPerAccess,
+  bool EnableResidualAccess
+>
+class PredicatedVectorAccessIterator<
+  Shape_,
+  WarpShape_,
+  Element_,
+  layout::RowMajor,
+  ElementsPerAccess,
+  EnableResidualAccess
+> {
+ public:
+
+  using Shape = Shape_;
+  using WarpShape = WarpShape_;
+  using Element = Element_;
+  using Layout = layout::RowMajor;
+
+  using Index = typename Layout::Index;
+  using LongIndex = typename Layout::LongIndex;
+
+  using TensorRef = TensorRef<Element, Layout>;
+  using TensorView = TensorView<Element, Layout>;
   using TensorCoord = typename Layout::TensorCoord;
-  using ThreadMap = typename TileAccessIterator::ThreadMap;
-  using AccessType = typename TileAccessIterator::AccessType;
-  using TensorRef = typename TileAccessIterator::TensorRef;
-  using Index = typename TileAccessIterator::Index;
-  using LongIndex = typename TileAccessIterator::LongIndex;
-  static IteratorAlgorithm const kIteratorAlgorithm = TileAccessIterator::kIteratorAlgorithm;
-  static StrideSupport const kStrideSupport = TileAccessIterator::kStrideSupport;
-  using Params = typename TileAccessIterator::Params;
-  static int const kConvDim = TileAccessIterator::kConvDim;
-  using ConvProblemSize = typename TileAccessIterator::ConvProblemSize;
-
-  /// Fragment object to be loaded or stored
-  using Fragment = cutlass::Array<
-    Element, 
-    ThreadMap::Iterations::kCount * ThreadMap::kElementsPerAccess>;
-
-private:
-
-  /// Internal state
-  TileAccessIterator tile_access_iterator_;
-
-public:
-
-  /// Constructor (output gradient (Dy) OperandA ctor)
-  CUTLASS_HOST_DEVICE
-  TileIteratorStridedDgrad(
-    Params const &params,
-    ConvProblemSize const &problem_size,
-    Element const *ptr,
-    int thread_idx,
-    FastDivmod const &stride_h_divmod, FastDivmod const &stride_w_divmod,
-    int start_r, int start_s,
-    MatrixCoord const &threadblock_offset = MatrixCoord()
-  ):
-    tile_access_iterator_(
-      params, 
-      problem_size, 
-      ptr, 
-      thread_idx, 
-      stride_h_divmod, stride_w_divmod, 
-      start_r, start_s, 
-      threadblock_offset) { }
-
-  /// Constructor (filter (w) OperandB ctor)
-  CUTLASS_HOST_DEVICE
-  TileIteratorStridedDgrad(
-    Params const &params,
-    ConvProblemSize const &problem_size,
-    Element const *ptr,
-    int thread_idx,
-    int start_r, int start_s,
-    MatrixCoord const &threadblock_offset = MatrixCoord()
-  ):
-    tile_access_iterator_(params, 
-      problem_size, 
-      ptr, 
-      thread_idx, 
-      start_r, start_s, 
-      threadblock_offset) { }
-
-  CUTLASS_HOST_DEVICE
-  static Params getParams(ConvProblemSize const &problem_size, Layout const &layout) {
-    return TileAccessIterator::getParams(problem_size, layout);
-  }
 
+  using ConstPointer = const Element *;
+  using NonConstPointer = typename platform::remove_const<Element>::type *;
+
+  using UnderlyingIterator = PredicatedVectorAccessIterator<
+      layout::PitchLinearShape<Shape::kColumn, Shape::kRow>, 
+      layout::PitchLinearShape<WarpShape::kColumn, WarpShape::kRow>, 
+      Element,
+      layout::PitchLinear,
+      ElementsPerAccess,
+      EnableResidualAccess>;
+
+  using AccessType = typename UnderlyingIterator::AccessType;
+  static int const kElementsPerAccess = UnderlyingIterator::kElementsPerAccess;
+  static int const kRowsPerIteration = UnderlyingIterator::kRowsPerIteration;
+  static int const kThreads = UnderlyingIterator::kThreads;
+  static int const kIterations = UnderlyingIterator::kIterations;
+
+ private:
+  //
+  // Data members
+  //
+
+  /// Underlying pitch-linear tile iterator
+  UnderlyingIterator iterator_;
+
+ public:
+  /// Constructs a TileIterator from its precomputed state, threadblock offset,
+  /// and thread ID
+  CUTLASS_HOST_DEVICE
+  PredicatedVectorAccessIterator(
+      ///< Pointer to the start of the vector
+      ConstPointer pointer,
+      ///< Extent of tensor
+      TensorCoord extent,
+      ///< ID of each participating thread
+      int thread_id,
+      ///< ID of each participating warp
+      int warp_id,
+      ///< Initial offset of threadblock
+      TensorCoord const &threadblock_offset)
+      : iterator_(pointer, layout::PitchLinearCoord(extent.column(), extent.row()),
+                  thread_id, warp_id,
+                  layout::PitchLinearCoord(threadblock_offset.column(),
+                                           threadblock_offset.row())) {}
+
+  /// Construct a PredicatedVectorAccessIterator with zero threadblock offset
+  CUTLASS_HOST_DEVICE
+  PredicatedVectorAccessIterator(
+      ConstPointer pointer,   ///< Pointer to the start of the vector
+      TensorCoord extent,     ///< Extent of tensor
+      int thread_id,          ///< ID of each participating thread
+      int warp_id             ///< ID of each participating warp
+      )
+      : PredicatedVectorAccessIterator(pointer, extent, thread_id, warp_id, 
+                                        make_Coord(0, 0)) {}
+
+  /// Overrides the internal iteration index
+  CUTLASS_HOST_DEVICE
+  void set_iteration_index(int index) { iterator_.set_iteration_index(index); }
 
-  /// Adds a pointer offset in units of Element
+  /// Advances an iterator along logical dimensions of matrix in units of whole
+  /// tiles
   CUTLASS_HOST_DEVICE
-  void add_pointer_offset(LongIndex pointer_offset) {
-    tile_access_iterator_.add_pointer_offset(pointer_offset);
+  void add_tile_offset(TensorCoord const &tile_offset) {
+    iterator_.add_tile_offset({tile_offset.column(), tile_offset.row()});
+  }
+
+  /// Returns a pointer
+  CUTLASS_HOST_DEVICE
+  AccessType *get() const {
+    return reinterpret_cast<AccessType *>(iterator_.get());
   }
 
   /// Advances to the next tile in memory.
+  ///
+  /// The first time this method is called, predicates are updated, and the
+  /// iterator's internal pointer is reverted to the first "steady state" tile.
+  /// Subsequent calls are lightweight and must only update the internal
+  /// pointer.
   CUTLASS_HOST_DEVICE
-  TileIteratorStridedDgrad &operator++() {
-    tile_access_iterator_.advance();
+  PredicatedVectorAccessIterator &operator++() {
+    ++iterator_;
     return *this;
   }
 
   /// Advances to the next tile in memory.
+  ///
+  /// The first time this method is called, predicates are updated, and the
+  /// iterator's internal pointer is reverted to the first "steady state" tile.
+  /// Subsequent calls are lightweight and must only update the internal
+  /// pointer.
   CUTLASS_HOST_DEVICE
-  TileIteratorStridedDgrad operator++(int) {
-    TileIteratorStridedDgrad self(*this);
+  PredicatedVectorAccessIterator operator++(int) {
+    PredicatedVectorAccessIterator self(*this);
     operator++();
     return self;
   }
 
-  /// Loads a fragment from memory
-  CUTLASS_DEVICE
-  void load_with_pointer_offset(Fragment &frag, Index pointer_offset) {
-
-    frag.clear();
-    AccessType *frag_ptr = reinterpret_cast<AccessType *>(&frag);
-
-    CUTLASS_PRAGMA_UNROLL
-    for (int s = 0; s < ThreadMap::Iterations::kStrided; ++s) {
-      CUTLASS_PRAGMA_UNROLL
-      for (int c = 0; c < ThreadMap::Iterations::kContiguous; ++c) {
-
-        cutlass::arch::global_load<
-          AccessType,
-          sizeof(AccessType)
-        >(
-          frag_ptr[c + s * ThreadMap::Iterations::kContiguous],
-          tile_access_iterator_.get() + pointer_offset,
-          tile_access_iterator_.valid()
-        );
-
-        ++tile_access_iterator_;
-      }
-    }
-  }
-
-  /// Loads a fragment from memory
-  CUTLASS_DEVICE
-  void load(Fragment &frag) {
-    tile_access_iterator_.set_iteration_index(0);
-    load_with_pointer_offset(frag, 0);
-  }
-
-  CUTLASS_DEVICE
+  /// Increment and return an instance to self.
+  CUTLASS_HOST_DEVICE
   void advance() {
-    tile_access_iterator_.advance();
+    iterator_.advance();
   }
 
-  /// Determines whether the Implicit GEMM can execute the given problem.
+  /// Returns whether access is valid or not
   CUTLASS_HOST_DEVICE
-  static Status can_implement(ConvProblemSize const &problem_size) {
-
-    // dispatch to iterator implementation
-    return TileAccessIterator::can_implement(problem_size);
+  bool valid() {
+    return iterator_.valid();
   }
 };
-/////////////////////////////////////////////////////////////////////////////////////////////////
 
-} // namespace threadblock
-} // namespace conv
-} // namespace cutlass
 
-/////////////////////////////////////////////////////////////////////////////////////////////////
+////////////////////////////////////////////////////////////////////////////////
+
+}  // namespace threadblock
+}  // namespace transform 
+}  // namespace cutlass
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/conv2d_wgrad_activation_tile_access_iterator_analytic.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/conv2d_wgrad_activation_tile_access_iterator_analytic.h`

 * *Files 2% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/conv2d_wgrad_activation_tile_access_iterator_optimized.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/conv2d_wgrad_activation_tile_access_iterator_optimized.h`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/conv2d_wgrad_output_gradient_tile_access_iterator_analytic.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/conv2d_wgrad_output_gradient_tile_access_iterator_analytic.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/conv2d_wgrad_output_gradient_tile_access_iterator_optimized.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/conv2d_wgrad_output_gradient_tile_access_iterator_optimized.h`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/conv3d_dgrad_filter_tile_access_iterator_analytic.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/conv3d_dgrad_filter_tile_access_iterator_analytic.h`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/conv3d_dgrad_filter_tile_access_iterator_optimized.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/conv3d_dgrad_filter_tile_access_iterator_optimized.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/conv3d_dgrad_output_gradient_tile_access_iterator_analytic.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/conv3d_dgrad_output_gradient_tile_access_iterator_analytic.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/conv3d_dgrad_output_gradient_tile_access_iterator_optimized.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/conv3d_dgrad_output_gradient_tile_access_iterator_optimized.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/conv3d_fprop_activation_tile_access_iterator_analytic.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/conv3d_fprop_activation_tile_access_iterator_analytic.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/conv3d_fprop_activation_tile_access_iterator_optimized.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/conv3d_fprop_activation_tile_access_iterator_optimized.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/conv3d_fprop_filter_tile_access_iterator_analytic.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/conv3d_fprop_filter_tile_access_iterator_analytic.h`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/conv3d_fprop_filter_tile_access_iterator_optimized.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/conv3d_fprop_filter_tile_access_iterator_optimized.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/conv3d_params.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/conv3d_params.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/conv3d_wgrad_activation_tile_access_iterator_analytic.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/conv3d_wgrad_activation_tile_access_iterator_analytic.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/conv3d_wgrad_activation_tile_access_iterator_optimized.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/conv3d_wgrad_activation_tile_access_iterator_optimized.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/conv3d_wgrad_output_gradient_tile_access_iterator_analytic.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/conv3d_wgrad_output_gradient_tile_access_iterator_analytic.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/conv3d_wgrad_output_gradient_tile_access_iterator_optimized.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/conv3d_wgrad_output_gradient_tile_access_iterator_optimized.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/depthwise_direct_conv_params.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/depthwise_direct_conv_params.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/depthwise_fprop_activation_tile_access_iterator_direct_conv_fixed_stride_dilation.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/depthwise_fprop_activation_tile_access_iterator_direct_conv_fixed_stride_dilation.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/depthwise_fprop_activation_tile_access_iterator_direct_conv_optimized.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/depthwise_fprop_activation_tile_access_iterator_direct_conv_optimized.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/depthwise_fprop_direct_conv_multistage.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/depthwise_fprop_direct_conv_multistage.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/depthwise_fprop_filter_tile_access_iterator_direct_conv_optimized.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/depthwise_fprop_filter_tile_access_iterator_direct_conv_optimized.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/depthwise_fprop_pipelined.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/depthwise_fprop_pipelined.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/depthwise_mma_base.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/depthwise_mma_base.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/depthwise_mma_core_with_lane_access_size.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/default_mma_core_with_access_size.h`

 * *Files 11% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -28,38 +28,34 @@
  * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
  *
  **************************************************************************************************/
 /*! \file
     \brief Defines basic properties needed by CTA-level GEMMs assuming expectations about data
       layout of the global memory fragments, data types, and internal tile sizes.
 
-      Partial specializations for threadblock::Mma operations targeting depthwise related simt instructions.
+      Partial specializations for threadblock::Mma operations targeting simt instructions.
 */
 
 #pragma once
 
 #include "cutlass/cutlass.h"
 #include "cutlass/array.h"
 
 #include "cutlass/numeric_types.h"
 #include "cutlass/matrix_shape.h"
 
 #include "cutlass/gemm/warp/mma.h"
 #include "cutlass/gemm/threadblock/mma_pipelined.h"
 #include "cutlass/gemm/threadblock/mma_singlestage.h"
-
-#include "cutlass/gemm/threadblock/mma_base.h"
-#include "cutlass/conv/warp/mma_depthwise_simt.h"
-
 #include "cutlass/arch/cache_operation.h" 
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
 namespace cutlass {
-namespace conv {
+namespace gemm {
 namespace threadblock {
 
 template <
     /// Shape of threadblock-scoped matrix multiply operator
     typename Shape,
     /// Shape of warp-level matrix multiply operator
     typename WarpShape,
@@ -75,18 +71,16 @@
     typename LayoutB,
     /// Data type of accumulator
     typename ElementC,
     /// Layout of accumulator
     typename LayoutC,
     /// Indicates type of math operator (arch::OpClassSimt or arch::OpClassTensorOp)
     typename OperatorClass,
-    /// Size of a warp-scoped per thread access
-    int kLaneAccessSizeA_ = 0,
-    /// Size of a warp-scoped per thread access 
-    int kLaneAccessSizeB_ = 0,
+    /// Size of a threadblock-scoped access
+    int kAccessSizeInBits = -1, // -1 denoting the default
     /// Number of stages
     int Stages = 2,
     /// Operation performed by MMA
     typename Operator = typename platform::conditional<
         (platform::is_same<OperatorClass,
                            cutlass::arch::OpClassTensorOp>::value) &&
             (platform::is_same<ElementA, int8_t>::value ||
@@ -106,17 +100,15 @@
         cutlass::arch::CacheOperation::Global,
     /// per-element transformation for elements of A
     ComplexTransform TransformA = ComplexTransform::kNone,
     /// per-element transformation for elements of B
     ComplexTransform TransformB = ComplexTransform::kNone,
     bool IsComplex = false // (is_complex<ElementA>::value || is_complex<ElementB>::value)
 >
-struct DepthwiseMmaCoreWithLaneAccessSize;
-
-/////////////////////////////////////////////////////////////////////////////////////////////////
+struct DefaultMmaCoreWithAccessSize;
 
 template <
     /// Shape of threadblock-scoped matrix multiply operator
     typename Shape,
     /// Shape of warp-level matrix multiply operator
     typename WarpShape,
     /// Shape of one matrix production operation (concept: GemmShape)
@@ -148,32 +140,35 @@
     cutlass::arch::CacheOperation::Kind CacheOpB,
     /// per-element transformation for elements of A
     ComplexTransform TransformA,
     /// per-element transformation for elements of B
     ComplexTransform TransformB,
     bool IsComplex
 >
-struct DepthwiseMmaCoreWithLaneAccessSize<
+struct DefaultMmaCoreWithAccessSize<
     Shape, WarpShape, InstructionShape,
     ElementA, LayoutA, ElementB, LayoutB, ElementC, LayoutC,
-    OperatorClass, -1, -1, Stages, Operator, AccumulatorsInRowMajor,
+    OperatorClass, -1, Stages, Operator, AccumulatorsInRowMajor,
     CacheOpA, CacheOpB, TransformA, TransformB, IsComplex
-> : cutlass::gemm::threadblock::DefaultMmaCore<
+> : DefaultMmaCore<
     Shape, WarpShape, InstructionShape,
     ElementA, LayoutA, ElementB, LayoutB, ElementC, LayoutC,
     OperatorClass, Stages, Operator, AccumulatorsInRowMajor,
     CacheOpA, CacheOpB, TransformA, TransformB, IsComplex
 > {};
 
+
+/////////////////////////////////////////////////////////////////////////////////////////////////
+
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
 /// Partial specialization:
 ///
-///   A: row-major
-///   B: column-major
+///   A: column-major
+///   B: row-major
 ///   Operator: simt class
 ///
 /// This uses the default warp-level operator given tile sizes
 template <
     /// Shape of threadblock-scoped matrix multiply operator (concept:
     /// GemmShape)
     typename Shape_,
@@ -183,155 +178,151 @@
     typename ElementA_,
     /// Data type of B operand
     typename ElementB_,
     /// Data type of accumulator
     typename ElementC_,
     /// Layout of accumulator
     typename LayoutC_,
-    /// Size of a warp-scoped per thread access (a value of -1 indicates the default)
-    int kLaneAccessSizeA_,
-    /// Size of a warp-scoped per thread access (a value of -1 indicates the default)
-    int kLaneAccessSizeB_,
+    /// Size of a threadblock-scoped access (a value of -1 indicates the default)
+    int kAccessSizeInBits_,
     /// Operation performed by GEMM
     typename Operator_>
-struct DepthwiseMmaCoreWithLaneAccessSize<Shape_,
-                                        WarpShape_,
-                                        cutlass::gemm::GemmShape<1, 1, 1>,
-                                        ElementA_,
-                                        layout::RowMajor,
-                                        ElementB_,
-                                        layout::ColumnMajor,
-                                        ElementC_,
-                                        LayoutC_,
-                                        arch::OpClassSimt,
-                                        kLaneAccessSizeA_,
-                                        kLaneAccessSizeB_,
-                                        2,
-                                        Operator_> : public cutlass::gemm::threadblock::DefaultMmaCore<Shape_,
-                                                                           WarpShape_,
-                                                                           cutlass::gemm::GemmShape<1, 1, 1>,
-                                                                           ElementA_,
-                                                                           layout::RowMajor,
-                                                                           ElementB_,
-                                                                           layout::ColumnMajor,
-                                                                           ElementC_,
-                                                                           LayoutC_,
-                                                                           arch::OpClassSimt,
-                                                                           2,
-                                                                           Operator_> {
-  using Base = cutlass::gemm::threadblock::DefaultMmaCore<Shape_,
-                              WarpShape_,
-                              cutlass::gemm::GemmShape<1, 1, 1>,
-                              ElementA_,
-                              layout::RowMajor,
-                              ElementB_,
-                              layout::ColumnMajor,
-                              ElementC_,
-                              LayoutC_,
-                              arch::OpClassSimt,
-                              2,
-                              Operator_>;
-
+struct DefaultMmaCoreWithAccessSize<Shape_, WarpShape_, typename std::enable_if<kAccessSizeInBits_ != -1, GemmShape<1, 1, 1>>::type, ElementA_,
+                      layout::ColumnMajor, ElementB_, layout::RowMajor,
+                      ElementC_, LayoutC_, arch::OpClassSimt, kAccessSizeInBits_, 2, Operator_
+                     > {
   using Shape = Shape_;
   using WarpShape = WarpShape_;
-  using InstructionShape = cutlass::gemm::GemmShape<1, 1, 1>;
+  using InstructionShape = GemmShape<1, 1, 1>;
   using ElementA = ElementA_;
-  using LayoutA = layout::RowMajor;
+  using LayoutA = layout::ColumnMajor;
   using ElementB = ElementB_;
-  using LayoutB = layout::ColumnMajor;
+  using LayoutB = layout::RowMajor;
   using ElementC = ElementC_;
   using LayoutC = LayoutC_;
   using OperatorClass = arch::OpClassSimt;
-
-  static int const kLaneAccessSizeA = kLaneAccessSizeA_;
-  static int const kLaneAccessSizeB = kLaneAccessSizeB_;
-
-  // Divisility requirements
-  static_assert( kLaneAccessSizeA > 0 && kLaneAccessSizeB > 0,
-    "Size of a warp-scoped per thread access should be larger then ZERO" );
+  static int const PartitionsK = Shape::kK / WarpShape::kK;
 
   /// Default Operator
   using Operator = Operator_;
 
   /// Number of warps present
-  using WarpCount = typename Base::WarpCount;
+  using WarpCount = GemmShape<
+    Shape::kM / WarpShape::kM,
+    Shape::kN / WarpShape::kN,
+    PartitionsK
+  >;
 
   // Divisility requirements
   static_assert(
     !(Shape::kM % WarpShape::kM) &&
     !(Shape::kN % WarpShape::kN),
     "Threadblock-scoped GEMM should be divisible by warp-scoped GEMM size."
   );
 
   /// Number of threads per warp
-  static int const kWarpSize = cutlass::gemm::warp::WarpSize<arch::OpClassSimt>::value;
+  static int const kWarpSize = warp::WarpSize<arch::OpClassSimt>::value;
+
+  /// Number of threads total
+  static int const kThreads = WarpCount::kCount * kWarpSize;
 
-  static int const kElementsPerAccess = 1;
+  static int const kElementsPerAccessDefault = 1;
+  static_assert(kAccessSizeInBits_ == -1 ||
+          sizeof_bits<ElementA>::value == sizeof_bits<ElementB>::value ||
+          kAccessSizeInBits_ / sizeof_bits<ElementA>::value == kElementsPerAccessDefault,
+          "Non-default value for kAccessSizeInBits_ is only allowed if size(elementA) == sizeof(elementB)");
+  static int const kElementsPerAccess = (kAccessSizeInBits_ != -1) ? kAccessSizeInBits_ / sizeof_bits<ElementA>::value : kElementsPerAccessDefault;
 
   //
   // Shared memory layouts
   //
 
   using SmemLayoutA = layout::ColumnMajor;
   using SmemLayoutB = layout::RowMajor;
 
   //
-  // Iterators to write to shared memory are same as base class
+  // Iterators to write to shared memory
   //
 
+  /// ThreadMap of iterator A
+  using IteratorThreadMapA = transform::PitchLinearStripminedThreadMap<
+    layout::PitchLinearShape<Shape::kM, Shape::kK>,
+    kThreads,
+    kElementsPerAccess
+  >;
+
+  /// Shared memory iterator to A operand
+  using SmemIteratorA = transform::threadblock::RegularTileIterator<
+    MatrixShape<Shape::kM, Shape::kK>, 
+    ElementA, 
+    SmemLayoutA,
+    1,
+    IteratorThreadMapA
+  >;
+
+  /// Policy of iterator B
+  using IteratorThreadMapB = transform::PitchLinearStripminedThreadMap<
+    layout::PitchLinearShape<Shape::kN, Shape::kK>,
+    kThreads,
+    kElementsPerAccess
+  >;
+
+  /// Shared memory iterator to B operand
+  using SmemIteratorB = transform::threadblock::RegularTileIterator<
+    MatrixShape<Shape::kK, Shape::kN>, 
+    ElementB, 
+    SmemLayoutB,
+    0,
+    IteratorThreadMapB
+  >;
+
   //
   // Warp-level matrix multiply operator
   //
 
   // Define the warp-level op
-  static const int WarpNumThreadsM = cutlass::gemm::threadblock::detail::simt_get_warp_threads_m<WarpShape>(); 
+  static const int WarpNumThreadsM = detail::simt_get_warp_threads_m<WarpShape>();
   static const int WarpNumThreadsN = kWarpSize / WarpNumThreadsM;
   static const int ThreadTileM = WarpShape::kM / WarpNumThreadsM;
   static const int ThreadTileN = WarpShape::kN / WarpNumThreadsN;
   static_assert(!(WarpShape::kM % WarpNumThreadsM) && !(WarpShape::kN % WarpNumThreadsN),
       "WarpShape must be divisible by ThreadTile shape.");
   static const int LaneLayout = ThreadTileM > 4 && ThreadTileN > 4 ? 2 : 1;
-  static const int numElementsA = kLaneAccessSizeA / sizeof_bits<ElementA>::value;
-  static const int numElementsB = kLaneAccessSizeB / sizeof_bits<ElementB>::value;
+  static const int numElementsA = 128 / sizeof_bits<ElementA>::value;
+  static const int numElementsB = 128 / sizeof_bits<ElementB>::value;
   static const int LaneM = cutlass::const_min(numElementsA, ThreadTileM);
   static const int LaneN = cutlass::const_min(numElementsB, ThreadTileN);
-
-  static int const kPaddingM = cutlass::gemm::threadblock::detail::simt_transpose_padding(kWarpSize, Shape::kK, sizeof_bits<ElementA>::value);
-  static int const kPaddingN = cutlass::gemm::threadblock::detail::simt_transpose_padding(kWarpSize, Shape::kK, sizeof_bits<ElementB>::value);
-
-  static_assert(!(kPaddingM % LaneM) && !(kPaddingN % LaneN),
-                "Padding must be divisible by Lane");
-
   // these should have max of thread tile also
   using LaneMmaShape = cutlass::gemm::GemmShape<
       LaneM,
       LaneN,
       1>;
   using Policy = cutlass::gemm::warp::MmaSimtPolicy<
       cutlass::MatrixShape<WarpNumThreadsM, WarpNumThreadsN>,   // WarpShape
       cutlass::layout::RowMajorInterleaved<LaneLayout>,         // LaneLayout
       LaneMmaShape
   >;
 
-  using MmaWarpSimt = cutlass::conv::warp::MmaDepthwiseSimt<
-      WarpShape,      /// Size of the Gemm problem - concept: gemm::GemmShape<>
-      ElementA,       /// Data type of A elements
-      SmemLayoutA,    /// Layout of A matrix (concept: MatrixLayout)
-      ElementB,       /// Data type of B elements
-      SmemLayoutB,    /// Layout of B matrix (concept: MatrixLayout)
-      ElementC,       /// Element type of C matrix
-      LayoutC,        /// Layout of C matrix (concept: MatrixLayout)
-      Policy          /// Policy describing warp-level MmaSimtOp (concept: MmaSimtOp policy)
-  >;
+  using MmaWarpSimt = cutlass::gemm::warp::MmaSimt<
+    WarpShape,    /// Size of the Gemm problem - concept: gemm::GemmShape<> 128, 128, 8
+    ElementA,     /// Data type of A elements
+    SmemLayoutA,  /// Layout of A matrix (concept: MatrixLayout)
+    ElementB,     /// Data type of B elements
+    SmemLayoutB,  /// Layout of B matrix (concept: MatrixLayout)
+    ElementC,     /// Element type of C matrix
+    LayoutC,      /// Layout of C matrix (concept: MatrixLayout)
+    Policy        /// Policy describing warp-level MmaSimtOp (concept: MmaSimtOp policy)
+    >;            /// Used for partial specialization
 
-  /// Policy used to define MmaPipelined 
-  using MmaPolicy = cutlass::gemm::threadblock::MmaPolicy<
+  /// Policy used to define MmaPipelined
+  using MmaPolicy = MmaPolicy<
     MmaWarpSimt,
-    MatrixShape<kPaddingM, 0>,    // skew for A matrix to avoid SMEM bank conflicts
-    MatrixShape<0, kPaddingN>,    // skew for B matrix to avoid SMEM bank conflicts
+    MatrixShape<0, 0>,
+    MatrixShape<0, 0>,
     WarpCount::kK
   >;
 };
 
+/////////////////////////////////////////////////////////////////////////////////////////////////
 } // namespace threadblock
-} // namespace conv
+} // namespace gemm
 } // namespace cutlass
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/implicit_gemm_fprop_fusion_multistage.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/implicit_gemm_fprop_fusion_multistage.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/implicit_gemm_multistage.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/implicit_gemm_multistage.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/implicit_gemm_pipelined.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/implicit_gemm_pipelined.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/implicit_gemm_wgrad_fusion_multistage.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/implicit_gemm_wgrad_fusion_multistage.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/predicated_scale_bias_vector_access_iterator.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/predicated_scale_bias_vector_access_iterator.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/predicated_scale_bias_vector_iterator.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/predicated_scale_bias_vector_iterator.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/threadblock_swizzle.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/threadblock_swizzle.h`

 * *Files 5% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -161,11 +161,33 @@
   /// For GEMM problem size (MxNxK) (Do not use base class get_tiled_shape())
   private:
     using Base::get_tiled_shape;
 };
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
+/// Threadblock swizzling function for GEMMs
+template <int N = 1, int Output_N = 1, int Output_P = 1, int Output_Q = 1>
+struct DepthwiseDirect2dConvIdentityThreadblockSwizzle
+    : public gemm::threadblock::GemmIdentityThreadblockSwizzle<N> {
+  CUTLASS_HOST_DEVICE
+  DepthwiseDirect2dConvIdentityThreadblockSwizzle() {}
+
+  /// Returns the shape of the problem in units of logical tiles
+  CUTLASS_HOST_DEVICE
+  gemm::GemmCoord get_tiled_shape(cutlass::conv::Operator conv_operator,
+                            cutlass::conv::Conv2dProblemSize const &problem_size,
+                            gemm::GemmCoord tile_size,
+                            int split_k_slices) const {
+        
+    gemm::GemmCoord implicit_gemm_problem_size =
+        cutlass::conv::implicit_gemm_problem_size(conv_operator, problem_size);
+
+    return gemm::GemmCoord(1,
+                     (implicit_gemm_problem_size.n() + tile_size.n() - 1) / tile_size.n(),
+                     split_k_slices);
+  }
+};
 
 } // namespace threadblock
-} // namespace gemm
+} // namespace conv
 } // namespace cutlass
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/conv/warp/mma_depthwise_simt.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/default_gemv.h`

 * *Files 24% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -24,140 +24,109 @@
  * DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR
  * SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER
  * CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,
  * OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
  * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
  *
  **************************************************************************************************/
-/*! \file
-    \brief Templates implementing warp-level matrix multiply-accumulate operations.
-*/
 
 #pragma once
 
-#include "cutlass/cutlass.h"
-#include "cutlass/array.h"
-#include "cutlass/numeric_types.h"
-#include "cutlass/matrix_shape.h"
-#include "cutlass/gemm/gemm.h"
-#include "cutlass/gemm/warp/mma.h"
-
-#include "cutlass/gemm/thread/mma.h"
-
-#include "cutlass/gemm/warp/mma_simt_tile_iterator.h"
-#include "cutlass/gemm/warp/mma_simt_policy.h"
-
-#include "cutlass/gemm/warp/mma_simt.h"
-#include "cutlass/conv/warp/mma_depthwise_simt_tile_iterator.h"
-
-/////////////////////////////////////////////////////////////////////////////////////////////////
+#include "cutlass/gemm/threadblock/gemv.h"
+#include "cutlass/gemm/threadblock/default_gemv_core.h"
+#include "cutlass/gemm/threadblock/threadblock_swizzle.h"
 
 namespace cutlass {
-namespace conv {
-namespace warp {
+namespace gemm {
+namespace kernel {
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
-/// Structure to compute the matrix product targeting CUDA cores and SIMT math instructions.
 template <
-    /// Size of the Gemm problem - concept: gemm::GemmShape<>
-    typename Shape_,
+    /// Size of the ThreadBlock tile - concept: gemm::GemmShape<>
+    typename ThreadBlockShape_,
+    /// Size of the per-thread shape - concept: gemm::GemmShape<>
+    typename ThreadShape_,
     /// Data type of A elements
     typename ElementA_,
     /// Layout of A matrix (concept: MatrixLayout)
     typename LayoutA_,
     /// Data type of B elements
     typename ElementB_,
     /// Layout of B matrix (concept: MatrixLayout)
     typename LayoutB_,
-    /// Element type of C matrix
-    typename ElementC_,
-    /// Layout of C matrix (concept: MatrixLayout)
-    typename LayoutC_,
-    /// Shape of the warp in units of thread (concept: MmaSimtPolicy)
-    typename Policy_,
-    /// Number of partitions along K dimension
-    int PartitionsK = 1,
-    /// Complex transformation on operand A
-    ComplexTransform TransformA = ComplexTransform::kNone,
-    /// Complex transformation on operand B
-    ComplexTransform TransformB = ComplexTransform::kNone,
-    /// Used for partial specialization
-    typename Enable = bool>
-class MmaDepthwiseSimt
-    : public cutlass::gemm::warp::
-          MmaSimt<Shape_, ElementA_, LayoutA_, ElementB_, LayoutB_, ElementC_, LayoutC_, Policy_> {
-  using Base = cutlass::gemm::warp::
-      MmaSimt<Shape_, ElementA_, LayoutA_, ElementB_, LayoutB_, ElementC_, LayoutC_, Policy_>;
-      
-public:
+    /// Element type of C/D matrix
+    typename ElementCD_,
+    /// Layout of C/D matrix (concept: MatrixLayout)
+    typename LayoutCD_,
+    ///  Data type of the accumulator
+    typename ElementAccumulator_ = ElementCD_>
+struct DefaultGemv {
+
+  /// Shape of Threadblock-level matrix operation (concept: GemmShape)
+  using ThreadBlockShape = ThreadBlockShape_;
+
   /// Shape of warp-level matrix operation (concept: GemmShape)
-  using Shape = Shape_;  // < 64, 16 , 8>
+  using ThreadShape = ThreadShape_;
 
   /// Data type of multiplicand A
   using ElementA = ElementA_;
 
   /// Layout of multiplicand A
   using LayoutA = LayoutA_;
 
   /// Data type of multiplicand B
   using ElementB = ElementB_;
 
   /// Layout of multiplicand B
   using LayoutB = LayoutB_;
 
-  /// Data type of accumulator matrix C
-  using ElementC = ElementC_;
-
-  /// Layout of accumulator matrix C
-  using LayoutC = LayoutC_;
-
-  /// Shape of the warp in units of thread (concept: MmaLanePolicySimt)
-  using Policy = Policy_;
+  /// Data type of accumulators
+  using ElementAccumulator = ElementAccumulator_;
 
-  /// Indicates class of matrix operator
-  using OperatorClass = arch::OpClassSimt;
+  /// Data type of accumulators (same as C/D)
+  using LayoutAccumulator = LayoutCD_;
 
-  /// Hard-coded for now
-  using ArchTag = arch::Sm50;
+  /// Data type of input/output matrix C/D
+  using ElementCD = ElementCD_;
 
-  /// Complex transform on A operand
-  static ComplexTransform const kTransformA = TransformA;
+  /// Layout of input/output matrix C/D
+  using LayoutCD = LayoutCD_;
 
-  /// Complex transform on B operand
-  static ComplexTransform const kTransformB = TransformB;
+  // Define the core components
+  using Core = typename cutlass::gemm::threadblock::DefaultGemvCore<
+      ThreadBlockShape, ThreadShape, ElementA, LayoutA, ElementB, LayoutB,
+      ElementAccumulator, LayoutAccumulator>;
 
-public:
+  // Define the threadblock-scoped gemv
+  using ThreadBlockGemv = cutlass::gemm::threadblock::Gemv<Core>;
 
-  /// Iterates over the B operand in memory
-  using IteratorB = cutlass::conv::warp::DepthwiseMmaSimtTileIterator<
-    MatrixShape<Policy::LaneMmaShape::kK, Shape::kN>,
-    cutlass::gemm::Operand::kB,
-    ElementB,
-    LayoutB,
-    Policy,
-    PartitionsK,
-    Shape::kK
-  >;
+  // Iterator for multiplicand A
+  using IteratorA = typename ThreadBlockGemv::IteratorA;
 
-  /// Storage for B tile
-  using FragmentB = typename IteratorB::Fragment;
+  // Iterator for multiplicand B
+  using IteratorB = typename ThreadBlockGemv::IteratorB;
 
-  /// Storage for transformed A tile
-  using TransformedFragmentB = FragmentB;
+  /// Policy for the iterator that reads/writes C/D
+  using IteratorPolicyCD = typename platform::conditional<
+        platform::is_same<LayoutCD, layout::RowMajor>::value,
+        cutlass::transform::PitchLinearTilePolicyStripminedThreadContiguous<
+          layout::PitchLinearShape<ThreadBlockShape::kN, ThreadBlockShape::kM>, Core::kThreadsPerN, ThreadShape::kN>,
+        cutlass::transform::PitchLinearTilePolicyStripminedThreadStrided<
+          layout::PitchLinearShape<ThreadBlockShape::kM, ThreadBlockShape::kN>, Core::kThreadsPerN, ThreadShape::kM>>::type;
 
-public:
+  /// Iterator that reads/writes C/D
+  using IteratorCD = cutlass::transform::threadblock::PredicatedTileIterator<
+   cutlass::MatrixShape<ThreadBlockShape::kM, ThreadBlockShape::kN>, ElementCD, LayoutCD, 0, IteratorPolicyCD>;
 
-  //
-  // Methods
-  //
+  /// Fragment storage for C/D
+  using FragmentCD = typename IteratorCD::Fragment;
 
-  /// Ctor
-  CUTLASS_DEVICE
-  MmaDepthwiseSimt():Base() {}
+  // Define the threadblock swizzle
+  using ThreadBlockSwizzle = cutlass::gemm::threadblock::GemvBatchedStridedThreadblockDefaultSwizzle;
 };
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
-} // namespace warp
-} // namespace gemm
-} // namespace cutlass
+}  // namespace kernel
+}  // namespace gemm
+}  // namespace cutlass
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/conv/warp/mma_depthwise_simt_tile_iterator.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/gemm/warp/mma_simt.h`

 * *Files 22% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -25,231 +25,240 @@
  * SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER
  * CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,
  * OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
  * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
  *
  **************************************************************************************************/
 /*! \file
-    \brief Describes the lane policy used by warp-level matrix multiply operators targeting SIMT
-      instructions
+    \brief Templates implementing warp-level matrix multiply-accumulate operations.
 */
 
 #pragma once
 
 #include "cutlass/cutlass.h"
 #include "cutlass/array.h"
-#include "cutlass/tensor_ref.h"
+#include "cutlass/numeric_types.h"
 #include "cutlass/matrix_shape.h"
+#include "cutlass/gemm/gemm.h"
+#include "cutlass/gemm/warp/mma.h"
 
-#include "cutlass/arch/memory_sm75.h"
-
-#include "cutlass/layout/matrix.h"
+#include "cutlass/gemm/thread/mma.h"
 
-#include "cutlass/gemm/gemm.h"
-#include "cutlass/gemm/warp/mma_simt_policy.h"
 #include "cutlass/gemm/warp/mma_simt_tile_iterator.h"
+#include "cutlass/gemm/warp/mma_simt_policy.h"
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
 namespace cutlass {
-namespace conv {
+namespace gemm {
 namespace warp {
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
-/// Iterates over operands to warp-level matrix multiply operations targeting SIMT instructions
-///
-/// concept: MutableRandomAccessContiguousTileIteratorConcept
-///
+/// Structure to compute the matrix product targeting CUDA cores and SIMT math instructions.
 template <
-  /// Size of the matrix to load (concept: MatrixShape)
+  /// Size of the Gemm problem - concept: gemm::GemmShape<>
   typename Shape_,
-  /// Operand identity
-  cutlass::gemm::Operand Operand,
   /// Data type of A elements
-  typename Element_,
-  /// Layout of operand
-  typename Layout_,
+  typename ElementA_,
+  /// Layout of A matrix (concept: MatrixLayout)
+  typename LayoutA_,
+  /// Data type of B elements
+  typename ElementB_,
+  /// Layout of B matrix (concept: MatrixLayout)
+  typename LayoutB_,
+  /// Element type of C matrix
+  typename ElementC_,
+  /// Layout of C matrix (concept: MatrixLayout)
+  typename LayoutC_,
   /// Shape of the warp in units of thread (concept: MmaSimtPolicy)
   typename Policy_,
-  /// Number of partitions along K dimension - used in sliced-K
+  /// Number of partitions along K dimension
   int PartitionsK = 1,
-  /// Group Size along kPartition - used in sliced-K
-  int PartitionGroupSize = 1
+  /// Complex transformation on operand A
+  ComplexTransform TransformA = ComplexTransform::kNone,
+  /// Complex transformation on operand B
+  ComplexTransform TransformB = ComplexTransform::kNone,
+  /// Used for partial specialization
+  typename Enable = bool
 >
-class DepthwiseMmaSimtTileIterator;
-
-/////////////////////////////////////////////////////////////////////////////////////////////////
-
-/// Specialization for B operands of row-major layouts
-///
-/// Concept: MutableRandomAccessContiguousTileIteratorConcept
-///
-template <
-    /// Size of the matrix to load (concept: MatrixShape)
-    typename Shape_,
-    /// Data type of A elements
-    typename Element_,
-    /// Shape of the warp in units of thread (concept: MmaSimtPolicy)
-    typename Policy_,
-    /// Number of partitions along K dimension
-    int PartitionsK,
-    /// Group Size along kPartition - used in sliced-K
-    int PartitionGroupSize>
-class DepthwiseMmaSimtTileIterator<Shape_,
-                                   cutlass::gemm::Operand::kB,
-                                   Element_,
-                                   layout::RowMajor,
-                                   Policy_,
-                                   PartitionsK,
-                                   PartitionGroupSize>
-    : public cutlass::gemm::warp::MmaSimtTileIterator<Shape_,
-                                               cutlass::gemm::Operand::kB,
-                                               Element_,
-                                               layout::RowMajor,
-                                               Policy_,
-                                               PartitionsK,
-                                               PartitionGroupSize> {
-
-  using Base = cutlass::gemm::warp::MmaSimtTileIterator<Shape_,
-                                               cutlass::gemm::Operand::kB,
-                                               Element_,
-                                               layout::RowMajor,
-                                               Policy_,
-                                               PartitionsK,
-                                               PartitionGroupSize>;
- public:
-  /// Shape of tile to load (concept: MatrixShape)
+class MmaSimt {
+public:
+  /// Shape of warp-level matrix operation (concept: GemmShape)
   using Shape = Shape_;
 
-  /// Operand tag
-  static cutlass::gemm::Operand const kOperand = cutlass::gemm::Operand::kB;
-
-  /// Element type
-  using Element = Element_;
+  /// Data type of multiplicand A
+  using ElementA = ElementA_;
 
-  /// Layout of policy
-  using Layout = layout::RowMajor;
+  /// Layout of multiplicand A
+  using LayoutA = LayoutA_;
 
-  /// Decomposition of elements among threads
-  using Policy = Policy_;
+  /// Data type of multiplicand B
+  using ElementB = ElementB_;
 
-  /// TensorRef type for loading element from a tensor
-  using TensorRef = typename Base::TensorRef;
+  /// Layout of multiplicand B
+  using LayoutB = LayoutB_;
 
-  /// Index type
-  using Index = typename TensorRef::Index;
+  /// Data type of accumulator matrix C
+  using ElementC = ElementC_;
 
-  /// Long Index type
-  using LongIndex = typename TensorRef::LongIndex;
+  /// Layout of accumulator matrix C
+  using LayoutC = LayoutC_;
 
-  /// Coordinate for an element in the tensor
-  using TensorCoord = typename TensorRef::TensorCoord;
+  /// Shape of the warp in units of thread (concept: MmaLanePolicySimt)
+  using Policy = Policy_;
 
-  /// Thread-level shape of a fragment
-  using ThreadShape = typename Base::ThreadShape;
+  /// Indicates class of matrix operator
+  using OperatorClass = arch::OpClassSimt;
 
-  /// Number of individual loads
-  using Iterations =  typename Base::Iterations;
+  /// Hard-coded for now
+  using ArchTag = arch::Sm50;
 
-  /// Fragment object holding a thread's part of a tile
-  using Fragment = typename Base::Fragment;
+  /// Complex transform on A operand
+  static ComplexTransform const kTransformA = TransformA;
 
-  static_assert(Policy::LaneMmaShape::kN == 1, "Each thread should be 1 element per LDS along the k-dim");
+  /// Complex transform on B operand
+  static ComplexTransform const kTransformB = TransformB;
+
+  /// Layout of threads
+  using ThreadLayoutA = typename platform::conditional< platform::is_same< layout::ColumnMajorInterleaved<4>, LayoutA >::value,
+                  layout::ColumnMajor,
+                  typename platform::conditional < platform::is_same< layout::RowMajorInterleaved<4>, LayoutA >::value,
+                      layout::RowMajor,
+                      LayoutA>::type
+                 >::type;
   
-private:
+  using ThreadLayoutB = typename platform::conditional< platform::is_same< layout::ColumnMajorInterleaved<4>, LayoutB >::value,
+                  layout::ColumnMajor,
+                  typename platform::conditional < platform::is_same< layout::RowMajorInterleaved<4>, LayoutB >::value,
+                      layout::RowMajor,
+                      LayoutB>::type
+                 >::type;
+
+  static constexpr bool use_dp4a = (platform::is_same< layout::ColumnMajorInterleaved<4>, LayoutA>::value || 
+                                    platform::is_same< layout::RowMajorInterleaved<4>, LayoutA >::value) && 
+                                    platform::is_same< ElementA, int8_t >::value && 
+                                    platform::is_same< ElementB, int8_t >::value;
+
+  using dp4a_type = typename platform::conditional< use_dp4a , int8_t, bool >::type;
+
+  /// Thread-level matrix multiply accumulate operator
+  using ThreadMma = thread::Mma<
+    GemmShape<
+      Shape::kM / Policy::WarpShape::kRow,
+      Shape::kN / Policy::WarpShape::kColumn,
+      Policy::LaneMmaShape::kK>,
+    ElementA,
+    ThreadLayoutA,
+    ElementB,
+    ThreadLayoutB,
+    ElementC,
+    LayoutC,
+    arch::OpMultiplyAdd,
+    dp4a_type
+  >;
 
-  MatrixCoord lane_offset_;
-  int channel_idx_;
-  int base_channel_idx_;
-  int warps_n_;
+  /// Underlying matrix multiply operator (concept: arch::Mma)
+  using ArchMmaOperator = typename ThreadMma::ArchMmaOperator;
 
- public:
+  /// Indicates math operator 
+  using MathOperator = typename ArchMmaOperator::Operator;
   
-  /// Default ctor constructs null iterator
-  CUTLASS_HOST_DEVICE
-  DepthwiseMmaSimtTileIterator():Base() { }
-
-  /// Constructor from TensorRef
-  CUTLASS_HOST_DEVICE
-  DepthwiseMmaSimtTileIterator(
-    TensorRef ref, 
-    int lane_id
-  ) : Base(ref, lane_id) {
-
-    // compute offset based on thread ID and lane layout
-    typename Policy::LaneLayout lane_layout = Policy::get_lane_layout();
-
-    warps_n_ = -1;
-    channel_idx_ = 0;
-    base_channel_idx_ = 0;
-    lane_offset_ = lane_layout.inverse(lane_id) * MatrixCoord(0, Policy::LaneMmaShape::kN);
-  }
-  
-  /// Advances an iterator along logical dimensions of matrix in units of whole tiles
-  CUTLASS_HOST_DEVICE
-  DepthwiseMmaSimtTileIterator &add_tile_offset(TensorCoord const &coord) {
+  /// Shape of the underlying instruction
+  using InstructionShape = GemmShape<1,1,use_dp4a ? 4 : 1>;
+
+public:
+
+  /// Iterates over the A operand in memory
+  using IteratorA = MmaSimtTileIterator<
+    MatrixShape<Shape::kM, Policy::LaneMmaShape::kK>,
+    Operand::kA,
+    ElementA,
+    LayoutA,
+    Policy,
+    PartitionsK,
+    Shape::kK
+  >;
+
+  /// Storage for A tile
+  using FragmentA = typename IteratorA::Fragment;
+
+  /// Storage for transformed A tile
+  using TransformedFragmentA = FragmentA;
+
+  /// Iterates over the B operand in memory
+  using IteratorB = MmaSimtTileIterator<
+    MatrixShape<Policy::LaneMmaShape::kK, Shape::kN>,
+    Operand::kB,
+    ElementB,
+    LayoutB,
+    Policy,
+    PartitionsK,
+    Shape::kK
+  >;
+
+  /// Storage for B tile
+  using FragmentB = typename IteratorB::Fragment;
+
+  /// Storage for transformed A tile
+  using TransformedFragmentB = FragmentB;
+
+  /// Iterates over the C operand in memory
+  using IteratorC = MmaSimtTileIterator<
+    MatrixShape<Shape::kM, Shape::kN>,
+    Operand::kC,
+    ElementC,
+    LayoutC,
+    Policy
+  >;
+
+  /// Storage for C tile
+  using FragmentC = typename ThreadMma::FragmentC;
+
+public:
+
+  //
+  // Methods
+  //
 
-    if(warps_n_ == -1){
-        warps_n_ = coord.column();
+  /// Ctor
+  CUTLASS_DEVICE
+  MmaSimt() {}
+
+  /// Performs a warp-level matrix multiply-accumulate operation
+  CUTLASS_DEVICE
+  void operator()(
+    FragmentC &d, 
+    FragmentA a, 
+    FragmentB b, 
+    FragmentC const &c, int group_idx = 0) const {
+
+    ThreadMma mma;
+
+    if (kTransformA == ComplexTransform::kConjugate) {
+      conjugate<FragmentA> conj_a;
+      a = conj_a(a);
     }
-    
-    Base::add_tile_offset(coord);
-    return *this;
-  }
 
-  /// Loads a fragment from memory at the location pointed to by the iterator. (vector loads)
-  CUTLASS_HOST_DEVICE
-  void load_with_pointer_offset(Fragment &frag, Index pointer_offset) const {
-    Array<Element, Policy::LaneMmaShape::kN> *dst_ptr =
-        reinterpret_cast<Array<Element, Policy::LaneMmaShape::kN> *>(&frag);
-
-    CUTLASS_PRAGMA_UNROLL
-    for (int k = 0; k < Iterations::kRow; ++k) {
-      CUTLASS_PRAGMA_UNROLL
-      for (int n = 0; n < Iterations::kColumn; ++n) {
-
-        void const *ptr = this->ref_.data() +
-                          this->ref_.offset({-(channel_idx_ - base_channel_idx_),
-                                             n * Policy::WarpShape::kColumn}) +
-                          pointer_offset / Policy::LaneMmaShape::kN;
-
-        // Base_k of a warp +  Base_k of current threads.
-        int thread_k_base_idx =
-            warps_n_ * Shape::kColumn / Policy::LaneMmaShape::kN + lane_offset_.column();
-
-        if (channel_idx_ + k == thread_k_base_idx + n * Policy::WarpShape::kColumn) {
-          // Depthwise kernel would only do computation when channel == k.
-          // Loads an element when the current computation channel == the k corresponding to this thread.
-          arch::shared_load(dst_ptr[n + k * Iterations::kColumn], ptr);
-        } else {
-          // Reduce SMEM load
-          dst_ptr[n + k * Iterations::kColumn].fill(Element(0));
-        }
-      }
+    if (kTransformB == ComplexTransform::kConjugate) {
+      conjugate<FragmentB> conj_b;
+      b = conj_b(b);
     }
-  }
 
-  /// Loads a fragment from memory at the location pointed to by the iterator.
-  CUTLASS_HOST_DEVICE
-  void load(Fragment &frag) const {
-    load_with_pointer_offset(frag, 0);
+    mma(d, a, b, c);
   }
-  
-  /// Notify the iterator which k-group it is currently pointing to.
-  ///
-  /// This does not advance the iterator. Rather, it overrides its internal
-  /// tracking with constant-valued k-group index
+
+  /// Transform the mma operands to the required types
   CUTLASS_DEVICE
-  void set_kgroup_index(int k_group) {
-    if(k_group % PartitionGroupSize == 0 && k_group != 0){
-      base_channel_idx_ = k_group;
-    }
-    channel_idx_ = k_group;
+  void transform(TransformedFragmentA &dst_A, TransformedFragmentB &dst_B,
+                 FragmentA const &A, FragmentB const &B) const {
+    //TODO: Implement this
+    dst_A = A;
+    dst_B = B;
   }
 };
 
-///////////////////////////////////////////////////////////////////////////////////////////////////
+/////////////////////////////////////////////////////////////////////////////////////////////////
 
 } // namespace warp
 } // namespace gemm
 } // namespace cutlass
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/conv/warp/scale_bias_relu_transform.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/conv/warp/scale_bias_relu_transform.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/coord.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/coord.h`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -96,20 +96,29 @@
   CUTLASS_HOST_DEVICE
   Coord(Index const (&_idx)[kRank]) {
     for (int i = 0; i < kRank; ++i) {
       idx[i] = _idx[i];
     }
   }
 
+  /// Constructs from some other Coord
+  template <int R, typename I, typename L>
+  CUTLASS_HOST_DEVICE
+  Coord(Coord<R, I, L> other) {
+    for (int i = 0; i < kRank; ++i) {
+      idx[i] = other[i];
+    }
+  }
+
   /// Returns a slice of the Coord which may be larger or smaller in rank
   /// than this.
   template <int Slice>
   CUTLASS_HOST_DEVICE
-  Coord<Slice> slice(int start = 0, Index identity = 0) const {
-    Coord<Slice> result;
+  Coord<Slice, Index, LongIndex> slice(int start = 0, Index identity = 0) const {
+    Coord<Slice, Index, LongIndex> result;
     for (int i = 0; i < Slice; ++i) {
       if (i + start < kRank) {
         result[i] = idx[i + start];
       }
       else {
         result[i] = identity;
       }
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/core_io.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/core_io.h`

 * *Files 2% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -55,15 +55,17 @@
 /// Output operator for CUDA built-in dim3 type
 inline std::ostream &operator<<(std::ostream &out, dim3 d) {
   return out << d.x << ", " << d.y << ", " << d.z;
 }
 
 /// Output operator for CUDA built-in error type
 inline std::ostream &operator<<(std::ostream &out, cudaError_t error) {
+#if defined(__NVCC__) || (defined(__clang__) && defined(__CUDA__)) || defined(__CUDACC_RTC__)
   return out << cudaGetErrorString(error);
+#endif
 }
 
 ///////////////////////////////////////////////////////////////////////////////////////////////////
 
 namespace cutlass {
 
 ///////////////////////////////////////////////////////////////////////////////////////////////////
@@ -248,16 +250,17 @@
 //                         stream operators for cutlass::conv namespace                          //
 ///////////////////////////////////////////////////////////////////////////////////////////////////
 namespace conv {
 /// Default printing to ostream for Conv2dProblemSize
 inline
 std::ostream& operator<<(std::ostream& out, Conv2dProblemSize const& problem) {
   out << "NHWC: (" << problem.N << ", " << problem.H << ", " << problem.W << ", " << problem.C << ")" << std::endl
-      << "KRSC: (" << problem.K << ", " << problem.R << ", " << problem.S << ", " << problem.C << ")" << std::endl
+      << "KRSC: (" << problem.K << ", " << problem.R << ", " << problem.S << ", " << problem.C / problem.groups << ")" << std::endl
       << "NPQK: (" << problem.N << ", " << problem.P << ", " << problem.Q << ", " << problem.K << ")" << std::endl
+      << "groups: (" << problem.groups << ")" << std::endl
       << "Pad_h, Pad_w: (" << problem.pad_h << ", " << problem.pad_w << ")" << std::endl
       << "Stride_h, Stride_w: (" << problem.stride_h << ", " << problem.stride_w << ")" << std::endl
       << "Dilation_h, Dilation_w: (" << problem.dilation_h << ", " << problem.dilation_w << ")" << std::endl
       << "split_k_slices: (" << problem.split_k_slices << ")" << std::endl
       << "mode: (" << ((problem.mode==conv::Mode::kConvolution) ? "conv" : "xcross") << ")";
 
   return out;
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/cutlass.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/cutlass.h`

 * *Files 3% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -34,20 +34,43 @@
 */
 
 #pragma once
 
 ////////////////////////////////////////////////////////////////////////////////////////////////////
 
 #ifdef CUTLASS_NAMESPACE
-#define cutlass CUTLASS_NAMESPACE
+#define concat_tok(a, b) a ## b
+#define mkcutlassnamespace(pre, ns) concat_tok(pre, ns)
+#define cutlass mkcutlassnamespace(cutlass_, CUTLASS_NAMESPACE)
 #endif
 
 ////////////////////////////////////////////////////////////////////////////////////////////////////
 
-#define CUTLASS_UNUSED(expr) do { ; } while (&expr != &expr)
+#if defined(__NVCC__) || (defined(__clang__) && defined(__CUDA__))
+#define CUTLASS_HOST_DEVICE __forceinline__ __device__ __host__
+#define CUTLASS_DEVICE __forceinline__ __device__
+#elif defined(__CUDACC_RTC__)
+#define CUTLASS_HOST_DEVICE __forceinline__ __device__
+#define CUTLASS_DEVICE __forceinline__ __device__
+#else
+#define CUTLASS_HOST_DEVICE inline
+#define CUTLASS_DEVICE inline
+#endif
+
+////////////////////////////////////////////////////////////////////////////////////////////////////
+
+template<typename T>
+CUTLASS_HOST_DEVICE void __CUTLASS_UNUSED(T const &) 
+{ }
+
+#if defined(__GNUC__)
+  #define CUTLASS_UNUSED(expr) __CUTLASS_UNUSED(expr)
+#else
+  #define CUTLASS_UNUSED(expr) do { ; } while (&expr != &expr)
+#endif
 
 #if !defined(__CUDACC_RTC__)
 
 #include <assert.h>
 
 #if defined(_MSC_VER)
   #define CUTLASS_NOT_IMPLEMENTED() assert(0 && __FUNCSIG__)
@@ -65,27 +88,14 @@
 
 #endif
 
 ////////////////////////////////////////////////////////////////////////////////////////////////////
 
 namespace cutlass {
 
-////////////////////////////////////////////////////////////////////////////////////////////////////
-
-#if defined(__NVCC__) || (defined(__clang__) && defined(__CUDA__))
-#define CUTLASS_HOST_DEVICE __forceinline__ __device__ __host__
-#define CUTLASS_DEVICE __forceinline__ __device__
-#elif defined(__CUDACC_RTC__)
-#define CUTLASS_HOST_DEVICE __forceinline__ __device__
-#define CUTLASS_DEVICE __forceinline__ __device__
-#else
-#define CUTLASS_HOST_DEVICE inline
-#define CUTLASS_DEVICE inline
-#endif
-
 /// Status code returned by CUTLASS operations
 enum class Status {
   kSuccess,                    ///< Operation was successful.
   kErrorMisalignedOperand,     ///< operands fail alignment requirements.
   kErrorInvalidDataType,       ///< DataType fails requirement.
   kErrorInvalidLayout,         ///< Layout fails alignment requirement.
   kErrorInvalidProblem,        ///< Specified problem size is not supported by operator.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/device_kernel.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/cute.cpp`

 * *Files 18% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -24,39 +24,31 @@
  * DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR
  * SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER
  * CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,
  * OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
  * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
  *
  **************************************************************************************************/
-/*! \file
-    \brief Template for generic CUTLASS kernel.
+/* \file
+   \brief binding CuTe C++ APIs to Python
 */
 
-#pragma once
+#include <pybind11/pybind11.h>
+#include <pybind11/stl_bind.h>
 
-#include "cutlass/cutlass.h"
-////////////////////////////////////////////////////////////////////////////////
+#include "cute/arch/mma_sm90_gmma.hpp"
 
-namespace cutlass {
+namespace py = pybind11;
 
-////////////////////////////////////////////////////////////////////////////////
 
-/// Generic CUTLASS kernel template.
-template <typename Operator>
-__global__
-void Kernel(typename Operator::Params params) {
-  // Dynamic shared memory base pointer
-  extern __shared__ int SharedStorageBase[];
+PYBIND11_MODULE(cute, m) {
 
-  // Declare pointer to dynamic shared memory.
-  typename Operator::SharedStorage *shared_storage =
-      reinterpret_cast<typename Operator::SharedStorage *>(SharedStorageBase);
+    // module doc
+    m.doc() = "CuTe C++ bindings";
 
-  Operator op;
-
-  op(params, *shared_storage);
+    py::enum_<cute::GMMA::Major>(m, "GMMAMajor",
+        R"pbdoc(classification of CuTe GMMA tensor major specification)pbdoc")
+        .value("K", cute::GMMA::Major::K,
+            R"pbdoc(Tensor is contiguous in reduction dimension)pbdoc")
+        .value("MN", cute::GMMA::Major::MN,
+            R"pbdoc(Tensor is contiguous in non-reduction dimension)pbdoc");
 }
-
-////////////////////////////////////////////////////////////////////////////////
-} /// namespace cutlass
-
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/epilogue/thread/activation.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/epilogue/thread/activation.h`

 * *Files 6% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -46,24 +46,14 @@
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
 namespace cutlass {
 namespace epilogue {
 namespace thread {
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
-
-template <typename T>
-struct Identity {
-  CUTLASS_HOST_DEVICE
-  T operator()(T value) const {
-    return value;
-  }
-};
-
-/////////////////////////////////////////////////////////////////////////////////////////////////
 template <typename T>
 struct LinearCombinationGenericParams {
   T alpha;                  ///< scales accumulators
   T beta;                   ///< scales source tensor
   T const *alpha_ptr;       ///< pointer to accumulator scalar - if not null, loads it from memory
   T const *beta_ptr;        ///< pointer to source scalar - if not null, loads it from memory
 
@@ -89,14 +79,47 @@
     T const *alpha_ptr,
     T const *beta_ptr = nullptr
   ): alpha(0), beta(0), alpha_ptr(alpha_ptr), beta_ptr(beta_ptr) { }
 };
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
+// Identity operator
+template <typename T>
+struct Identity {
+  static const bool kIsHeavy=false;
+
+  CUTLASS_HOST_DEVICE
+  T operator()(T value) const {
+    return value;
+  }
+
+  using Params = LinearCombinationGenericParams<T>;
+
+  CUTLASS_HOST_DEVICE
+  T operator()(T const &value, Params const &params_) const {
+    return this->operator()(value);
+  }
+};
+
+template <typename T, int N>
+struct Identity<Array<T, N> > {
+  CUTLASS_HOST_DEVICE
+  Array<T, N> operator()(Array<T, N> const &value) const {
+    return value;
+  }
+
+  using Params = LinearCombinationGenericParams<T>;
+
+  CUTLASS_HOST_DEVICE
+  Array<T, N> operator()(Array<T, N> const &value, Params const &params_) const {
+    return this->operator()(value);
+  }
+};
+
 /// ReLu operator - propagates NaNs
 /// Always put threshold in the right hand side of max to propagate NaN.
 template <typename T>
 struct ReLu {
   static const bool kIsHeavy=false;
   CUTLASS_HOST_DEVICE
   T operator()(T const & threshold, T value) const {
@@ -156,15 +179,15 @@
     // Methods
     using LinearCombinationGenericParams<T>::LinearCombinationGenericParams;
 
     CUTLASS_HOST_DEVICE
     Params():
       LinearCombinationGenericParams<T>(),
       leaky_alpha(T(1)) {}
-    
+ 
     CUTLASS_HOST_DEVICE
     Params(
       T alpha,
       T beta,
       T leaky_alpha = T(1)
     ): LinearCombinationGenericParams<T>(alpha, beta), leaky_alpha(leaky_alpha) {}
   };
@@ -190,40 +213,40 @@
 
     // Methods
 
     CUTLASS_HOST_DEVICE
     Params():
       LinearCombinationGenericParams<T>(),
       leaky_alpha(T(1)) {}
-    
+
     CUTLASS_HOST_DEVICE
     Params(
       T alpha,
       T beta,
       T leaky_alpha = T(1)
     ): LinearCombinationGenericParams<T>(alpha, beta), leaky_alpha(leaky_alpha) {}
   };
 
 
   CUTLASS_HOST_DEVICE
-  Array<T, N> operator()(Array<T, N> const &rhs, T const & alpha_recip) const {
+  Array<T, N> operator()(Array<T, N> const &value, T const & alpha_recip) const {
     Array<T, N> y;
     LeakyReLU<T> leaky_op;
 
     CUTLASS_PRAGMA_UNROLL
-    for (int i = 0; i < int(rhs.size()); ++i) {
-      y[i] = leaky_op(rhs[i], alpha_recip);
+    for (int i = 0; i < int(value.size()); ++i) {
+      y[i] = leaky_op(value[i], alpha_recip);
     }
 
     return y;
   }
 
   CUTLASS_HOST_DEVICE
-  Array<T, N> operator()(Array<T, N> const &rhs, Params const &params_) const {
-    return this->operator()(rhs, params_.leaky_alpha);
+  Array<T, N> operator()(Array<T, N> const &value, Params const &params_) const {
+    return this->operator()(value, params_.leaky_alpha);
   }
 };
 
 // Tanh operator
 template <typename T>
 struct Tanh {
   CUTLASS_HOST_DEVICE
@@ -238,31 +261,31 @@
     return this->operator()(scalar);
   }
 };
 
 template <typename T, int N>
 struct Tanh<Array<T, N> > {
   CUTLASS_HOST_DEVICE
-  Array<T, N> operator()(Array<T, N> const &rhs) const {
+  Array<T, N> operator()(Array<T, N> const &value) const {
     Array<T, N> y;
     Tanh<T> tanh_op;
 
     CUTLASS_PRAGMA_UNROLL
     for (int i = 0; i < N; ++i) {
-      y[i] = tanh_op(rhs[i]);
+      y[i] = tanh_op(value[i]);
     }
 
     return y;
   }
 
   using Params = LinearCombinationGenericParams<T>;
 
   CUTLASS_HOST_DEVICE
-  Array<T, N> operator()(Array<T, N> const &rhs, Params const &params_) const {
-    return this->operator()(rhs);
+  Array<T, N> operator()(Array<T, N> const &value, Params const &params_) const {
+    return this->operator()(value);
   }
 };
 
 template <int N>
 struct Tanh<Array<half_t, N>> {
   using T = half_t;
 
@@ -272,16 +295,16 @@
     return tanh(z);
 
   }
 
   using Params = LinearCombinationGenericParams<T>;
 
   CUTLASS_HOST_DEVICE
-  Array<T, N> operator()(Array<T, N> const &rhs, Params const &params_) const {
-    return this->operator()(rhs);
+  Array<T, N> operator()(Array<T, N> const &value, Params const &params_) const {
+    return this->operator()(value);
   }
 };
 
 // Sigmoid operator
 template <typename T>
 struct Sigmoid {
   CUTLASS_HOST_DEVICE
@@ -296,31 +319,31 @@
     return this->operator()(scalar);
   }
 };
 
 template <typename T, int N>
 struct Sigmoid<Array<T, N> > {
   CUTLASS_HOST_DEVICE
-  Array<T, N> operator()(Array<T, N> const &rhs) const {
+  Array<T, N> operator()(Array<T, N> const &value) const {
     Array<T, N> y;
     Sigmoid<T> sigmoid_op;
 
     CUTLASS_PRAGMA_UNROLL
     for (int i = 0; i < N; ++i) {
-      y[i] = sigmoid_op(rhs[i]);
+      y[i] = sigmoid_op(value[i]);
     }
 
     return y;
   }
 
   using Params = LinearCombinationGenericParams<T>;
 
   CUTLASS_HOST_DEVICE
-  Array<T, N> operator()(Array<T, N> const &rhs, Params const &params_) const {
-    return this->operator()(rhs);
+  Array<T, N> operator()(Array<T, N> const &value, Params const &params_) const {
+    return this->operator()(value);
   }
 };
 
 template <int N>
 struct Sigmoid<Array<half_t, N>> {
   using T = half_t;
 
@@ -341,14 +364,15 @@
                add(cutlass::constants::one<T>(),
                    fast_exp(neg(z))));
 #endif
   }
 
   using Params = LinearCombinationGenericParams<T>;
 
+  CUTLASS_HOST_DEVICE
   Array<T, N> operator()(Array<T, N> const &z, Params const &params_) const {
     return this->operator()(z);
   }
 };
 
 // SiLu (swish) operator introduced by Elfwing et al. in the following paper
 // "Sigmoid-Weighted Linear Units for Neural Network Function Approximation in Reinforcement Learning" (2017)
@@ -370,25 +394,25 @@
     return this->operator()(scalar);
   }
 };
 
 template <typename T, int N>
 struct SiLu<Array<T, N>> {
   CUTLASS_HOST_DEVICE
-  Array<T, N> operator()(Array<T, N> const &rhs) const {
+  Array<T, N> operator()(Array<T, N> const &value) const {
     Sigmoid<Array<T, N>> sigmoid_op;
     multiplies<Array<T, N>>     mul;
-    return mul(rhs, sigmoid_op(rhs));
+    return mul(value, sigmoid_op(value));
   }
 
   using Params = LinearCombinationGenericParams<T>;
 
   CUTLASS_HOST_DEVICE
-  Array<T, N> operator()(Array<T, N> const &rhs, Params const &params_) const {
-    return this->operator()(rhs);
+  Array<T, N> operator()(Array<T, N> const &value, Params const &params_) const {
+    return this->operator()(value);
   }
 };
 
 // Hardswish operator introduced by Howard et al. in the following paper
 // "Searching for MobileNetV3" (2019)
 // https://arxiv.org/pdf/1905.02244.pdf
 // It is used in models based on MobilenetNetV3.
@@ -430,21 +454,21 @@
     return this->operator()(x);
   }
 };
 
 template <typename T, int N>
 struct HardSwish<Array<T, N> > {
   CUTLASS_HOST_DEVICE
-  Array<T, N> operator()(Array<T, N> const &rhs) const {
+  Array<T, N> operator()(Array<T, N> const &value) const {
     Array<T, N> y;
     HardSwish<T> hardswish_op;
 
     CUTLASS_PRAGMA_UNROLL
     for (int i = 0; i < N; ++i) {
-      y[i] = hardswish_op(rhs[i]);
+      y[i] = hardswish_op(value[i]);
     }
 
     return y;
   }
 
   using Params = LinearCombinationGenericParams<T>;
 
@@ -455,21 +479,21 @@
 };
 
 template <int N>
 struct HardSwish<Array<half_t, N> > {
   using T = half_t;
 
   CUTLASS_HOST_DEVICE
-  Array<T, N> operator()(Array<T, N> const &rhs) const {
+  Array<T, N> operator()(Array<T, N> const &value) const {
     minimum<Array<T, N> > mn;
     maximum<Array<T, N> > mx;
     multiplies<Array<T, N> > mul;
     plus<Array<T, N> > add;
- 
-    return mul(mul(mn(mx(add(rhs, T(3)), T(0)), T(6)), rhs), T(0.16666667f));
+
+    return mul(mul(mn(mx(add(value, T(3)), T(0)), T(6)), value), T(0.16666667f));
   }
 
   using Params = LinearCombinationGenericParams<T>;
 
   CUTLASS_HOST_DEVICE
   Array<T, N> operator()(Array<T, N> const &x, Params const &params_) const {
     return this->operator()(x);
@@ -489,15 +513,15 @@
 template <typename T>
 struct GELU {
   CUTLASS_HOST_DEVICE
   T operator()(T const &scalar) const {
     return T(cutlass::constants::half<T>() * scalar *
       (cutlass::constants::one<T>() + (T)erff((float)(scalar / cutlass::constants::root_two<T>()))));
   }
-  
+
   using Params = LinearCombinationGenericParams<T>;
 
   CUTLASS_HOST_DEVICE
   T operator()(T const &scalar, Params const &params_) const {
     return this->operator()(scalar);
   }
 };
@@ -505,15 +529,15 @@
 template <>
 struct GELU<float> {
   CUTLASS_HOST_DEVICE
   float operator()(float const &scalar) const {
     return cutlass::constants::half<float>() * scalar *
       (cutlass::constants::one<float>() + erff( scalar / cutlass::constants::root_two<float>() ));
   }
-  
+
   using Params = LinearCombinationGenericParams<float>;
 
   CUTLASS_HOST_DEVICE
   float operator()(float const &scalar, Params const &params_) const {
     return this->operator()(scalar);
   }
 };
@@ -521,43 +545,43 @@
 template <>
 struct GELU<double> {
   CUTLASS_HOST_DEVICE
   double operator()(double const &scalar) const {
     return cutlass::constants::half<double>() * scalar *
       (cutlass::constants::one<double>() + erf( scalar / cutlass::constants::root_two<double>() ));
   }
-  
+
   using Params = LinearCombinationGenericParams<double>;
 
   CUTLASS_HOST_DEVICE
   double operator()(double const &scalar, Params const &params_) const {
     return this->operator()(scalar);
   }
 };
 
 template <typename T, int N>
 struct GELU<Array<T, N> > {
   CUTLASS_HOST_DEVICE
-  Array<T, N> operator()(Array<T, N> const &rhs) const {
+  Array<T, N> operator()(Array<T, N> const &value) const {
     Array<T, N> y;
     GELU<T> gelu_op;
 
     CUTLASS_PRAGMA_UNROLL
     for (int i = 0; i < N; ++i) {
-      y[i] = gelu_op(rhs[i]);
+      y[i] = gelu_op(value[i]);
     }
 
     return y;
   }
-  
+
   using Params = LinearCombinationGenericParams<T>;
 
   CUTLASS_HOST_DEVICE
-  Array<T, N> operator()(Array<T, N> const &rhs, Params const &params_) const {
-    return this->operator()(rhs);
+  Array<T, N> operator()(Array<T, N> const &value, Params const &params_) const {
+    return this->operator()(value);
   }
 };
 
 // GELU operator implemented using the Taylor series approximation
 template <typename T>
 struct GELU_taylor {
   static const bool kIsHeavy=true;
@@ -568,15 +592,19 @@
     T k1 = T(0.044715);
 
     return T(cutlass::constants::half<T>() * z *
       (cutlass::constants::one<T>() + fast_tanh(k0 * z * (cutlass::constants::one<T>() + k1 * z * z))));
   }
 
   using Params = LinearCombinationGenericParams<T>;
-  
+
+  CUTLASS_HOST_DEVICE
+  T operator()(T const &scalar, Params const &params_) const {
+    return this->operator()(scalar);
+  }
 };
 
 template <int N>
 struct GELU_taylor<Array<half_t, N> > {
   static const bool kIsHeavy=true;
   CUTLASS_HOST_DEVICE
   Array<half_t, N> operator()(Array<half_t, N> const &z) const {
@@ -597,33 +625,43 @@
 
     y = mul(mul(z, cutlass::constants::half<T>()), add(cutlass::constants::one<T>(), tanh(u)));
 
     return y;
   }
 
   using Params = LinearCombinationGenericParams<half_t>;
+
+  CUTLASS_HOST_DEVICE
+  Array<half_t, N> operator()(Array<half_t, N> const &value, Params const &params_) const {
+    return this->operator()(value);
+  }
 };
 
 template <typename T, int N>
 struct GELU_taylor<Array<T, N> > {
   static const bool kIsHeavy=true;
   CUTLASS_HOST_DEVICE
-  Array<T, N> operator()(Array<T, N> const &rhs) const {
+  Array<T, N> operator()(Array<T, N> const &value) const {
     Array<T, N> y;
     GELU_taylor<T> gelu_op;
 
     CUTLASS_PRAGMA_UNROLL
     for (int i = 0; i < N; ++i) {
-      y[i] = gelu_op(rhs[i]);
+      y[i] = gelu_op(value[i]);
     }
 
     return y;
   }
-  
+
   using Params = LinearCombinationGenericParams<T>;
+
+  CUTLASS_HOST_DEVICE
+  Array<T, N> operator()(Array<T, N> const &value, Params const &params_) const {
+    return this->operator()(value);
+  }
 };
 
 /// Computes backwards pass for GELU operator assuming d_t is the layer gradient and
 /// z is computed from the forward pass.
 template <typename T>
 struct dGELU {
   CUTLASS_HOST_DEVICE
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/epilogue/thread/conversion_op.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/epilogue/thread/conversion_op.h`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/epilogue/thread/linear_combination.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/epilogue/thread/linear_combination.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/epilogue/thread/linear_combination_bias_elementwise.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/epilogue/thread/linear_combination_bias_elementwise.h`

 * *Files 2% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -74,14 +74,17 @@
   using ElementT = ElementT_;
   static int const kElementsPerAccess = ElementsPerAccess;
   static int const kCount = kElementsPerAccess;
 
   using ElementwiseOp = ElementwiseOp_;
   using BinaryOp = BinaryOp_;
 
+  // Indicates that this epilogue applies only one binary operation
+  static bool const kIsSingleSource = true;
+
   using FragmentAccumulator = Array<ElementAccumulator, kElementsPerAccess>;
   using FragmentCompute = Array<ElementCompute, kElementsPerAccess>;
   using FragmentC = Array<ElementOutput, kElementsPerAccess>;
   using FragmentZ = Array<ElementZ, kElementsPerAccess>;
   using FragmentT = Array<ElementT, kElementsPerAccess>;
 
   using FragmentOutput = FragmentZ;
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/epilogue/thread/linear_combination_bias_relu.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/epilogue/thread/linear_combination_bias_relu.h`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -219,14 +219,17 @@
 
   static int const kElementsPerAccess = ElementsPerAccess;
   static int const kCount = kElementsPerAccess;
 
   using ElementwiseOp = ReLu<ElementCompute>;
   using BinaryOp = plus<ElementCompute>;
 
+  // Indicates that this epilogue applies only one binary operation
+  static bool const kIsSingleSource = true;
+
   using FragmentAccumulator = Array<ElementAccumulator, kElementsPerAccess>;
   using FragmentCompute = Array<ElementCompute, kElementsPerAccess>;
   using FragmentC = Array<ElementOutput, kElementsPerAccess>;
   using FragmentZ = Array<ElementZ, kElementsPerAccess>;
   using FragmentT = Array<ElementT, kElementsPerAccess>;
 
   /// If true, the 'Z' tensor is stored
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/epilogue/thread/linear_combination_clamp.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/epilogue/thread/linear_combination_clamp.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/epilogue/thread/linear_combination_dgelu.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/epilogue/thread/linear_combination_dgelu.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/epilogue/thread/linear_combination_drelu.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/epilogue/thread/linear_combination_drelu.h`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/epilogue/thread/linear_combination_gelu.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/epilogue/thread/linear_combination_gelu.h`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/epilogue/thread/linear_combination_generic.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/epilogue/thread/linear_combination_generic.h`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/epilogue/thread/linear_combination_hardswish.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/epilogue/thread/linear_combination_hardswish.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /*************************************************************************************************** 
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/epilogue/thread/linear_combination_leaky_relu.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/epilogue/thread/linear_combination_leaky_relu.h`

 * *Files 2% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -33,14 +33,15 @@
 
 #include "cutlass/cutlass.h"
 #include "cutlass/numeric_types.h"
 #include "cutlass/array.h"
 #include "cutlass/functional.h"
 #include "cutlass/numeric_conversion.h"
 #include "cutlass/epilogue/thread/activation.h"
+#include "cutlass/epilogue/thread/scale_type.h"
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
 namespace cutlass {
 namespace epilogue {
 namespace thread {
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/epilogue/thread/linear_combination_params.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/epilogue/thread/linear_combination_params.h`

 * *Files 4% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/epilogue/thread/linear_combination_planar_complex.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/epilogue/thread/linear_combination_planar_complex.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/epilogue/thread/linear_combination_relu.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/epilogue/thread/linear_combination_relu.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/epilogue/thread/linear_combination_relu0.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/epilogue/thread/linear_combination_relu0.h`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/epilogue/thread/linear_combination_residual_block.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/epilogue/thread/linear_combination_with_elementwise.h`

 * *Files 25% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -24,144 +24,209 @@
  * DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR
  * SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER
  * CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,
  * OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
  * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
  *
  **************************************************************************************************/
-
 /*! \file
-  \brief Epilogue functor specialized for residual blocks in deep neural network.
+  
+  \brief Functor performing linear combination with elementwise
 */
 
 #pragma once
 
+#include <cutlass/half.h>
+#include "cutlass/cutlass.h"
+#include "cutlass/numeric_types.h"
 #include "cutlass/array.h"
+#include "cutlass/constants.h"
+#include "cutlass/fast_math.h"
 #include "cutlass/functional.h"
 #include "cutlass/numeric_conversion.h"
+#include "cutlass/epilogue/thread/activation.h"
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
 namespace cutlass {
 namespace epilogue {
 namespace thread {
 
-// /// Models a residual block of the form: UnaryOp(BinaryOp(ActivationOp(TensorOp(X) + bias), residual))
-template <typename ElementOutput_, typename ElementAccumulator_,
-          typename ElementCompute_, typename ElementC_, int ElementsPerAccess,
-          template <typename T> class ActivationOp_,
-          template <typename T> class BinaryOp_,
-          template <typename T> class UnaryOp_>
-class LinearCombinationResidualBlock {
+/////////////////////////////////////////////////////////////////////////////////////////////////
+
+/// Applies a linear combination operator to an array of elements.
+///
+/// D = alpha * accumulator + beta * source + uniform
+///
+template <
+  typename ElementCompute_,                            ///< Data type returned by this functor
+  typename ElementAccumulator_,                        ///< Data type of accumulators
+  typename ElementSource_,                             ///< Data type of source tensor
+  typename ElementTensor_,                             ///< Data type of additional tensor
+  int Count,                                           ///< Number of elements computed per operation
+                                                       ///< Usually it is 128/sizeof_bits<ElementOutput_>,
+                                                       ///< but we use 64 or 32 sometimes when there are not enough data to store
+  FloatRoundStyle Round = FloatRoundStyle::round_to_nearest
+>
+class LinearCombinationWithElementwise {
 public:
 
-  using ElementOutput = ElementC_;
-  using ElementC = ElementC_;
-  using ElementAccumulator = ElementAccumulator_;
+  using ElementOutput = ElementSource_;
   using ElementCompute = ElementCompute_;
-  static int const kElementsPerAccess = ElementsPerAccess;
-  static int const kCount = kElementsPerAccess;
-
-  using UnaryOp = UnaryOp_<Array<ElementCompute, kCount>>;
-  using BinaryOp = BinaryOp_<Array<ElementCompute, kCount>>;
-  using ActivationOp = ActivationOp_<Array<ElementCompute, kCount>>;
-
-  using FragmentAccumulator = Array<ElementAccumulator, kElementsPerAccess>;
-  using FragmentCompute = Array<ElementCompute, kElementsPerAccess>;
-  using FragmentC = Array<ElementC, kElementsPerAccess>;
-  using FragmentOutput = Array<ElementOutput, kElementsPerAccess>;
-
-  using ElementZ = ElementOutput_;
-  using ElementT = ElementZ;
-  using FragmentZ = Array<ElementZ, kElementsPerAccess>;
-  using FragmentT = Array<ElementT, kElementsPerAccess>;
+  using ElementAccumulator = ElementAccumulator_;
+  using ElementSource = ElementSource_;
+  using ElementTensor = ElementTensor_;
 
   static bool const kIsHeavy = true;
-  static bool const kStoreZ = true;
-  static bool const kStoreT = false;
+
+  static int const kCount = Count;
+
+  using FragmentCompute = Array<ElementCompute, kCount>;
+  using FragmentAccumulator = Array<ElementAccumulator, kCount>;
+  using FragmentSource = Array<ElementSource, kCount>;
+  using FragmentTensor = Array<ElementTensor, kCount>;
+
+  static FloatRoundStyle const kRound = Round;
 
   /// Host-constructable parameters structure
   struct Params {
 
     ElementCompute alpha;                  ///< scales accumulators
-    ElementCompute beta;                   ///< scales residual input
-    ElementCompute const *alpha_ptr{nullptr};       ///< pointer to accumulator scalar - if not null, loads it from memory
-    ElementCompute const *beta_ptr{nullptr};        ///< pointer to residual scalar - if not null, loads it from memory
+    ElementCompute beta;                   ///< scales source tensor
+    ElementCompute threshold;              ///< minimum value that is output 
+    ElementCompute const *alpha_ptr;       ///< pointer to accumulator scalar - if not null, loads it from memory
+    ElementCompute const *beta_ptr;        ///< pointer to source scalar - if not null, loads it from memory
+    //
+    // Methods
+    //
 
     CUTLASS_HOST_DEVICE
-    Params() : alpha(ElementCompute(1)), beta(ElementCompute(1)) {}
+    Params(): 
+      alpha(ElementCompute(1)), 
+      beta(ElementCompute(0)),
+      threshold(ElementCompute(0)), 
+      alpha_ptr(nullptr), 
+      beta_ptr(nullptr) { }
 
     CUTLASS_HOST_DEVICE
-    Params(ElementCompute alpha, ElementCompute beta)
-        : alpha(alpha), beta(beta) {}
+    Params(
+      ElementCompute alpha,
+      ElementCompute beta,
+      ElementCompute threshold = ElementCompute(0)
+    ): alpha(alpha), beta(beta), threshold(threshold), alpha_ptr(nullptr), beta_ptr(nullptr) {
+
+    }
 
     CUTLASS_HOST_DEVICE
-    Params(ElementCompute const *alpha_ptr, ElementCompute const *beta_ptr)
-        : alpha(0), beta(0), alpha_ptr(alpha_ptr), beta_ptr(beta_ptr) {}
+    Params(
+      ElementCompute const *alpha_ptr,
+      ElementCompute const *beta_ptr,
+      ElementCompute threshold = ElementCompute(0)
+    ): alpha(0), beta(0), threshold(threshold), alpha_ptr(alpha_ptr), beta_ptr(beta_ptr) {
+
+    }
   };
 
 private:
 
+  //
+  // Data members
+  //
+
   ElementCompute alpha_;
   ElementCompute beta_;
-  bool skip_elementwise_;
+  ElementCompute threshold_;
+  bool participates_in_reduction_;
 
 public:
 
-  /// Constructor from Params
+  /// Constructs the function object, possibly loading from pointers in host memory
   CUTLASS_HOST_DEVICE
-  LinearCombinationResidualBlock(Params const &params) {
+  LinearCombinationWithElementwise(Params const &params) {
+
     alpha_ = (params.alpha_ptr ? *params.alpha_ptr : params.alpha);
     beta_ = (params.beta_ptr ? *params.beta_ptr : params.beta);
-    skip_elementwise_ = false;
+    threshold_ = params.threshold;
+    participates_in_reduction_ = true;
   }
 
-  /// The "source" tensor corresponds to the residual input
+  /// Returns true if source is needed
   CUTLASS_HOST_DEVICE
-  bool is_source_needed() const { return true; }
+  bool is_source_needed() const {
+    return beta_ != ElementCompute(0);
+  }
+
+  /// Returns true if the threadblock computes the reduction
+  CUTLASS_HOST_DEVICE
+  bool participates_in_reduction() const {
+    return participates_in_reduction_;
+  }
 
   /// Functionally required for serial reduction in the epilogue
-  /// IMPORTANT: Split-k is supported only when ActivationOp is Identity.
   CUTLASS_HOST_DEVICE
   void set_k_partition(int k_partition, int k_partition_count) {
     if (k_partition) {
       beta_ = ElementCompute(1);
     }
 
     if (k_partition != k_partition_count - 1) {
-      skip_elementwise_ = true;
+      // set to NaN to make ReLU no-op for all except last k partitions
+      int64_t allones = -1;
+      threshold_ = reinterpret_cast<ElementCompute const &>(allones);
+      // Avoid computing the reduction if this isn't the final Split-K slice
+      participates_in_reduction_ = false;
     }
   }
-
-  /// Applies the operation UnaryOp(BinaryOp(ActivationOp(AB + bias), residual))
+  
+  /// Computes linear scaling: D = alpha * accumulator + beta * source
   CUTLASS_HOST_DEVICE
-  void operator()(FragmentOutput &frag_Z, FragmentOutput &, FragmentAccumulator const &AB,
-                  FragmentC const &residual,
-                  FragmentCompute const &bias) const {
-    UnaryOp unary_op;
-    BinaryOp binary_op;
-    ActivationOp activation;
+  FragmentCompute operator()(
+    FragmentAccumulator const &accumulator, 
+    FragmentSource const &source,
+    FragmentTensor const &tensor) const {
+
+    // Convert source to interal compute numeric type
+    NumericArrayConverter<ElementCompute, ElementSource, kCount, Round> source_converter;
+    NumericArrayConverter<ElementCompute, ElementAccumulator, kCount, Round> accumulator_converter;
+
+    FragmentCompute converted_source = source_converter(source);
+    FragmentCompute converted_accumulator = accumulator_converter(accumulator);
+
+    // Perform binary operations
+    FragmentCompute intermediate;
 
-    FragmentCompute tmp_Accum =
-        NumericArrayConverter<ElementCompute, ElementAccumulator, kElementsPerAccess>()(AB);
-    FragmentCompute tmp_residual =
-        NumericArrayConverter<ElementCompute, ElementC, kElementsPerAccess>()(residual);
+    multiplies<FragmentCompute> mul_add_source;
+    multiply_add<FragmentCompute> mul_add_accumulator;
 
-    FragmentCompute z =
-        binary_op(activation(alpha_ * tmp_Accum + bias), beta_ * tmp_residual);
-    FragmentCompute result_Z = skip_elementwise_ ? z : unary_op(z);
+    intermediate = mul_add_source(beta_, converted_source);                             // X =  beta * C + uniform
+    intermediate = mul_add_accumulator(alpha_, converted_accumulator, intermediate);    // D = alpha * Accum + X
 
-    NumericArrayConverter<ElementOutput, ElementCompute, kElementsPerAccess> convert_z;
-    frag_Z = convert_z(result_Z);
+    return intermediate;
   }
 
-  /// Should never be called
+  /// Computes linear scaling: D = alpha * accumulator
   CUTLASS_HOST_DEVICE
-  void operator()(FragmentOutput &, FragmentOutput &, FragmentAccumulator const &,
-                  FragmentCompute const &) const {}
+  FragmentCompute operator()(
+    FragmentAccumulator const &accumulator,
+    FragmentTensor const &tensor) const {
+
+    // Convert source to interal compute numeric type
+    NumericArrayConverter<ElementCompute, ElementAccumulator, kCount, Round> accumulator_converter;
+
+    FragmentCompute converted_accumulator = accumulator_converter(accumulator);
+
+    // Perform binary operations
+    FragmentCompute intermediate;
+
+    multiplies<FragmentCompute> mul_accumulator;
+
+    intermediate = mul_accumulator(alpha_, converted_accumulator);    // D = alpha * Accum
+
+    return intermediate;
+  }
 };
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
 } // namespace thread
 } // namespace epilogue
 } // namespace cutlass
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/epilogue/thread/linear_combination_sigmoid.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/epilogue/thread/linear_combination_sigmoid.h`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/epilogue/thread/linear_combination_silu.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/epilogue/thread/linear_combination_silu.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/epilogue/thread/linear_combination_with_elementwise.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/epilogue_base_streamk.h`

 * *Files 24% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -25,210 +25,173 @@
  * SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER
  * CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,
  * OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
  * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
  *
  **************************************************************************************************/
 /*! \file
-  
-  \brief Functor performing linear combination with elementwise
+  \brief Basic subset of epilogue functionality for supporting StreamK decompositions
 */
 
+
 #pragma once
 
-#include <cutlass/half.h>
 #include "cutlass/cutlass.h"
-#include "cutlass/numeric_types.h"
-#include "cutlass/array.h"
-#include "cutlass/constants.h"
-#include "cutlass/fast_math.h"
 #include "cutlass/functional.h"
-#include "cutlass/numeric_conversion.h"
-#include "cutlass/epilogue/thread/activation.h"
+#include "cutlass/block_striped.h"
 
-/////////////////////////////////////////////////////////////////////////////////////////////////
+////////////////////////////////////////////////////////////////////////////////
 
 namespace cutlass {
 namespace epilogue {
-namespace thread {
+namespace threadblock {
+
+////////////////////////////////////////////////////////////////////////////////
 
-/////////////////////////////////////////////////////////////////////////////////////////////////
 
-/// Applies a linear combination operator to an array of elements.
-///
-/// D = alpha * accumulator + beta * source + uniform
-///
+/// StreamK epilogue functionality for cross-block accumulator fragment reduction
 template <
-  typename ElementCompute_,                            ///< Data type returned by this functor
-  typename ElementAccumulator_,                        ///< Data type of accumulators
-  typename ElementSource_,                             ///< Data type of source tensor
-  typename ElementTensor_,                             ///< Data type of additional tensor
-  int Count,                                           ///< Number of elements computed per operation
-                                                       ///< Usually it is 128/sizeof_bits<ElementOutput_>,
-                                                       ///< but we use 64 or 32 sometimes when there are not enough data to store
-  FloatRoundStyle Round = FloatRoundStyle::round_to_nearest
->
-class LinearCombinationWithElementwise {
-public:
+  typename Shape,                          ///< Shape of threadblock tile (concept: GemmShape)
+  int PartitionsK,
+  typename WarpMmaOperator,                ///< Warp-level MMA operator (concept: gemm::warp::MmaTensorOp)
+  typename AccumulatorFragmentIterator>    ///< Iterator for enumerating fragments within the per-thread tile of raw accumulators
+class EpilogueBaseStreamK
+{
+
+protected:
+
+  /// The per-thread tile of raw accumulators
+  using AccumulatorTile = typename AccumulatorFragmentIterator::AccumulatorTile;
+
+  /// Number of warps
+  using WarpCount = gemm::GemmShape<
+                        Shape::kM / WarpMmaOperator::Shape::kM,
+                        Shape::kN / WarpMmaOperator::Shape::kN,
+                        PartitionsK>;
 
-  using ElementOutput = ElementSource_;
-  using ElementCompute = ElementCompute_;
-  using ElementAccumulator = ElementAccumulator_;
-  using ElementSource = ElementSource_;
-  using ElementTensor = ElementTensor_;
-
-  static bool const kIsHeavy = true;
-
-  static int const kCount = Count;
-
-  using FragmentCompute = Array<ElementCompute, kCount>;
-  using FragmentAccumulator = Array<ElementAccumulator, kCount>;
-  using FragmentSource = Array<ElementSource, kCount>;
-  using FragmentTensor = Array<ElementTensor, kCount>;
-
-  static FloatRoundStyle const kRound = Round;
-
-  /// Host-constructable parameters structure
-  struct Params {
-
-    ElementCompute alpha;                  ///< scales accumulators
-    ElementCompute beta;                   ///< scales source tensor
-    ElementCompute threshold;              ///< minimum value that is output 
-    ElementCompute const *alpha_ptr;       ///< pointer to accumulator scalar - if not null, loads it from memory
-    ElementCompute const *beta_ptr;        ///< pointer to source scalar - if not null, loads it from memory
-    //
-    // Methods
-    //
-
-    CUTLASS_HOST_DEVICE
-    Params(): 
-      alpha(ElementCompute(1)), 
-      beta(ElementCompute(0)),
-      threshold(ElementCompute(0)), 
-      alpha_ptr(nullptr), 
-      beta_ptr(nullptr) { }
-
-    CUTLASS_HOST_DEVICE
-    Params(
-      ElementCompute alpha,
-      ElementCompute beta,
-      ElementCompute threshold = ElementCompute(0)
-    ): alpha(alpha), beta(beta), threshold(threshold), alpha_ptr(nullptr), beta_ptr(nullptr) {
+  /// Number of threads per block
+  static int const kBlockThreads = 32 * WarpCount::kCount;
 
-    }
+  /// Numerical accumulation element type
+  using ElementAccumulator = typename WarpMmaOperator::ElementC;
 
-    CUTLASS_HOST_DEVICE
-    Params(
-      ElementCompute const *alpha_ptr,
-      ElementCompute const *beta_ptr,
-      ElementCompute threshold = ElementCompute(0)
-    ): alpha(0), beta(0), threshold(threshold), alpha_ptr(alpha_ptr), beta_ptr(beta_ptr) {
+  /// Fragment type used by the accumulator tile's fragment iterator
+  using AccumulatorFragment = typename AccumulatorFragmentIterator::Fragment;
 
-    }
-  };
+public:
 
-private:
+  /// Number of AccumulatorTile fragments per thread
+  static int const kAccumulatorFragments = AccumulatorFragmentIterator::Policy::kIterations;
 
-  //
-  // Data members
-  //
-
-  ElementCompute alpha_;
-  ElementCompute beta_;
-  ElementCompute threshold_;
-  bool participates_in_reduction_;
+protected:
 
-public:
+  /// Number of AccumulatorTile fragments per block output tile
+  static int const kOutputTileFragments = kBlockThreads * kAccumulatorFragments;
 
-  /// Constructs the function object, possibly loading from pointers in host memory
-  CUTLASS_HOST_DEVICE
-  LinearCombinationWithElementwise(Params const &params) {
-
-    alpha_ = (params.alpha_ptr ? *params.alpha_ptr : params.alpha);
-    beta_ = (params.beta_ptr ? *params.beta_ptr : params.beta);
-    threshold_ = params.threshold;
-    participates_in_reduction_ = true;
-  }
+  /// Block-striped transfer utility for sharing AccumulatorFragment
+  using BlockStripedT = BlockStriped<kBlockThreads, AccumulatorFragment>;
 
-  /// Returns true if source is needed
-  CUTLASS_HOST_DEVICE
-  bool is_source_needed() const {
-    return beta_ != ElementCompute(0);
-  }
+  /// AccumulatorFragment stride in the shared workspace between different peer blocks (each thread block can share accumulators for up to two block output tiles)
+  static const int kPeerFragmentStride = kOutputTileFragments * 2;
 
-  /// Returns true if the threadblock computes the reduction
-  CUTLASS_HOST_DEVICE
-  bool participates_in_reduction() const {
-    return participates_in_reduction_;
-  }
+public:
 
-  /// Functionally required for serial reduction in the epilogue
-  CUTLASS_HOST_DEVICE
-  void set_k_partition(int k_partition, int k_partition_count) {
-    if (k_partition) {
-      beta_ = ElementCompute(1);
-    }
+  /// Workspace bytes per thread block
+  static size_t const kWorkspaceBytesPerBlock =sizeof(AccumulatorFragment) * kPeerFragmentStride;
 
-    if (k_partition != k_partition_count - 1) {
-      // set to NaN to make ReLU no-op for all except last k partitions
-      int64_t allones = -1;
-      threshold_ = reinterpret_cast<ElementCompute const &>(allones);
-      // Avoid computing the reduction if this isn't the final Split-K slice
-      participates_in_reduction_ = false;
-    }
-  }
-  
-  /// Computes linear scaling: D = alpha * accumulator + beta * source
-  CUTLASS_HOST_DEVICE
-  FragmentCompute operator()(
-    FragmentAccumulator const &accumulator, 
-    FragmentSource const &source,
-    FragmentTensor const &tensor) const {
-
-    // Convert source to interal compute numeric type
-    NumericArrayConverter<ElementCompute, ElementSource, kCount, Round> source_converter;
-    NumericArrayConverter<ElementCompute, ElementAccumulator, kCount, Round> accumulator_converter;
-
-    FragmentCompute converted_source = source_converter(source);
-    FragmentCompute converted_accumulator = accumulator_converter(accumulator);
-
-    // Perform binary operations
-    FragmentCompute intermediate;
+public:
 
-    multiplies<FragmentCompute> mul_add_source;
-    multiply_add<FragmentCompute> mul_add_accumulator;
+  /// Thread index in the threadblock
+  int thread_idx;
 
-    intermediate = mul_add_source(beta_, converted_source);                             // X =  beta * C + uniform
-    intermediate = mul_add_accumulator(alpha_, converted_accumulator, intermediate);    // D = alpha * Accum + X
+public:
 
-    return intermediate;
-  }
+  /// Constructor
+  CUTLASS_DEVICE
+  EpilogueBaseStreamK(
+      int thread_idx)                                       ///< ID of a thread within the threadblock
+  :
+      thread_idx(thread_idx)
+  {}
+
+
+  /// Aggregates the accumulator sets shared by peer blocks in the global workspace
+  CUTLASS_DEVICE
+  void reduce(
+      AccumulatorFragment &accum_fragment,                  ///< [out] sum of all shared accumulator fragments for these peer partials
+      int peer_idx_begin,
+      int peer_idx_end,
+      int reduce_fragment_idx,
+      void *workspace_ptr)
+  {
+    plus<AccumulatorFragment> add_fragments;
+
+    AccumulatorFragment *fragment_workspace = reinterpret_cast<AccumulatorFragment *>(workspace_ptr);
+
+    int fragment_offset = (peer_idx_begin * kPeerFragmentStride) + (reduce_fragment_idx * kBlockThreads);
+
+    // Load first peer fragment
+    BlockStripedT::load(accum_fragment, fragment_workspace + fragment_offset, this->thread_idx);
+
+    fragment_offset += kPeerFragmentStride;         // Move to next peer
+    fragment_offset += kOutputTileFragments;        // Move to the set of fragments for this peer's "non-started" output tile
+
+    // Reduce fragments from additional peers
+    #pragma unroll 2
+    for (; fragment_offset < peer_idx_end * kPeerFragmentStride; fragment_offset += kPeerFragmentStride)
+    {
+      // Load peer fragment
+      AccumulatorFragment addend_fragment;
+      BlockStripedT::load(addend_fragment, fragment_workspace + fragment_offset, this->thread_idx);
 
-  /// Computes linear scaling: D = alpha * accumulator
-  CUTLASS_HOST_DEVICE
-  FragmentCompute operator()(
-    FragmentAccumulator const &accumulator,
-    FragmentTensor const &tensor) const {
+      // Add peer fragment
+      accum_fragment = add_fragments(accum_fragment, addend_fragment);
+    }
+  }
 
-    // Convert source to interal compute numeric type
-    NumericArrayConverter<ElementCompute, ElementAccumulator, kCount, Round> accumulator_converter;
 
-    FragmentCompute converted_accumulator = accumulator_converter(accumulator);
+  /// Shares the accumulator set with peers in the global workspace
+  CUTLASS_DEVICE
+  void share(
+      int peer_idx,
+      void *workspace_ptr,
+      AccumulatorTile const &accumulators,
+      bool started_tile)                      ///< Whether this thread block computed the first work volume for the current output tile
+  {
+    AccumulatorFragment *fragment_workspace = reinterpret_cast<AccumulatorFragment *>(workspace_ptr);
+
+    int fragment_offset = peer_idx * kPeerFragmentStride;
+
+    if (!started_tile) {
+      // Move to the set of fragments for the "non-started" output tile
+      fragment_offset += kOutputTileFragments;
+    }
 
-    // Perform binary operations
-    FragmentCompute intermediate;
+    AccumulatorFragmentIterator accum_fragment_iterator(accumulators);
 
-    multiplies<FragmentCompute> mul_accumulator;
+    // Convert raw accumulator tile to fragments and store
+    CUTLASS_PRAGMA_UNROLL
+    for (int iter = 0; iter < kAccumulatorFragments; ++iter)
+    {
+      // Acquire reordered accumulator fragment
+      AccumulatorFragment accum_fragment;
+      accum_fragment_iterator.load(accum_fragment);
+      ++accum_fragment_iterator;
 
-    intermediate = mul_accumulator(alpha_, converted_accumulator);    // D = alpha * Accum
+      // Store accumulator fragment
+      BlockStripedT::store(fragment_workspace + fragment_offset, accum_fragment, this->thread_idx);
 
-    return intermediate;
+      fragment_offset += kBlockThreads;
+    }
   }
+
 };
 
-/////////////////////////////////////////////////////////////////////////////////////////////////
 
-} // namespace thread
+
+////////////////////////////////////////////////////////////////////////////////
+
+} // namespace threadblock
 } // namespace epilogue
 } // namespace cutlass
 
-/////////////////////////////////////////////////////////////////////////////////////////////////
+////////////////////////////////////////////////////////////////////////////////
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/epilogue/thread/reduction_op.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/epilogue/thread/reduction_op.h`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/epilogue/thread/scale_type.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/epilogue/thread/scale_type.h`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/default_epilogue_complex_tensor_op.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/default_epilogue_complex_tensor_op.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/default_epilogue_complex_tensor_op_blas3.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/default_epilogue_complex_tensor_op_blas3.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/default_epilogue_direct_store.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/default_epilogue_direct_store.h`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/default_epilogue_planar_complex.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/default_epilogue_planar_complex.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/default_epilogue_simt.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/default_epilogue_volta_tensor_op.h`

 * *Files 12% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -25,15 +25,15 @@
  * SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER
  * CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,
  * OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
  * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
  *
  **************************************************************************************************/
 /*! \file
-  \brief Epilogue for threadblock scoped GEMMs using SIMT.
+  \brief Epilogue for threadblock scoped GEMMs using Tensor Ops on Volta.
 
   The epilogue rearranges the result of a matrix product through shared memory to match canonical
   tensor layouts in global memory. Epilogues support conversion and reduction operations.
 
 */
 
 #pragma once
@@ -46,267 +46,283 @@
 
 #include "cutlass/epilogue/thread/linear_combination.h"
 #include "cutlass/epilogue/thread/linear_combination_clamp.h"
 #include "cutlass/epilogue/thread/linear_combination_relu.h"
 #include "cutlass/epilogue/thread/linear_combination_gelu.h"
 #include "cutlass/epilogue/thread/linear_combination_sigmoid.h"
 #include "cutlass/epilogue/thread/linear_combination_planar_complex.h"
+
 #include "cutlass/epilogue/thread/conversion_op.h"
 #include "cutlass/epilogue/thread/reduction_op.h"
 
 #include "cutlass/transform/threadblock/regular_tile_iterator_pitch_linear.h"
-
-#include "cutlass/epilogue/warp/fragment_iterator_simt.h"
-#include "cutlass/epilogue/warp/tile_iterator_simt.h"
-#include "cutlass/epilogue/threadblock/default_thread_map_simt.h"
-
-#include "cutlass/epilogue/threadblock/predicated_tile_iterator.h"
 #include "cutlass/epilogue/threadblock/predicated_tile_iterator_strided_dgrad.h"
+#include "cutlass/epilogue/threadblock/predicated_tile_iterator.h"
 #include "cutlass/epilogue/threadblock/predicated_tile_iterator_affine.h"
 #include "cutlass/epilogue/threadblock/shared_load_iterator.h"
+
+#include "cutlass/epilogue/warp/fragment_iterator_volta_tensor_op.h"
+#include "cutlass/epilogue/warp/tile_iterator_volta_tensor_op.h"
+#include "cutlass/epilogue/threadblock/default_thread_map_volta_tensor_op.h"
+
 #include "cutlass/epilogue/threadblock/epilogue.h"
 
 #include "cutlass/layout/permute.h"
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
 namespace cutlass {
 namespace epilogue {
 namespace threadblock {
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
-/// Defines sensible defaults for epilogues for SimtOps.
+/// Defines sensible defaults for epilogues for TensorOps.
 template <
   typename Shape_,
-  typename WarpMmaSimt_,
+  typename WarpMmaTensorOp_,
+  int PartitionsK,
   typename OutputOp_,
   int ElementsPerAccess,
   bool ScatterD = false,
   typename PermuteDLayout = layout::NoPermute
 >
-struct DefaultEpilogueSimt {
+struct DefaultEpilogueVoltaTensorOp {
 
   using Shape = Shape_;
-  using WarpMmaSimt = WarpMmaSimt_;
+  using WarpMmaTensorOp = WarpMmaTensorOp_;
+  static int const kPartitionsK = PartitionsK;
   using OutputOp = OutputOp_;
   static int const kElementsPerAccess = ElementsPerAccess;
-  static const int kPartitionsK = Shape::kK / WarpMmaSimt::Shape::kK;
 
   using ElementOutput = typename OutputOp::ElementOutput;
-  using LayoutC = typename WarpMmaSimt::LayoutC;
-  using ElementAccumulator = typename WarpMmaSimt::ElementC;
+  using LayoutC = typename WarpMmaTensorOp::LayoutC;
+  using ElementAccumulator = typename WarpMmaTensorOp::ElementC;
 
   //
   // Thread map
   //
 
-  using OutputTileThreadMap = typename cutlass::epilogue::threadblock::DefaultThreadMapSimt<
+  using OutputTileThreadMap = typename cutlass::epilogue::threadblock::DefaultThreadMapVoltaTensorOp<
     Shape,
-    typename WarpMmaSimt::Shape,
-    typename WarpMmaSimt::Policy,
+    typename WarpMmaTensorOp::Shape,
     kPartitionsK,
     ElementOutput,
-    kElementsPerAccess
+    kElementsPerAccess,
+    ElementAccumulator
   >::Type;
 
   using OutputTileIterator = cutlass::epilogue::threadblock::PredicatedTileIterator<
     OutputTileThreadMap,
     ElementOutput,
     ScatterD,
     PermuteDLayout
   >;
 
-  using AccumulatorFragmentIterator = cutlass::epilogue::warp::FragmentIteratorSimt<
-    typename WarpMmaSimt::Shape,
-    typename WarpMmaSimt::ThreadMma,
-    layout::RowMajor,
-    typename WarpMmaSimt::Policy
+  using AccumulatorFragmentIterator = cutlass::epilogue::warp::FragmentIteratorVoltaTensorOp<
+    typename WarpMmaTensorOp::Shape,
+    gemm::GemmShape<32, 32, 4>,
+    ElementAccumulator,
+    LayoutC
   >;
 
-  using WarpTileIterator = cutlass::epilogue::warp::TileIteratorSimt<
-    typename WarpMmaSimt::Shape,
-    typename WarpMmaSimt::ThreadMma,
+  using WarpTileIterator = cutlass::epilogue::warp::TileIteratorVoltaTensorOp<
+    typename WarpMmaTensorOp::Shape,
+    gemm::GemmShape<32, 32, 4>,
     ElementAccumulator,
-    layout::RowMajor,
-    typename WarpMmaSimt::Policy
+    LayoutC
   >;
 
+  static int const kSharedMemAlignment = sizeof_bits<ElementAccumulator>::value * WarpTileIterator::kElementsPerAccess / 8;
+
+  static_assert(kSharedMemAlignment == 8, "Shared memory alignment must be 8B");
+
   using SharedLoadIterator = cutlass::epilogue::threadblock::SharedLoadIterator<
     typename OutputTileThreadMap::CompactedThreadMap,
-    ElementAccumulator
+    ElementAccumulator,
+    kSharedMemAlignment
   >;
 
   /// Hard-coded padding elements added 
   using Padding = typename WarpTileIterator::Padding;
 
   //
   // Define the epilogue
   //
   using Epilogue = cutlass::epilogue::threadblock::Epilogue<
     Shape,
-    WarpMmaSimt,
+    WarpMmaTensorOp,
     kPartitionsK,
     OutputTileIterator,
     AccumulatorFragmentIterator,
     WarpTileIterator,
     SharedLoadIterator,
     OutputOp,
     Padding
   >;
 };
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
-/// Defines sensible defaults for epilogues for SimtOps.
+/// Defines sensible defaults for epilogues for TensorOps.
 template <
   typename Shape_,
-  typename WarpMmaSimt_,
+  typename WarpMmaTensorOp_,
+  int PartitionsK,
   typename OutputOp_,
   int ElementsPerAccess
 >
-struct DefaultEpilogueSimtStridedDgrad {
+struct DefaultEpilogueVoltaTensorOpStridedDgrad {
 
   using Shape = Shape_;
-  using WarpMmaSimt = WarpMmaSimt_;
+  using WarpMmaTensorOp = WarpMmaTensorOp_;
+  static int const kPartitionsK = PartitionsK;
   using OutputOp = OutputOp_;
   static int const kElementsPerAccess = ElementsPerAccess;
-  static const int kPartitionsK = Shape::kK / WarpMmaSimt::Shape::kK;
 
   using ElementOutput = typename OutputOp::ElementOutput;
-  using LayoutC = typename WarpMmaSimt::LayoutC;
-  using ElementAccumulator = typename WarpMmaSimt::ElementC;
+  using LayoutC = typename WarpMmaTensorOp::LayoutC;
+  using ElementAccumulator = typename WarpMmaTensorOp::ElementC;
 
   //
   // Thread map
   //
 
-  using OutputTileThreadMap = typename cutlass::epilogue::threadblock::DefaultThreadMapSimt<
+  using OutputTileThreadMap = typename cutlass::epilogue::threadblock::DefaultThreadMapVoltaTensorOp<
     Shape,
-    typename WarpMmaSimt::Shape,
-    typename WarpMmaSimt::Policy,
+    typename WarpMmaTensorOp::Shape,
     kPartitionsK,
     ElementOutput,
-    kElementsPerAccess
+    kElementsPerAccess,
+    ElementAccumulator
   >::Type;
 
   using OutputTileIterator = cutlass::epilogue::threadblock::PredicatedTileIteratorStridedDgrad<
     OutputTileThreadMap,
     ElementOutput
   >;
 
-  using AccumulatorFragmentIterator = cutlass::epilogue::warp::FragmentIteratorSimt<
-    typename WarpMmaSimt::Shape,
-    typename WarpMmaSimt::ThreadMma,
-    layout::RowMajor,
-    typename WarpMmaSimt::Policy
+  using AccumulatorFragmentIterator = cutlass::epilogue::warp::FragmentIteratorVoltaTensorOp<
+    typename WarpMmaTensorOp::Shape,
+    gemm::GemmShape<32, 32, 4>,
+    ElementAccumulator,
+    LayoutC
   >;
 
-  using WarpTileIterator = cutlass::epilogue::warp::TileIteratorSimt<
-    typename WarpMmaSimt::Shape,
-    typename WarpMmaSimt::ThreadMma,
+  using WarpTileIterator = cutlass::epilogue::warp::TileIteratorVoltaTensorOp<
+    typename WarpMmaTensorOp::Shape,
+    gemm::GemmShape<32, 32, 4>,
     ElementAccumulator,
-    layout::RowMajor,
-    typename WarpMmaSimt::Policy
+    LayoutC
   >;
 
+  static int const kSharedMemAlignment = sizeof_bits<ElementAccumulator>::value * WarpTileIterator::kElementsPerAccess / 8;
+
+  static_assert(kSharedMemAlignment == 8, "Shared memory alignment must be 8B");
+
   using SharedLoadIterator = cutlass::epilogue::threadblock::SharedLoadIterator<
     typename OutputTileThreadMap::CompactedThreadMap,
-    ElementAccumulator
+    ElementAccumulator,
+    kSharedMemAlignment
   >;
 
   /// Hard-coded padding elements added 
   using Padding = typename WarpTileIterator::Padding;
 
   //
   // Define the epilogue
   //
   using Epilogue = cutlass::epilogue::threadblock::Epilogue<
     Shape,
-    WarpMmaSimt,
+    WarpMmaTensorOp,
     kPartitionsK,
     OutputTileIterator,
     AccumulatorFragmentIterator,
     WarpTileIterator,
     SharedLoadIterator,
     OutputOp,
     Padding
   >;
 };
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
-/// Defines sensible defaults for epilogues for SimtOps.
+/// Defines sensible defaults for epilogues for TensorOps.
 template <
   int Rank,
   typename Shape_,
-  typename WarpMmaSimt_,
+  typename WarpMmaTensorOp_,
+  int PartitionsK,
   typename OutputOp_,
   int ElementsPerAccess
 >
-struct DefaultEpilogueSimtAffineRankN {
+struct DefaultEpilogueVoltaTensorOpAffineRankN {
 
   using Shape = Shape_;
-  using WarpMmaSimt = WarpMmaSimt_;
+  using WarpMmaTensorOp = WarpMmaTensorOp_;
+  static int const kPartitionsK = PartitionsK;
   using OutputOp = OutputOp_;
   static int const kElementsPerAccess = ElementsPerAccess;
-  static const int kPartitionsK = Shape::kK / WarpMmaSimt::Shape::kK;
 
   using ElementOutput = typename OutputOp::ElementOutput;
-  using LayoutC = typename WarpMmaSimt::LayoutC;
-  using ElementAccumulator = typename WarpMmaSimt::ElementC;
+  using LayoutC = typename WarpMmaTensorOp::LayoutC;
+  using ElementAccumulator = typename WarpMmaTensorOp::ElementC;
 
   //
   // Thread map
   //
 
-  using OutputTileThreadMap = typename cutlass::epilogue::threadblock::DefaultThreadMapSimt<
+  using OutputTileThreadMap = typename cutlass::epilogue::threadblock::DefaultThreadMapVoltaTensorOp<
     Shape,
-    typename WarpMmaSimt::Shape,
-    typename WarpMmaSimt::Policy,
+    typename WarpMmaTensorOp::Shape,
     kPartitionsK,
     ElementOutput,
-    kElementsPerAccess
+    kElementsPerAccess,
+    ElementAccumulator
   >::Type;
 
   using OutputTileIterator = cutlass::epilogue::threadblock::PredicatedTileIteratorAffineRankN<
     OutputTileThreadMap,
     ElementOutput,
     Rank
   >;
 
-  using AccumulatorFragmentIterator = cutlass::epilogue::warp::FragmentIteratorSimt<
-    typename WarpMmaSimt::Shape,
-    typename WarpMmaSimt::ThreadMma,
-    layout::RowMajor,
-    typename WarpMmaSimt::Policy
+  using AccumulatorFragmentIterator = cutlass::epilogue::warp::FragmentIteratorVoltaTensorOp<
+    typename WarpMmaTensorOp::Shape,
+    gemm::GemmShape<32, 32, 4>,
+    ElementAccumulator,
+    LayoutC
   >;
 
-  using WarpTileIterator = cutlass::epilogue::warp::TileIteratorSimt<
-    typename WarpMmaSimt::Shape,
-    typename WarpMmaSimt::ThreadMma,
+  using WarpTileIterator = cutlass::epilogue::warp::TileIteratorVoltaTensorOp<
+    typename WarpMmaTensorOp::Shape,
+    gemm::GemmShape<32, 32, 4>,
     ElementAccumulator,
-    layout::RowMajor,
-    typename WarpMmaSimt::Policy
+    LayoutC
   >;
 
+  static int const kSharedMemAlignment = sizeof_bits<ElementAccumulator>::value * WarpTileIterator::kElementsPerAccess / 8;
+
+  static_assert(kSharedMemAlignment == 8, "Shared memory alignment must be 8B");
+
   using SharedLoadIterator = cutlass::epilogue::threadblock::SharedLoadIterator<
     typename OutputTileThreadMap::CompactedThreadMap,
-    ElementAccumulator
+    ElementAccumulator,
+    kSharedMemAlignment
   >;
 
   /// Hard-coded padding elements added 
   using Padding = typename WarpTileIterator::Padding;
 
   //
   // Define the epilogue
   //
   using Epilogue = cutlass::epilogue::threadblock::Epilogue<
     Shape,
-    WarpMmaSimt,
+    WarpMmaTensorOp,
     kPartitionsK,
     OutputTileIterator,
     AccumulatorFragmentIterator,
     WarpTileIterator,
     SharedLoadIterator,
     OutputOp,
     Padding
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/default_epilogue_tensor_op.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/default_epilogue_tensor_op.h`

 * *Files 7% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -289,14 +289,149 @@
   using SharedLoadIterator = typename platform::conditional<
                              (ThreadblockShape::kN == 256),
                              SharedLoadIteratorNotMixed,
                              SharedLoadIteratorMixed>::type;
 
   static int const kFragmentsPerIteration = 1;
 };
+
+/// Partial specialization for float_e4m3_t <= float x 16/8 epilogues avoids shared memory bank conflicts.
+/// Threadblock::kN = 256 still has bank conflicts.
+template <
+  int ElementsPerAccess,
+  typename ThreadblockShape,
+  typename WarpShape,
+  typename InstructionShape,
+  typename ThreadMap
+>
+struct DefaultIteratorsTensorOp<
+  cutlass::float_e4m3_t,
+  float, 
+  ElementsPerAccess,
+  ThreadblockShape, 
+  WarpShape, 
+  InstructionShape, 
+  ThreadMap> {
+
+  using ElementOutput = cutlass::float_e4m3_t;
+
+  static_assert((ElementsPerAccess == 16 || ElementsPerAccess == 8),
+              "ElementsPerAccess needs to be 16 or 8.");
+  
+  using WarpTileIteratorMixed = cutlass::epilogue::warp::TileIteratorTensorOpMixed<
+    WarpShape,
+    InstructionShape,
+    float,
+    32,
+    cutlass::sizeof_bits<ElementOutput>::value,
+    ElementsPerAccess,
+    8
+  >;
+
+  using WarpTileIteratorNotMixed =  cutlass::epilogue::warp::TileIteratorTensorOp<
+    WarpShape,
+    InstructionShape,
+    float,
+    layout::RowMajor
+  >;
+
+  using WarpTileIterator = typename platform::conditional<
+                             (ThreadblockShape::kN == 256),
+                             WarpTileIteratorNotMixed,
+                             WarpTileIteratorMixed>::type;
+
+  using SharedLoadIteratorMixed = cutlass::epilogue::threadblock::SharedLoadIteratorMixed<
+    ThreadMap,
+    float,
+    32,
+    cutlass::sizeof_bits<ElementOutput>::value,
+    ElementsPerAccess,
+    8
+  >;
+
+  using SharedLoadIteratorNotMixed = cutlass::epilogue::threadblock::SharedLoadIterator<
+    ThreadMap,
+    float
+  >;
+
+  using SharedLoadIterator = typename platform::conditional<
+                             (ThreadblockShape::kN == 256),
+                             SharedLoadIteratorNotMixed,
+                             SharedLoadIteratorMixed>::type;
+
+  static int const kFragmentsPerIteration = 1;
+};
+
+/// Partial specialization for float_e5m2_t <= float x 16/8 epilogues avoids shared memory bank conflicts.
+/// Threadblock::kN = 256 still has bank conflicts.
+template <
+  int ElementsPerAccess,
+  typename ThreadblockShape,
+  typename WarpShape,
+  typename InstructionShape,
+  typename ThreadMap
+>
+struct DefaultIteratorsTensorOp<
+  cutlass::float_e5m2_t,
+  float, 
+  ElementsPerAccess,
+  ThreadblockShape, 
+  WarpShape, 
+  InstructionShape, 
+  ThreadMap> {
+
+  using ElementOutput = cutlass::float_e5m2_t;
+
+  static_assert((ElementsPerAccess == 16 || ElementsPerAccess == 8),
+              "ElementsPerAccess needs to be 16 or 8.");
+  
+  using WarpTileIteratorMixed = cutlass::epilogue::warp::TileIteratorTensorOpMixed<
+    WarpShape,
+    InstructionShape,
+    float,
+    32,
+    cutlass::sizeof_bits<ElementOutput>::value,
+    ElementsPerAccess,
+    8
+  >;
+
+  using WarpTileIteratorNotMixed =  cutlass::epilogue::warp::TileIteratorTensorOp<
+    WarpShape,
+    InstructionShape,
+    float,
+    layout::RowMajor
+  >;
+
+  using WarpTileIterator = typename platform::conditional<
+                             (ThreadblockShape::kN == 256),
+                             WarpTileIteratorNotMixed,
+                             WarpTileIteratorMixed>::type;
+
+  using SharedLoadIteratorMixed = cutlass::epilogue::threadblock::SharedLoadIteratorMixed<
+    ThreadMap,
+    float,
+    32,
+    cutlass::sizeof_bits<ElementOutput>::value,
+    ElementsPerAccess,
+    8
+  >;
+
+  using SharedLoadIteratorNotMixed = cutlass::epilogue::threadblock::SharedLoadIterator<
+    ThreadMap,
+    float
+  >;
+
+  using SharedLoadIterator = typename platform::conditional<
+                             (ThreadblockShape::kN == 256),
+                             SharedLoadIteratorNotMixed,
+                             SharedLoadIteratorMixed>::type;
+
+  static int const kFragmentsPerIteration = 1;
+};
+
 } // namespace detail
 
 ////////////////////////////////////////////////////////////////////////////////
 
 /// Defines sensible defaults for epilogues for TensorOps.
 template <
   typename Shape_,
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/default_epilogue_tensor_op_blas3.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/default_epilogue_tensor_op_blas3.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/default_epilogue_volta_tensor_op.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/default_epilogue_simt.h`

 * *Files 25% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -25,313 +25,398 @@
  * SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER
  * CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,
  * OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
  * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
  *
  **************************************************************************************************/
 /*! \file
-  \brief Epilogue for threadblock scoped GEMMs using Tensor Ops on Volta.
+  \brief Epilogue for threadblock scoped GEMMs using SIMT.
 
   The epilogue rearranges the result of a matrix product through shared memory to match canonical
   tensor layouts in global memory. Epilogues support conversion and reduction operations.
 
 */
 
 #pragma once
 
 #include "cutlass/cutlass.h"
 #include "cutlass/numeric_types.h"
 #include "cutlass/array.h"
 
+#include "cutlass/arch/mma.h"
+
 #include "cutlass/gemm/gemm.h"
+#include "cutlass/gemm/warp/mma.h"
 
 #include "cutlass/epilogue/thread/linear_combination.h"
 #include "cutlass/epilogue/thread/linear_combination_clamp.h"
 #include "cutlass/epilogue/thread/linear_combination_relu.h"
 #include "cutlass/epilogue/thread/linear_combination_gelu.h"
 #include "cutlass/epilogue/thread/linear_combination_sigmoid.h"
 #include "cutlass/epilogue/thread/linear_combination_planar_complex.h"
-
 #include "cutlass/epilogue/thread/conversion_op.h"
 #include "cutlass/epilogue/thread/reduction_op.h"
 
 #include "cutlass/transform/threadblock/regular_tile_iterator_pitch_linear.h"
-#include "cutlass/epilogue/threadblock/predicated_tile_iterator_strided_dgrad.h"
+
+#include "cutlass/epilogue/warp/fragment_iterator_simt.h"
+#include "cutlass/epilogue/warp/tile_iterator_simt.h"
+#include "cutlass/epilogue/threadblock/default_thread_map_simt.h"
+#include "cutlass/transform/pitch_linear_thread_map.h"
+
 #include "cutlass/epilogue/threadblock/predicated_tile_iterator.h"
+#include "cutlass/epilogue/threadblock/predicated_tile_iterator_strided_dgrad.h"
 #include "cutlass/epilogue/threadblock/predicated_tile_iterator_affine.h"
+#include "cutlass/epilogue/threadblock/predicated_tile_iterator_direct_conv.h" 
 #include "cutlass/epilogue/threadblock/shared_load_iterator.h"
-
-#include "cutlass/epilogue/warp/fragment_iterator_volta_tensor_op.h"
-#include "cutlass/epilogue/warp/tile_iterator_volta_tensor_op.h"
-#include "cutlass/epilogue/threadblock/default_thread_map_volta_tensor_op.h"
-
+#include "cutlass/epilogue/threadblock/shared_load_iterator_pitch_liner.h"
 #include "cutlass/epilogue/threadblock/epilogue.h"
+#include "cutlass/epilogue/threadblock/epilogue_depthwise.h"
 
 #include "cutlass/layout/permute.h"
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
 namespace cutlass {
 namespace epilogue {
 namespace threadblock {
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
-/// Defines sensible defaults for epilogues for TensorOps.
+/// Defines sensible defaults for epilogues for SimtOps.
 template <
   typename Shape_,
-  typename WarpMmaTensorOp_,
-  int PartitionsK,
+  typename WarpMmaSimt_,
   typename OutputOp_,
   int ElementsPerAccess,
   bool ScatterD = false,
   typename PermuteDLayout = layout::NoPermute
 >
-struct DefaultEpilogueVoltaTensorOp {
+struct DefaultEpilogueSimt {
 
   using Shape = Shape_;
-  using WarpMmaTensorOp = WarpMmaTensorOp_;
-  static int const kPartitionsK = PartitionsK;
+  using WarpMmaSimt = WarpMmaSimt_;
   using OutputOp = OutputOp_;
   static int const kElementsPerAccess = ElementsPerAccess;
+  static const int kPartitionsK = Shape::kK / WarpMmaSimt::Shape::kK;
 
   using ElementOutput = typename OutputOp::ElementOutput;
-  using LayoutC = typename WarpMmaTensorOp::LayoutC;
-  using ElementAccumulator = typename WarpMmaTensorOp::ElementC;
+  using LayoutC = typename WarpMmaSimt::LayoutC;
+  using ElementAccumulator = typename WarpMmaSimt::ElementC;
 
   //
   // Thread map
   //
 
-  using OutputTileThreadMap = typename cutlass::epilogue::threadblock::DefaultThreadMapVoltaTensorOp<
+  using OutputTileThreadMap = typename cutlass::epilogue::threadblock::DefaultThreadMapSimt<
     Shape,
-    typename WarpMmaTensorOp::Shape,
+    typename WarpMmaSimt::Shape,
+    typename WarpMmaSimt::Policy,
     kPartitionsK,
     ElementOutput,
-    kElementsPerAccess,
-    ElementAccumulator
+    kElementsPerAccess
   >::Type;
 
   using OutputTileIterator = cutlass::epilogue::threadblock::PredicatedTileIterator<
     OutputTileThreadMap,
     ElementOutput,
     ScatterD,
     PermuteDLayout
   >;
 
-  using AccumulatorFragmentIterator = cutlass::epilogue::warp::FragmentIteratorVoltaTensorOp<
-    typename WarpMmaTensorOp::Shape,
-    gemm::GemmShape<32, 32, 4>,
-    ElementAccumulator,
-    LayoutC
+  using AccumulatorFragmentIterator = cutlass::epilogue::warp::FragmentIteratorSimt<
+    typename WarpMmaSimt::Shape,
+    typename WarpMmaSimt::ThreadMma,
+    layout::RowMajor,
+    typename WarpMmaSimt::Policy
   >;
 
-  using WarpTileIterator = cutlass::epilogue::warp::TileIteratorVoltaTensorOp<
-    typename WarpMmaTensorOp::Shape,
-    gemm::GemmShape<32, 32, 4>,
+  using WarpTileIterator = cutlass::epilogue::warp::TileIteratorSimt<
+    typename WarpMmaSimt::Shape,
+    typename WarpMmaSimt::ThreadMma,
     ElementAccumulator,
-    LayoutC
+    layout::RowMajor,
+    typename WarpMmaSimt::Policy
   >;
 
-  static int const kSharedMemAlignment = sizeof_bits<ElementAccumulator>::value * WarpTileIterator::kElementsPerAccess / 8;
-
-  static_assert(kSharedMemAlignment == 8, "Shared memory alignment must be 8B");
-
   using SharedLoadIterator = cutlass::epilogue::threadblock::SharedLoadIterator<
     typename OutputTileThreadMap::CompactedThreadMap,
-    ElementAccumulator,
-    kSharedMemAlignment
+    ElementAccumulator
   >;
 
   /// Hard-coded padding elements added 
   using Padding = typename WarpTileIterator::Padding;
 
   //
   // Define the epilogue
   //
   using Epilogue = cutlass::epilogue::threadblock::Epilogue<
     Shape,
-    WarpMmaTensorOp,
+    WarpMmaSimt,
     kPartitionsK,
     OutputTileIterator,
     AccumulatorFragmentIterator,
     WarpTileIterator,
     SharedLoadIterator,
     OutputOp,
     Padding
   >;
 };
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
-/// Defines sensible defaults for epilogues for TensorOps.
+/// Defines sensible defaults for epilogues for SimtOps.
 template <
   typename Shape_,
-  typename WarpMmaTensorOp_,
-  int PartitionsK,
+  typename WarpMmaSimt_,
   typename OutputOp_,
   int ElementsPerAccess
 >
-struct DefaultEpilogueVoltaTensorOpStridedDgrad {
+struct DefaultEpilogueSimtStridedDgrad {
 
   using Shape = Shape_;
-  using WarpMmaTensorOp = WarpMmaTensorOp_;
-  static int const kPartitionsK = PartitionsK;
+  using WarpMmaSimt = WarpMmaSimt_;
   using OutputOp = OutputOp_;
   static int const kElementsPerAccess = ElementsPerAccess;
+  static const int kPartitionsK = Shape::kK / WarpMmaSimt::Shape::kK;
 
   using ElementOutput = typename OutputOp::ElementOutput;
-  using LayoutC = typename WarpMmaTensorOp::LayoutC;
-  using ElementAccumulator = typename WarpMmaTensorOp::ElementC;
+  using LayoutC = typename WarpMmaSimt::LayoutC;
+  using ElementAccumulator = typename WarpMmaSimt::ElementC;
 
   //
   // Thread map
   //
 
-  using OutputTileThreadMap = typename cutlass::epilogue::threadblock::DefaultThreadMapVoltaTensorOp<
+  using OutputTileThreadMap = typename cutlass::epilogue::threadblock::DefaultThreadMapSimt<
     Shape,
-    typename WarpMmaTensorOp::Shape,
+    typename WarpMmaSimt::Shape,
+    typename WarpMmaSimt::Policy,
     kPartitionsK,
     ElementOutput,
-    kElementsPerAccess,
-    ElementAccumulator
+    kElementsPerAccess
   >::Type;
 
   using OutputTileIterator = cutlass::epilogue::threadblock::PredicatedTileIteratorStridedDgrad<
     OutputTileThreadMap,
     ElementOutput
   >;
 
-  using AccumulatorFragmentIterator = cutlass::epilogue::warp::FragmentIteratorVoltaTensorOp<
-    typename WarpMmaTensorOp::Shape,
-    gemm::GemmShape<32, 32, 4>,
-    ElementAccumulator,
-    LayoutC
+  using AccumulatorFragmentIterator = cutlass::epilogue::warp::FragmentIteratorSimt<
+    typename WarpMmaSimt::Shape,
+    typename WarpMmaSimt::ThreadMma,
+    layout::RowMajor,
+    typename WarpMmaSimt::Policy
   >;
 
-  using WarpTileIterator = cutlass::epilogue::warp::TileIteratorVoltaTensorOp<
-    typename WarpMmaTensorOp::Shape,
-    gemm::GemmShape<32, 32, 4>,
+  using WarpTileIterator = cutlass::epilogue::warp::TileIteratorSimt<
+    typename WarpMmaSimt::Shape,
+    typename WarpMmaSimt::ThreadMma,
     ElementAccumulator,
-    LayoutC
+    layout::RowMajor,
+    typename WarpMmaSimt::Policy
   >;
 
-  static int const kSharedMemAlignment = sizeof_bits<ElementAccumulator>::value * WarpTileIterator::kElementsPerAccess / 8;
-
-  static_assert(kSharedMemAlignment == 8, "Shared memory alignment must be 8B");
-
   using SharedLoadIterator = cutlass::epilogue::threadblock::SharedLoadIterator<
     typename OutputTileThreadMap::CompactedThreadMap,
-    ElementAccumulator,
-    kSharedMemAlignment
+    ElementAccumulator
   >;
 
   /// Hard-coded padding elements added 
   using Padding = typename WarpTileIterator::Padding;
 
   //
   // Define the epilogue
   //
   using Epilogue = cutlass::epilogue::threadblock::Epilogue<
     Shape,
-    WarpMmaTensorOp,
+    WarpMmaSimt,
     kPartitionsK,
     OutputTileIterator,
     AccumulatorFragmentIterator,
     WarpTileIterator,
     SharedLoadIterator,
     OutputOp,
     Padding
   >;
 };
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
-/// Defines sensible defaults for epilogues for TensorOps.
+/// Defines sensible defaults for epilogues for SimtOps.
 template <
   int Rank,
   typename Shape_,
-  typename WarpMmaTensorOp_,
-  int PartitionsK,
+  typename WarpMmaSimt_,
   typename OutputOp_,
   int ElementsPerAccess
 >
-struct DefaultEpilogueVoltaTensorOpAffineRankN {
+struct DefaultEpilogueSimtAffineRankN {
 
   using Shape = Shape_;
-  using WarpMmaTensorOp = WarpMmaTensorOp_;
-  static int const kPartitionsK = PartitionsK;
+  using WarpMmaSimt = WarpMmaSimt_;
   using OutputOp = OutputOp_;
   static int const kElementsPerAccess = ElementsPerAccess;
+  static const int kPartitionsK = Shape::kK / WarpMmaSimt::Shape::kK;
 
   using ElementOutput = typename OutputOp::ElementOutput;
-  using LayoutC = typename WarpMmaTensorOp::LayoutC;
-  using ElementAccumulator = typename WarpMmaTensorOp::ElementC;
+  using LayoutC = typename WarpMmaSimt::LayoutC;
+  using ElementAccumulator = typename WarpMmaSimt::ElementC;
 
   //
   // Thread map
   //
 
-  using OutputTileThreadMap = typename cutlass::epilogue::threadblock::DefaultThreadMapVoltaTensorOp<
+  using OutputTileThreadMap = typename cutlass::epilogue::threadblock::DefaultThreadMapSimt<
     Shape,
-    typename WarpMmaTensorOp::Shape,
+    typename WarpMmaSimt::Shape,
+    typename WarpMmaSimt::Policy,
     kPartitionsK,
     ElementOutput,
-    kElementsPerAccess,
-    ElementAccumulator
+    kElementsPerAccess
   >::Type;
 
   using OutputTileIterator = cutlass::epilogue::threadblock::PredicatedTileIteratorAffineRankN<
     OutputTileThreadMap,
     ElementOutput,
     Rank
   >;
 
-  using AccumulatorFragmentIterator = cutlass::epilogue::warp::FragmentIteratorVoltaTensorOp<
-    typename WarpMmaTensorOp::Shape,
-    gemm::GemmShape<32, 32, 4>,
-    ElementAccumulator,
-    LayoutC
+  using AccumulatorFragmentIterator = cutlass::epilogue::warp::FragmentIteratorSimt<
+    typename WarpMmaSimt::Shape,
+    typename WarpMmaSimt::ThreadMma,
+    layout::RowMajor,
+    typename WarpMmaSimt::Policy
   >;
 
-  using WarpTileIterator = cutlass::epilogue::warp::TileIteratorVoltaTensorOp<
-    typename WarpMmaTensorOp::Shape,
-    gemm::GemmShape<32, 32, 4>,
+  using WarpTileIterator = cutlass::epilogue::warp::TileIteratorSimt<
+    typename WarpMmaSimt::Shape,
+    typename WarpMmaSimt::ThreadMma,
     ElementAccumulator,
-    LayoutC
+    layout::RowMajor,
+    typename WarpMmaSimt::Policy
   >;
 
-  static int const kSharedMemAlignment = sizeof_bits<ElementAccumulator>::value * WarpTileIterator::kElementsPerAccess / 8;
-
-  static_assert(kSharedMemAlignment == 8, "Shared memory alignment must be 8B");
-
   using SharedLoadIterator = cutlass::epilogue::threadblock::SharedLoadIterator<
     typename OutputTileThreadMap::CompactedThreadMap,
-    ElementAccumulator,
-    kSharedMemAlignment
+    ElementAccumulator
   >;
 
   /// Hard-coded padding elements added 
   using Padding = typename WarpTileIterator::Padding;
 
   //
   // Define the epilogue
   //
   using Epilogue = cutlass::epilogue::threadblock::Epilogue<
     Shape,
-    WarpMmaTensorOp,
+    WarpMmaSimt,
     kPartitionsK,
     OutputTileIterator,
     AccumulatorFragmentIterator,
     WarpTileIterator,
     SharedLoadIterator,
     OutputOp,
     Padding
   >;
 };
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
+/////////////////////////////////////////////////////////////////////////////////////////////////
+
+/// Defines sensible defaults for epilogues for SimtOps.
+template <typename Shape_,        // ThreadBlock Shape
+          typename WarpMmaSimt_,  // mma_depthwise_simt
+          typename OutputOp_,
+          int ElementsPerAccess_,
+          typename ThreadOutputShape_ = cutlass::conv::TensorNHWCShape<1, 1, 1, 1>,
+          typename ThreadBlockOutputShape_ = cutlass::conv::TensorNHWCShape<1, 1, 1, 1> >
+struct DefaultDirectConvEpilogueSimt {
+  using Shape = Shape_;
+  using WarpMmaSimt = WarpMmaSimt_;
+  using WarpShape = typename WarpMmaSimt::Shape;
+  using OutputOp = OutputOp_;
+  using ThreadOutputShape = ThreadOutputShape_;
+  using ThreadBlockOutputShape = ThreadBlockOutputShape_;
+  static int const kElementsPerAccess = ElementsPerAccess_;
+
+
+  using ElementOutput = typename OutputOp::ElementOutput;
+  using LayoutC = typename WarpMmaSimt::LayoutC;
+  using ElementAccumulator = typename WarpMmaSimt::ElementC;
+
+  /// Number of threads total
+  using WarpCount = gemm::GemmShape<
+    Shape::kM / WarpShape::kM,
+    Shape::kN / WarpShape::kN
+  >;
+
+  static int const kWarpSize = cutlass::gemm::warp::WarpSize<arch::OpClassSimt>::value;
+
+  static int const kThreads = WarpCount::kCount * kWarpSize;
+
+  //
+  // Thread map
+  //
+  
+  using OutputTileThreadMap = cutlass::transform::PitchLinearStripminedThreadMap<
+    layout::PitchLinearShape<ThreadBlockOutputShape::kC, ThreadBlockOutputShape::kNHW>,
+    kThreads,
+    kElementsPerAccess
+  >;
+
+
+  using OutputTileIterator = cutlass::epilogue::threadblock::PredicatedTileIteratorDirectConv<
+    OutputTileThreadMap,
+    ElementOutput,
+    ThreadOutputShape,
+    ThreadBlockOutputShape 
+  >;
+
+  using AccumulatorFragmentIterator = cutlass::epilogue::warp::FragmentIteratorSimt<
+    typename WarpMmaSimt::Shape,
+    typename WarpMmaSimt::ThreadMma,
+    layout::RowMajor,
+    typename WarpMmaSimt::Policy
+  >;
+  
+  using WarpTileIterator = cutlass::epilogue::warp::TileIteratorSimtDirect2dConv<
+    typename WarpMmaSimt::Shape,
+    ThreadOutputShape,
+    ThreadBlockOutputShape,
+    typename WarpMmaSimt::ThreadMma,
+    ElementAccumulator,
+    layout::RowMajor,
+    typename WarpMmaSimt::Policy
+  >;
+
+  using SharedLoadIterator = cutlass::epilogue::threadblock::SharedLoadIteratorPitchLiner<
+    OutputTileThreadMap,
+    ElementAccumulator
+  >;
+
+  /// Hard-coded padding elements added 
+  using Padding = typename WarpTileIterator::Padding;
+  //
+  // Define the epilogue
+  //
+  using Epilogue = cutlass::epilogue::threadblock::EpilogueDepthwise<
+    Shape,
+    ThreadOutputShape,
+    ThreadBlockOutputShape,
+    WarpMmaSimt,
+    OutputTileIterator,
+    AccumulatorFragmentIterator,
+    WarpTileIterator,
+    SharedLoadIterator,
+    OutputOp,
+    Padding
+  >;
+};
+
+/////////////////////////////////////////////////////////////////////////////////////////////////
+
 } // namespace threadblock
 } // namespace epilogue
 } // namespace cutlass
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/default_epilogue_with_broadcast.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/default_epilogue_with_broadcast.h`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/default_epilogue_with_reduction.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/default_epilogue_with_reduction.h`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/default_epilogue_wmma_tensor_op.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/default_epilogue_wmma_tensor_op.h`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/default_thread_map_simt.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/default_thread_map_simt.h`

 * *Files 2% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/default_thread_map_tensor_op.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/default_thread_map_tensor_op.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/default_thread_map_volta_tensor_op.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/default_thread_map_volta_tensor_op.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/default_thread_map_wmma_tensor_op.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/default_thread_map_wmma_tensor_op.h`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/direct_store_epilogue_iterator.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/direct_store_epilogue_iterator.h`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/epilogue.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/epilogue.h`

 * *Files 8% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -30,14 +30,15 @@
  **************************************************************************************************/
 /*! \file
   \brief Epilogue for threadblock scoped GEMMs using Tensor Ops.
 
   The epilogue rearranges the result of a matrix product through shared memory to match canonical
   tensor layouts in global memory. Epilogues support conversion and reduction operations.
 
+  The shared memory resource is time-sliced across warps.
 */
 
 #pragma once
 
 #if defined(__CUDACC_RTC__)
 #include <cuda/std/cassert>
 #else
@@ -55,23 +56,24 @@
 
 #include "cutlass/gemm/gemm.h"
 
 #include "cutlass/transform/pitch_linear_thread_map.h"
 #include "cutlass/transform/threadblock/regular_tile_iterator.h"
 
 #include "cutlass/epilogue/threadblock/epilogue_base.h"
+#include "cutlass/epilogue/threadblock/epilogue_base_streamk.h"
 #include "cutlass/epilogue/threadblock/predicated_tile_iterator.h"
-#include "cutlass/numeric_types.h"
 
 ////////////////////////////////////////////////////////////////////////////////
 
 namespace cutlass {
 namespace epilogue {
 namespace threadblock {
 
+
 ////////////////////////////////////////////////////////////////////////////////
 
 /// Epilogue operator
 template <
   typename Shape_,                          ///< Shape of threadblock tile (concept: GemmShape)
   typename WarpMmaOperator_,                ///< Warp-level MMA operator (concept: gemm::warp::MmaTensorOp)
   int PartitionsK,                          ///< Number of partitions of the K dimension
@@ -81,53 +83,73 @@
   typename SharedLoadIterator_,             ///< Threadblock-scoped tile iterator loading from SMEM
   typename OutputOp_,                       ///< Output operator
   typename Padding_,                        ///< Padding added to SMEM allocation to avoid bank conflicts (concept: MatrixShape)
   int FragmentsPerPartition = 1,            ///< Used to coarsten the epilogue granularity
   int IterationsUnroll =                    ///< Used to reduce binary size when epilogue op is large
     (!IsEpilogueFunctorHeavy<OutputOp_>::value)
 >
-class Epilogue : 
+class Epilogue :
   public EpilogueBase<
-    Shape_, 
-    typename WarpMmaOperator_::Shape, 
-    PartitionsK, 
-    AccumulatorFragmentIterator_, 
-    WarpTileIterator_, 
+    Shape_,
+    typename WarpMmaOperator_::Shape,
+    PartitionsK,
+    AccumulatorFragmentIterator_,
+    WarpTileIterator_,
     Padding_,
-    FragmentsPerPartition> {
+    FragmentsPerPartition>,
+  public EpilogueBaseStreamK<
+    Shape_,
+    PartitionsK,
+    WarpMmaOperator_,
+    AccumulatorFragmentIterator_>
+{
 
 public:
 
   using Base = EpilogueBase<
-    Shape_, 
-    typename WarpMmaOperator_::Shape, 
-    PartitionsK, 
-    AccumulatorFragmentIterator_, 
-    WarpTileIterator_, 
+    Shape_,
+    typename WarpMmaOperator_::Shape,
+    PartitionsK,
+    AccumulatorFragmentIterator_,
+    WarpTileIterator_,
     Padding_,
     FragmentsPerPartition>;
 
+  using BaseStreamK = EpilogueBaseStreamK<
+    Shape_,
+    PartitionsK,
+    WarpMmaOperator_,
+    AccumulatorFragmentIterator_>;
+
   using Shape = Shape_;
   using WarpMmaOperator = WarpMmaOperator_;
   static int const kPartitionsK = PartitionsK;
   using OutputTileIterator = OutputTileIterator_;
   using AccumulatorFragmentIterator = AccumulatorFragmentIterator_;
   using WarpTileIterator = WarpTileIterator_;
   using SharedLoadIterator = SharedLoadIterator_;
   using OutputOp = OutputOp_;
   using Padding = Padding_;
-
   using Layout = layout::RowMajor;
   using LongIndex = typename Layout::LongIndex;
 
-  /// The complete warp-level accumulator tile
+  /// Number of warps per block
+  using WarpCount = typename Base::WarpCount;
+
+  /// Number of threads per block
+  static int const kBlockThreads = 32 * WarpCount::kCount;
+
+  /// Per-thread accumulator tile type
   using AccumulatorTile = typename Base::AccumulatorTile;
 
-  /// Accumulator element
-  using ElementAccumulator = typename WarpTileIterator::Element;
+  /// Numerical accumulation element type
+  using ElementAccumulator = typename WarpMmaOperator::ElementC;
+
+  /// Fragment type used by the accumulator tile's fragment iterator
+  using AccumulatorFragment = typename AccumulatorFragmentIterator::Fragment;
 
   /// Output element
   using ElementOutput = typename OutputTileIterator::Element;
 
   /// Output access size
   static int const kElementsPerAccess = OutputTileIterator::kElementsPerAccess;
 
@@ -136,374 +158,376 @@
 
   /// Tensor reference to sync tensor
   using SyncTensorRef = typename cutlass::TensorRef<int, cutlass::layout::PackedVectorLayout>;
 
   /// Const tensor reference to source tensor
   using ConstTensorRef = typename OutputTileIterator::ConstTensorRef;
 
-  /// Array type used to output
+  /// Vector type used by the global output iterator
   using OutputAccessType = Array<
     typename OutputTileIterator::Element, OutputTileIterator::kElementsPerAccess>;
 
-  /// Array type used by output functor
-  using AccumulatorAccessType = Array<typename WarpTileIterator::Element, OutputTileIterator::kElementsPerAccess>; 
-  
-  /// Number of warps
-  using WarpCount = typename Base::WarpCount;
+  /// Vector type used by the shared output iterator
+  using AccumulatorAccessType = Array<typename WarpTileIterator::Element, OutputTileIterator::kElementsPerAccess>;
 
   static int constexpr kSmemTiles = Base::kFragmentsPerIteration > 1 ? Base::kFragmentsPerIteration : kPartitionsK;
+
   static int constexpr kSmemPointerOffset = Base::SharedStorage::StorageShape::kCount / kSmemTiles;
 
+
 public:
 
   static_assert(SharedLoadIterator::Fragment::kElements == OutputTileIterator::Fragment::kElements,
     "Mismatch between shared load iterator and output tile iterator.");
 
   static_assert(OutputTileIterator::kElementsPerAccess, "OutputTileIterator::kElementsPerAccess must not be zero.");
 
   static_assert(!(OutputTileIterator::Fragment::kElements % OutputTileIterator::kElementsPerAccess), 
     "Divisibility");
 
-private:
+  static_assert(kPartitionsK == 1 || Base::kFragmentsPerIteration == 1, "One of these must be exactly 1.");
 
-  /// Loads fragment from shared memory aligned with output tensor
-  SharedLoadIterator shared_load_iterator_;
 
 public:
 
-  /// Constructor
-  CUTLASS_DEVICE
-  Epilogue(
-    typename Base::SharedStorage &shared_storage,    ///< Shared storage object    
-    int thread_idx,                   ///< ID of a thread within the threadblock
-    int warp_idx,                     ///< ID of warp within threadblock
-    int lane_idx                     ///< Id of thread within warp
-  ):
-    Base(shared_storage, thread_idx, warp_idx, lane_idx),
-    shared_load_iterator_(shared_storage.reference(), thread_idx) 
+  /// Aspect for when epilogue source is not needed
+  struct SourceAspectNotNeeded
   {
-    
-  }
+    /// Constructor
+    CUTLASS_DEVICE
+    SourceAspectNotNeeded()
+    {}
 
-  /// Streams the result to global memory
-  CUTLASS_DEVICE
-  void operator()(
-    OutputOp const &output_op,                    ///< Output operator
-    OutputTileIterator destination_iterator,      ///< Tile iterator for destination
-    AccumulatorTile const &accumulators,          ///< Complete warp-level accumulator tile
-    OutputTileIterator source_iterator) {         ///< Threadblock tile coordinate in GEMM (in units of threadblock tiles)
-    
-    if (!output_op.is_source_needed()) {
-      compute_source_not_needed_(output_op, destination_iterator, accumulators);  
-    }
-    else {
-      compute_source_needed_(output_op, destination_iterator, accumulators, source_iterator);
-    }
-  }
+    /// Invoke the output functor over each vector of output
+    CUTLASS_DEVICE
+    void apply_output_operator(
+      typename OutputTileIterator::Fragment &output_fragment,
+      OutputOp const &output_op,
+      typename SharedLoadIterator::Fragment const &aligned_accum_fragment)
+    {
+      OutputAccessType *output_frag_ptr =
+        reinterpret_cast<OutputAccessType *>(&output_fragment);
 
-private:
+      AccumulatorAccessType const *compute_frag_ptr =
+        reinterpret_cast<AccumulatorAccessType const *>(&aligned_accum_fragment);
 
-  template <class Seq>
-  struct acc2smem_source_not_needed;
+      int const kOutputOpIterations =
+        OutputTileIterator::Fragment::kElements / OutputTileIterator::kElementsPerAccess;
 
-  template <size_t... Seq>
-  struct acc2smem_source_not_needed<cutlass::index_sequence<Seq...>> {
-    template <int Advance>
-    CUTLASS_DEVICE static void helper(AccumulatorFragmentIterator accum_fragment_iterator,
-                                      WarpTileIterator &warp_tile_iterator) {
       CUTLASS_PRAGMA_UNROLL
-      for (int i = 0; i < Advance; i++) {
-        ++accum_fragment_iterator;
+      for (int i = 0; i < kOutputOpIterations; ++i)
+      {
+        // Call the output operator
+        output_frag_ptr[i] = output_op(compute_frag_ptr[i]);
       }
+    }
+  };
 
-      CUTLASS_PRAGMA_UNROLL
-      for (int p = 0; p < Base::kFragmentsPerIteration; ++p) {
-        typename AccumulatorFragmentIterator::Fragment accum_fragment;
-
-        accum_fragment_iterator.load(accum_fragment);
-        ++accum_fragment_iterator;
 
-        warp_tile_iterator.store(accum_fragment);
-        if (p < Base::kFragmentsPerIteration - 1) {
-          warp_tile_iterator.add_pointer_offset(kSmemPointerOffset);
-        }
-      }
+  /// Aspect for when epilogue source is needed
+  struct SourceAspectNeeded
+  {
+    OutputTileIterator source_iterator;
 
-      if (Base::kFragmentsPerIteration > 1) {
-        warp_tile_iterator.add_pointer_offset(kSmemPointerOffset *
-                                              (1 - Base::kFragmentsPerIteration));
-      }
-    }
+    typename OutputTileIterator::Fragment source_fragment;
 
+    /// Invoke the output functor over each vector of output
     CUTLASS_DEVICE
-    static void push(size_t pos,
-                     AccumulatorFragmentIterator const &iterator_begin,
-                     WarpTileIterator &warp_tile_iterator) {
-      int dummy[] = {
-          (pos == (Seq * Base::kFragmentsPerIteration)) &&
-          (helper<Seq * Base::kFragmentsPerIteration>(iterator_begin, warp_tile_iterator), 0)...};
+    static void apply_output_operator(
+      typename OutputTileIterator::Fragment &output_fragment,
+      OutputOp const &output_op,
+      typename SharedLoadIterator::Fragment const &aligned_accum_fragment,
+      typename OutputTileIterator::Fragment const &source_fragment)
+    {
+      OutputAccessType *output_frag_ptr =
+        reinterpret_cast<OutputAccessType *>(&output_fragment);
 
-      CUTLASS_UNUSED(dummy[0]);
-    }
-  };
+      AccumulatorAccessType const *compute_frag_ptr =
+        reinterpret_cast<AccumulatorAccessType const *>(&aligned_accum_fragment);
 
-  static_assert(kPartitionsK == 1 || Base::kFragmentsPerIteration == 1, "One of these must be exactly 1.");
+      OutputAccessType const *source_frag_ptr =
+        reinterpret_cast<OutputAccessType const *>(&source_fragment);
 
-  /// Streams the result to global memory
-  CUTLASS_DEVICE
-  void compute_source_not_needed_(
-    OutputOp const &output_op,                    ///< Output operator
-    OutputTileIterator destination_iterator,      ///< Tile iterator for destination
-    AccumulatorTile const &accumulators          ///< Complete warp-level accumulator tile 
-    ) { 
+      int const kOutputOpIterations =
+        OutputTileIterator::Fragment::kElements / OutputTileIterator::kElementsPerAccess;
 
-    //
-    // Iterator over warp-level accumulator fragment
-    //
+      CUTLASS_PRAGMA_UNROLL
+      for (int i = 0; i < kOutputOpIterations; ++i)
+      {
+        // Call the output operator
+        output_frag_ptr[i] = output_op(compute_frag_ptr[i], source_frag_ptr[i]);
+      }
+    }
 
-    AccumulatorFragmentIterator accum_fragment_iterator(accumulators);
+    /// Constructor
+    CUTLASS_DEVICE
+    SourceAspectNeeded(OutputTileIterator source_iterator) :
+      source_iterator(source_iterator)
+    {
+      source_fragment.clear();
+    }
 
-    //
-    // Iterate over accumulator tile
-    // 
+    /// Invoke the output functor over each vector of output
+    CUTLASS_DEVICE
+    void apply_output_operator(
+      typename OutputTileIterator::Fragment &output_fragment,
+      OutputOp const &output_op,
+      typename SharedLoadIterator::Fragment const &aligned_accum_fragment)
+    {
+      // Load addend source fragment from global memory
+      source_iterator.load(source_fragment);
+      ++source_iterator;
 
-    #pragma unroll(IterationsUnroll ? OutputTileIterator::kIterations / Base::kFragmentsPerIteration : 1)
-    for (int iter = 0; iter < OutputTileIterator::kIterations; iter += Base::kFragmentsPerIteration) {
+      apply_output_operator(output_fragment, output_op, aligned_accum_fragment, source_fragment);
+    }
+  };
 
-      //
-      // Convert and store fragment
-      //
-      
-      __syncthreads();
 
+private:
 
-      acc2smem_source_not_needed<
-          cutlass::make_index_sequence<OutputTileIterator::kIterations /
-                                   Base::kFragmentsPerIteration>>::push(iter,
-                                                                        accum_fragment_iterator,
-                                                                        this->warp_tile_iterator_);
+  /// Loads fragment from shared memory aligned with output tensor
+  SharedLoadIterator shared_load_iterator_;
 
-      __syncthreads();
+  /// Thread index in the threadblock
+  int thread_idx;
 
-      //
-      // Load fragments from shared memory
-      //
+  /// Warp index in the threadblock
+  int warp_idx;
 
-      CUTLASS_PRAGMA_UNROLL
-      for (int p = 0; p < Base::kFragmentsPerIteration; ++p) {
+public:
 
+  /// Constructor
+  CUTLASS_DEVICE
+  Epilogue(
+      typename Base::SharedStorage &shared_storage,   ///< Shared storage object
+      int thread_idx,                                 ///< ID of a thread within the threadblock
+      int warp_idx,                                   ///< ID of warp within threadblock
+      int lane_idx)                                   ///< Id of thread within warp
+  :
+      Base(shared_storage, thread_idx, warp_idx, lane_idx),
+      BaseStreamK(thread_idx),
+      shared_load_iterator_(shared_storage.reference(), thread_idx),
+      thread_idx(thread_idx),
+      warp_idx(warp_idx)
+  {}
 
-        typename SharedLoadIterator::Fragment aligned_accum_fragment[kPartitionsK];
 
-        shared_load_iterator_.load(aligned_accum_fragment[0]);
+  /// Aggregates the accumulator sets shared by peer blocks in the global workspace,
+  /// performing epilogue computations, writing to output
+  CUTLASS_DEVICE
+  void reduce(
+      int peer_idx_begin,
+      int peer_idx_end,
+      int reduce_fragment_idx,
+      void *element_workspace,
+      OutputOp const &output_op,                      ///< Output operator
+      OutputTileIterator destination_iterator,        ///< Tile iterator for destination
+      OutputTileIterator source_iterator)             ///< Threadblock tile coordinate in GEMM (in units of threadblock tiles)
+  {
+    // Redcuce peer accumulator fragments into one fragment
+    AccumulatorFragment accum_fragment;
+    BaseStreamK::reduce(accum_fragment, peer_idx_begin, peer_idx_end, reduce_fragment_idx, element_workspace);
 
-        if (p < Base::kFragmentsPerIteration - 1) {
-          shared_load_iterator_.add_pointer_offset(kSmemPointerOffset);
-        }
-        else if (kPartitionsK > 1) {
+    // Store fragment to shared memory
+    this->warp_tile_iterator_.store(accum_fragment);
 
-          plus <typename SharedLoadIterator::Fragment> add_fragments;
+    __syncthreads();
 
-          CUTLASS_PRAGMA_UNROLL
-          for ( int i = 1; i < kPartitionsK; ++i) {
-            shared_load_iterator_.add_pointer_offset(kSmemPointerOffset);
-            shared_load_iterator_.load(aligned_accum_fragment[i]);
-            aligned_accum_fragment[0] = add_fragments(aligned_accum_fragment[0], aligned_accum_fragment[i]);
-          }
+    // Initialize/load source-fragment data
+    typename OutputTileIterator::Fragment source_fragment;
+    source_fragment.clear();
 
-          shared_load_iterator_.add_pointer_offset((1 - kPartitionsK) * kSmemPointerOffset);
-        }
+    if (output_op.is_source_needed())
+    {
+      source_iterator += reduce_fragment_idx;
+      source_iterator.load(source_fragment);
+    }
 
-        //
-        // Compute the output result
-        //
+    // Load fragment from shared memory
+    typename SharedLoadIterator::Fragment aligned_accum_fragment;
+    shared_load_iterator_.load(aligned_accum_fragment);
+
+    // Add fragments shared by other k partitions
+    if (kPartitionsK > 1)
+    {
+      plus <typename SharedLoadIterator::Fragment> add_fragments;
 
-        typename OutputTileIterator::Fragment output_fragment;
+      CUTLASS_PRAGMA_UNROLL
+      for ( int i = 1; i < kPartitionsK; ++i) {
+        typename SharedLoadIterator::Fragment aligned_addend_fragment;
+        shared_load_iterator_.add_pointer_offset(kSmemPointerOffset);
+        shared_load_iterator_.load(aligned_addend_fragment);
+        aligned_accum_fragment = add_fragments(aligned_accum_fragment, aligned_addend_fragment);
+      }
+    }
 
-        apply_output_operator_source_not_needed_(output_fragment, output_op, aligned_accum_fragment[0]);
+    // Compute the output result
+    typename OutputTileIterator::Fragment output_fragment;
 
+    // Apply the output operator
+    SourceAspectNeeded::apply_output_operator(
+        output_fragment,
+        output_op,
+        aligned_accum_fragment,
+        source_fragment);
+
+    // Store the final result
+    destination_iterator += reduce_fragment_idx;
+    destination_iterator.store(output_fragment);
+  }
 
-        //
-        // Store the final result
-        //
 
-        destination_iterator.store(output_fragment);
-        ++destination_iterator;
-      }
+  /// Perform the epilogue computations and stream the result to global memory.
+  CUTLASS_DEVICE
+  void operator()(
+    OutputOp const &output_op,                      ///< Output operator
+    OutputTileIterator destination_iterator,        ///< Tile iterator for destination
+    AccumulatorTile const &accumulators)            ///< Complete warp-level accumulator tile
+  {
+    operator()(output_op, destination_iterator, accumulators, SourceAspectNotNeeded());
+  }
 
-      if (Base::kFragmentsPerIteration > 1) {
-        shared_load_iterator_.add_pointer_offset(kSmemPointerOffset * (1 - Base::kFragmentsPerIteration));
-      }
+
+  /// Perform the epilogue computations and stream the result to global memory.  Implements
+  /// two alternative codepaths, depending on whether the output op requires addend data to be loaded.
+  CUTLASS_DEVICE
+  void operator()(
+    OutputOp const &output_op,                      ///< Output operator
+    OutputTileIterator destination_iterator,        ///< Tile iterator for destination
+    AccumulatorTile const &accumulators,            ///< Complete warp-level accumulator tile
+    OutputTileIterator source_iterator )            ///< Tile iterator for addend source
+  {
+    if (output_op.is_source_needed())
+    {
+      operator()(output_op, destination_iterator, accumulators, SourceAspectNeeded(source_iterator));
+    }
+    else
+    {
+      operator()(output_op, destination_iterator, accumulators, SourceAspectNotNeeded());
     }
   }
 
-  template<class Seq>
-  struct acc2smem_source_needed;
 
-  template <size_t... Seq>
-  struct acc2smem_source_needed<cutlass::index_sequence<Seq...>> {
-    template<int Advance>
-    CUTLASS_DEVICE
-    static void helper(AccumulatorFragmentIterator accum_fragment_iterator,
-                       WarpTileIterator &warp_tile_iterator) {
-      CUTLASS_PRAGMA_UNROLL
-      for (int i = 0; i < Advance; i++) {
-        ++accum_fragment_iterator;
-      }
-
-      typename AccumulatorFragmentIterator::Fragment accum_fragment;
-      accum_fragment_iterator.load(accum_fragment);
-      warp_tile_iterator.store(accum_fragment);
+  /// Perform the epilogue computations and stream the result to global memory.  Implements a
+  /// single codepath, regardless of whether the output op requires addend data to be loaded
+  CUTLASS_DEVICE
+  void unified(
+    OutputOp const &output_op,                      ///< Output operator
+    OutputTileIterator destination_iterator,        ///< Tile iterator for destination
+    AccumulatorTile const &accumulators,            ///< Complete warp-level accumulator tile
+    OutputTileIterator source_iterator )            ///< Tile iterator for addend source
+  {
+    if (!output_op.is_source_needed())
+    {
+      source_iterator.clear_mask();
+      __syncthreads();  // Dummy (CUDA 11.0)
     }
 
-    CUTLASS_DEVICE
-    static void push(size_t pos,
-                     AccumulatorFragmentIterator const &iterator_begin,
-                     WarpTileIterator &warp_tile_iterator) {
-      int dummy[] = {(pos == Seq) && (helper<Seq>(iterator_begin, warp_tile_iterator), 0)...};
-    }
-  };
+    operator()(output_op, destination_iterator, accumulators, SourceAspectNeeded(source_iterator));
+  }
+
 
   /// Streams the result to global memory
+  template <typename SourceAspect>
   CUTLASS_DEVICE
-  void compute_source_needed_(
-    OutputOp const &output_op,                    ///< Output operator
-    OutputTileIterator destination_iterator,      ///< Tile iterator for destination
-    AccumulatorTile const &accumulators,          ///< Complete warp-level accumulator tile
-    OutputTileIterator source_iterator           ///< Threadblock tile coordinate in GEMM (in units of threadblock tiles)
-    ) { 
-    
-    typename OutputTileIterator::Fragment source_fragment;
-
-    source_fragment.clear();
-
-    //
+  void operator()(
+    OutputOp const &output_op,                      ///< Output operator
+    OutputTileIterator destination_iterator,        ///< Tile iterator for destination
+    AccumulatorTile const &accumulators,            ///< Complete warp-level accumulator tile
+    SourceAspect source)
+  {
     // Iterator over warp-level accumulator fragment
-    //
-
     AccumulatorFragmentIterator accum_fragment_iterator(accumulators);
 
     //
     // Iterate over accumulator tile
-    // 
-
-    #pragma unroll(IterationsUnroll ? OutputTileIterator::kIterations : 1)
-    for (int iter = 0; iter < OutputTileIterator::kIterations; ++iter) {
-
-      //
-      // Load the source
-      //
+    //
 
-      source_iterator.load(source_fragment);
-      ++source_iterator;
+    #pragma unroll(IterationsUnroll ? OutputTileIterator::kIterations / Base::kFragmentsPerIteration : 1)
+    for (int iter = 0; iter < OutputTileIterator::kIterations; iter += Base::kFragmentsPerIteration)
+    {
 
       //
       // Convert and store fragment
       //
-      
-      __syncthreads();
-
-      acc2smem_source_needed<cutlass::make_index_sequence<OutputTileIterator::kIterations>>::push(
-          iter, accum_fragment_iterator, this->warp_tile_iterator_);
 
       __syncthreads();
 
-      //
-      // Load fragments from shared memory
-      //
-
-      typename SharedLoadIterator::Fragment aligned_accum_fragment[kPartitionsK];
-
-      shared_load_iterator_.load(aligned_accum_fragment[0]);
+      CUTLASS_PRAGMA_UNROLL
+      for (int p = 0; p < Base::kFragmentsPerIteration; ++p)
+      {
+        typename AccumulatorFragmentIterator::Fragment accum_fragment;
 
-      // If the number of k-slices is > 1 - perform a reduction amongst the k-slices
-      if (kPartitionsK > 1) {
+        accum_fragment_iterator.load(accum_fragment);
+        ++accum_fragment_iterator;
 
-        plus <typename SharedLoadIterator::Fragment> add_fragments;
+        this->warp_tile_iterator_.store(accum_fragment);
 
-        CUTLASS_PRAGMA_UNROLL
-        for ( int i = 1; i < kPartitionsK; ++i) {
-          shared_load_iterator_.add_pointer_offset(kSmemPointerOffset);
-          shared_load_iterator_.load(aligned_accum_fragment[i]);
-          aligned_accum_fragment[0] = add_fragments(aligned_accum_fragment[0], aligned_accum_fragment[i]);
+        if (p < Base::kFragmentsPerIteration - 1) {
+          this->warp_tile_iterator_.add_pointer_offset(kSmemPointerOffset);
         }
-
-        shared_load_iterator_.add_pointer_offset((1 - kPartitionsK) * kSmemPointerOffset);
       }
 
-      //
-      // Compute the output result
-      //
-     
-      typename OutputTileIterator::Fragment output_fragment;
-
-      apply_output_operator_(output_fragment, output_op, aligned_accum_fragment[0], source_fragment);
+      if (Base::kFragmentsPerIteration > 1) {
+        this->warp_tile_iterator_.add_pointer_offset(kSmemPointerOffset * (1 - Base::kFragmentsPerIteration));
+      }
 
 
       //
-      // Store the final result
+      // Load fragments from shared memory
       //
 
-      destination_iterator.store(output_fragment);      
-      ++destination_iterator;
+      __syncthreads();
 
-    }
-  }
+      CUTLASS_PRAGMA_UNROLL
+      for (int p = 0; p < Base::kFragmentsPerIteration; ++p)
+      {
+        typename SharedLoadIterator::Fragment aligned_accum_fragment;
+        shared_load_iterator_.load(aligned_accum_fragment);
 
-  /// Helper to invoke the output functor over each vector of output
-  CUTLASS_DEVICE
-  void apply_output_operator_(
-    typename OutputTileIterator::Fragment &output_fragment,
-    OutputOp const &output_op,                    ///< Output operator
-    typename SharedLoadIterator::Fragment const &aligned_accum_fragment,
-    typename OutputTileIterator::Fragment const &source_fragment) {
-      
-    OutputAccessType *output_frag_ptr = 
-      reinterpret_cast<OutputAccessType *>(&output_fragment);
-
-    AccumulatorAccessType const *compute_frag_ptr = 
-      reinterpret_cast<AccumulatorAccessType const *>(&aligned_accum_fragment);
-
-    OutputAccessType const *source_frag_ptr = 
-      reinterpret_cast<OutputAccessType const *>(&source_fragment);
+        if (p < Base::kFragmentsPerIteration - 1)
+        {
+          shared_load_iterator_.add_pointer_offset(kSmemPointerOffset);
+        }
+        else if (kPartitionsK > 1)
+        {
+          plus <typename SharedLoadIterator::Fragment> add_fragments;
 
-    int const kOutputOpIterations = 
-      OutputTileIterator::Fragment::kElements / OutputTileIterator::kElementsPerAccess;
+          CUTLASS_PRAGMA_UNROLL
+          for ( int i = 1; i < kPartitionsK; ++i) {
+            typename SharedLoadIterator::Fragment aligned_accum_fragment_addend;
+            shared_load_iterator_.add_pointer_offset(kSmemPointerOffset);
+            shared_load_iterator_.load(aligned_accum_fragment_addend);
+            aligned_accum_fragment = add_fragments(aligned_accum_fragment, aligned_accum_fragment_addend);
+          }
 
-    CUTLASS_PRAGMA_UNROLL
-    for (int i = 0; i < kOutputOpIterations; ++i) {
+          shared_load_iterator_.add_pointer_offset((1 - kPartitionsK) * kSmemPointerOffset);
+        }
 
-      // Call the output operator
-      output_frag_ptr[i] = output_op(compute_frag_ptr[i], source_frag_ptr[i]);
-    }
-  }
+        //
+        // Compute the output result
+        //
 
-  /// Helper to invoke the output functor over each vector of output
-  CUTLASS_DEVICE
-  void apply_output_operator_source_not_needed_(
-    typename OutputTileIterator::Fragment &output_fragment,
-    OutputOp const &output_op,                    ///< Output operator
-    typename SharedLoadIterator::Fragment const &aligned_accum_fragment) {
-    
-    OutputAccessType *output_frag_ptr = 
-      reinterpret_cast<OutputAccessType *>(&output_fragment);
-
-    AccumulatorAccessType const *compute_frag_ptr = 
-      reinterpret_cast<AccumulatorAccessType const *>(&aligned_accum_fragment);
+        typename OutputTileIterator::Fragment output_fragment;
+        source.apply_output_operator(output_fragment, output_op, aligned_accum_fragment);
 
-    int const kOutputOpIterations = 
-      OutputTileIterator::Fragment::kElements / OutputTileIterator::kElementsPerAccess;
+        //
+        // Store the final result
+        //
 
-    CUTLASS_PRAGMA_UNROLL
-    for (int i = 0; i < kOutputOpIterations; ++i) {
+        destination_iterator.store(output_fragment);
+        ++destination_iterator;
+      }
 
-      // Call the output operator
-      output_frag_ptr[i] = output_op(compute_frag_ptr[i]);
+      if (Base::kFragmentsPerIteration > 1) {
+        shared_load_iterator_.add_pointer_offset(kSmemPointerOffset * (1 - Base::kFragmentsPerIteration));
+      }
     }
   }
+
 };
 
 ////////////////////////////////////////////////////////////////////////////////
 
 } // namespace threadblock
 } // namespace epilogue
 } // namespace cutlass
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/epilogue_base.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/epilogue_base.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/epilogue_base_streamk.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/python/cutlass/cpp/include/epilogue/epilogue_visitor_op/visitor_op_unary.h`

 * *Files 24% similar despite different names*

```diff
@@ -10,188 +10,217 @@
  *
  * 2. Redistributions in binary form must reproduce the above copyright notice,
  * this list of conditions and the following disclaimer in the documentation
  * and/or other materials provided with the distribution.
  *
  * 3. Neither the name of the copyright holder nor the names of its
  * contributors may be used to endorse or promote products derived from
- * this software without specific prior written permission.
+ * this layernormware without specific prior written permission.
  *
  * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS IS"
  * AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
  * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE
  * DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE
  * FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL
  * DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR
  * SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER
  * CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,
  * OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
  * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
  *
  **************************************************************************************************/
+
 /*! \file
-  \brief Basic subset of epilogue functionality for supporting StreamK decompositions
+  
+  \brief A file contains the epilogue visitor Op with Unary operation
 */
 
-
 #pragma once
-
 #include "cutlass/cutlass.h"
-#include "cutlass/functional.h"
-#include "cutlass/block_striped.h"
+#include "unary_ops.h"
 
-////////////////////////////////////////////////////////////////////////////////
+/////////////////////////////////////////////////////////////////////////////////////////////////
 
 namespace cutlass {
 namespace epilogue {
 namespace threadblock {
 
-////////////////////////////////////////////////////////////////////////////////
+/////////////////////////////////////////////////////////////////////////////////////////////////
 
 
-/// StreamK epilogue functionality for cross-block accumulator fragment reduction
+/// Epilogue Visitor operator for the following computation:
+///
+///  ElementCompute alpha;
+///  ElementCompute beta;
+///  ElementCompute C = UnaryOp(ElementCompute(Visitor)) 
+///  Return C;
+///
 template <
-  typename Shape,                          ///< Shape of threadblock tile (concept: GemmShape)
-  int PartitionsK,
-  typename WarpMmaOperator,                ///< Warp-level MMA operator (concept: gemm::warp::MmaTensorOp)
-  typename AccumulatorFragmentIterator>    ///< Iterator for enumerating fragments within the per-thread tile of raw accumulators
-class EpilogueBaseStreamK
-{
-
-protected:
-
-  /// The per-thread tile of raw accumulators
-  using AccumulatorTile = typename AccumulatorFragmentIterator::AccumulatorTile;
-
-  /// Number of warps
-  using WarpCount = gemm::GemmShape<
-                        Shape::kM / WarpMmaOperator::Shape::kM,
-                        Shape::kN / WarpMmaOperator::Shape::kN,
-                        PartitionsK>;
-
-  /// Number of threads per block
-  static int const kBlockThreads = 32 * WarpCount::kCount;
-
-  /// Numerical accumulation element type
-  using ElementAccumulator = typename WarpMmaOperator::ElementC;
-
-  /// Fragment type used by the accumulator tile's fragment iterator
-  using AccumulatorFragment = typename AccumulatorFragmentIterator::Fragment;
-
+    typename ElementAccumulator_,  ///< Data type of the Accumulator
+    typename ElementCompute_,      ///< Data type used to compute linear combination
+    int      kElementsPerAccess_,  ///< Number of elements computed per operation
+    typename Visitor_,              ///< Child node
+    template<typename T, int N> typename UnaryOp_
+>
+class VisitorOpUnary{
 public:
+    using ElementAccumulator = ElementAccumulator_;
+    using ElementCompute = ElementCompute_;
+    static int const kElementsPerAccess = kElementsPerAccess_;
+
+    using Visitor = Visitor_;
+
+    /// Fragment type returned from Visitor.visit
+    using VisitAccessTypeVisitor = typename Visitor::VisitAccessType;
+    using ElementVisit = typename VisitAccessTypeVisitor::Element;
+
+    /// Fragment type returned by this visitor
+    using VisitAccessType = Array<ElementCompute, kElementsPerAccess>; 
+
+    /// Fragment type of accumulator
+    using AccumulatorAccessType = Array<ElementAccumulator, kElementsPerAccess>;
+
+    /// Combination Op
+    using UnaryOp = UnaryOp_<ElementCompute, kElementsPerAccess>;
+
+    static_assert(kElementsPerAccess==VisitAccessTypeVisitor::kElements, "kElementsPerAccess mismatches with Visitor");
+
+    /// SMEM buffer class required in the epilogue visitor
+    struct SharedStorage {
+        typename Visitor::SharedStorage storage_visitor;
+
+        CUTLASS_HOST_DEVICE
+        SharedStorage() {}
+    };
+
+
+    /// Host-constructable Arguments structure
+    struct Arguments {
+        typename UnaryOp::Arguments unary_arg;
+        typename Visitor::Arguments visitor_arg;    ///< Argument type for visitor
+
+        //
+        // Methods
+        //
+        CUTLASS_HOST_DEVICE
+        Arguments():unary_arg() { }
+        
+        CUTLASS_HOST_DEVICE
+        Arguments(
+            typename UnaryOp::Arguments unary_arg,
+            typename Visitor::Arguments visitor_arg
+        ):
+            unary_arg(unary_arg),
+            visitor_arg(visitor_arg)
+        { }
+    };
+
+    /// Parameter structure
+    struct Params {
+        typename UnaryOp::Params unary_param;
+        typename Visitor::Params visitor_param;    ///< Argument type for visitor
+
+        //
+        // Methods
+        //
+        CUTLASS_HOST_DEVICE
+        Params():unary_param() { }
+        
+        CUTLASS_HOST_DEVICE
+        Params(Arguments const &args):
+            unary_param(args.unary_arg),
+            visitor_param(args.visitor_arg)
+        { }
+    };
+
+private:
+    //
+    // Data members
+    //
+    UnaryOp unary_op;
 
-  /// Number of AccumulatorTile fragments per thread
-  static int const kAccumulatorFragments = AccumulatorFragmentIterator::Policy::kIterations;
-
-protected:
+    Visitor visitor_op;
 
-  /// Number of AccumulatorTile fragments per block output tile
-  static int const kOutputTileFragments = kBlockThreads * kAccumulatorFragments;
+public:
 
-  /// Block-striped transfer utility for sharing AccumulatorFragment
-  using BlockStripedT = BlockStriped<kBlockThreads, AccumulatorFragment>;
+    /// Constructs the function object
+    CUTLASS_HOST_DEVICE
+    VisitorOpUnary(
+        Params const &params,
+        SharedStorage &shared_storage,
+        int thread_idx,
+        MatrixCoord threadblock_offset,
+        MatrixCoord problem_size
+    ):
+        unary_op(params.unary_param),
+        visitor_op(params.visitor_param, shared_storage.storage_visitor, thread_idx, threadblock_offset, problem_size)
+    { }
+
+    CUTLASS_DEVICE
+    void set_batch_index(int batch_idx) {
+        visitor_op.set_batch_index(batch_idx);
+    }
 
-  /// AccumulatorFragment stride in the shared workspace between different peer blocks (each thread block can share accumulators for up to two block output tiles)
-  static const int kPeerFragmentStride = kOutputTileFragments * 2;
+    CUTLASS_DEVICE
+    void begin_epilogue() {
+        if (unary_op.guard()) visitor_op.begin_epilogue();
+    }
 
-public:
+    CUTLASS_DEVICE
+    void begin_step(int step_idx) {
+        if (unary_op.guard()) visitor_op.begin_step(step_idx);
+    }
 
-  /// Workspace bytes per thread block
-  static size_t const kWorkspaceBytesPerBlock =sizeof(AccumulatorFragment) * kPeerFragmentStride;
+    CUTLASS_DEVICE
+    void begin_row(int row_idx) {
+        if (unary_op.guard()) visitor_op.begin_row(row_idx);
+    }
 
-public:
+    CUTLASS_DEVICE
+    VisitAccessType visit(
+        int iter_idx,
+        int row_idx,
+        int column_idx,
+        int frag_idx,
+        AccumulatorAccessType const &accum
+    ) { 
+        /// Get result from visitor A and visitor B
+        VisitAccessTypeVisitor result;
+
+        if (unary_op.guard()){
+            result = visitor_op.visit(iter_idx, row_idx, column_idx, frag_idx, accum);
+        } else {
+            result.clear();
+        }
 
-  /// Thread index in the threadblock
-  int thread_idx;
+        /// Type conversion
+        NumericArrayConverter<ElementCompute, ElementVisit, kElementsPerAccess> source_converter;
 
-public:
+        cutlass::multiplies<VisitAccessType> multiply_op;
 
-  /// Constructor
-  CUTLASS_DEVICE
-  EpilogueBaseStreamK(
-      int thread_idx)                                       ///< ID of a thread within the threadblock
-  :
-      thread_idx(thread_idx)
-  {}
-
-
-  /// Aggregates the accumulator sets shared by peer blocks in the global workspace
-  CUTLASS_DEVICE
-  void reduce(
-      AccumulatorFragment &accum_fragment,                  ///< [out] sum of all shared accumulator fragments for these peer partials
-      int peer_idx_begin,
-      int peer_idx_end,
-      int reduce_fragment_idx,
-      void *workspace_ptr)
-  {
-    plus<AccumulatorFragment> add_fragments;
-
-    AccumulatorFragment *fragment_workspace = reinterpret_cast<AccumulatorFragment *>(workspace_ptr);
-
-    int fragment_offset = (peer_idx_begin * kPeerFragmentStride) + (reduce_fragment_idx * kBlockThreads);
-
-    // Load first peer fragment
-    BlockStripedT::load(accum_fragment, fragment_workspace + fragment_offset, this->thread_idx);
-
-    fragment_offset += kPeerFragmentStride;         // Move to next peer
-    fragment_offset += kOutputTileFragments;        // Move to the set of fragments for this peer's "non-started" output tile
-
-    // Reduce fragments from additional peers
-    #pragma unroll 2
-    for (; fragment_offset < peer_idx_end * kPeerFragmentStride; fragment_offset += kPeerFragmentStride)
-    {
-      // Load peer fragment
-      AccumulatorFragment addend_fragment;
-      BlockStripedT::load(addend_fragment, fragment_workspace + fragment_offset, this->thread_idx);
-
-      // Add peer fragment
-      accum_fragment = add_fragments(accum_fragment, addend_fragment);
-    }
-  }
-
-
-  /// Shares the accumulator set with peers in the global workspace
-  CUTLASS_DEVICE
-  void share(
-      int peer_idx,
-      void *workspace_ptr,
-      AccumulatorTile const &accumulators,
-      bool started_tile)                      ///< Whether this thread block computed the first work volume for the current output tile
-  {
-    AccumulatorFragment *fragment_workspace = reinterpret_cast<AccumulatorFragment *>(workspace_ptr);
-
-    int fragment_offset = peer_idx * kPeerFragmentStride;
-
-    if (!started_tile) {
-      // Move to the set of fragments for the "non-started" output tile
-      fragment_offset += kOutputTileFragments;
-    }
-
-    AccumulatorFragmentIterator accum_fragment_iterator(accumulators);
-
-    // Convert raw accumulator tile to fragments and store
-    CUTLASS_PRAGMA_UNROLL
-    for (int iter = 0; iter < kAccumulatorFragments; ++iter)
-    {
-      // Acquire reordered accumulator fragment
-      AccumulatorFragment accum_fragment;
-      accum_fragment_iterator.load(accum_fragment);
-      ++accum_fragment_iterator;
+        return unary_op(source_converter(result));
+    }
 
-      // Store accumulator fragment
-      BlockStripedT::store(fragment_workspace + fragment_offset, accum_fragment, this->thread_idx);
+    CUTLASS_DEVICE
+    void end_row(int row_idx) {
+        if (unary_op.guard()) visitor_op.end_row(row_idx);
+    }
 
-      fragment_offset += kBlockThreads;
+    CUTLASS_DEVICE
+    void end_step(int step_idx) {
+        if (unary_op.guard()) visitor_op.end_step(step_idx);
     }
-  }
 
+    CUTLASS_DEVICE
+    void end_epilogue() {
+        if (unary_op.guard()) visitor_op.end_epilogue();
+    }
 };
 
 
+/////////////////////////////////////////////////////////////////////////////////////////////////
 
-////////////////////////////////////////////////////////////////////////////////
-
-} // namespace threadblock
-} // namespace epilogue
+} // namespace kernel
+} // namespace gemm
 } // namespace cutlass
 
-////////////////////////////////////////////////////////////////////////////////
+/////////////////////////////////////////////////////////////////////////////////////////////////
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/epilogue_depthwise.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/epilogue_depthwise.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/epilogue_direct_store.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/epilogue_direct_store.h`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -73,15 +73,14 @@
   using Shape = Shape_;
   using WarpMmaOperator = WarpMmaOperator_;
   using WarpShape = typename WarpMmaOperator_::Shape;
   static int const kPartitionsK = PartitionsK;
   using OutputTileIterator = OutputTileIterator_;
   using AccumulatorFragmentIterator = AccumulatorFragmentIterator_;
   using WarpTileIterator = WarpTileIterator_;
-  using SharedLoadIterator = SharedLoadIterator_;
   using OutputOp = OutputOp_;
   using Padding = MatrixShape<0, 0>;
 
   using Layout = layout::RowMajor;
   using LongIndex = typename Layout::LongIndex;
 
   /// The complete warp-level accumulator tile
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/epilogue_gemm_k_reduction.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/epilogue_gemm_k_reduction.h`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -145,21 +145,20 @@
       }
 
       Array<ElementOutput, kIterations / 4> source;
       source.clear();
 
       CUTLASS_PRAGMA_UNROLL
       for (int i = 0; i < kIterations / 4; ++i) {
-        ElementOutput tmp;
+        ElementOutput *source_ptr = reinterpret_cast<ElementOutput *>(&source);
         cutlass::arch::global_load<ElementOutput, sizeof(ElementOutput)>(
-                                                  tmp,
+                                                  source_ptr[i],
                                                   (void *)(pointer_ + i * 32),
                                                   guard[i] && LoadForSerialSplitK);
 
-        source[i] = tmp;
       }
 
       FragmentAccumulator sum = gemm_k_with_reduction_accumulation;
 
       CUTLASS_PRAGMA_UNROLL
       for (int i = 0; i < kIterations; ++i) {
         sum[i] += __shfl_xor_sync(0xffffffff, sum[i], 1);
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/epilogue_planar_complex.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/epilogue_planar_complex.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/epilogue_smem_accumulator.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/epilogue_smem_accumulator.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/epilogue_visitor_with_softmax.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/epilogue_visitor_with_softmax.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/epilogue_with_broadcast.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/epilogue_with_reduction.h`

 * *Files 10% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -37,18 +37,16 @@
 
 */
 
 #pragma once
 
 #if defined(__CUDACC_RTC__)
 #include <cuda/std/cassert>
-#include <cuda/std/utility>
 #else
 #include <assert.h>
-#include <utility>
 #endif
 
 #include "cutlass/cutlass.h"
 #include "cutlass/array.h"
 #include "cutlass/numeric_types.h"
 #include "cutlass/numeric_conversion.h"
 #include "cutlass/tensor_coord.h"
@@ -62,166 +60,76 @@
 
 #include "cutlass/transform/pitch_linear_thread_map.h"
 #include "cutlass/transform/threadblock/regular_tile_iterator.h"
 
 #include "cutlass/epilogue/threadblock/epilogue_base.h"
 #include "cutlass/epilogue/threadblock/predicated_tile_iterator.h"
 
-#include "cutlass/numeric_types.h"
-
-/////////////////////////////////////////////////////////////////////////////////////////////////
+////////////////////////////////////////////////////////////////////////////////
 
 namespace cutlass {
 namespace epilogue {
 namespace threadblock {
 
-/////////////////////////////////////////////////////////////////////////////////////////////////
-
-/// This base class is meant to define the concept required of the
-/// EpilogueWithBroadcast::OutputOp
-template <
-  typename ElementC_,
-  typename ElementAccumulator_,
-  typename ElementCompute_,
-  typename ElementZ_,
-  typename ElementT_,
-  int ElementsPerAccess,
-  bool StoreZ = true,
-  bool StoreT = true
->
-struct EpilogueWithBroadcastOpBase {
-  
-  using ElementOutput = ElementC_;
-  using ElementAccumulator = ElementAccumulator_;
-  using ElementCompute = ElementCompute_;
-  using ElementZ = ElementZ_;
-  using ElementT = ElementT_;
-  static int const kElementsPerAccess = ElementsPerAccess;
-
-  using FragmentAccumulator = Array<ElementAccumulator, kElementsPerAccess>;
-  using FragmentCompute = Array<ElementCompute, kElementsPerAccess>;
-  using FragmentC = Array<ElementOutput, kElementsPerAccess>;
-  using FragmentZ = Array<ElementZ, kElementsPerAccess>;
-  using FragmentT = Array<ElementT, kElementsPerAccess>;
-
-  /// If true, the 'Z' tensor is stored
-  static bool const kStoreZ = StoreZ;
-
-  /// If true, the 'T' tensor is stored
-  static bool const kStoreT = StoreT;
-
-  /// Parameters structure - required
-  struct Params { };
-
-  //
-  // Methods
-  //
-
-  /// Constructor from Params
-  EpilogueWithBroadcastOpBase(Params const &params_) { }
-
-  /// Determine if the source is needed. May return false if 
-  bool is_source_needed() const {
-    return true;
-  }
-
-  CUTLASS_HOST_DEVICE
-  void set_k_partition(int k_partition, int k_partition_count) { }
-
-  /// Applies the operation when is_source_needed() is true
-  CUTLASS_HOST_DEVICE
-  void operator()(
-    FragmentZ &frag_Z, 
-    FragmentT &frag_T, 
-    FragmentAccumulator const &AB,
-    FragmentC const &frag_C,
-    FragmentCompute const &V) const {
-
-  }
-
-  /// Applies the operation when is_source_needed() is false
-  CUTLASS_HOST_DEVICE
-  void operator()(
-    FragmentZ &frag_Z, 
-    FragmentT &frag_T, 
-    FragmentAccumulator const &AB,
-    FragmentCompute const &V) const {
-
-  }
-};
-
 ////////////////////////////////////////////////////////////////////////////////
 
-/// Epilogue operator with bias vector broadcast over columns.
-///
-/// Computes the following:
-///
-///
-///  Z, T = OutputOp(AB, C, Broadcast)
-///
-///  if (ElementwiseOp::kStoreZ) {
-///    store(converted_u);
-///  }  
-///
-///  if (ElementwiseOp::kStoreT) {
-///    store(v);
-///  }  
-///
+/// Epilogue operator with reduction over each column 
 template <
   typename Shape_,                          ///< Shape of threadblock tile (concept: GemmShape)
   typename WarpMmaOperator_,                ///< Warp-level MMA operator (concept: gemm::warp::MmaTensorOp)
   int PartitionsK,                          ///< Number of partitions of the K dimension
-  typename OutputTileIterator_,             ///< Tile iterator reading and writing output tensors (z)
-  typename TensorTileIterator_,             ///< Additional tile iterator for tensor-valued operands (t)
-  typename ElementVector_,                  ///< Pointer to broadcast vector
+  typename OutputTileIterator_,             ///< Tile iterator reading and writing output tensors
+  typename TensorTileIterator_,             ///< Additional tile iterator for tensor-valued operands
+  typename ElementVector_,                  ///< Pointer to reduction vector
   typename AccumulatorFragmentIterator_,    ///< Fragment iterator selecting accumulators
   typename WarpTileIterator_,               ///< Warp-scoped tile iterator writing accumulators to SMEM
   typename SharedLoadIterator_,             ///< Threadblock-scoped tile iterator loading from SMEM
-  typename OutputOp_,                       ///< Output operator - concept is EpilogueWithBroadcastOp
+  typename OutputOp_,                       ///< Output operator
+  typename ReductionOp_,                    ///< Reduction operator
   typename Padding_,                        ///< Padding added to SMEM allocation to avoid bank conflicts (concept: MatrixShape)
-  int FragmentsPerPartition = 1,            ///< Used to coarsten the epilogue granularity
   int IterationsUnroll =                    ///< Used to reduce binary size when epilogue op is large
     (!IsEpilogueFunctorHeavy<OutputOp_>::value)
 >
-class EpilogueWithBroadcast : 
+class EpilogueWithReduction : 
   public EpilogueBase<
     Shape_, 
     typename WarpMmaOperator_::Shape, 
     PartitionsK, 
     AccumulatorFragmentIterator_, 
     WarpTileIterator_, 
-    Padding_,
-    FragmentsPerPartition> {
+    Padding_> {
 
 public:
 
   using Base = EpilogueBase<
     Shape_, 
     typename WarpMmaOperator_::Shape, 
     PartitionsK, 
     AccumulatorFragmentIterator_, 
     WarpTileIterator_, 
-    Padding_,
-    FragmentsPerPartition>;
+    Padding_>;
 
   using Shape = Shape_;
   using WarpMmaOperator = WarpMmaOperator_;
   static int const kPartitionsK = PartitionsK;
   using OutputTileIterator = OutputTileIterator_;
   using TensorTileIterator = TensorTileIterator_;
   using ElementVector = ElementVector_;
   using AccumulatorFragmentIterator = AccumulatorFragmentIterator_;
   using WarpTileIterator = WarpTileIterator_;
   using SharedLoadIterator = SharedLoadIterator_;
   using OutputOp = OutputOp_;
+  using ReductionOp = ReductionOp_;
   using Padding = Padding_;
 
   using Layout = layout::RowMajor;
   using LongIndex = typename Layout::LongIndex;
 
+  static bool const kIsSingleSource = true;
+
   /// The complete warp-level accumulator tile
   using AccumulatorTile = typename Base::AccumulatorTile;
 
   /// Accumulator element
   using ElementAccumulator = typename WarpTileIterator::Element;
 
   /// Compute data type produced by the output op
@@ -229,17 +137,17 @@
 
   /// Compute fragment
   using FragmentCompute = Array<ElementCompute, OutputTileIterator::Fragment::kElements>;
 
   /// Thread map used by output tile iterators
   using ThreadMap = typename OutputTileIterator::ThreadMap;
 
-  /// Fragment object used to store the broadcast values
-  using BroadcastFragment = Array<
-    ElementCompute, 
+  /// Fragment object used in reduction
+  using ReductionFragment = Array<
+    ElementAccumulator, 
     ThreadMap::Iterations::kColumn * ThreadMap::kElementsPerAccess>;
 
   /// Output element
   using ElementOutput = typename OutputTileIterator::Element;
 
   /// Data type of additional tensor
   using ElementTensor = typename TensorTileIterator::Element;
@@ -271,25 +179,24 @@
   
   /// Number of warps
   using WarpCount = typename Base::WarpCount;
 
   /// Shared memory allocation from epilogue base class
   using BaseSharedStorage = typename Base::SharedStorage;
 
-  static int constexpr kSmemTiles = Base::kFragmentsPerIteration > 1 ? Base::kFragmentsPerIteration : kPartitionsK;
-  static int constexpr kSmemPointerOffset = Base::SharedStorage::StorageShape::kCount / kSmemTiles;
+  /// Used for the reduction
+  struct ReductionDetail {
 
-  /// Used for the broadcast
-  struct BroadcastDetail {
+    /// If true, accumulator coordinates are computed and out-of-bounds checks are enabled when
+    /// performing the reduction.
+    static bool const kOobCheck = false;
 
     /// Number of threads per warp
     static int const kWarpSize = 32;
 
-    static int const kElementsPerAccess = ThreadMap::kElementsPerAccess;
-
     /// Number of distinct scalar column indices handled by each thread
     static int const kColumnsPerThread = ThreadMap::Iterations::kColumn * ThreadMap::kElementsPerAccess;
 
     /// Number of distinct scalar row indices handled by each thread
     static int const kRowsPerThread = ThreadMap::Iterations::kCount / ThreadMap::Iterations::kColumn;
 
     /// Number of threads per threadblock
@@ -310,18 +217,19 @@
       Shape::kN
     >;
 
     /// Debug printing
     CUTLASS_DEVICE
     static void print() {
 #if 0
-      printf("BroadcastDetail {\n");
+      printf("ReductionDetail {\n");
       printf(
-        "  kColumnsPerThread: %d\nkRowsPerThread: %d\n,kThreadCount: %d\nkThreadsPerRow: %d\n"
+        "  kElementsPerAccess:%d\nkColumnsPerThread: %d\nkRowsPerThread: %d\n,kThreadCount: %d\nkThreadsPerRow: %d\n"
         "kThreadRows: %d\nThreadAccessesPerRow: %d\nStorageShape: %d x %d (count: %d)\n",
+        kElementsPerAccess,
         kColumnsPerThread,
         kRowsPerThread,
         kThreadCount,
         kThreadsPerRow,
         kThreadRows,
         kThreadAccessesPerRow,
         StorageShape::kRow,
@@ -333,14 +241,15 @@
     }
   };
 
   /// Shared storage structure (shadows base) with additional SMEM buffer for reduction
   struct SharedStorage {
     union {
       BaseSharedStorage base;
+      AlignedArray<ElementAccumulator, ReductionDetail::StorageShape::kCount, 16> reduction;    ///< Shared storage for reduction
     };
 
     CUTLASS_HOST_DEVICE
     SharedStorage() { }
   };
 
 public:
@@ -355,308 +264,305 @@
     "Divisibility");
 
 private:
 
   /// Loads fragment from shared memory aligned with output tensor
   SharedLoadIterator shared_load_iterator_;
 
+  /// Shared memory pointer fo rreduction
+  ElementAccumulator *reduction_ptr_;
+
   /// Thread index within the threadblock
   int thread_idx_;
 
 public:
 
   /// Constructor
   CUTLASS_DEVICE
-  EpilogueWithBroadcast(
+  EpilogueWithReduction(
     SharedStorage &shared_storage,                    ///< Shared storage object    
     int thread_idx,                                   ///< ID of a thread within the threadblock
     int warp_idx,                                     ///< ID of warp within threadblock
     int lane_idx                                      ///< Id of thread within warp
   ):
     Base(shared_storage.base, thread_idx, warp_idx, lane_idx),
     shared_load_iterator_(shared_storage.base.reference(), thread_idx),
+    reduction_ptr_(shared_storage.reduction.data()),
     thread_idx_(thread_idx)
   {
 
   }
 
   /// Streams the result to global memory
   CUTLASS_DEVICE
   void operator()(
     OutputOp const &output_op,                        ///< Output operator
-    ElementVector const * broadcast_ptr,           ///< Broadcast vector
+    ElementVector * reduction_output_ptr,             ///< Reduction output vector
     OutputTileIterator destination_iterator,          ///< Tile iterator for destination
     AccumulatorTile const &accumulators,              ///< Complete warp-level accumulator tile
     OutputTileIterator source_iterator,               ///< Tile iterator for source accumulator matrix
     TensorTileIterator tensor_iterator,               ///< Threadblock tile iterator for additional tensor operand
     MatrixCoord const &problem_size =                 ///< Problem size needed to guard against out-of-bounds accesses
         MatrixCoord(Shape::kM, Shape::kN),
     MatrixCoord const &threadblock_offset =           ///< Threadblock's initial offset within the problem size space
         MatrixCoord()) {
     
-    BroadcastFragment broadcast_fragment;
-
-    load_broadcast_fragment_(broadcast_fragment, broadcast_ptr, problem_size, threadblock_offset);
+    ReductionFragment reduction_fragment;
+    reduction_fragment.clear();
 
     if (!output_op.is_source_needed()) {
       compute_source_not_needed_(
         output_op, 
-        broadcast_fragment, 
+        reduction_fragment, 
         destination_iterator, 
         accumulators,
-        tensor_iterator);
+        tensor_iterator,
+        problem_size,
+        threadblock_offset);
     }
     else {
       compute_source_needed_(
         output_op, 
-        broadcast_fragment, 
+        reduction_fragment, 
         destination_iterator, 
         accumulators, 
         source_iterator,
-        tensor_iterator);
+        tensor_iterator,
+        problem_size,
+        threadblock_offset);
+    }
+
+    if (output_op.participates_in_reduction()) {
+      reduction_(problem_size, threadblock_offset, reduction_output_ptr, reduction_fragment);
     }
   }
 
 private:
 
+  /// Perform the reduction
   CUTLASS_DEVICE
-  void load_broadcast_fragment_(
-    BroadcastFragment & broadcast_fragment,      ///< Fragment containing the accumulated partial reduction over columns
-    ElementVector const * broadcast_ptr,         ///< Broadcast vector
-    MatrixCoord const &problem_size,             ///< Problem size needed to guard against out-of-bounds accesses
-    MatrixCoord const &threadblock_offset        ///< Threadblock's initial offset within the problem size space
-    ) {
+  void reduction_(
+    MatrixCoord const &problem_size,                  ///< Problem size needed to guard against out-of-bounds accesses
+    MatrixCoord const &threadblock_offset,            ///< Problem size needed to guard against out-of-bounds accesses
+    ElementVector * reduction_output_ptr,          ///< Reduction output vector
+    ReductionFragment const & reduction_fragment) {
+
+    //
+    // Store the partially reduced value to SMEM
+    //
 
-    broadcast_fragment.clear();
+    // Guard against uses of the existing SMEM tile
+    __syncthreads();
     
-    // If no pointer is supplied, set with all zeros and avoid memory accesses
-    if (!broadcast_ptr) {
-      return;
-    }
+    using AccessType = AlignedArray<ElementAccumulator, ThreadMap::kElementsPerAccess>;
+
+    //
+    // Determine a compacted thread arrangement to store to SMEM.
+    //
+    int const kThreadsPerRow = Shape::kN / (ThreadMap::Iterations::kColumn * ThreadMap::kElementsPerAccess);
+
+    MatrixCoord thread_offset(
+      thread_idx_ / kThreadsPerRow, 
+      (thread_idx_ % kThreadsPerRow) * ThreadMap::kElementsPerAccess);
+   
+    //
+    // Each thread store its fragment to a SMEM
+    //
+
+    AccessType *aligned_reduction_ptr = reinterpret_cast<AccessType *>(
+      &reduction_ptr_[thread_offset.row() * Shape::kN + thread_offset.column()]);
 
-    int thread_initial_column = ThreadMap::initial_offset(thread_idx_).column();
+    AccessType const *frag_ptr = reinterpret_cast<AccessType const *>(&reduction_fragment);
+    
+    CUTLASS_PRAGMA_UNROLL
+    for (int column = 0; column < ThreadMap::Iterations::kColumn; ++column) {
+      int col_idx = column * ThreadMap::Delta::kColumn / ThreadMap::kElementsPerAccess;
 
-    int thread_column_idx = threadblock_offset.column() + thread_initial_column;
-    broadcast_ptr += thread_initial_column;
+      aligned_reduction_ptr[col_idx] = frag_ptr[column];
+    }
 
-    NumericArrayConverter<ElementCompute, ElementVector, BroadcastDetail::kElementsPerAccess> converter;
-    using AccessType = AlignedArray<ElementVector, BroadcastDetail::kElementsPerAccess>;
-    using ComputeFragmentType = Array<ElementCompute, BroadcastDetail::kElementsPerAccess>;
+    __syncthreads();
 
-    ComputeFragmentType *frag_ptr = reinterpret_cast<ComputeFragmentType *>(&broadcast_fragment);
+    //
+    // Now, threads are assigned several columns of the output. They fetch over all rows from
+    // the compacted SMEM tile and perform a reduction.
+    //
 
     CUTLASS_PRAGMA_UNROLL
-    for (int j = 0; j < ThreadMap::Iterations::kColumn; ++j) {
+    for (int j = 0; j < ReductionDetail::kThreadAccessesPerRow; ++j) {
+      int column_idx = thread_idx_ + j * ReductionDetail::kThreadCount;
 
-      AccessType loaded;
+      ReductionOp reduction_op;
+      ElementAccumulator reduction_element = ElementAccumulator();
 
-      loaded.clear();
+      int output_column_idx = threadblock_offset.column() + column_idx;
 
-      if (thread_column_idx < problem_size.column()) {
-        loaded = *reinterpret_cast<AccessType const *>(broadcast_ptr);
-      }
+      if (column_idx < Shape::kN && output_column_idx < problem_size.column()) {
 
-      ComputeFragmentType cvt = converter(loaded);
-      frag_ptr[j] = cvt;
+        CUTLASS_PRAGMA_UNROLL
+        for (int row = 0; row < ReductionDetail::kThreadRows; ++row) {
+          if (row) {
+            auto frag = reduction_ptr_[row * Shape::kN + column_idx];
+
+            reduction_element = reduction_op(reduction_element, frag);
+          }
+          else {
+
+            reduction_element = reduction_ptr_[column_idx];
+          }
+        }
 
-      thread_column_idx += ThreadMap::Delta::kColumn;
-      broadcast_ptr += ThreadMap::Delta::kColumn;
+        // Store
+        reduction_output_ptr[column_idx] = ElementVector(reduction_element);
+      }
     }
   }
 
-  template <class Seq>
-  struct acc2smem_source_not_needed;
+  template<class Seq>
+  struct acc2smem;
 
   template <size_t... Seq>
-  struct acc2smem_source_not_needed<cutlass::index_sequence<Seq...>> {
-    template <int Advance>
-    CUTLASS_DEVICE static void helper(AccumulatorFragmentIterator accum_fragment_iterator,
-                                      WarpTileIterator &warp_tile_iterator) {
+  struct acc2smem<cutlass::index_sequence<Seq...>> {
+    template<int Advance>
+    CUTLASS_DEVICE
+    static void helper(AccumulatorFragmentIterator accum_fragment_iterator,
+                       WarpTileIterator &warp_tile_iterator) {
       CUTLASS_PRAGMA_UNROLL
       for (int i = 0; i < Advance; i++) {
         ++accum_fragment_iterator;
       }
 
-      CUTLASS_PRAGMA_UNROLL
-      for (int p = 0; p < Base::kFragmentsPerIteration; ++p) {
-        typename AccumulatorFragmentIterator::Fragment accum_fragment;
-
-        accum_fragment_iterator.load(accum_fragment);
-        ++accum_fragment_iterator;
-
-        warp_tile_iterator.store(accum_fragment);
-        if (p < Base::kFragmentsPerIteration - 1) {
-          warp_tile_iterator.add_pointer_offset(kSmemPointerOffset);
-        }
-      }
-
-      if (Base::kFragmentsPerIteration > 1) {
-        warp_tile_iterator.add_pointer_offset(kSmemPointerOffset *
-                                              (1 - Base::kFragmentsPerIteration));
-      }
+      typename AccumulatorFragmentIterator::Fragment accum_fragment;
+      accum_fragment_iterator.load(accum_fragment);
+      warp_tile_iterator.store(accum_fragment);
     }
 
     CUTLASS_DEVICE
     static void push(size_t pos,
                      AccumulatorFragmentIterator const &iterator_begin,
                      WarpTileIterator &warp_tile_iterator) {
-      int dummy[] = {
-          (pos == (Seq * Base::kFragmentsPerIteration)) &&
-          (helper<Seq * Base::kFragmentsPerIteration>(iterator_begin, warp_tile_iterator), 0)...};
-
-      CUTLASS_UNUSED(dummy[0]);
+      int dummy[] = {(pos == Seq) && (helper<Seq>(iterator_begin, warp_tile_iterator), 0)...};
     }
   };
 
   /// Streams the result to global memory
   CUTLASS_DEVICE
   void compute_source_not_needed_(
     OutputOp const &output_op,                        ///< Output operator
-    BroadcastFragment const &broadcast_fragment,      ///< Fragment containing the accumulated partial reduction over columns
+    ReductionFragment &reduction_fragment,            ///< Fragment containing the accumulated partial reduction over columns
     OutputTileIterator destination_iterator,          ///< Tile iterator for destination
     AccumulatorTile const &accumulators,              ///< Complete warp-level accumulator tile 
-    TensorTileIterator tensor_iterator                ///< Threadblock tile iterator for additioanl tensor operand
+    TensorTileIterator tensor_iterator,               ///< Threadblock tile iterator for additioanl tensor operand
+    MatrixCoord const &problem_size,                  ///< Problem size needed to guard against out-of-bounds accesses
+    MatrixCoord const &threadblock_offset             ///< Threadblock's initial offset within the problem size space
     ) { 
 
     //
     // Iterator over warp-level accumulator fragment
     //
 
+    typename TensorTileIterator::Fragment tensor_fragment;
+    tensor_fragment.clear();
+
     AccumulatorFragmentIterator accum_fragment_iterator(accumulators);
 
     //
     // Iterate over accumulator tile
     // 
 
-    // CUTLASS_PRAGMA_UNROLL
-    #pragma unroll(IterationsUnroll ? OutputTileIterator::kIterations / Base::kFragmentsPerIteration : 1)
-    for (int iter = 0; iter < OutputTileIterator::kIterations; iter += Base::kFragmentsPerIteration) {
+    #pragma unroll(IterationsUnroll ? OutputTileIterator::kIterations : 1)
+    for (int iter = 0; iter < OutputTileIterator::kIterations; ++iter) {
 
       //
       // Convert and store fragment
       //
-      
 
+      tensor_iterator.load(tensor_fragment);
+      ++tensor_iterator;
+      
       __syncthreads();
 
-      acc2smem_source_not_needed<
-          cutlass::make_index_sequence<OutputTileIterator::kIterations /
-                                   Base::kFragmentsPerIteration>>::push(iter,
-                                                                        accum_fragment_iterator,
-                                                                        this->warp_tile_iterator_);
+      acc2smem<cutlass::make_index_sequence<OutputTileIterator::kIterations>>::push(
+          iter, accum_fragment_iterator, this->warp_tile_iterator_);
 
       __syncthreads();
 
       //
       // Load fragments from shared memory
       //
 
-      CUTLASS_PRAGMA_UNROLL
-      for (int p = 0; p < Base::kFragmentsPerIteration; ++p) {
-
-
-        typename SharedLoadIterator::Fragment aligned_accum_fragment[kPartitionsK];
-
-        shared_load_iterator_.load(aligned_accum_fragment[0]);
-
-        if (p < Base::kFragmentsPerIteration - 1) {
-          shared_load_iterator_.add_pointer_offset(kSmemPointerOffset);
-        }
-        else if (kPartitionsK > 1) {
-
-          plus <typename SharedLoadIterator::Fragment> add_fragments;
-
-          CUTLASS_PRAGMA_UNROLL
-          for ( int i = 1; i < kPartitionsK; ++i) {
-            shared_load_iterator_.add_pointer_offset(kSmemPointerOffset);
-            shared_load_iterator_.load(aligned_accum_fragment[i]);
-            aligned_accum_fragment[0] = add_fragments(aligned_accum_fragment[0], aligned_accum_fragment[i]);
-          }
+      typename SharedLoadIterator::Fragment aligned_accum_fragment[kPartitionsK];
 
-          shared_load_iterator_.add_pointer_offset((1 - kPartitionsK) * kSmemPointerOffset);
-        }
+      shared_load_iterator_.load(aligned_accum_fragment[0]);
 
-        //
-        // Apply output operation
-        //
-
-        typename OutputTileIterator::Fragment frag_Z;
-        typename TensorTileIterator::Fragment frag_T;
-
-        apply_output_operator_source_not_needed_(
-          frag_Z,
-          frag_T,
-          output_op,
-          aligned_accum_fragment[0],
-          broadcast_fragment);
-
-        //
-        // Conditionally store fragments
-        //
-
-        if (OutputOp::kStoreZ) {
-          destination_iterator.store(frag_Z);
-          ++destination_iterator;
-        }
+      //
+      // If the number of k-slices is > 1 - perform a reduction amongst the k-slices
+      //
+      if (kPartitionsK > 1)
+      {
+        plus <typename SharedLoadIterator::Fragment> add_fragments;
+        const int tile_row_offset = Base::SharedStorage::StorageShape::kRow / PartitionsK;
 
-        if (OutputOp::kStoreT) {
-          tensor_iterator.store(frag_T);
-          ++tensor_iterator;
+        CUTLASS_PRAGMA_UNROLL
+        for ( int i = 1; i < kPartitionsK; ++i) {
+          shared_load_iterator_.add_tile_offset({tile_row_offset , 0});
+          shared_load_iterator_.load(aligned_accum_fragment[i]);
+          aligned_accum_fragment[0] = add_fragments(aligned_accum_fragment[0], aligned_accum_fragment[i]);
         }
-      }
 
-      if (Base::kFragmentsPerIteration > 1) {
-        shared_load_iterator_.add_pointer_offset(kSmemPointerOffset * (1 - Base::kFragmentsPerIteration));
+        shared_load_iterator_.add_tile_offset({-1 * (kPartitionsK-1) * tile_row_offset, 0});
       }
-    }
-  }
 
+      //
+      // Compute the output result
+      //
+     
+      FragmentCompute compute_fragment;
 
-  template<class Seq>
-  struct acc2smem_source_needed;
+      apply_output_operator_source_not_needed_(
+        reduction_fragment,
+        compute_fragment, 
+        output_op, 
+        aligned_accum_fragment[0],
+        tensor_fragment,
+        destination_iterator);
 
-  template <size_t... Seq>
-  struct acc2smem_source_needed<cutlass::index_sequence<Seq...>> {
-    template<int Advance>
-    CUTLASS_DEVICE
-    static void helper(AccumulatorFragmentIterator accum_fragment_iterator,
-                       WarpTileIterator &warp_tile_iterator) {
-      CUTLASS_PRAGMA_UNROLL
-      for (int i = 0; i < Advance; i++) {
-        ++accum_fragment_iterator;
-      }
+      //
+      // Store the final result
+      //
+      
+      NumericArrayConverter<ElementOutput, ElementCompute, FragmentCompute::kElements> converter;
 
-      typename AccumulatorFragmentIterator::Fragment accum_fragment;
-      accum_fragment_iterator.load(accum_fragment);
-      warp_tile_iterator.store(accum_fragment);
-    }
+      typename OutputTileIterator::Fragment output_fragment = converter(compute_fragment);
 
-    CUTLASS_DEVICE
-    static void push(size_t pos,
-                     AccumulatorFragmentIterator const &iterator_begin,
-                     WarpTileIterator &warp_tile_iterator) {
-      int dummy[] = {(pos == Seq) && (helper<Seq>(iterator_begin, warp_tile_iterator), 0)...};
+      destination_iterator.store(output_fragment);
+      ++destination_iterator;
     }
-  };
+  }
 
   
   /// Streams the result to global memory
   CUTLASS_DEVICE
   void compute_source_needed_(
     OutputOp const &output_op,                    ///< Output operator
-    BroadcastFragment const &broadcast_fragment,  ///< Fragment containing the accumulated partial reduction over columns
+    ReductionFragment &reduction_fragment,        ///< Fragment containing the accumulated partial reduction over columns
     OutputTileIterator destination_iterator,      ///< Tile iterator for destination
     AccumulatorTile const &accumulators,          ///< Complete warp-level accumulator tile
     OutputTileIterator source_iterator,           ///< Threadblock tile coordinate in GEMM (in units of threadblock tiles)
-    TensorTileIterator tensor_iterator            ///< Threadblock tile iterator for additioanl tensor operand
+    TensorTileIterator tensor_iterator,            ///< Threadblock tile iterator for additioanl tensor operand
+    MatrixCoord const &problem_size,                  ///< Problem size needed to guard against out-of-bounds accesses
+    MatrixCoord const &threadblock_offset             ///< Threadblock's initial offset within the problem size space
     ) { 
     
     typename OutputTileIterator::Fragment source_fragment;
     source_fragment.clear();
 
+    typename TensorTileIterator::Fragment tensor_fragment;
+    tensor_fragment.clear();
+
     //
     // Iterator over warp-level accumulator fragment
     //
 
     AccumulatorFragmentIterator accum_fragment_iterator(accumulators);
 
     //
@@ -666,24 +572,28 @@
     #pragma unroll(IterationsUnroll ? OutputTileIterator::kIterations : 1)
     for (int iter = 0; iter < OutputTileIterator::kIterations; ++iter) {
 
       //
       // Load the source
       //
 
+      source_fragment.clear();
       source_iterator.load(source_fragment);
       ++source_iterator;
 
+      tensor_iterator.load(tensor_fragment);
+      ++tensor_iterator;
+
       //
       // Convert and store fragment
       //
       
       __syncthreads();
 
-      acc2smem_source_needed<cutlass::make_index_sequence<OutputTileIterator::kIterations>>::push(
+      acc2smem<cutlass::make_index_sequence<OutputTileIterator::kIterations>>::push(
           iter, accum_fragment_iterator, this->warp_tile_iterator_);
 
       __syncthreads();
 
       //
       // Load fragments from shared memory
       //
@@ -705,118 +615,205 @@
           aligned_accum_fragment[0] = add_fragments(aligned_accum_fragment[0], aligned_accum_fragment[i]);
         }
 
         shared_load_iterator_.add_tile_offset({-1 * (kPartitionsK-1) * tile_row_offset, 0});
       }
 
       //
-      // Apply output operation
+      // Compute the output result
       //
-
-      typename OutputTileIterator::Fragment frag_Z;
-      typename TensorTileIterator::Fragment frag_T;
+     
+      FragmentCompute compute_fragment;
 
       apply_output_operator_(
-        frag_Z,
-        frag_T,
-        output_op,
-        aligned_accum_fragment[0],
+        reduction_fragment, 
+        compute_fragment, 
+        output_op, 
+        aligned_accum_fragment[0], 
         source_fragment,
-        broadcast_fragment);
+        tensor_fragment,
+        destination_iterator);
 
       //
-      // Conditionally store fragments
+      // Convert and store the final result
       //
 
-      if (OutputOp::kStoreZ) {
-        destination_iterator.store(frag_Z);
-        ++destination_iterator;
-      }
+      NumericArrayConverter<ElementOutput, ElementCompute, FragmentCompute::kElements> converter;
 
-      if (OutputOp::kStoreT) {
-        tensor_iterator.store(frag_T);
-        ++tensor_iterator;
-      }
+      typename OutputTileIterator::Fragment output_fragment = converter(compute_fragment);
+
+      destination_iterator.store(output_fragment);      
+      ++destination_iterator;
     }
   }
 
   /// Helper to invoke the output functor over each vector of output
   CUTLASS_DEVICE
   void apply_output_operator_(
-    typename OutputTileIterator::Fragment &frag_Z,
-    typename TensorTileIterator::Fragment &frag_T,
-    OutputOp const &output_op,
-    typename SharedLoadIterator::Fragment const &frag_AB,
-    typename OutputTileIterator::Fragment const &frag_C,
-    BroadcastFragment const &frag_Broadcast) {
-
-    using AccessTypeZ = Array<typename OutputTileIterator::Element, kElementsPerAccess>;
-    using AccessTypeT = Array<typename TensorTileIterator::Element, kElementsPerAccess>;
-    using AccessTypeBroadcast = Array<ElementCompute, kElementsPerAccess>;
+    ReductionFragment &reduction_fragment,
+    FragmentCompute &compute_fragment,
+    OutputOp const &output_op,                    ///< Output operator
+    typename SharedLoadIterator::Fragment const &aligned_accum_fragment,
+    typename OutputTileIterator::Fragment const &source_fragment,
+    typename TensorTileIterator::Fragment const &tensor_fragment,
+    OutputTileIterator const & destination_iterator) {
+      
+    ComputeAccessType *compute_frag_ptr = 
+      reinterpret_cast<ComputeAccessType *>(&compute_fragment);
 
-    AccessTypeZ *frag_Z_ptr = reinterpret_cast<AccessTypeZ *>(&frag_Z);
-    AccessTypeT *frag_T_ptr = reinterpret_cast<AccessTypeT *>(&frag_T);
-    
-    AccumulatorAccessType const *frag_AB_ptr = 
-      reinterpret_cast<AccumulatorAccessType const *>(&frag_AB);
+    AccumulatorAccessType const *accum_frag_ptr = 
+      reinterpret_cast<AccumulatorAccessType const *>(&aligned_accum_fragment);
 
-    OutputAccessType const *frag_C_ptr = 
-      reinterpret_cast<OutputAccessType const *>(&frag_C);
+    OutputAccessType const *source_frag_ptr = 
+      reinterpret_cast<OutputAccessType const *>(&source_fragment);
 
-    AccessTypeBroadcast const *frag_Broadcast_ptr =
-      reinterpret_cast<AccessTypeBroadcast const *>(&frag_Broadcast);
+    TensorAccessType const *tensor_frag_ptr =
+      reinterpret_cast<TensorAccessType const *>(&tensor_fragment);
 
     int const kOutputOpIterations = 
       OutputTileIterator::Fragment::kElements / OutputTileIterator::kElementsPerAccess;
 
     CUTLASS_PRAGMA_UNROLL
     for (int i = 0; i < kOutputOpIterations; ++i) {
 
-      output_op(
-        frag_Z_ptr[i], 
-        frag_T_ptr[i], 
-        frag_AB_ptr[i], 
-        frag_C_ptr[i], 
-        frag_Broadcast_ptr[i % ThreadMap::Iterations::kColumn]);
+      // Call the output operator
+      compute_frag_ptr[i] = output_op(accum_frag_ptr[i], source_frag_ptr[i], tensor_frag_ptr[i]);
+    }
+
+    //
+    // Partial reduction over each column
+    //
+
+    ReductionOp reduction_op;
+
+    typename OutputTileIterator::Mask mask;
+    destination_iterator.get_mask(mask);
+
+    CUTLASS_PRAGMA_UNROLL
+    for (int column = 0; column < ReductionDetail::kColumnsPerThread; ++column) {
+
+      int column_vector_idx = column / ThreadMap::kElementsPerAccess;
+      bool column_guard = mask.predicates[column_vector_idx];
+
+      CUTLASS_PRAGMA_UNROLL
+      for (int row = 0; row < ReductionDetail::kRowsPerThread; ++row) {
+
+        bool fetch;
+        if (ReductionDetail::kOobCheck) {
+          int row_idx = (row % ThreadMap::Iterations::kRow);
+          int residual = (row / ThreadMap::Iterations::kRow);
+
+          int group_idx = (residual % ThreadMap::Iterations::kGroup);
+          residual = (residual / ThreadMap::Iterations::kGroup);
+
+          int cluster_idx = (residual % ThreadMap::Iterations::kCluster);
+
+          int row_offset = row_idx * ThreadMap::Delta::kRow 
+            + group_idx * ThreadMap::Delta::kGroup 
+            + cluster_idx * ThreadMap::Delta::kCluster;
+
+          int output_row = destination_iterator.thread_start_row() + row_offset;
+
+          fetch = (output_row < destination_iterator.extent_row() && column_guard);
+        }
+        else {
+          fetch = true;
+        }
+
+        ElementCompute value = ElementCompute();
+        if (fetch) {
+          value = compute_fragment[row * ReductionDetail::kColumnsPerThread + column];
+        }
+
+        reduction_fragment[column] = reduction_op(
+          reduction_fragment[column], 
+          value);
+      }
     }
   }
 
   /// Helper to invoke the output functor over each vector of output
   CUTLASS_DEVICE
   void apply_output_operator_source_not_needed_(
-    typename OutputTileIterator::Fragment &frag_Z,
-    typename TensorTileIterator::Fragment &frag_T,
-    OutputOp const &output_op,
-    typename SharedLoadIterator::Fragment const &frag_AB,
-    BroadcastFragment const &frag_Broadcast) {
-
-    using AccessTypeZ = Array<typename OutputTileIterator::Element, kElementsPerAccess>;
-    using AccessTypeT = Array<typename TensorTileIterator::Element, kElementsPerAccess>;
-    using AccessTypeBroadcast = Array<ElementCompute, kElementsPerAccess>;
-
-    AccessTypeZ *frag_Z_ptr = reinterpret_cast<AccessTypeZ *>(&frag_Z);
-    AccessTypeT *frag_T_ptr = reinterpret_cast<AccessTypeT *>(&frag_T);
+    ReductionFragment &reduction_fragment,
+    FragmentCompute &compute_fragment,
+    OutputOp const &output_op,                    ///< Output operator
+    typename SharedLoadIterator::Fragment const &aligned_accum_fragment,
+    typename TensorTileIterator::Fragment const &tensor_fragment,
+    OutputTileIterator const & destination_iterator
+  ) {
     
-    AccumulatorAccessType const *frag_AB_ptr = 
-      reinterpret_cast<AccumulatorAccessType const *>(&frag_AB);
+    ComputeAccessType *compute_frag_ptr = 
+      reinterpret_cast<ComputeAccessType *>(&compute_fragment);
+
+    AccumulatorAccessType const *accum_frag_ptr = 
+      reinterpret_cast<AccumulatorAccessType const *>(&aligned_accum_fragment);
 
-    AccessTypeBroadcast const *frag_Broadcast_ptr =
-      reinterpret_cast<AccessTypeBroadcast const *>(&frag_Broadcast);
+    TensorAccessType const *tensor_frag_ptr =
+      reinterpret_cast<TensorAccessType const *>(&tensor_fragment);
 
     int const kOutputOpIterations = 
       OutputTileIterator::Fragment::kElements / OutputTileIterator::kElementsPerAccess;
 
     CUTLASS_PRAGMA_UNROLL
     for (int i = 0; i < kOutputOpIterations; ++i) {
 
-      output_op(
-        frag_Z_ptr[i], 
-        frag_T_ptr[i], 
-        frag_AB_ptr[i], 
-        frag_Broadcast_ptr[i % ThreadMap::Iterations::kColumn]);
+      // Call the output operator
+      compute_frag_ptr[i] = output_op(accum_frag_ptr[i], tensor_frag_ptr[i]);
+    }
+
+    //
+    // Partial reduction over each column
+    //
+
+    ReductionOp reduction_op;
+
+    typename OutputTileIterator::Mask mask;
+    destination_iterator.get_mask(mask);
+
+    CUTLASS_PRAGMA_UNROLL
+    for (int column = 0; column < ReductionDetail::kColumnsPerThread; ++column) {
+
+      int column_vector_idx = column / ThreadMap::kElementsPerAccess;
+      bool column_guard = mask.predicates[column_vector_idx];
+
+      CUTLASS_PRAGMA_UNROLL
+      for (int row = 0; row < ReductionDetail::kRowsPerThread; ++row) {
+
+        bool fetch;
+        if (ReductionDetail::kOobCheck) {
+          int row_idx = (row % ThreadMap::Iterations::kRow);
+          int residual = (row / ThreadMap::Iterations::kRow);
+
+          int group_idx = (residual % ThreadMap::Iterations::kGroup);
+          residual = (residual / ThreadMap::Iterations::kGroup);
+
+          int cluster_idx = (residual % ThreadMap::Iterations::kCluster);
+
+          int row_offset = row_idx * ThreadMap::Delta::kRow 
+            + group_idx * ThreadMap::Delta::kGroup 
+            + cluster_idx * ThreadMap::Delta::kCluster;
+
+          int output_row = destination_iterator.thread_start_row() + row_offset;
+
+          fetch = (output_row < destination_iterator.extent_row() && column_guard);
+        }
+        else {
+          fetch = true;
+        }
+
+        ElementCompute value = ElementCompute();
+        if (fetch) {
+          value = compute_fragment[row * ReductionDetail::kColumnsPerThread + column];
+        }
+
+        reduction_fragment[column] = reduction_op(
+          reduction_fragment[column], 
+          value);
+      }
     }
   }
 };
 
 ////////////////////////////////////////////////////////////////////////////////
 
 } // namespace threadblock
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/epilogue_with_reduction.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/transform/threadblock/predicated_tile_iterator_triangular_matrix.h`

 * *Files 22% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -25,797 +25,794 @@
  * SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER
  * CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,
  * OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
  * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
  *
  **************************************************************************************************/
 /*! \file
+    \brief Templates implementing loading of tiles from pitch-linear rank=2 tensors. 
 
-  \brief Epilogue for threadblock scoped GEMMs using Tensor Ops.
-
-  The epilogue rearranges the result of a matrix product through shared memory to match canonical
-  tensor layouts in global memory. Epilogues support conversion and reduction operations.
+    This iterator uses masks to guard out-of-bounds accesses and visits the last "residue" tile
+    first, with the objective of minimizing predicate mask updates during steady-state operation.
 
+    A precomputed "Params" object minimizes the amount of state that must be stored in registers,
+    and integer addition is used to advance the pointer through memory.
 */
 
 #pragma once
 
-#if defined(__CUDACC_RTC__)
-#include <cuda/std/cassert>
-#else
-#include <assert.h>
-#endif
-
-#include "cutlass/cutlass.h"
-#include "cutlass/array.h"
-#include "cutlass/numeric_types.h"
-#include "cutlass/numeric_conversion.h"
-#include "cutlass/tensor_coord.h"
-#include "cutlass/aligned_buffer.h"
-#include "cutlass/functional.h"
-#include "cutlass/fast_math.h"
-#include "cutlass/layout/vector.h"
-#include "cutlass/layout/tensor.h"
-
-#include "cutlass/gemm/gemm.h"
-
-#include "cutlass/transform/pitch_linear_thread_map.h"
-#include "cutlass/transform/threadblock/regular_tile_iterator.h"
-
-#include "cutlass/epilogue/threadblock/epilogue_base.h"
-#include "cutlass/epilogue/threadblock/predicated_tile_iterator.h"
+#include "cutlass/arch/memory.h"
+#include "cutlass/transform/threadblock/predicated_tile_access_iterator_triangular_matrix.h"
 
 ////////////////////////////////////////////////////////////////////////////////
 
 namespace cutlass {
-namespace epilogue {
+namespace transform {
 namespace threadblock {
 
 ////////////////////////////////////////////////////////////////////////////////
 
-/// Epilogue operator with reduction over each column 
+/// PredicatedTileIteratorTriangularMatrix
+///
+/// Satisfies: ForwardTileIteratorConcept | 
+///            ReadableContiguousTileIteratorConcept | 
+///            WriteableContiguousTileIteratorConcept |
+///            MaskedTileIteratorConcept
+///
+/// Regular tile iterator using a precomputed control structure to minimize register liveness
+/// and integer arithmetic.
+///
+/// Layout is assumed to be invariant at the time the precomputed "Params" object is constructed.
+///
+/// Base pointer and tensor extents may be specified at the time the iterator is constructed.
+/// Subsequently, they are assumed to be immutable.
+///
+/// Adding a logical coordinate offset may be performed at the time the iterator is constructed.
+/// Subsequent additions to logical coordinate offset may be performed but are relatively expensive.
+///
+/// Vistitation order is intended to first visit a "residual" tile that may be partially full in
+/// both the advance dimension and the steady-state dimension. This is assumed to be the last
+/// tile in the iteration sequence. Advancing an iterator that has just been constructed moves to
+/// the first tile that is full in the advance dimension and recomputes predicates. Subsequent
+/// accesses may be performed without updating internal predicates and are efficient in terms of
+/// live register state and pointer arithmetic instructions.
+///
+/// To be efficient, this assumes the iteraor will be dereferenced and advanced at least once
+/// outside any looping structure to minimize integer arithmetic. 
+///
+/// Acceses out of bounds are safe so long as `clear_mask()` is called prior to dereferencing
+/// the iterator.
+///
+///
+/// Example:
+///
+/// An efficient pipeline structure may be constructed as follows:
+///
+// template <typename Iterator>
+// __global__ void kernel(
+//   typename Iterator::Params params, 
+//   typename Iterator::Element *ptr,
+//   TensorCoord extent) {
+//
+//   typename Iterator::Fragment fragment;
+//
+//   TensorCoord threadblock_offset(0, 0);
+//
+//   Iterator iter(params, ptr, extent, threadIdx.x, threadblock_offsets);
+//
+//
+//   fragment = *iter;        // load "residue" tile first
+//   ++iter;                  // advance to first "steady state" tile and update internal masks
+//
+//
+//   #pragma unroll
+//   for (int i = Remaining - 1; i >= 0; --i) {
+//
+//     f(fragment);
+//
+//     if (!i) {
+//       iter.clear_mask();   // light-weight operation to clear masks - subsequent loads become NO-OPs.
+//     }
+//  
+//     fragment = *iter;      // load tile during "steady state" phase
+//     ++iter;                // advance to next tile - lightweight due to steady-state masks
+//   }
+// }
+//
+// void host(TensorView<Element, 2, layout::PitchLinear> view) {
+//
+//   using Iterator = transform::threadblock::PredicatedTileIteratorTriangularMatrix;
+//
+//   typename Iterator::Params params(view.layout());
+//
+//   kernel<Iterator>(params, view.data());
+// }
+///
+///
 template <
-  typename Shape_,                          ///< Shape of threadblock tile (concept: GemmShape)
-  typename WarpMmaOperator_,                ///< Warp-level MMA operator (concept: gemm::warp::MmaTensorOp)
-  int PartitionsK,                          ///< Number of partitions of the K dimension
-  typename OutputTileIterator_,             ///< Tile iterator reading and writing output tensors
-  typename TensorTileIterator_,             ///< Additional tile iterator for tensor-valued operands
-  typename ElementVector_,                  ///< Pointer to reduction vector
-  typename AccumulatorFragmentIterator_,    ///< Fragment iterator selecting accumulators
-  typename WarpTileIterator_,               ///< Warp-scoped tile iterator writing accumulators to SMEM
-  typename SharedLoadIterator_,             ///< Threadblock-scoped tile iterator loading from SMEM
-  typename OutputOp_,                       ///< Output operator
-  typename ReductionOp_,                    ///< Reduction operator
-  typename Padding_,                        ///< Padding added to SMEM allocation to avoid bank conflicts (concept: MatrixShape)
-  int IterationsUnroll =                    ///< Used to reduce binary size when epilogue op is large
-    (!IsEpilogueFunctorHeavy<OutputOp_>::value)
+  typename Shape,
+  typename Element,
+  typename Layout,
+  int AdvanceRank,
+  typename ThreadMap,
+  SideMode kSideMode, 
+  FillMode kFillMode, 
+  DiagType kDiagType,
+  int AccessSize = ThreadMap::kElementsPerAccess
 >
-class EpilogueWithReduction : 
-  public EpilogueBase<
-    Shape_, 
-    typename WarpMmaOperator_::Shape, 
-    PartitionsK, 
-    AccumulatorFragmentIterator_, 
-    WarpTileIterator_, 
-    Padding_> {
+class PredicatedTileIteratorTriangularMatrix;
 
-public:
+////////////////////////////////////////////////////////////////////////////////
 
-  using Base = EpilogueBase<
-    Shape_, 
-    typename WarpMmaOperator_::Shape, 
-    PartitionsK, 
-    AccumulatorFragmentIterator_, 
-    WarpTileIterator_, 
-    Padding_>;
+/// Specialization of PredicatedTileIteratorTriangularMatrix for pitch-linear data.
+///
+/// Satisfies: ForwardTileIteratorConcept | 
+///            ReadableContiguousTileIteratorConcept | 
+///            WriteableContiguousTileIteratorConcept |
+///            MaskedTileIteratorConcept
+///
+template <typename Shape_, typename Element_, int AdvanceRank, typename ThreadMap_, 
+          SideMode kSideMode, FillMode kFillMode, DiagType kDiagType, 
+          int AccessSize>
+class PredicatedTileIteratorTriangularMatrix<Shape_, Element_, layout::PitchLinear, AdvanceRank, ThreadMap_, 
+                                             kSideMode, kFillMode, kDiagType,
+                                             AccessSize> {
+ public:
+  static_assert(
+      AdvanceRank == 0 || AdvanceRank == 1,
+      "Specialization for pitch-linear iterator may along advance along the "
+      "contiguous(rank=0) or strided(rank=1) dimension.");
 
   using Shape = Shape_;
-  using WarpMmaOperator = WarpMmaOperator_;
-  static int const kPartitionsK = PartitionsK;
-  using OutputTileIterator = OutputTileIterator_;
-  using TensorTileIterator = TensorTileIterator_;
-  using ElementVector = ElementVector_;
-  using AccumulatorFragmentIterator = AccumulatorFragmentIterator_;
-  using WarpTileIterator = WarpTileIterator_;
-  using SharedLoadIterator = SharedLoadIterator_;
-  using OutputOp = OutputOp_;
-  using ReductionOp = ReductionOp_;
-  using Padding = Padding_;
+  using Element = Element_;
+  using Layout = layout::PitchLinear;
+  static int const kAdvanceRank = AdvanceRank;
+  using ThreadMap = ThreadMap_;
 
-  using Layout = layout::RowMajor;
+  using Index = typename Layout::Index;
   using LongIndex = typename Layout::LongIndex;
 
-  /// The complete warp-level accumulator tile
-  using AccumulatorTile = typename Base::AccumulatorTile;
-
-  /// Accumulator element
-  using ElementAccumulator = typename WarpTileIterator::Element;
-
-  /// Compute data type produced by the output op
-  using ElementCompute = typename OutputOp::ElementCompute;
-
-  /// Compute fragment
-  using FragmentCompute = Array<ElementCompute, OutputTileIterator::Fragment::kElements>;
-
-  /// Thread map used by output tile iterators
-  using ThreadMap = typename OutputTileIterator::ThreadMap;
-
-  /// Fragment object used in reduction
-  using ReductionFragment = Array<
-    ElementAccumulator, 
-    ThreadMap::Iterations::kColumn * ThreadMap::kElementsPerAccess>;
+  using TensorRef = TensorRef<Element, Layout>;
+  using TensorView = TensorView<Element, Layout>;
+  using TensorCoord = typename Layout::TensorCoord;
 
-  /// Output element
-  using ElementOutput = typename OutputTileIterator::Element;
+  using Pointer = Element *;
+  using NonConstPointer = typename platform::remove_const<Element>::type *;
 
-  /// Data type of additional tensor
-  using ElementTensor = typename TensorTileIterator::Element;
+  /// Type used for internal memory accesses
+  using AccessType = AlignedArray<Element, AccessSize, (AccessSize * sizeof_bits<Element>::value / 8)>;
 
-  /// Output access size
-  static int const kElementsPerAccess = OutputTileIterator::kElementsPerAccess;
+  /// Underlying iterator to compute the addresses
+  using TileAccessIterator =
+      PredicatedTileAccessIteratorTriangularMatrix<Shape, Element, Layout, kAdvanceRank,
+                                   ThreadMap, kSideMode, kFillMode, kDiagType, AccessType>;
 
-  /// Tensor reference to destination tensor
-  using TensorRef = typename OutputTileIterator::TensorRef;
+  static int const kAccessesPerVector = TileAccessIterator::kAccessesPerVector;
 
-  /// Tensor reference to sync tensor
-  using SyncTensorRef = typename cutlass::TensorRef<int, cutlass::layout::PackedVectorLayout>;
+  /// Fragment object to be loaded or stored
+  using Fragment = cutlass::Array<Element, ThreadMap::Iterations::kCount *
+                                               ThreadMap::kElementsPerAccess>;
 
-  /// Const tensor reference to source tensor
-  using ConstTensorRef = typename OutputTileIterator::ConstTensorRef;
+  /// Predicate vector stores mask to guard accesses
+  using Mask = typename TileAccessIterator::Mask;
 
-  /// Array type used to output
-  using OutputAccessType = Array<
-    typename OutputTileIterator::Element, OutputTileIterator::kElementsPerAccess>;
+  /// Parameters object is precomputed state and is host-constructible
+  class Params {
+   public:
+    friend PredicatedTileIteratorTriangularMatrix;
 
-  /// Array type used by output functor
-  using AccumulatorAccessType = Array<typename WarpTileIterator::Element, OutputTileIterator::kElementsPerAccess>; 
-
-  /// Array type used by output functor
-  using ComputeAccessType = Array<ElementCompute, OutputTileIterator::kElementsPerAccess>;
-
-  /// Tensor access type
-  using TensorAccessType = Array<ElementTensor, OutputTileIterator::kElementsPerAccess>;
-  
-  /// Number of warps
-  using WarpCount = typename Base::WarpCount;
-
-  /// Shared memory allocation from epilogue base class
-  using BaseSharedStorage = typename Base::SharedStorage;
-
-  /// Used for the reduction
-  struct ReductionDetail {
-
-    /// If true, accumulator coordinates are computed and out-of-bounds checks are enabled when
-    /// performing the reduction.
-    static bool const kOobCheck = false;
-
-    /// Number of threads per warp
-    static int const kWarpSize = 32;
-
-    /// Number of distinct scalar column indices handled by each thread
-    static int const kColumnsPerThread = ThreadMap::Iterations::kColumn * ThreadMap::kElementsPerAccess;
-
-    /// Number of distinct scalar row indices handled by each thread
-    static int const kRowsPerThread = ThreadMap::Iterations::kCount / ThreadMap::Iterations::kColumn;
-
-    /// Number of threads per threadblock
-    static int const kThreadCount = kWarpSize * WarpCount::kCount;
-
-    /// Number of distinct threads per row of output tile
-    static int const kThreadsPerRow = (Shape::kN / kColumnsPerThread);
-
-    /// Number of distinct threads which must be reduced during the final reduction phase within the threadblock.
-    static int const kThreadRows = kThreadCount / kThreadsPerRow;
-
-    /// I'm not sure what I meant here.
-    static int const kThreadAccessesPerRow = const_max(1, (Shape::kN + kThreadCount - 1) / kThreadCount);
-
-    /// Shape of the shared memory allocation for the epilogue    
-    using StorageShape = MatrixShape<
-      kThreadRows,
-      Shape::kN
-    >;
-
-    /// Debug printing
-    CUTLASS_DEVICE
-    static void print() {
-#if 0
-      printf("ReductionDetail {\n");
-      printf(
-        "  kElementsPerAccess:%d\nkColumnsPerThread: %d\nkRowsPerThread: %d\n,kThreadCount: %d\nkThreadsPerRow: %d\n"
-        "kThreadRows: %d\nThreadAccessesPerRow: %d\nStorageShape: %d x %d (count: %d)\n",
-        kElementsPerAccess,
-        kColumnsPerThread,
-        kRowsPerThread,
-        kThreadCount,
-        kThreadsPerRow,
-        kThreadRows,
-        kThreadAccessesPerRow,
-        StorageShape::kRow,
-        StorageShape::kColumn,
-        StorageShape::kCount
-      );
-      printf("};\n");
-#endif
-    }
-  };
-
-  /// Shared storage structure (shadows base) with additional SMEM buffer for reduction
-  struct SharedStorage {
-    union {
-      BaseSharedStorage base;
-      AlignedArray<ElementAccumulator, ReductionDetail::StorageShape::kCount, 16> reduction;    ///< Shared storage for reduction
-    };
+   private:
+    /// Parameters object
+    typename TileAccessIterator::Params params_;
 
+   public:
+    /// Construct the Params object given a pitch-linear tensor's layout
     CUTLASS_HOST_DEVICE
-    SharedStorage() { }
+    Params(Layout const &layout) : params_(layout) { }
+    
+    CUTLASS_HOST_DEVICE
+    Params() { }
   };
 
-public:
-
-
-  static_assert(SharedLoadIterator::Fragment::kElements == OutputTileIterator::Fragment::kElements,
-    "Mismatch between shared load iterator and output tile iterator.");
-
-  static_assert(OutputTileIterator::kElementsPerAccess, "OutputTileIterator::kElementsPerAccess must not be zero.");
-
-  static_assert(!(OutputTileIterator::Fragment::kElements % OutputTileIterator::kElementsPerAccess), 
-    "Divisibility");
-
-private:
-
-  /// Loads fragment from shared memory aligned with output tensor
-  SharedLoadIterator shared_load_iterator_;
-
-  /// Shared memory pointer fo rreduction
-  ElementAccumulator *reduction_ptr_;
-
-  /// Thread index within the threadblock
-  int thread_idx_;
-
-public:
-
-  /// Constructor
-  CUTLASS_DEVICE
-  EpilogueWithReduction(
-    SharedStorage &shared_storage,                    ///< Shared storage object    
-    int thread_idx,                                   ///< ID of a thread within the threadblock
-    int warp_idx,                                     ///< ID of warp within threadblock
-    int lane_idx                                      ///< Id of thread within warp
-  ):
-    Base(shared_storage.base, thread_idx, warp_idx, lane_idx),
-    shared_load_iterator_(shared_storage.base.reference(), thread_idx),
-    reduction_ptr_(shared_storage.reduction.data()),
-    thread_idx_(thread_idx)
-  {
-
+ private:
+  /// Internal pointer type permits fast address arithmetic
+  using BytePointer = char *;
+
+ private:
+  //
+  // Data members
+  //
+
+  /// Data member to the tile access iterator
+  TileAccessIterator address_iterator_;
+
+ public:
+  /// Constructs a TileIterator from its precomputed state, threadblock offset,
+  /// and thread ID
+  CUTLASS_HOST_DEVICE
+  PredicatedTileIteratorTriangularMatrix(
+      /// Precomputed parameters object
+      Params const &params,
+      /// Pointer to start of tensor
+      Pointer pointer,
+      /// Extent of tensor
+      TensorCoord extent,
+      /// ID of each participating thread
+      int thread_id,
+      /// Initial offset of threadblock
+      TensorCoord const &threadblock_offset)
+      : address_iterator_(params.params_, pointer, extent, thread_id,
+                          threadblock_offset) {}
+
+  /// Construct a PredicatedTileIteratorTriangularMatrix with zero threadblock offset
+  CUTLASS_HOST_DEVICE
+  PredicatedTileIteratorTriangularMatrix(
+      Params const &params,  ///< Precomputed parameters object
+      Pointer pointer,       ///< Pointer to start of tensor
+      TensorCoord extent,    ///< Extent of tensor
+      int thread_id          ///< ID of each participating thread
+      )
+      : PredicatedTileIteratorTriangularMatrix(params, pointer, extent, thread_id,
+                               make_Coord(0, 0)) {}
+
+  /// Adds a pointer offset in units of Element
+  CUTLASS_HOST_DEVICE
+  void add_pointer_offset(LongIndex pointer_offset) {
+    address_iterator_.add_pointer_offset(pointer_offset);
   }
 
-  /// Streams the result to global memory
-  CUTLASS_DEVICE
-  void operator()(
-    OutputOp const &output_op,                        ///< Output operator
-    ElementVector * reduction_output_ptr,          ///< Reduction output vector
-    OutputTileIterator destination_iterator,          ///< Tile iterator for destination
-    AccumulatorTile const &accumulators,              ///< Complete warp-level accumulator tile
-    OutputTileIterator source_iterator,               ///< Tile iterator for source accumulator matrix
-    TensorTileIterator tensor_iterator,               ///< Threadblock tile iterator for additional tensor operand
-    MatrixCoord const &problem_size =                 ///< Problem size needed to guard against out-of-bounds accesses
-        MatrixCoord(Shape::kM, Shape::kN),
-    MatrixCoord const &threadblock_offset =           ///< Threadblock's initial offset within the problem size space
-        MatrixCoord()) {
-    
-    ReductionFragment reduction_fragment;
-    reduction_fragment.clear();
+  /// Advances to the next tile in memory.
+  ///
+  /// The first time this method is called, predicates are updated, and the
+  /// iterator's internal pointer is reverted to the first "steady state" tile.
+  /// Subsequent calls are lightweight and must only update the internal
+  /// pointer.
+  CUTLASS_HOST_DEVICE
+  PredicatedTileIteratorTriangularMatrix &operator++() {
+    if (kAdvanceRank)
+      address_iterator_.add_tile_offset({0, 1});
+    else
+      address_iterator_.add_tile_offset({1, 0});
 
-    if (!output_op.is_source_needed()) {
-      compute_source_not_needed_(
-        output_op, 
-        reduction_fragment, 
-        destination_iterator, 
-        accumulators,
-        tensor_iterator,
-        problem_size,
-        threadblock_offset);
-    }
-    else {
-      compute_source_needed_(
-        output_op, 
-        reduction_fragment, 
-        destination_iterator, 
-        accumulators, 
-        source_iterator,
-        tensor_iterator,
-        problem_size,
-        threadblock_offset);
-    }
+    return *this;
+  }
 
-    if (output_op.participates_in_reduction()) {
-      reduction_(problem_size, threadblock_offset, reduction_output_ptr, reduction_fragment);
-    }
+  /// Advances to the next tile in memory.
+  ///
+  /// The first time this method is called, predicates are updated, and the
+  /// iterator's internal pointer is reverted to the first "steady state" tile.
+  /// Subsequent calls are lightweight and must only update the internal
+  /// pointer.
+  CUTLASS_HOST_DEVICE
+  PredicatedTileIteratorTriangularMatrix operator++(int) {
+    PredicatedTileIteratorTriangularMatrix self(*this);
+    operator++();
+    return self;
   }
 
-private:
+  /// Clears the predicate set efficiently
+  CUTLASS_HOST_DEVICE
+  void clear_mask(bool enable = true) { address_iterator_.clear_mask(enable); }
+
+  /// Clears the predicate set efficiently
+  CUTLASS_HOST_DEVICE
+  void enable_mask() { address_iterator_.enable_mask(); }
+
+  /// Sets the predicate mask, overriding value stored in predicate iterator
+  CUTLASS_HOST_DEVICE
+  void set_mask(Mask const &mask) { address_iterator_.set_mask(mask); }
+
+  /// Gets the mask
+  CUTLASS_HOST_DEVICE
+  void get_mask(Mask &mask) { address_iterator_.get_mask(mask); }
 
-  /// Perform the reduction
   CUTLASS_DEVICE
-  void reduction_(
-    MatrixCoord const &problem_size,                  ///< Problem size needed to guard against out-of-bounds accesses
-    MatrixCoord const &threadblock_offset,            ///< Problem size needed to guard against out-of-bounds accesses
-    ElementVector * reduction_output_ptr,          ///< Reduction output vector
-    ReductionFragment const & reduction_fragment) {
-
-    //
-    // Store the partially reduced value to SMEM
-    //
-
-    // Guard against uses of the existing SMEM tile
-    __syncthreads();
-    
-    using AccessType = AlignedArray<ElementAccumulator, ThreadMap::kElementsPerAccess>;
-
-    //
-    // Determine a compacted thread arrangement to store to SMEM.
-    //
-    int const kThreadsPerRow = Shape::kN / (ThreadMap::Iterations::kColumn * ThreadMap::kElementsPerAccess);
-
-    MatrixCoord thread_offset(
-      thread_idx_ / kThreadsPerRow, 
-      (thread_idx_ % kThreadsPerRow) * ThreadMap::kElementsPerAccess);
-   
-    //
-    // Each thread store its fragment to a SMEM
-    //
-
-    AccessType *aligned_reduction_ptr = reinterpret_cast<AccessType *>(
-      &reduction_ptr_[thread_offset.row() * Shape::kN + thread_offset.column()]);
-
-    AccessType const *frag_ptr = reinterpret_cast<AccessType const *>(&reduction_fragment);
-    
-    CUTLASS_PRAGMA_UNROLL
-    for (int column = 0; column < ThreadMap::Iterations::kColumn; ++column) {
-      int col_idx = column * ThreadMap::Delta::kColumn / ThreadMap::kElementsPerAccess;
-
-      aligned_reduction_ptr[col_idx] = frag_ptr[column];
-    }
+  void load_with_pointer_offset(Fragment &frag, Index pointer_offset) {
+    load_with_byte_offset(frag, pointer_offset * sizeof_bits<Element>::value / 8);
+  }
 
-    __syncthreads();
+  CUTLASS_DEVICE
+  void load_with_byte_offset(Fragment &frag, LongIndex byte_offset) {
 
-    //
-    // Now, threads are assigned several columns of the output. They fetch over all rows from
-    // the compacted SMEM tile and perform a reduction.
-    //
+    AccessType *frag_ptr = reinterpret_cast<AccessType *>(&frag);
 
     CUTLASS_PRAGMA_UNROLL
-    for (int j = 0; j < ReductionDetail::kThreadAccessesPerRow; ++j) {
-      int column_idx = thread_idx_ + j * ReductionDetail::kThreadCount;
-
-      ReductionOp reduction_op;
-      ElementAccumulator reduction_element = ElementAccumulator();
+    for (int s = 0; s < ThreadMap::Iterations::kStrided; ++s) {
+      CUTLASS_PRAGMA_UNROLL
+      for (int c = 0; c < ThreadMap::Iterations::kContiguous; ++c) {
 
-      int output_column_idx = threadblock_offset.column() + column_idx;
+        CUTLASS_PRAGMA_UNROLL
+        for (int v = 0; v < kAccessesPerVector; ++v) {
 
-      if (column_idx < Shape::kN && output_column_idx < problem_size.column()) {
+          int idx = v + kAccessesPerVector * (c + s * ThreadMap::Iterations::kContiguous);
+          
+          address_iterator_.set_iteration_index(idx);
+          char const *byte_ptr = reinterpret_cast<char const *>(address_iterator_.get()) + byte_offset;
 
-        CUTLASS_PRAGMA_UNROLL
-        for (int row = 0; row < ReductionDetail::kThreadRows; ++row) {
-          if (row) {
-            auto frag = reduction_ptr_[row * Shape::kN + column_idx];
+          AccessType const *access_ptr = reinterpret_cast<AccessType const *>(byte_ptr);
 
-            reduction_element = reduction_op(reduction_element, frag);
-          }
-          else {
+          cutlass::arch::global_load<AccessType,
+                                     sizeof(AccessType)
+                                    >(
+              frag_ptr[idx], access_ptr, address_iterator_.valid());
 
-            reduction_element = reduction_ptr_[column_idx];
-          }
+          ++address_iterator_;
         }
-
-        // Store
-        reduction_output_ptr[column_idx] = ElementVector(reduction_element);
       }
     }
   }
 
-  template<class Seq>
-  struct acc2smem;
-
-  template <size_t... Seq>
-  struct acc2smem<cutlass::index_sequence<Seq...>> {
-    template<int Advance>
-    CUTLASS_DEVICE
-    static void helper(AccumulatorFragmentIterator accum_fragment_iterator,
-                       WarpTileIterator &warp_tile_iterator) {
-      CUTLASS_PRAGMA_UNROLL
-      for (int i = 0; i < Advance; i++) {
-        ++accum_fragment_iterator;
-      }
-
-      typename AccumulatorFragmentIterator::Fragment accum_fragment;
-      accum_fragment_iterator.load(accum_fragment);
-      warp_tile_iterator.store(accum_fragment);
-    }
+  /// Loads a fragment from memory
+  CUTLASS_DEVICE
+  void load(Fragment &frag) { load_with_byte_offset(frag, 0); }
 
-    CUTLASS_DEVICE
-    static void push(size_t pos,
-                     AccumulatorFragmentIterator const &iterator_begin,
-                     WarpTileIterator &warp_tile_iterator) {
-      int dummy[] = {(pos == Seq) && (helper<Seq>(iterator_begin, warp_tile_iterator), 0)...};
-    }
-  };
+  /// Store a fragment to memory
+  CUTLASS_DEVICE
+  void store_with_pointer_offset(Fragment const &frag, Index pointer_offset) {
+    store_with_byte_offset(frag, pointer_offset * sizeof_bits<Element>::value / 8);
+  }
 
-  /// Streams the result to global memory
+  /// Store a fragment to memory
   CUTLASS_DEVICE
-  void compute_source_not_needed_(
-    OutputOp const &output_op,                        ///< Output operator
-    ReductionFragment &reduction_fragment,            ///< Fragment containing the accumulated partial reduction over columns
-    OutputTileIterator destination_iterator,          ///< Tile iterator for destination
-    AccumulatorTile const &accumulators,              ///< Complete warp-level accumulator tile 
-    TensorTileIterator tensor_iterator,               ///< Threadblock tile iterator for additioanl tensor operand
-    MatrixCoord const &problem_size,                  ///< Problem size needed to guard against out-of-bounds accesses
-    MatrixCoord const &threadblock_offset             ///< Threadblock's initial offset within the problem size space
-    ) { 
-
-    //
-    // Iterator over warp-level accumulator fragment
-    //
-
-    typename TensorTileIterator::Fragment tensor_fragment;
-    tensor_fragment.clear();
-
-    AccumulatorFragmentIterator accum_fragment_iterator(accumulators);
-
-    //
-    // Iterate over accumulator tile
-    // 
-
-    #pragma unroll(IterationsUnroll ? OutputTileIterator::kIterations : 1)
-    for (int iter = 0; iter < OutputTileIterator::kIterations; ++iter) {
-
-      //
-      // Convert and store fragment
-      //
-
-      tensor_iterator.load(tensor_fragment);
-      ++tensor_iterator;
-      
-      __syncthreads();
-
-      acc2smem<cutlass::make_index_sequence<OutputTileIterator::kIterations>>::push(
-          iter, accum_fragment_iterator, this->warp_tile_iterator_);
-
-      __syncthreads();
-
-      //
-      // Load fragments from shared memory
-      //
-
-      typename SharedLoadIterator::Fragment aligned_accum_fragment[kPartitionsK];
-
-      shared_load_iterator_.load(aligned_accum_fragment[0]);
-
-      //
-      // If the number of k-slices is > 1 - perform a reduction amongst the k-slices
-      //
-      if (kPartitionsK > 1)
-      {
-        plus <typename SharedLoadIterator::Fragment> add_fragments;
-        const int tile_row_offset = Base::SharedStorage::StorageShape::kRow / PartitionsK;
+  void store_with_byte_offset(Fragment const &frag, LongIndex byte_offset) {
+    address_iterator_.set_iteration_index(0);
+    AccessType const *frag_ptr = reinterpret_cast<AccessType const *>(&frag);
 
+    CUTLASS_PRAGMA_UNROLL
+    for (int s = 0; s < ThreadMap::Iterations::kStrided; ++s) {
+      CUTLASS_PRAGMA_UNROLL
+      for (int c = 0; c < ThreadMap::Iterations::kContiguous; ++c) {
         CUTLASS_PRAGMA_UNROLL
-        for ( int i = 1; i < kPartitionsK; ++i) {
-          shared_load_iterator_.add_tile_offset({tile_row_offset , 0});
-          shared_load_iterator_.load(aligned_accum_fragment[i]);
-          aligned_accum_fragment[0] = add_fragments(aligned_accum_fragment[0], aligned_accum_fragment[i]);
-        }
-
-        shared_load_iterator_.add_tile_offset({-1 * (kPartitionsK-1) * tile_row_offset, 0});
-      }
+        for (int v = 0; v < kAccessesPerVector; ++v) {
 
-      //
-      // Compute the output result
-      //
-     
-      FragmentCompute compute_fragment;
-
-      apply_output_operator_source_not_needed_(
-        reduction_fragment,
-        compute_fragment, 
-        output_op, 
-        aligned_accum_fragment[0],
-        tensor_fragment,
-        destination_iterator);
-
-      //
-      // Store the final result
-      //
-      
-      NumericArrayConverter<ElementOutput, ElementCompute, FragmentCompute::kElements> converter;
+          int idx = v + kAccessesPerVector * (c + s * ThreadMap::Iterations::kContiguous);
 
-      typename OutputTileIterator::Fragment output_fragment = converter(compute_fragment);
+          char *byte_ptr = reinterpret_cast<char *>(address_iterator_.get()) + byte_offset;
+          AccessType *access_ptr = reinterpret_cast<AccessType *>(byte_ptr);
 
-      destination_iterator.store(output_fragment);
-      ++destination_iterator;
+          if (address_iterator_.valid()) {
+            *access_ptr = frag_ptr[idx];
+          }
+          ++address_iterator_;
+        }
+      }
     }
   }
 
-  
-  /// Streams the result to global memory
+  /// Store a fragment to memory
   CUTLASS_DEVICE
-  void compute_source_needed_(
-    OutputOp const &output_op,                    ///< Output operator
-    ReductionFragment &reduction_fragment,        ///< Fragment containing the accumulated partial reduction over columns
-    OutputTileIterator destination_iterator,      ///< Tile iterator for destination
-    AccumulatorTile const &accumulators,          ///< Complete warp-level accumulator tile
-    OutputTileIterator source_iterator,           ///< Threadblock tile coordinate in GEMM (in units of threadblock tiles)
-    TensorTileIterator tensor_iterator,            ///< Threadblock tile iterator for additioanl tensor operand
-    MatrixCoord const &problem_size,                  ///< Problem size needed to guard against out-of-bounds accesses
-    MatrixCoord const &threadblock_offset             ///< Threadblock's initial offset within the problem size space
-    ) { 
-    
-    typename OutputTileIterator::Fragment source_fragment;
-    source_fragment.clear();
-
-    typename TensorTileIterator::Fragment tensor_fragment;
-    tensor_fragment.clear();
-
-    //
-    // Iterator over warp-level accumulator fragment
-    //
-
-    AccumulatorFragmentIterator accum_fragment_iterator(accumulators);
-
-    //
-    // Iterate over accumulator tile
-    // 
+  void store(Fragment const &frag) { store_with_byte_offset(frag, 0); }
+};
 
-    #pragma unroll(IterationsUnroll ? OutputTileIterator::kIterations : 1)
-    for (int iter = 0; iter < OutputTileIterator::kIterations; ++iter) {
+////////////////////////////////////////////////////////////////////////////////
 
-      //
-      // Load the source
-      //
+/// Specialization of PredicatedTileIteratorTriangularMatrix for column-major data.
+///
+/// Satisfies: ForwardTileIteratorConcept | 
+///            ReadableContiguousTileIteratorConcept | 
+///            WriteableContiguousTileIteratorConcept |
+///            MaskedTileIteratorConcept
+///
+template <
+  typename Shape_,
+  typename Element_,
+  int AdvanceRank,
+  typename ThreadMap_,
+  SideMode kSideMode, 
+  FillMode kFillMode, 
+  DiagType kDiagType,
+  int AccessSize
+>
+class PredicatedTileIteratorTriangularMatrix<Shape_, Element_, layout::ColumnMajor, AdvanceRank, ThreadMap_, 
+                                              kSideMode, kFillMode, kDiagType,
+                                              AccessSize> {
+public:
 
-      source_fragment.clear();
-      source_iterator.load(source_fragment);
-      ++source_iterator;
+  static_assert(AdvanceRank == 0 || AdvanceRank == 1, 
+    "Specialization for pitch-linear iterator may along advance along the "
+    "contiguous(rank=0) or strided(rank=1) dimension.");
 
-      tensor_iterator.load(tensor_fragment);
-      ++tensor_iterator;
+  using Shape = Shape_;
+  using Element = Element_;
+  using Layout = layout::ColumnMajor;
+  static int const kAdvanceRank = AdvanceRank;
+  using ThreadMap = ThreadMap_;
 
-      //
-      // Convert and store fragment
-      //
-      
-      __syncthreads();
+  using Index = typename Layout::Index;
+  using LongIndex = typename Layout::LongIndex;
 
-      acc2smem<cutlass::make_index_sequence<OutputTileIterator::kIterations>>::push(
-          iter, accum_fragment_iterator, this->warp_tile_iterator_);
+  using TensorRef = TensorRef<Element, Layout>;
+  using TensorView = TensorView<Element, Layout>;
+  using TensorCoord = typename Layout::TensorCoord;
+
+  using Pointer = Element *;
+  using NonConstPointer = typename platform::remove_const<Element>::type *;
+
+  using UnderlyingIterator = PredicatedTileIteratorTriangularMatrix<
+    layout::PitchLinearShape<Shape::kRow, Shape::kColumn>,
+    Element,
+    layout::PitchLinear,
+    (kAdvanceRank == 0 ? 0 : 1),
+    ThreadMap,
+    kSideMode, 
+    kFillMode, 
+    kDiagType,
+    AccessSize
+  >;
+
+  using AccessType = typename UnderlyingIterator::AccessType;
+
+  /// Fragment object to be loaded or stored
+  using Fragment = cutlass::Array<Element, ThreadMap::Iterations::kCount * ThreadMap::kElementsPerAccess>;
+
+  /// Predicate vector stores mask to guard accesses
+  using Mask = typename UnderlyingIterator::Mask;
+
+  /// Parameters object is precomputed state and is host-constructible
+  class Params {
+  private:
 
-      __syncthreads();
+    friend PredicatedTileIteratorTriangularMatrix;
 
-      //
-      // Load fragments from shared memory
-      //
+    /// Parameters object
+    typename UnderlyingIterator::Params params_;
 
-      typename SharedLoadIterator::Fragment aligned_accum_fragment[kPartitionsK];
+  public:
+    
+    CUTLASS_HOST_DEVICE
+    Params() { }
 
-      shared_load_iterator_.load(aligned_accum_fragment[0]);
+    /// Construct the Params object given a pitch-linear tensor's layout
+    CUTLASS_HOST_DEVICE
+    Params(Layout const &layout): params_(layout::PitchLinear(layout.stride(0))) {
 
-      // If the number of k-slices is > 1 - perform a reduction amongst the k-slices
-      if (kPartitionsK > 1)
-      {
-        plus <typename SharedLoadIterator::Fragment> add_fragments;
-        const int tile_row_offset = Base::SharedStorage::StorageShape::kRow / PartitionsK;
+    }
+  };
 
-        CUTLASS_PRAGMA_UNROLL
-        for ( int i = 1; i < kPartitionsK; ++i) {
-          shared_load_iterator_.add_tile_offset({tile_row_offset , 0});
-          shared_load_iterator_.load(aligned_accum_fragment[i]);
-          aligned_accum_fragment[0] = add_fragments(aligned_accum_fragment[0], aligned_accum_fragment[i]);
-        }
 
-        shared_load_iterator_.add_tile_offset({-1 * (kPartitionsK-1) * tile_row_offset, 0});
-      }
+private:
 
-      //
-      // Compute the output result
-      //
-     
-      FragmentCompute compute_fragment;
-
-      apply_output_operator_(
-        reduction_fragment, 
-        compute_fragment, 
-        output_op, 
-        aligned_accum_fragment[0], 
-        source_fragment,
-        tensor_fragment,
-        destination_iterator);
-
-      //
-      // Convert and store the final result
-      //
+  //
+  // Data members
+  //
 
-      NumericArrayConverter<ElementOutput, ElementCompute, FragmentCompute::kElements> converter;
+  /// Underlying pitch-linear tile iterator
+  UnderlyingIterator iterator_;
 
-      typename OutputTileIterator::Fragment output_fragment = converter(compute_fragment);
+public:
 
-      destination_iterator.store(output_fragment);      
-      ++destination_iterator;
-    }
+  /// Constructs a TileIterator from its precomputed state, threadblock offset, and thread ID
+  CUTLASS_HOST_DEVICE
+  PredicatedTileIteratorTriangularMatrix(
+    Params const &params,                         ///< Precomputed parameters object 
+    Pointer pointer,                              ///< Pointer to start of tensor
+    TensorCoord extent,                           ///< Extent of tensor
+    int thread_id,                                ///< ID of each participating thread
+    TensorCoord const &threadblock_offset         ///< Initial offset of threadblock
+  ):
+    iterator_(
+      params.params_,
+      pointer,
+      layout::PitchLinearCoord(extent.row(), extent.column()),
+      thread_id,
+      layout::PitchLinearCoord(threadblock_offset.row(), threadblock_offset.column())
+    ) { }
+
+  /// Construct a PredicatedTileIteratorTriangularMatrix with zero threadblock offset
+  CUTLASS_HOST_DEVICE
+  PredicatedTileIteratorTriangularMatrix(
+    Params const &params,                         ///< Precomputed parameters object
+    Pointer pointer,                              ///< Pointer to start of tensor
+    TensorCoord extent,                           ///< Extent of tensor
+    int thread_id                                 ///< ID of each participating thread
+  ): PredicatedTileIteratorTriangularMatrix(params, pointer, extent, thread_id, make_Coord(0, 0)) { }
+
+  /// Adds a pointer offset in units of Element
+  CUTLASS_HOST_DEVICE
+  void add_pointer_offset(LongIndex pointer_offset) {
+    iterator_.add_pointer_offset(pointer_offset);
   }
 
-  /// Helper to invoke the output functor over each vector of output
-  CUTLASS_DEVICE
-  void apply_output_operator_(
-    ReductionFragment &reduction_fragment,
-    FragmentCompute &compute_fragment,
-    OutputOp const &output_op,                    ///< Output operator
-    typename SharedLoadIterator::Fragment const &aligned_accum_fragment,
-    typename OutputTileIterator::Fragment const &source_fragment,
-    typename TensorTileIterator::Fragment const &tensor_fragment,
-    OutputTileIterator const & destination_iterator) {
-      
-    ComputeAccessType *compute_frag_ptr = 
-      reinterpret_cast<ComputeAccessType *>(&compute_fragment);
-
-    AccumulatorAccessType const *accum_frag_ptr = 
-      reinterpret_cast<AccumulatorAccessType const *>(&aligned_accum_fragment);
+  /// Advances to the next tile in memory.
+  ///
+  /// The first time this method is called, predicates are updated, and the iterator's
+  /// internal pointer is reverted to the first "steady state" tile. Subsequent calls
+  /// are lightweight and must only update the internal pointer.
+  CUTLASS_HOST_DEVICE
+  PredicatedTileIteratorTriangularMatrix &operator++() {
+    ++iterator_;
+    return *this;
+  }
 
-    OutputAccessType const *source_frag_ptr = 
-      reinterpret_cast<OutputAccessType const *>(&source_fragment);
+  /// Advances to the next tile in memory.
+  ///
+  /// The first time this method is called, predicates are updated, and the iterator's
+  /// internal pointer is reverted to the first "steady state" tile. Subsequent calls
+  /// are lightweight and must only update the internal pointer.
+  CUTLASS_HOST_DEVICE
+  PredicatedTileIteratorTriangularMatrix operator++(int) {
+    PredicatedTileIteratorTriangularMatrix self(*this);
+    operator++();
+    return self;
+  }
 
-    TensorAccessType const *tensor_frag_ptr =
-      reinterpret_cast<TensorAccessType const *>(&tensor_fragment);
+  /// Clears the predicate set efficiently
+  CUTLASS_HOST_DEVICE
+  void clear_mask(bool enable = true) {
+    iterator_.clear_mask(enable);
+  }
 
-    int const kOutputOpIterations = 
-      OutputTileIterator::Fragment::kElements / OutputTileIterator::kElementsPerAccess;
+  /// Clears the predicate set efficiently
+  CUTLASS_HOST_DEVICE
+  void enable_mask() {
+    iterator_.enable_mask();
+  }
 
-    CUTLASS_PRAGMA_UNROLL
-    for (int i = 0; i < kOutputOpIterations; ++i) {
+  /// Sets the predicate mask, overriding value stored in predicate iterator
+  CUTLASS_HOST_DEVICE
+  void set_mask(Mask const &mask) {
+    iterator_.set_mask(mask);
+  }
 
-      // Call the output operator
-      compute_frag_ptr[i] = output_op(accum_frag_ptr[i], source_frag_ptr[i], tensor_frag_ptr[i]);
-    }
+  /// Gets the mask
+  CUTLASS_HOST_DEVICE
+  void get_mask(Mask &mask) {
+    iterator_.get_mask(mask);
+  }
 
-    //
-    // Partial reduction over each column
-    //
+  /// Loads a fragment from memory
+  CUTLASS_DEVICE
+  void load_with_pointer_offset(Fragment &frag, Index pointer_offset) {
+    iterator_.load_with_pointer_offset(frag, pointer_offset);
+  }
 
-    ReductionOp reduction_op;
+  /// Loads a fragment from memory
+  CUTLASS_DEVICE
+  void load_with_byte_offset(Fragment &frag, LongIndex byte_offset) {
+    iterator_.load_with_byte_offset(frag, byte_offset);
+  }
 
-    typename OutputTileIterator::Mask mask;
-    destination_iterator.get_mask(mask);
+  /// Loads a fragment from memory
+  CUTLASS_DEVICE
+  void load(Fragment &frag) {
+    load_with_pointer_offset(frag, 0);
+  }
 
-    CUTLASS_PRAGMA_UNROLL
-    for (int column = 0; column < ReductionDetail::kColumnsPerThread; ++column) {
+  /// Store a fragment to memory
+  CUTLASS_DEVICE
+  void store_with_pointer_offset(Fragment const &frag, Index pointer_offset) {
+    iterator_.store_with_pointer_offset(frag, pointer_offset);
+  }
 
-      int column_vector_idx = column / ThreadMap::kElementsPerAccess;
-      bool column_guard = mask.predicates[column_vector_idx];
+  /// Store a fragment to memory
+  CUTLASS_DEVICE
+  void store_with_byte_offset(Fragment const &frag, LongIndex byte_offset) {
+    iterator_.store_with_byte_offset(frag, byte_offset);
+  }
 
-      CUTLASS_PRAGMA_UNROLL
-      for (int row = 0; row < ReductionDetail::kRowsPerThread; ++row) {
+  /// Store a fragment to memory
+  CUTLASS_DEVICE
+  void store(Fragment const &frag) {
+    store_with_pointer_offset(frag, 0);
+  }
+};
 
-        bool fetch;
-        if (ReductionDetail::kOobCheck) {
-          int row_idx = (row % ThreadMap::Iterations::kRow);
-          int residual = (row / ThreadMap::Iterations::kRow);
+////////////////////////////////////////////////////////////////////////////////
 
-          int group_idx = (residual % ThreadMap::Iterations::kGroup);
-          residual = (residual / ThreadMap::Iterations::kGroup);
+/// Specialization of PredicatedTileIteratorTriangularMatrix for row-major data.
+///
+/// Satisfies: ForwardTileIteratorConcept | 
+///            ReadableContiguousTileIteratorConcept | 
+///            WriteableContiguousTileIteratorConcept |
+///            MaskedTileIteratorConcept
+///
+template <
+  typename Shape_,
+  typename Element_,
+  int AdvanceRank,
+  typename ThreadMap_,
+  SideMode kSideMode, 
+  FillMode kFillMode, 
+  DiagType kDiagType,
+  int AccessSize
+>
+class PredicatedTileIteratorTriangularMatrix<Shape_, Element_, layout::RowMajor, AdvanceRank, ThreadMap_, 
+                                            kSideMode, kFillMode, kDiagType,
+                                            AccessSize> {
+public:
 
-          int cluster_idx = (residual % ThreadMap::Iterations::kCluster);
+  static_assert(AdvanceRank == 0 || AdvanceRank == 1, 
+    "Specialization for pitch-linear iterator may along advance along the "
+    "contiguous(rank=0) or strided(rank=1) dimension.");
 
-          int row_offset = row_idx * ThreadMap::Delta::kRow 
-            + group_idx * ThreadMap::Delta::kGroup 
-            + cluster_idx * ThreadMap::Delta::kCluster;
+  using Shape = Shape_;
+  using Element = Element_;
+  using Layout = layout::RowMajor;
+  static int const kAdvanceRank = AdvanceRank;
+  using ThreadMap = ThreadMap_;
 
-          int output_row = destination_iterator.thread_start_row() + row_offset;
+  using Index = typename Layout::Index;
+  using LongIndex = typename Layout::LongIndex;
 
-          fetch = (output_row < destination_iterator.extent_row() && column_guard);
-        }
-        else {
-          fetch = true;
-        }
+  using TensorRef = TensorRef<Element, Layout>;
+  using TensorView = TensorView<Element, Layout>;
+  using TensorCoord = typename Layout::TensorCoord;
+
+  using Pointer = Element *;
+  using NonConstPointer = typename platform::remove_const<Element>::type *;
+
+  using UnderlyingIterator = PredicatedTileIteratorTriangularMatrix<
+    layout::PitchLinearShape<Shape::kColumn, Shape::kRow>,
+    Element,
+    layout::PitchLinear,
+    (kAdvanceRank == 0 ? 1 : 0),
+    ThreadMap,
+    kSideMode, 
+    kFillMode, 
+    kDiagType,
+    AccessSize
+  >;
+
+  using AccessType = typename UnderlyingIterator::AccessType;
+
+  /// Fragment object to be loaded or stored
+  using Fragment = cutlass::Array<Element, ThreadMap::Iterations::kCount * ThreadMap::kElementsPerAccess>;
+
+  /// Predicate vector stores mask to guard accesses
+  using Mask = typename UnderlyingIterator::Mask;
+
+  /// Parameters object is precomputed state and is host-constructible
+  class Params {
+  private:
 
-        ElementCompute value = ElementCompute();
-        if (fetch) {
-          value = compute_fragment[row * ReductionDetail::kColumnsPerThread + column];
-        }
+    friend PredicatedTileIteratorTriangularMatrix;
 
-        reduction_fragment[column] = reduction_op(
-          reduction_fragment[column], 
-          value);
-      }
-    }
-  }
+    /// Parameters object
+    typename UnderlyingIterator::Params params_;
 
-  /// Helper to invoke the output functor over each vector of output
-  CUTLASS_DEVICE
-  void apply_output_operator_source_not_needed_(
-    ReductionFragment &reduction_fragment,
-    FragmentCompute &compute_fragment,
-    OutputOp const &output_op,                    ///< Output operator
-    typename SharedLoadIterator::Fragment const &aligned_accum_fragment,
-    typename TensorTileIterator::Fragment const &tensor_fragment,
-    OutputTileIterator const & destination_iterator
-  ) {
+  public:
     
-    ComputeAccessType *compute_frag_ptr = 
-      reinterpret_cast<ComputeAccessType *>(&compute_fragment);
+    CUTLASS_HOST_DEVICE
+    Params() { } 
 
-    AccumulatorAccessType const *accum_frag_ptr = 
-      reinterpret_cast<AccumulatorAccessType const *>(&aligned_accum_fragment);
+    /// Construct the Params object given a pitch-linear tensor's layout
+    CUTLASS_HOST_DEVICE
+    Params(Layout const &layout): params_(layout::PitchLinear(layout.stride(0))) {
 
-    TensorAccessType const *tensor_frag_ptr =
-      reinterpret_cast<TensorAccessType const *>(&tensor_fragment);
+    };
+  };
 
-    int const kOutputOpIterations = 
-      OutputTileIterator::Fragment::kElements / OutputTileIterator::kElementsPerAccess;
 
-    CUTLASS_PRAGMA_UNROLL
-    for (int i = 0; i < kOutputOpIterations; ++i) {
+private:
 
-      // Call the output operator
-      compute_frag_ptr[i] = output_op(accum_frag_ptr[i], tensor_frag_ptr[i]);
-    }
+  //
+  // Data members
+  //
 
-    //
-    // Partial reduction over each column
-    //
+  /// Underlying pitch-linear tile iterator
+  UnderlyingIterator iterator_;
 
-    ReductionOp reduction_op;
+public:
 
-    typename OutputTileIterator::Mask mask;
-    destination_iterator.get_mask(mask);
+  /// Constructs a TileIterator from its precomputed state, threadblock offset, and thread ID
+  CUTLASS_HOST_DEVICE
+  PredicatedTileIteratorTriangularMatrix(
+    Params const &params,                         ///< Precomputed parameters object 
+    Pointer pointer,                              ///< Pointer to start of tensor
+    TensorCoord extent,                           ///< Extent of tensor
+    int thread_id,                                ///< ID of each participating thread
+    TensorCoord const &threadblock_offset         ///< Initial offset of threadblock
+  ):
+    iterator_(
+      params.params_,
+      pointer,
+      layout::PitchLinearCoord(extent.column(), extent.row()),
+      thread_id,
+      layout::PitchLinearCoord(threadblock_offset.column(), threadblock_offset.row())
+    ) { }
+
+  /// Construct a PredicatedTileIteratorTriangularMatrix with zero threadblock offset
+  CUTLASS_HOST_DEVICE
+  PredicatedTileIteratorTriangularMatrix(
+    Params const &params,                         ///< Precomputed parameters object
+    Pointer pointer,                              ///< Pointer to start of tensor
+    TensorCoord extent,                           ///< Extent of tensor
+    int thread_id                                 ///< ID of each participating thread
+  ): PredicatedTileIteratorTriangularMatrix(params, pointer, extent, thread_id, make_Coord(0, 0)) { }
+
+  /// Adds a pointer offset in units of Element
+  CUTLASS_HOST_DEVICE
+  void add_pointer_offset(LongIndex pointer_offset) {
+    iterator_.add_pointer_offset(pointer_offset);
+  }
 
-    CUTLASS_PRAGMA_UNROLL
-    for (int column = 0; column < ReductionDetail::kColumnsPerThread; ++column) {
+  /// Advances to the next tile in memory.
+  ///
+  /// The first time this method is called, predicates are updated, and the iterator's
+  /// internal pointer is reverted to the first "steady state" tile. Subsequent calls
+  /// are lightweight and must only update the internal pointer.
+  CUTLASS_HOST_DEVICE
+  PredicatedTileIteratorTriangularMatrix &operator++() {
+    ++iterator_;
+    return *this;
+  }
 
-      int column_vector_idx = column / ThreadMap::kElementsPerAccess;
-      bool column_guard = mask.predicates[column_vector_idx];
+  /// Advances to the next tile in memory.
+  ///
+  /// The first time this method is called, predicates are updated, and the iterator's
+  /// internal pointer is reverted to the first "steady state" tile. Subsequent calls
+  /// are lightweight and must only update the internal pointer.
+  CUTLASS_HOST_DEVICE
+  PredicatedTileIteratorTriangularMatrix operator++(int) {
+    PredicatedTileIteratorTriangularMatrix self(*this);
+    operator++();
+    return self;
+  }
 
-      CUTLASS_PRAGMA_UNROLL
-      for (int row = 0; row < ReductionDetail::kRowsPerThread; ++row) {
+  /// Clears the predicate set efficiently
+  CUTLASS_HOST_DEVICE
+  void clear_mask(bool enable = true) {
+    iterator_.clear_mask(enable);
+  }
 
-        bool fetch;
-        if (ReductionDetail::kOobCheck) {
-          int row_idx = (row % ThreadMap::Iterations::kRow);
-          int residual = (row / ThreadMap::Iterations::kRow);
+  /// Clears the predicate set efficiently
+  CUTLASS_HOST_DEVICE
+  void enable_mask() {
+    iterator_.enable_mask();
+  }
 
-          int group_idx = (residual % ThreadMap::Iterations::kGroup);
-          residual = (residual / ThreadMap::Iterations::kGroup);
+  /// Sets the predicate mask, overriding value stored in predicate iterator
+  CUTLASS_HOST_DEVICE
+  void set_mask(Mask const &mask) {
+    iterator_.set_mask(mask);
+  }
 
-          int cluster_idx = (residual % ThreadMap::Iterations::kCluster);
+  /// Gets the mask
+  CUTLASS_HOST_DEVICE
+  void get_mask(Mask &mask) {
+    iterator_.get_mask(mask);
+  }
 
-          int row_offset = row_idx * ThreadMap::Delta::kRow 
-            + group_idx * ThreadMap::Delta::kGroup 
-            + cluster_idx * ThreadMap::Delta::kCluster;
+  /// Loads a fragment from memory
+  CUTLASS_DEVICE
+  void load_with_pointer_offset(Fragment &frag, Index pointer_offset) {
+    iterator_.load_with_pointer_offset(frag, pointer_offset);
+  }
 
-          int output_row = destination_iterator.thread_start_row() + row_offset;
+  /// Loads a fragment from memory
+  CUTLASS_DEVICE
+  void load_with_byte_offset(Fragment &frag, LongIndex byte_offset) {
+    iterator_.load_with_byte_offset(frag, byte_offset);
+  }
 
-          fetch = (output_row < destination_iterator.extent_row() && column_guard);
-        }
-        else {
-          fetch = true;
-        }
+  /// Loads a fragment from memory
+  CUTLASS_DEVICE
+  void load(Fragment &frag) {
+    load_with_pointer_offset(frag, 0);
+  }
 
-        ElementCompute value = ElementCompute();
-        if (fetch) {
-          value = compute_fragment[row * ReductionDetail::kColumnsPerThread + column];
-        }
+  /// Store a fragment to memory
+  CUTLASS_DEVICE
+  void store_with_pointer_offset(Fragment const &frag, Index pointer_offset) {
+    iterator_.store_with_pointer_offset(frag, pointer_offset);
+  }
+  
+  /// Store a fragment to memory
+  CUTLASS_DEVICE
+  void store_with_byte_offset(Fragment const &frag, LongIndex byte_offset) {
+    iterator_.store_with_byte_offset(frag, byte_offset);
+  }
 
-        reduction_fragment[column] = reduction_op(
-          reduction_fragment[column], 
-          value);
-      }
-    }
+  /// Store a fragment to memory
+  CUTLASS_DEVICE
+  void store(Fragment const &frag) {
+    store_with_pointer_offset(frag, 0);
   }
 };
 
 ////////////////////////////////////////////////////////////////////////////////
 
 } // namespace threadblock
-} // namespace epilogue
+} // namespace transform
 } // namespace cutlass
 
 ////////////////////////////////////////////////////////////////////////////////
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/epilogue_with_visitor.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/epilogue_with_visitor.h`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/epilogue_workspace.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/epilogue_workspace.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/interleaved_epilogue.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/interleaved_epilogue.h`

 * *Files 14% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -47,15 +47,15 @@
 #include "cutlass/aligned_buffer.h"
 
 #include "cutlass/gemm/gemm.h"
 
 #include "cutlass/transform/pitch_linear_thread_map.h"
 #include "cutlass/transform/threadblock/regular_tile_iterator.h"
 
-#include "cutlass/epilogue/threadblock/epilogue_base.h"
+#include "cutlass/epilogue/threadblock/epilogue_base_streamk.h"
 #include "cutlass/epilogue/threadblock/predicated_tile_iterator.h"
 
 ////////////////////////////////////////////////////////////////////////////////
 
 namespace cutlass {
 namespace epilogue {
 namespace threadblock {
@@ -74,26 +74,42 @@
     typename OutputTileIterator_,
     /// Fragment iterator selecting accumulators
     typename AccumulatorFragmentIterator_,
     /// Output operator
     typename OutputOp_,
     /// Number of interleaved k
     int InterleavedK>
-class InterleavedEpilogue {
- public:
+class InterleavedEpilogue :
+  public EpilogueBaseStreamK<
+    Shape_,
+    PartitionsK,
+    WarpMmaOperator_,
+    AccumulatorFragmentIterator_>
+{
+public:
+
+  using BaseStreamK = EpilogueBaseStreamK<
+    Shape_,
+    PartitionsK,
+    WarpMmaOperator_,
+    AccumulatorFragmentIterator_>;
+
   using Shape = Shape_;
   using WarpMmaOperator = WarpMmaOperator_;
   static int const kPartitionsK = PartitionsK;
   using AccumulatorFragmentIterator = AccumulatorFragmentIterator_;
   using OutputTileIterator = OutputTileIterator_;
   using OutputOp = OutputOp_;
 
   /// The complete warp-level accumulator tile
   using AccumulatorTile = typename AccumulatorFragmentIterator::AccumulatorTile;
 
+  /// Fragment type used by the accumulator tile's fragment iterator
+  using AccumulatorFragment = typename AccumulatorFragmentIterator::Fragment;
+
   /// Accumulator element
   using ElementAccumulator = typename AccumulatorTile::Element;
 
   /// Output element
   using ElementOutput = typename OutputTileIterator::Element;
 
   /// Output access size
@@ -118,134 +134,246 @@
       Array<ElementAccumulator, OutputTileIterator::kElementsPerAccess>;
 
   /// Number of warps
   using WarpCount =
       gemm::GemmShape<Shape::kM / WarpMmaOperator::Shape::kM,
                       Shape::kN / WarpMmaOperator::Shape::kN, kPartitionsK>;
 
- public:
+public:
+
   static_assert(OutputTileIterator::kElementsPerAccess,
                 "This must not be zero.");
 
   static_assert(!(OutputTileIterator::Fragment::kElements %
                   OutputTileIterator::kElementsPerAccess),
                 "Divisibility");
 
+public:
+
+  /// Aspect for when epilogue source is not needed
+  struct SourceAspectNotNeeded
+  {
+    /// Constructor
+    CUTLASS_DEVICE
+    SourceAspectNotNeeded()
+    {}
+
+    /// Invoke the output functor over each vector of output
+    CUTLASS_DEVICE
+    void apply_output_operator(
+      typename OutputTileIterator::Fragment &output_fragment,
+      OutputOp const &output_op,
+      typename AccumulatorFragmentIterator::Fragment const &aligned_accum_fragment)
+    {
+      OutputAccessType *output_frag_ptr =
+        reinterpret_cast<OutputAccessType *>(&output_fragment);
+
+      AccumulatorAccessType const *compute_frag_ptr =
+        reinterpret_cast<AccumulatorAccessType const *>(&aligned_accum_fragment);
+
+      int const kOutputOpIterations =
+        OutputTileIterator::Fragment::kElements / OutputTileIterator::kElementsPerAccess;
+
+      CUTLASS_PRAGMA_UNROLL
+      for (int i = 0; i < kOutputOpIterations; ++i)
+      {
+        // Call the output operator
+        output_frag_ptr[i] = output_op(compute_frag_ptr[i]);
+      }
+    }
+  };
+
+
+  /// Aspect for when epilogue source is needed
+  struct SourceAspectNeeded
+  {
+    OutputTileIterator source_iterator;
+
+    typename OutputTileIterator::Fragment source_fragment;
+
+    /// Invoke the output functor over each vector of output
+    CUTLASS_DEVICE
+    static void apply_output_operator(
+      typename OutputTileIterator::Fragment &output_fragment,
+      OutputOp const &output_op,
+      typename AccumulatorFragmentIterator::Fragment const &aligned_accum_fragment,
+      typename OutputTileIterator::Fragment const &source_fragment)
+    {
+      OutputAccessType *output_frag_ptr =
+        reinterpret_cast<OutputAccessType *>(&output_fragment);
+
+      AccumulatorAccessType const *compute_frag_ptr =
+        reinterpret_cast<AccumulatorAccessType const *>(&aligned_accum_fragment);
+
+      OutputAccessType const *source_frag_ptr =
+        reinterpret_cast<OutputAccessType const *>(&source_fragment);
+
+      int const kOutputOpIterations =
+        OutputTileIterator::Fragment::kElements / OutputTileIterator::kElementsPerAccess;
+
+      CUTLASS_PRAGMA_UNROLL
+      for (int i = 0; i < kOutputOpIterations; ++i)
+      {
+        // Call the output operator
+        output_frag_ptr[i] = output_op(compute_frag_ptr[i], source_frag_ptr[i]);
+      }
+    }
+
+    /// Constructor
+    CUTLASS_DEVICE
+    SourceAspectNeeded(OutputTileIterator source_iterator) :
+      source_iterator(source_iterator)
+    {
+      source_fragment.clear();
+    }
+
+    /// Invoke the output functor over each vector of output
+    CUTLASS_DEVICE
+    void apply_output_operator(
+      typename OutputTileIterator::Fragment &output_fragment,
+      OutputOp const &output_op,
+      typename AccumulatorFragmentIterator::Fragment const &aligned_accum_fragment)
+    {
+      // Load addend source fragment from global memory
+      source_iterator.load(source_fragment);
+      ++source_iterator;
+
+      apply_output_operator(output_fragment, output_op, aligned_accum_fragment, source_fragment);
+    }
+  };
+
+
   /// Shared storage allocation needed by the epilogue
   struct SharedStorage {};
 
 
- public:
+public:
+
   /// Constructor
   CUTLASS_DEVICE
   InterleavedEpilogue(
       SharedStorage &shared_storage,  ///< Shared storage object
       int thread_idx,                 ///< ID of a thread within the threadblock
       int warp_idx,                   ///< ID of warp within threadblock
-      int lane_idx                    ///< Id of thread within warp
-    ) {}
+      int lane_idx)                   ///< Id of thread within warp
+  :
+      BaseStreamK(thread_idx)
+  {}
 
-  /// Streams the result to global memory
-  CUTLASS_DEVICE
-  void operator()(
-    OutputOp const &output_op,                    ///< Output operator
-    OutputTileIterator destination_iterator,      ///< Tile iterator for destination
-    AccumulatorTile const &accumulators,          ///< Complete warp-level accumulator tile
-    OutputTileIterator source_iterator) {         ///< Threadblock tile coordinate in GEMM (in units of threadblock tiles)
-    if (!output_op.is_source_needed()) {
-      compute_source_not_needed_(output_op, destination_iterator, accumulators);  
-    }
-    else {
-      compute_source_needed_(output_op, destination_iterator, accumulators, source_iterator);
-    }
-  }
-   
-  /// Streams the result to global memory
-  CUTLASS_DEVICE
-  void compute_source_not_needed_(
-    OutputOp const &output_op,                    ///< Output operator
-    OutputTileIterator destination_iterator,      ///< Tile iterator for destination
-    AccumulatorTile const &accumulators           ///< Complete warp-level accumulator tile
-    ) { 
 
-    //
-    // Iterator over warp-level accumulator fragment
-    //
+  /// Aggregates the accumulator sets shared by peer blocks in the global workspace,
+  /// performing epilogue computations, writing to output
+  CUTLASS_DEVICE
+  void reduce(
+      int peer_idx_begin,
+      int peer_idx_end,
+      int reduce_fragment_idx,
+      void *element_workspace,
+      OutputOp const &output_op,                      ///< Output operator
+      OutputTileIterator destination_iterator,        ///< Tile iterator for destination
+      OutputTileIterator source_iterator)             ///< Threadblock tile coordinate in GEMM (in units of threadblock tiles)
+  {
+    // Redcuce peer accumulator fragments into one fragment
+    AccumulatorFragment accum_fragment;
+    BaseStreamK::reduce(accum_fragment, peer_idx_begin, peer_idx_end, reduce_fragment_idx, element_workspace);
 
-    AccumulatorFragmentIterator accum_fragment_iterator(accumulators);
+    // Source-fragment data (zero-initialized for scenarios where the
+    // output operator allows us to skip loading it from global input)
+    typename OutputTileIterator::Fragment source_fragment;
+    source_fragment.clear();
 
-    //
-    // Iterate over accumulator tile
-    //
+    if (output_op.is_source_needed())
+    {
+      source_iterator += reduce_fragment_idx;
+      source_iterator.load(source_fragment);
+    }
 
-    CUTLASS_PRAGMA_UNROLL
-    for (int iter = 0; iter < OutputTileIterator::kIterations; ++iter) {
+    // Compute the output result
+    typename OutputTileIterator::Fragment output_fragment;
 
-      //
-      // Convert fragment
-      //
+    // Apply the output operator
+    SourceAspectNeeded::apply_output_operator(output_fragment, output_op, accum_fragment, source_fragment);
 
-      typename AccumulatorFragmentIterator::Fragment accum_fragment;
-
-      accum_fragment_iterator.load(accum_fragment);
-      ++accum_fragment_iterator;
+    // Store the final result
+    destination_iterator += reduce_fragment_idx;
+    destination_iterator.store(output_fragment);
+  }
 
-      //
-      // Compute the output result
-      //
 
-      typename OutputTileIterator::Fragment output_fragment;
-      apply_output_operator_source_not_needed_(output_op, output_fragment, accum_fragment);
+  /// Perform the epilogue computations and stream the result to global memory.
+  CUTLASS_DEVICE
+  void operator()(
+    OutputOp const &output_op,                      ///< Output operator
+    OutputTileIterator destination_iterator,        ///< Tile iterator for destination
+    AccumulatorTile const &accumulators)            ///< Complete warp-level accumulator tile
+  {
+    operator()(output_op, destination_iterator, accumulators, SourceAspectNotNeeded());
+  }
 
-      //
-      // Store the final result
-      //
 
-      destination_iterator.set_iteration_index(iter);
-      destination_iterator.store(output_fragment);
-      ++destination_iterator;
+  /// Perform the epilogue computations and stream the result to global memory.  Implements
+  /// two alternative codepaths, depending on whether the output op requires addend data to be loaded.
+  CUTLASS_DEVICE
+  void operator()(
+    OutputOp const &output_op,                      ///< Output operator
+    OutputTileIterator destination_iterator,        ///< Tile iterator for destination
+    AccumulatorTile const &accumulators,            ///< Complete warp-level accumulator tile
+    OutputTileIterator source_iterator )            ///< Tile iterator for addend source
+  {
+    if (output_op.is_source_needed())
+    {
+      operator()(output_op, destination_iterator, accumulators, SourceAspectNeeded(source_iterator));
     }
-  } 
+    else
+    {
+      operator()(output_op, destination_iterator, accumulators, SourceAspectNotNeeded());
+    }
+  }
 
-  /// Streams the result to global memory
+
+  /// Perform the epilogue computations and stream the result to global memory.  Implements a
+  /// single codepath, regardless of whether the output op requires addend data to be loaded
   CUTLASS_DEVICE
-  void compute_source_needed_(
-    OutputOp const &output_op,                    ///< Output operator
-    OutputTileIterator destination_iterator,      ///< Tile iterator for destination
-    AccumulatorTile const &accumulators,          ///< Complete warp-level accumulator tile
-    OutputTileIterator source_iterator           ///< Threadblock tile coordinate in GEMM (in units of threadblock tiles)
-    ) { 
- 
-    //
-    // Predicated tile iterators constructed from members
-    //
+  void unified(
+    OutputOp const &output_op,                      ///< Output operator
+    OutputTileIterator destination_iterator,        ///< Tile iterator for destination
+    AccumulatorTile const &accumulators,            ///< Complete warp-level accumulator tile
+    OutputTileIterator source_iterator )            ///< Tile iterator for addend source
+  {
+    if (!output_op.is_source_needed())
+    {
+      source_iterator.clear_mask();
+      __syncthreads();  // Dummy (CUDA 11.0)
+    }
 
-    typename OutputTileIterator::Fragment source_fragment;
+    operator()(output_op, destination_iterator, accumulators, SourceAspectNeeded(source_iterator));
+  }
 
-    source_fragment.clear();
 
+  /// Streams the result to global memory
+  template <typename SourceAspect>
+  CUTLASS_DEVICE
+  void operator()(
+    OutputOp const &output_op,                      ///< Output operator
+    OutputTileIterator destination_iterator,        ///< Tile iterator for destination
+    AccumulatorTile const &accumulators,            ///< Complete warp-level accumulator tile
+    SourceAspect source)
+  {
     //
     // Iterator over warp-level accumulator fragment
     //
 
     AccumulatorFragmentIterator accum_fragment_iterator(accumulators);
 
     //
     // Iterate over accumulator tile
     //
 
     CUTLASS_PRAGMA_UNROLL
     for (int iter = 0; iter < OutputTileIterator::kIterations; ++iter) {
-      //
-      // Load the source
-      //
-
-      source_iterator.set_iteration_index(iter);
-      source_iterator.load(source_fragment);
-      ++source_iterator;
 
       //
       // Convert fragment
       //
 
       typename AccumulatorFragmentIterator::Fragment accum_fragment;
 
@@ -253,78 +381,25 @@
       ++accum_fragment_iterator;
 
       //
       // Compute the output result
       //
 
       typename OutputTileIterator::Fragment output_fragment;
-      apply_output_operator_source_needed_(output_op, output_fragment, accum_fragment, source_fragment);
+      source.apply_output_operator(output_fragment, output_op, accum_fragment);
 
       //
       // Store the final result
       //
 
       destination_iterator.set_iteration_index(iter);
       destination_iterator.store(output_fragment);
       ++destination_iterator;
     }
   }
-
- private:
-  /// Helper to invoke the output functor over each vector of output
-  CUTLASS_DEVICE
-  void apply_output_operator_source_needed_(
-    OutputOp const &output_op,                    ///< Output operator
-      typename OutputTileIterator::Fragment &output_fragment,
-      typename AccumulatorFragmentIterator::Fragment const
-          &aligned_accum_fragment,
-      typename OutputTileIterator::Fragment const &source_fragment) {
-    OutputAccessType *output_frag_ptr =
-        reinterpret_cast<OutputAccessType *>(&output_fragment);
-
-    AccumulatorAccessType const *compute_frag_ptr =
-        reinterpret_cast<AccumulatorAccessType const *>(
-            &aligned_accum_fragment);
-
-    OutputAccessType const *source_frag_ptr =
-        reinterpret_cast<OutputAccessType const *>(&source_fragment);
-
-    int const kOutputOpIterations = OutputTileIterator::Fragment::kElements /
-                                    OutputTileIterator::kElementsPerAccess;
-
-    CUTLASS_PRAGMA_UNROLL
-    for (int i = 0; i < kOutputOpIterations; ++i) {
-      // Call the output operator
-      output_frag_ptr[i] = output_op(compute_frag_ptr[i], source_frag_ptr[i]);
-    }
-  }
-
-  /// Helper to invoke the output functor over each vector of output
-  CUTLASS_DEVICE
-  void apply_output_operator_source_not_needed_(
-    OutputOp const &output_op,                    ///< Output operator
-      typename OutputTileIterator::Fragment &output_fragment,
-      typename AccumulatorFragmentIterator::Fragment const
-          &aligned_accum_fragment) {
-    OutputAccessType *output_frag_ptr =
-        reinterpret_cast<OutputAccessType *>(&output_fragment);
-
-    AccumulatorAccessType const *compute_frag_ptr =
-        reinterpret_cast<AccumulatorAccessType const *>(
-            &aligned_accum_fragment);
-
-    int const kOutputOpIterations = OutputTileIterator::Fragment::kElements /
-                                    OutputTileIterator::kElementsPerAccess;
-
-    CUTLASS_PRAGMA_UNROLL
-    for (int i = 0; i < kOutputOpIterations; ++i) {
-      // Call the output operator
-      output_frag_ptr[i] = output_op(compute_frag_ptr[i]);
-    }
-  }
 };
 
 ////////////////////////////////////////////////////////////////////////////////
 
 } // namespace threadblock
 } // namespace epilogue
 } // namespace cutlass
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/output_iterator_parameter.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/output_iterator_parameter.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/output_tile_thread_map.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/output_tile_thread_map.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/predicated_tile_iterator.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/predicated_tile_iterator.h`

 * *Files 6% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -676,21 +676,78 @@
         thread_start_row_ += ThreadMap::Count::kGroup * 
           ThreadMap::Shape::kGroup * ThreadMap::Count::kRow * ThreadMap::Shape::kRow;
 
         if (state_[2] == ThreadMap::Count::kCluster) {
           state_[2] = 0;
           byte_pointer_ += params_.advance_tile;
           store_byte_pointer_ += params_.advance_tile;
+
+          thread_start_row_ += ThreadMap::Shape::kGroup * ThreadMap::Shape::kRow
+            * ThreadMap::Shape::kCluster * ThreadMap::Shape::kTile;
         }
       }
     }
 
     return *this;
   }
 
+  /// Advances a number of positions to load or store
+  CUTLASS_HOST_DEVICE
+  PredicatedTileIterator &operator+=(int increment)
+  {
+    // Row
+    state_[0] += increment;
+    int increment_row = state_[0] / ThreadMap::Count::kRow;
+    state_[0] = state_[0] % ThreadMap::Count::kRow;
+
+    byte_pointer_ += (params_.advance_row * increment);
+    store_byte_pointer_ += (params_.advance_row * increment);
+    thread_start_row_ += (ThreadMap::Shape::kRow * increment);
+
+    // Group
+    state_[1] += increment_row;
+    int increment_group = state_[1] / ThreadMap::Count::kGroup;
+    state_[1] = state_[1] % ThreadMap::Count::kGroup;
+
+    byte_pointer_ += (params_.advance_group * increment_row);
+    store_byte_pointer_ += (params_.advance_group * increment_row);
+    thread_start_row_ +=
+        (ThreadMap::Shape::kGroup - 1) *
+        ThreadMap::Shape::kRow *
+        ThreadMap::Count::kRow *
+        increment_row;
+
+
+    // Cluster
+    state_[2] += increment_group;
+    int increment_cluster = state_[2] / ThreadMap::Count::kCluster;
+    state_[2] = state_[2] % ThreadMap::Count::kCluster;
+
+    byte_pointer_ += (params_.advance_cluster * increment_group);
+    store_byte_pointer_ += (params_.advance_cluster * increment_group);
+    thread_start_row_ +=
+        ThreadMap::Count::kGroup *
+        ThreadMap::Shape::kGroup *
+        ThreadMap::Count::kRow *
+        ThreadMap::Shape::kRow *
+        increment_group;
+
+    // Tile
+    byte_pointer_ += (params_.advance_tile * increment_cluster);
+    store_byte_pointer_ += (params_.advance_tile * increment_cluster);
+    thread_start_row_ +=
+        ThreadMap::Shape::kGroup *
+        ThreadMap::Shape::kRow *
+        ThreadMap::Shape::kCluster *
+        ThreadMap::Shape::kTile *
+        increment_cluster;
+
+    return *this;
+  }
+
   ///< Efficiently disables all accesses guarded by mask
   CUTLASS_DEVICE void clear_mask() {
     mask_.clear();
   }
 
   ///< Efficiently enables all accesses guarded by mask
   CUTLASS_DEVICE void enable_mask() {
@@ -940,14 +997,31 @@
         iteration_strided_ = 0;
       }
     }
 
     return *this;
   }
 
+  /// Advances a number of positions to load or store
+  CUTLASS_HOST_DEVICE
+  InterleavedPredicatedTileIterator &operator+=(int increment)
+  {
+    // Contiguous
+    iteration_contiguous_ += increment;
+    int increment_strided = iteration_contiguous_ / ThreadMap::Iterations::kContiguous;
+    iteration_contiguous_ = iteration_contiguous_ % ThreadMap::Iterations::kContiguous;
+    byte_pointer_ += (params_.advance_row * increment);
+
+    // Strided
+    iteration_strided_ += increment_strided;
+    byte_pointer_ += (params_.advance_column * increment_strided);
+
+    return *this;
+  }
+
   ///< Efficiently disables all accesses guarded by mask
   CUTLASS_DEVICE void clear_mask() {
     mask_.clear();
   }
 
   ///< Efficiently enables all accesses guarded by mask
   CUTLASS_DEVICE void enable_mask() {
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/predicated_tile_iterator_affine.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/predicated_tile_iterator_affine.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/predicated_tile_iterator_affine_layout_params.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/predicated_tile_iterator_affine_layout_params.h`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/predicated_tile_iterator_blas3.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/predicated_tile_iterator_blas3.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/predicated_tile_iterator_direct_conv.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/predicated_tile_iterator_direct_conv.h`

 * *Files 0% similar despite different names*

```diff
@@ -194,15 +194,15 @@
 
   /// A thread's starting row position (assuming steady-state predicates have been computed)
   Index thread_start_row_;
 
   /// A thread's starting column
   Index thread_start_column_;
 
-  /// Initial thread output location
+  /// Initial thread ouput location
   int thread_start_n_, thread_start_p_, thread_start_q_;
 
   /// Current threadblock tile index
   int tile_index_;
 
   //
   // Static asserts about internal strides
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/predicated_tile_iterator_params.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/predicated_tile_iterator_params.h`

 * *Files 12% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -31,17 +31,20 @@
 /*! \file
   \brief 
 */
 
 #pragma once
 
 #include "cutlass/cutlass.h"
+
 #include "cutlass/layout/pitch_linear.h"
 #include "cutlass/layout/matrix.h"
 
+#include "cutlass/conv/conv2d_problem_size.h"
+
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
 namespace cutlass {
 namespace epilogue {
 namespace threadblock {
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
@@ -241,14 +244,95 @@
   CUTLASS_HOST_DEVICE
   PredicatedTileIteratorParams(LongIndex stride, OutputTileThreadMapDesc thread_map) {
     initialize(stride, thread_map);
   }
 };
 
 
+
+///////////////////////////////////////////////////////////////////////////////
+
+//
+// Parameters struct for PredicatedTileIteratorDirect2dConv
+//
+
+struct PredicatedTileIteratorDirect2dConvParams{
+  using Index = int32_t;
+  using LongIndex = int64_t;
+
+  //
+  // Data members
+  //
+  FastDivmod pq_divmod;
+  FastDivmod q_divmod;
+
+  LongIndex stride;
+  LongIndex stride_n;
+  LongIndex stride_p;
+
+  int N;
+  int P;
+  int Q;
+
+  //
+  // Methods
+  //
+
+  CUTLASS_HOST_DEVICE
+  Status initialize(LongIndex stride_,
+                    cutlass::conv::Conv2dProblemSize const &problem_size,
+                    MatrixCoord threadblock_output_shape) {
+    stride = stride_; // The stride per row of output tensor (bytes)
+    stride_n = problem_size.P * problem_size.Q;
+    stride_p = problem_size.Q ;
+
+    N = problem_size.N;
+    P = problem_size.P;
+    Q = problem_size.Q;
+
+    // Fastdivmod for output O, P, Q
+    if(threadblock_output_shape.row() != 0 && threadblock_output_shape.column() !=0 ){
+      int tiles_p =
+          (problem_size.P + (threadblock_output_shape.row() - 1)) / (threadblock_output_shape.row());
+      int tiles_q = (problem_size.Q + (threadblock_output_shape.column() - 1)) /
+                    (threadblock_output_shape.column());
+
+      pq_divmod = FastDivmod(tiles_p * tiles_q);
+      q_divmod = FastDivmod(tiles_q);
+    }
+
+    return Status::kSuccess;
+  }
+
+  CUTLASS_HOST_DEVICE
+  Status initialize(
+      Index stride_,
+      cutlass::conv::Conv2dProblemSize const &problem_size = cutlass::conv::Conv2dProblemSize(),
+      MatrixCoord threadblock_output_shape = MatrixCoord()) {
+    return initialize(LongIndex(stride_), problem_size, threadblock_output_shape);
+  }
+
+  CUTLASS_HOST_DEVICE
+  PredicatedTileIteratorDirect2dConvParams() { initialize(LongIndex(0)); }
+
+  CUTLASS_HOST_DEVICE
+  PredicatedTileIteratorDirect2dConvParams(Index stride,
+                               cutlass::conv::Conv2dProblemSize const &problem_size,
+                               MatrixCoord threadblock_output_shape) {
+    initialize(stride, problem_size, threadblock_output_shape);
+  }
+
+  CUTLASS_HOST_DEVICE
+  PredicatedTileIteratorDirect2dConvParams(LongIndex stride,
+                               cutlass::conv::Conv2dProblemSize const &problem_size,
+                               MatrixCoord threadblock_output_shape) {
+    initialize(stride, problem_size, threadblock_output_shape);
+  }
+};
+
 ///////////////////////////////////////////////////////////////////////////////
 //  InterleavedPredicatedTileIterator
 ///////////////////////////////////////////////////////////////////////////////
 
 
 /// Predicated tile access iterator descriptor object containing template dependent state
 struct InterleavedPredicatedTileIteratorDesc {
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/predicated_tile_iterator_predicates.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/predicated_tile_iterator_predicates.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/predicated_tile_iterator_strided_dgrad.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/predicated_tile_iterator_strided_dgrad.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/shared_load_iterator.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/shared_load_iterator.h`

 * *Files 2% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -197,14 +197,19 @@
             }
           }
         }
       }
     }
   }
 
+  /// Loads a fragment from memory
+  CUTLASS_DEVICE
+  void set_smem_base_address(Index address) {
+  }
+
   /// Loads a fragment
   CUTLASS_DEVICE
   void load(Fragment &frag) const {
 
     load_with_pointer_offset(frag, 0);
   }
 };
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/shared_load_iterator_mixed.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/shared_load_iterator_mixed.h`

 * *Files 2% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -230,14 +230,18 @@
             }
           }
         }
       }
     }
   }
 
+  /// Set base smem address
+  CUTLASS_DEVICE
+  void set_smem_base_address(Index address) {}
+
   /// Loads a fragment
   CUTLASS_DEVICE
   void load(Fragment &frag) const {
 
     load_with_pointer_offset(frag, 0);
   }
 };
@@ -391,14 +395,18 @@
             }
           }
         }
       }
     }
   }
 
+  /// Set base smem address
+  CUTLASS_DEVICE
+  void set_smem_base_address(Index address) {}
+
   /// Loads a fragment
   CUTLASS_DEVICE
   void load(Fragment &frag) {
 
     load_with_pointer_offset(frag, 0);
   }
 };
@@ -552,14 +560,18 @@
             }
           }
         }
       }
     }
   }
 
+  /// Set base smem address
+  CUTLASS_DEVICE
+  void set_smem_base_address(Index address) {}
+
   /// Loads a fragment
   CUTLASS_DEVICE
   void load(Fragment &frag) {
 
     load_with_pointer_offset(frag, 0);
   }
 };
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/shared_load_iterator_pitch_liner.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/shared_load_iterator_pitch_liner.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/epilogue/warp/fragment_iterator_complex_tensor_op.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/epilogue/warp/fragment_iterator_complex_tensor_op.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/epilogue/warp/fragment_iterator_gaussian_complex_tensor_op.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/epilogue/warp/fragment_iterator_gaussian_complex_tensor_op.h`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/epilogue/warp/fragment_iterator_simt.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/epilogue/warp/fragment_iterator_simt.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/epilogue/warp/fragment_iterator_tensor_op.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/epilogue/warp/fragment_iterator_tensor_op.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/epilogue/warp/fragment_iterator_volta_tensor_op.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/epilogue/warp/fragment_iterator_volta_tensor_op.h`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/epilogue/warp/fragment_iterator_wmma_tensor_op.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/epilogue/warp/fragment_iterator_wmma_tensor_op.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/epilogue/warp/simt_policy.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/epilogue/warp/simt_policy.h`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/epilogue/warp/tensor_op_policy.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/epilogue/warp/tensor_op_policy.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/epilogue/warp/tile_iterator_simt.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/epilogue/warp/tile_iterator_volta_tensor_op.h`

 * *Files 13% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -34,459 +34,407 @@
 
 #pragma once
 
 #include "cutlass/array.h"
 #include "cutlass/layout/matrix.h"
 #include "cutlass/layout/pitch_linear.h"
 
-#include "cutlass/epilogue/warp/simt_policy.h"
-
-#define CUTLASS_SIMT_EPILOGUE_USE_SCALAR_STORES 1
+#include "cutlass/epilogue/warp/tensor_op_policy.h"
+#include "cutlass/epilogue/warp/volta_tensor_op_policy.h"
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
 namespace cutlass {
 namespace epilogue {
 namespace warp {
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
 /// Template for reading and writing tiles of accumulators to shared memory
 template <
-  typename WarpShape,     ///< shape of warp-level GEMM (concept: MatrixShape)
-  typename Operator,      ///< matrix multiply operation (concept: arch::Mma)
-  typename Element,       ///< data type of element to be written
-  typename Layout,        ///< target shared memory layout
-  typename MmaSimtPolicy          ///< policy defining lane arrangement (concept: MmaSimtPolicy)
+  typename WarpShape,             ///< shape of warp-level GEMM (concept: MatrixShape)
+  typename InterleavedTileShape,  ///< shape of indivisible instruction-level arrangement (concept: GemmShape)
+  typename ElementC,              ///< Accumulator layout
+  typename Layout                 ///< target shared memory layout
 >
-class TileIteratorSimt;
+struct TileIteratorVoltaTensorOp; 
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
 /// Template for reading and writing tiles of accumulators to shared memory
 template <
-  typename WarpShape_,     ///< shape of warp-level GEMM (concept: GemmShape)
-  typename Operator_,      ///< matrix multiply operation (concept: arch::Mma)
-  typename Element_,       ///< data type of element to be written
-  typename MmaSimtPolicy_         ///< policy defining lane arrangement (concept: MmaSimtPolicy)
+  typename WarpShape_         ///< shape of warp-level GEMM (concept: MatrixShape)
 >
-class TileIteratorSimt<WarpShape_, Operator_, Element_, layout::RowMajor, MmaSimtPolicy_> {
+struct TileIteratorVoltaTensorOp<WarpShape_, gemm::GemmShape<32, 32, 4>, half_t, layout::RowMajor> {
 public:
 
   using WarpShape = WarpShape_;
-  using Operator = Operator_;
-  using Element = Element_;
+  using InterleavedTileShape = gemm::GemmShape<32, 32, 4>;
+  using Element = half_t;
   using Layout = layout::RowMajor;
 
   using TensorRef = TensorRef<Element, Layout>;         ///< Tensor Reference object
   using TensorCoord = MatrixCoord;                      ///< Logical coordinate in referenced tensor
   using Index = typename TensorRef::Index;
   using LongIndex = typename TensorRef::LongIndex;
 
-  using Policy = SimtPolicy<WarpShape, Operator, Layout, MmaSimtPolicy_>;
+  using Policy = VoltaTensorOpPolicy<WarpShape, InterleavedTileShape, Element, Layout>;
 
   /// Shape of the tile in memory
   using Shape = MatrixShape<
     Policy::kRowsPerIteration,
     WarpShape::kN
   >;
 
+  /// Array type for aligned memory accesses
+  using AccessType = typename Policy::AccessType;
+  
   /// This is the fragment size produced by one access of the iterator.
-  using Fragment = Array<
-    typename Operator::ElementC, 
-    Policy::kElementsPerIteration>;
+  using Fragment = typename Policy::Fragment;
 
   /// This is the complete warp-level accumulator tile.
-  using AccumulatorTile = Array<
-    typename Operator::ElementC, 
-    Policy::kAccumulatorElementCount>;
+  using AccumulatorTile = typename Policy::AccumulatorTile;
 
   /// Number of times this iterator can be incremented
   static int const kIterations = Policy::kIterations;
 
+  /// Number of elements per access
+  static int const kElementsPerAccess = Policy::kElementsPerAccess;
+
+  // Internal constants
+  struct Detail {
+    static int const kLanesInQuad = 4;
+    static int const kRowsPerQuad = 4;
+    static int const kColumnsPerQuad = 8;
+    static int const kAccessesPerQuad = kColumnsPerQuad / Policy::kElementsPerAccess;
+    static int const kAccessQuadDelta = 16;
+  };
+
   /// Padding quantity
   using Padding = MatrixShape<
     0,
-    4 * Policy::kElementsPerAccess
-#if CUTLASS_SIMT_EPILOGUE_USE_SCALAR_STORES
-    + 1
-#endif
-  >;
+    Policy::kElementsPerAccess>;
 
 private:
 
-#if CUTLASS_SIMT_EPILOGUE_USE_SCALAR_STORES
-  /// Storage type for accessing memory
-  using AccessType = AlignedArray<
-    Element, 
-    1
-  >;
-
-#else
-  /// Storage type for accessing memory
-  using AccessType = AlignedArray<
-    Element, 
-    Policy::kElementsPerAccess
-  >;
-#endif
-
   //
   // Data members
   //
 
   /// Internal pointer to memory
   AccessType *pointer_;
 
   /// Internal layout object
   Layout layout_;
 
 public:
 
   /// Default constructor
   CUTLASS_HOST_DEVICE
-  TileIteratorSimt(): pointer_(nullptr) { }
+  TileIteratorVoltaTensorOp(): pointer_(nullptr) { }
 
   /// Constructor from TensorRef
-  CUTLASS_HOST_DEVICE
-  TileIteratorSimt(
+  CUTLASS_DEVICE
+  TileIteratorVoltaTensorOp(
     TensorRef const &ref,
     unsigned lane_id
   ):
     pointer_(reinterpret_cast<AccessType *>(ref.data())),
-    layout_(ref.stride()[0] / AccessType::kElements) { 
+    layout_(ref.stride()[0] / Policy::kElementsPerAccess) { 
 
-    auto lane_layout = Policy::MmaSimtPolicy::get_lane_layout();
-    MatrixCoord lane_offset = lane_layout.inverse(lane_id);
+    int quad_id = lane_id / Detail::kLanesInQuad;
+    int lane_in_quad = (lane_id % Detail::kLanesInQuad);
 
-    pointer_ += layout_({
-      lane_offset.row(),
-      lane_offset.column() * Policy::kElementsPerAccess / int(AccessType::kElements)
-    });
+    int quad_row_idx = ((quad_id & 4) >> 1) + (quad_id & 1);
+    int quad_col_idx = ((quad_id & 2) >> 1);
+
+    int row = quad_row_idx * Detail::kRowsPerQuad + lane_in_quad;
+    int column = quad_col_idx * Detail::kColumnsPerQuad;
+
+    pointer_ += layout_({row, column / kElementsPerAccess});
   }
 
   /// Adds a pointer offset
   CUTLASS_HOST_DEVICE
-  TileIteratorSimt & add_pointer_offset(Index pointer_offset) {
-    pointer_ += pointer_offset / AccessType::kElements;
+  TileIteratorVoltaTensorOp & add_pointer_offset(Index pointer_offset) {
+    pointer_ += pointer_offset / Policy::kElementsPerAccess;
     return *this;
   }
 
   ///< advances in units of whole tiles along the logical coordinate space of the tensor
   CUTLASS_HOST_DEVICE
-  TileIteratorSimt & add_tile_offset(TensorCoord const &tile_offset) {
+  TileIteratorVoltaTensorOp & add_tile_offset(TensorCoord const &tile_offset) {
 
     pointer_ += layout_({
       tile_offset.row() * Shape::kRow, 
-      (tile_offset.column() * Shape::kColumn / int(AccessType::kElements))
-    });
+      tile_offset.column() * Shape::kColumn / Policy::kElementsPerAccess});
 
     return *this;
   }
 
   ///< advances in units of whole tiles along the logical coordinate space of the tensor
   CUTLASS_HOST_DEVICE
-  TileIteratorSimt & operator+=(TensorCoord const &tile_offset) {
-
+  TileIteratorVoltaTensorOp & operator+=(TensorCoord const &tile_offset) {
     add_tile_offset(tile_offset);
-    
     return *this;
   }
 
   /// Store
-  CUTLASS_HOST_DEVICE
+  CUTLASS_DEVICE
   void store_with_pointer_offset(Fragment const &frag, Index pointer_offset) {
-#if CUTLASS_SIMT_EPILOGUE_USE_SCALAR_STORES
-      // de-vectorized stores
-      using ScalarAccessType = AlignedArray<Element, 1>;
-      ScalarAccessType const *scalarFragPtr = reinterpret_cast<ScalarAccessType const *>(&frag);
-      ScalarAccessType *scalarPointer = reinterpret_cast<ScalarAccessType *>(pointer_) + pointer_offset;
 
-      CUTLASS_PRAGMA_UNROLL
-      for (int n = 0; n < Policy::kAccessesPerIteration; ++n) {
-        CUTLASS_PRAGMA_UNROLL
-        for (int s = 0; s < Policy::kElementsPerAccess; s++) {
-          scalarPointer[n * Policy::MmaSimtPolicy::WarpShape::kColumn * Policy::kElementsPerAccess + s] = scalarFragPtr[n * Policy::kElementsPerAccess + s];
-        }
-      }
-#else
-    // original vector stores
     AccessType const *frag_ptr = reinterpret_cast<AccessType const *>(&frag);
+
     CUTLASS_PRAGMA_UNROLL
-    for (int n = 0; n < Policy::kAccessesPerIteration; ++n) {
-      pointer_[n * Policy::MmaSimtPolicy::WarpShape::kColumn + pointer_offset / int(AccessType::kElements)] = frag_ptr[n];
+    for (int tile_idx = 0; tile_idx < Policy::TileIterations::kColumn; ++tile_idx) {
+
+      CUTLASS_PRAGMA_UNROLL
+      for (int access_idx = 0; access_idx < Policy::kAccessesPerInterleavedTile; ++access_idx) {
+
+        int access_quad = access_idx / 2;
+        int access = access_idx % 2;
+
+        int ptr_offset = tile_idx * InterleavedTileShape::kN / Policy::kElementsPerAccess +
+          access_quad * Detail::kAccessQuadDelta / Policy::kElementsPerAccess + 
+          access + pointer_offset / Policy::kElementsPerAccess;
+
+        int frag_idx = tile_idx * Policy::kAccessesPerInterleavedTile + access_idx;
+
+        AccessType access_vector = frag_ptr[frag_idx];
+
+        pointer_[ptr_offset] = access_vector;
+      }
     }
-#endif
   }
 
   /// Store
   CUTLASS_HOST_DEVICE
   void store(Fragment const &frag) {
     store_with_pointer_offset(frag, 0);
   }
 
   /// Load
   CUTLASS_HOST_DEVICE
-  void load_with_pointer_offset(Fragment &frag, Index pointer_offset) const {
+  void load_with_pointer_offset(Fragment const &frag, Index pointer_offset) {
 
     AccessType *frag_ptr = reinterpret_cast<AccessType *>(&frag);
 
     CUTLASS_PRAGMA_UNROLL
-    for (int n = 0; n < Policy::kAccessesPerIteration; ++n) {
-      frag_ptr[n] = pointer_[n * Policy::MmaSimtPolicy::WarpShape::kColumn + pointer_offset / int(AccessType::kElements)];
+    for (int tile_idx = 0; tile_idx < Policy::TileIterations::kColumn; ++tile_idx) {
+
+      CUTLASS_PRAGMA_UNROLL
+      for (int access_idx = 0; access_idx < Policy::kAccessesPerInterleavedTile; ++access_idx) {
+
+        int access_quad = access_idx / 2;
+        int access = access_idx % 2;
+
+        int ptr_offset = tile_idx * Detail::kTileDelta + access_quad * Detail::kAccessQuadDelta + 
+          access + pointer_offset / Policy::kElementsPerAccess;
+
+        int frag_idx = tile_idx * Policy::kAccessesPerInterleavedTile + access_idx;
+
+        frag_ptr[frag_idx] = pointer_[ptr_offset];
+      }
     }
   }
 
   /// Load
   CUTLASS_HOST_DEVICE
-  void load(Fragment &frag) const {
+  void load(Fragment const &frag) {
     load_with_pointer_offset(frag, 0);
   }
+  
+  /// Set smem base address
+  CUTLASS_HOST_DEVICE
+  void set_smem_base_address(Index address) {
+  }
 };
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
-/////////////////////////////////////////////////////////////////////////////////////////////////
-
 /// Template for reading and writing tiles of accumulators to shared memory
 template <
-  typename WarpShape_,        ///< shape of warp-level GEMM (concept: GemmShape)
-  typename Operator_,         ///< matrix multiply operation (concept: arch::Mma)
-  typename Element_,          ///< data type of element to be written
-  typename Layout_,            ///< target shared memory layout
-  typename MmaSimtPolicy_     ///< policy defining lane arrangement (concept: MmaSimtPolicy)
+  typename WarpShape_         ///< shape of warp-level GEMM (concept: MatrixShape)
 >
-class TileIteratorSimtCanonical {
+struct TileIteratorVoltaTensorOp<WarpShape_, gemm::GemmShape<32, 32, 4>, float, layout::RowMajor> {
 public:
 
   using WarpShape = WarpShape_;
-  using Operator = Operator_;
-  using Element = Element_;
-  using Layout = Layout_;
+  using InterleavedTileShape = gemm::GemmShape<32, 32, 4>;
+  using Element = float;
+  using Layout = layout::RowMajor;
 
   using TensorRef = TensorRef<Element, Layout>;         ///< Tensor Reference object
   using TensorCoord = MatrixCoord;                      ///< Logical coordinate in referenced tensor
   using Index = typename TensorRef::Index;
   using LongIndex = typename TensorRef::LongIndex;
 
-  using Policy = SimtPolicy<WarpShape, Operator, Layout, MmaSimtPolicy_>;
+  using Policy = VoltaTensorOpPolicy<WarpShape, InterleavedTileShape, Element, Layout>;
 
   /// Shape of the tile in memory
   using Shape = MatrixShape<
     Policy::kRowsPerIteration,
     WarpShape::kN
   >;
 
+  /// Array type for aligned memory accesses
+  using AccessType = typename Policy::AccessType;
+  
   /// This is the fragment size produced by one access of the iterator.
-  using Fragment = Array<
-    typename Operator::ElementC, 
-    Policy::kElementsPerIteration>;
+  using Fragment = typename Policy::Fragment;
 
   /// This is the complete warp-level accumulator tile.
-  using AccumulatorTile = Array<
-    typename Operator::ElementC, 
-    Policy::kAccumulatorElementCount>;
+  using AccumulatorTile = typename Policy::AccumulatorTile;
 
   /// Number of times this iterator can be incremented
   static int const kIterations = Policy::kIterations;
 
+  /// Number of elements per access
+  static int const kElementsPerAccess = Policy::kElementsPerAccess;
+
+  // Internal constants
+  struct Detail {
+    static int const kLanesInQuad = 4;
+    static int const kRowsPerQuad = 4;
+    static int const kColumnsPerQuad = 8;
+    static int const kAccessesPerQuad = kColumnsPerQuad / Policy::kElementsPerAccess;
+    static int const kAccessQuadDelta = 16;
+  };
+
   /// Padding quantity
   using Padding = MatrixShape<
     0,
-    4 * Policy::kElementsPerAccess + 1
-  >;
+    Policy::kElementsPerAccess>;
 
 private:
 
-  /// Storage type for accessing memory
-  using AccessType = AlignedArray<
-    Element, 
-    1
-  >;
-
   //
   // Data members
   //
 
   /// Internal pointer to memory
   AccessType *pointer_;
 
   /// Internal layout object
   Layout layout_;
 
-  /// Guard to indicate whether the shape is divisible
-  bool divisible_;
-
-  /// Extent of the output tensor
-  MatrixCoord extent_;
-
-  /// Thread offset
-  MatrixCoord thread_offset_;
-
 public:
 
   /// Default constructor
   CUTLASS_HOST_DEVICE
-  TileIteratorSimtCanonical(): pointer_(nullptr) { }
+  TileIteratorVoltaTensorOp(): pointer_(nullptr) { }
 
   /// Constructor from TensorRef
-  CUTLASS_HOST_DEVICE
-  TileIteratorSimtCanonical(
+  CUTLASS_DEVICE
+  TileIteratorVoltaTensorOp(
     TensorRef const &ref,
     unsigned lane_id
   ):
     pointer_(reinterpret_cast<AccessType *>(ref.data())),
-    layout_(ref.stride()[0] / AccessType::kElements),
-    divisible_(true),
-    extent_(WarpShape::kM, WarpShape::kN) { 
-
-    auto lane_layout = Policy::MmaSimtPolicy::get_lane_layout();
-    MatrixCoord lane_offset = lane_layout.inverse(lane_id);
-
-    thread_offset_ = {
-      lane_offset.row() * Shape::kRow, 
-      lane_offset.column() * Policy::kElementsPerAccess
-    };
+    layout_(ref.stride()[0] / Policy::kElementsPerAccess) { 
 
-    pointer_ += layout_({
-      lane_offset.row() * Shape::kRow,
-      lane_offset.column() * Policy::kElementsPerAccess / int(AccessType::kElements)
-    });
-  }
+    int quad_id = lane_id / Detail::kLanesInQuad;
+    int lane_in_quad = (lane_id % Detail::kLanesInQuad);
 
-  /// Constructor from TensorRef
-  CUTLASS_HOST_DEVICE
-  TileIteratorSimtCanonical(
-    TensorRef const &ref,
-    TensorCoord const &extent,
-    unsigned lane_id
-  ):
-    pointer_(reinterpret_cast<AccessType *>(ref.data())),
-    layout_(ref.stride()[0] / AccessType::kElements),
-    divisible_(false),
-    extent_(extent) { 
-
-    auto lane_layout = Policy::MmaSimtPolicy::get_lane_layout();
-    MatrixCoord lane_offset = lane_layout.inverse(lane_id);
-
-    thread_offset_ = {
-      lane_offset.row() * Shape::kRow, 
-      lane_offset.column() * Policy::kElementsPerAccess
-    };
+    int const kQuadRowDelta = 4;
+    int const kQuadColumnDelta = 2 * Policy::MmaIterations::kColumn;
 
-    pointer_ += layout_({
-      lane_offset.row() * Shape::kRow,
-      lane_offset.column() * Policy::kElementsPerAccess / int(AccessType::kElements)
-    });
+    int quad_row_offset = ((quad_id & 4) / 2 + (quad_id & 1)) * kQuadRowDelta;
+    int quad_column_offset = (quad_id & 2) / 2 * kQuadColumnDelta;
+
+    int thread_row_offset = (lane_in_quad & 1);
+    int thread_column_offset = (lane_in_quad & 2) / 2;
+
+    int row = quad_row_offset + thread_row_offset;
+    int column = quad_column_offset + thread_column_offset;
+
+    pointer_ += layout_({row, column});
   }
 
   /// Adds a pointer offset
   CUTLASS_HOST_DEVICE
-  TileIteratorSimtCanonical & add_pointer_offset(Index pointer_offset) {
-    pointer_ += pointer_offset / AccessType::kElements;
+  TileIteratorVoltaTensorOp & add_pointer_offset(Index pointer_offset) {
+    pointer_ += pointer_offset / Policy::kElementsPerAccess;
     return *this;
   }
 
   ///< advances in units of whole tiles along the logical coordinate space of the tensor
   CUTLASS_HOST_DEVICE
-  TileIteratorSimtCanonical & add_tile_offset(TensorCoord const &tile_offset) {
-
-    MatrixCoord coord_offset(
-      tile_offset.row(), 
-      tile_offset.column() * Shape::kColumn
-    );
-
-    thread_offset_ += coord_offset;
+  TileIteratorVoltaTensorOp & add_tile_offset(TensorCoord const &tile_offset) {
 
     pointer_ += layout_({
-      coord_offset.row(), 
-      coord_offset.column()
-    });
+      tile_offset.row() * Shape::kRow, 
+      tile_offset.column() * Shape::kColumn / Policy::kElementsPerAccess});
 
     return *this;
   }
 
   ///< advances in units of whole tiles along the logical coordinate space of the tensor
   CUTLASS_HOST_DEVICE
-  TileIteratorSimtCanonical & operator+=(TensorCoord const &tile_offset) {
-
+  TileIteratorVoltaTensorOp & operator+=(TensorCoord const &tile_offset) {
     add_tile_offset(tile_offset);
-    
     return *this;
   }
 
   /// Store
-  CUTLASS_HOST_DEVICE
+  CUTLASS_DEVICE
   void store_with_pointer_offset(Fragment const &frag, Index pointer_offset) {
 
-    // de-vectorized stores
-    using ScalarAccessType = AlignedArray<Element, 1>;
-    ScalarAccessType const *scalarFragPtr = reinterpret_cast<ScalarAccessType const *>(&frag);
-    ScalarAccessType *scalarPointer = reinterpret_cast<ScalarAccessType *>(pointer_) + pointer_offset;
+    AccessType const *frag_ptr = reinterpret_cast<AccessType const *>(&frag);
+
+    int const kAccessesPerRow = Policy::TileIterations::kColumn * Policy::MmaIterations::kColumn * 2;
 
     CUTLASS_PRAGMA_UNROLL
-    for (int n = 0; n < Policy::kAccessesPerIteration; ++n) {
+    for (int row_idx = 0; row_idx < Policy::kRowsPerMmaTile; ++row_idx) {
+
       CUTLASS_PRAGMA_UNROLL
-      for (int s = 0; s < Policy::kElementsPerAccess; s++) {
-        
-        int ptr_idx = n * Policy::MmaSimtPolicy::WarpShape::kColumn * Policy::kElementsPerAccess + s;
-        int frag_idx = n * Policy::kElementsPerAccess + s;
-        
-        int col = thread_offset_.column() + ptr_idx;
-
-        if (divisible_ || (thread_offset_.row() < extent_.row() && col < extent_.column())) {
-          scalarPointer[ptr_idx] = scalarFragPtr[frag_idx];
-        }
+      for (int access_idx = 0; access_idx < kAccessesPerRow; ++access_idx) {
+
+        int frag_idx = row_idx * kAccessesPerRow + access_idx;
+
+        int ptr_column_offset = (access_idx & 1) * 2 + 
+          (access_idx & 2) * Policy::MmaIterations::kColumn * 2 + 
+          (access_idx & 4) * Policy::MmaIterations::kColumn * 2;
+
+        int ptr_row_offset = row_idx * 2;
+
+        int ptr_offset = layout_({ptr_row_offset, ptr_column_offset}) + pointer_offset / Policy::kElementsPerAccess;
+
+        pointer_[ptr_offset] = frag_ptr[frag_idx];
       }
     }
   }
 
   /// Store
   CUTLASS_HOST_DEVICE
   void store(Fragment const &frag) {
     store_with_pointer_offset(frag, 0);
   }
 
   /// Load
   CUTLASS_HOST_DEVICE
-  void load_with_pointer_offset(Fragment &frag, Index pointer_offset) const {
+  void load_with_pointer_offset(Fragment const &frag, Index pointer_offset) {
 
-      // de-vectorized loads
-      using ScalarAccessType = AlignedArray<Element, 1>;
-      ScalarAccessType *scalarFragPtr = reinterpret_cast<ScalarAccessType *>(&frag);
-      ScalarAccessType const *scalarPointer = reinterpret_cast<ScalarAccessType const*>(pointer_) + pointer_offset;
+    AccessType *frag_ptr = reinterpret_cast<AccessType *>(&frag);
 
-      CUTLASS_PRAGMA_UNROLL
-      for (int n = 0; n < Policy::kAccessesPerIteration; ++n) {
-        CUTLASS_PRAGMA_UNROLL
-        for (int s = 0; s < Policy::kElementsPerAccess; s++) {
-          
-          int ptr_idx = n * Policy::MmaSimtPolicy::WarpShape::kColumn * Policy::kElementsPerAccess + s;
-          int frag_idx = n * Policy::kElementsPerAccess + s;
-          
-          int col = thread_offset_.column() + ptr_idx;
-
-          if (divisible_ || (thread_offset_.row() < extent_.row() && col < extent_.column())) {
-            scalarFragPtr[frag_idx] = scalarPointer[ptr_idx];
-          }
-        }
-      }
+    assert(0); // TODO
   }
 
   /// Load
   CUTLASS_HOST_DEVICE
-  void load(Fragment &frag) const {
+  void load(Fragment const &frag) {
     load_with_pointer_offset(frag, 0);
   }
-
+  
+  /// Set smem base address
   CUTLASS_HOST_DEVICE
-  TileIteratorSimtCanonical & operator++() {
-    return add_tile_offset({1, 0});
+  void set_smem_base_address(Index address) {
   }
-
 };
 
+/////////////////////////////////////////////////////////////////////////////////////////////////
 
 } // namespace warp
 } // namespace epilogue
 } // namespace cutlass
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/epilogue/warp/tile_iterator_tensor_op.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/epilogue/warp/tile_iterator_tensor_op.h`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -224,14 +224,19 @@
     load_with_pointer_offset(frag, 0);
   }
 
   CUTLASS_HOST_DEVICE
   TileIteratorTensorOp & operator++() {
     return add_tile_offset({1, 0});
   }
+  
+  /// Set smem base address
+  CUTLASS_HOST_DEVICE
+  void set_smem_base_address(Index address) {
+  }
 };
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
 /// Template for reading and writing tiles of accumulators to shared memory
 template <
   typename WarpShape_,     ///< shape of warp-level GEMM (concept: GemmShape)
@@ -416,14 +421,19 @@
     load_with_pointer_offset(frag, 0);
   }
 
   CUTLASS_HOST_DEVICE
   TileIteratorTensorOp & operator++() {
     return add_tile_offset({0, 1});
   }
+
+  /// Set smem base address
+  CUTLASS_HOST_DEVICE
+  void set_smem_base_address(Index address) {
+  }
 };
 
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
 /// Template for reading and writing tiles of accumulators to shared memory
 template <
@@ -641,14 +651,19 @@
     load_with_pointer_offset(frag, 0);
   }
 
   CUTLASS_HOST_DEVICE
   TileIteratorTensorOpCanonical & operator++() {
     return add_tile_offset({1, 0});
   }
+  
+  /// Set smem base address
+  CUTLASS_HOST_DEVICE
+  void set_smem_base_address(Index address) {
+  }
 };
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
 } // namespace warp
 } // namespace epilogue
 } // namespace cutlass
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/epilogue/warp/tile_iterator_tensor_op_mixed.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/epilogue/warp/tile_iterator_tensor_op_mixed.h`

 * *Files 2% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -300,14 +300,19 @@
   }
 
   /// Load
   CUTLASS_HOST_DEVICE
   void load(Fragment &frag) const {
     load_with_pointer_offset(frag, 0);
   }
+  
+  /// Set smem base address
+  CUTLASS_HOST_DEVICE
+  void set_smem_base_address(Index address) {
+  }
 };
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
 /// Partial specialization for int32_t x 16 => int8_t/int4b_t x 16
 template <
   typename WarpShape_,            ///< shape of warp-level GEMM (concept: GemmShape)
@@ -502,14 +507,19 @@
   }
 
   /// Store
   CUTLASS_HOST_DEVICE
   void store(Fragment const &frag) {
     store_with_pointer_offset(frag, 0);
   }
+
+  /// Set smem base address
+  CUTLASS_HOST_DEVICE
+  void set_smem_base_address(Index address) {
+  }
 };
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
 /// Partial specialization for int32_t x 8 => int8_t/int4b_t x 8
 template <
   typename WarpShape_,            ///< shape of warp-level GEMM (concept: GemmShape)
@@ -693,14 +703,19 @@
   }
 
   /// Store
   CUTLASS_HOST_DEVICE
   void store(Fragment const &frag) {
     store_with_pointer_offset(frag, 0);
   }
+
+  /// Set smem base address
+  CUTLASS_HOST_DEVICE
+  void set_smem_base_address(Index address) {
+  }
 };
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
 } // namespace warp
 } // namespace epilogue
 } // namespace cutlass
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/epilogue/warp/tile_iterator_volta_tensor_op.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/transform/threadblock/regular_tile_access_iterator_pitch_linear.h`

 * *Files 27% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -25,406 +25,384 @@
  * SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER
  * CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,
  * OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
  * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
  *
  **************************************************************************************************/
 /*! \file
-    \brief 
+    \brief Templates implementing computing the addresses of storing of tiles
+   from pitch-linear rank=2 tensors.
 */
 
 #pragma once
 
+#include "cutlass/cutlass.h"
 #include "cutlass/array.h"
-#include "cutlass/layout/matrix.h"
 #include "cutlass/layout/pitch_linear.h"
+#include "cutlass/layout/matrix.h"
+#include "cutlass/matrix_coord.h"
+#include "cutlass/matrix_shape.h"
+#include "cutlass/tensor_ref.h"
 
-#include "cutlass/epilogue/warp/tensor_op_policy.h"
-#include "cutlass/epilogue/warp/volta_tensor_op_policy.h"
+#include "cutlass/transform/threadblock/regular_tile_access_iterator.h"
 
-/////////////////////////////////////////////////////////////////////////////////////////////////
+////////////////////////////////////////////////////////////////////////////////
 
 namespace cutlass {
-namespace epilogue {
-namespace warp {
+namespace transform {
+namespace threadblock {
 
-/////////////////////////////////////////////////////////////////////////////////////////////////
+////////////////////////////////////////////////////////////////////////////////
 
-/// Template for reading and writing tiles of accumulators to shared memory
-template <
-  typename WarpShape,             ///< shape of warp-level GEMM (concept: MatrixShape)
-  typename InterleavedTileShape,  ///< shape of indivisible instruction-level arrangement (concept: GemmShape)
-  typename ElementC,              ///< Accumulator layout
-  typename Layout                 ///< target shared memory layout
->
-struct TileIteratorVoltaTensorOp; 
-
-/////////////////////////////////////////////////////////////////////////////////////////////////
-
-/// Template for reading and writing tiles of accumulators to shared memory
-template <
-  typename WarpShape_         ///< shape of warp-level GEMM (concept: MatrixShape)
->
-struct TileIteratorVoltaTensorOp<WarpShape_, gemm::GemmShape<32, 32, 4>, half_t, layout::RowMajor> {
-public:
-
-  using WarpShape = WarpShape_;
-  using InterleavedTileShape = gemm::GemmShape<32, 32, 4>;
-  using Element = half_t;
-  using Layout = layout::RowMajor;
+/// Tile iterator specialized for congruous arrangements for TensorOps
+///
+///
+/// Satisfies: ForwardTileIteratorConcept |
+///            ReadableContiguousTileIteratorConcept |
+///            WriteableContiguousTileIteratorConcept
+///
+template <typename Shape_, typename Element_, int AdvanceRank,
+          typename ThreadMap_, int Alignment>
+class RegularTileAccessIterator<
+    Shape_, Element_,
+    layout::PitchLinear,
+    AdvanceRank, ThreadMap_, Alignment> {
+ public:
+  static_assert(
+      AdvanceRank == 0 || AdvanceRank == 1,
+      "Specialization for pitch-linear iterator may along advance along the "
+      "contiguous(rank=0) or strided(rank=1) dimension.");
+
+  using Shape = Shape_;
+  using Element = Element_;
+  using Layout = layout::PitchLinear;
+  static int const kAdvanceRank = AdvanceRank;
+  static int const kAlignment = Alignment;
+
+  using Index = typename Layout::Index;
+  using LongIndex = typename Layout::LongIndex;
+  using StrideIndex = typename Layout::Stride::Index;
+
+  using TensorRef = TensorRef<Element, Layout>;
+  using TensorCoord = typename Layout::TensorCoord;
 
-  using TensorRef = TensorRef<Element, Layout>;         ///< Tensor Reference object
-  using TensorCoord = MatrixCoord;                      ///< Logical coordinate in referenced tensor
-  using Index = typename TensorRef::Index;
-  using LongIndex = typename TensorRef::LongIndex;
-
-  using Policy = VoltaTensorOpPolicy<WarpShape, InterleavedTileShape, Element, Layout>;
-
-  /// Shape of the tile in memory
-  using Shape = MatrixShape<
-    Policy::kRowsPerIteration,
-    WarpShape::kN
-  >;
-
-  /// Array type for aligned memory accesses
-  using AccessType = typename Policy::AccessType;
-  
-  /// This is the fragment size produced by one access of the iterator.
-  using Fragment = typename Policy::Fragment;
-
-  /// This is the complete warp-level accumulator tile.
-  using AccumulatorTile = typename Policy::AccumulatorTile;
-
-  /// Number of times this iterator can be incremented
-  static int const kIterations = Policy::kIterations;
-
-  /// Number of elements per access
-  static int const kElementsPerAccess = Policy::kElementsPerAccess;
-
-  // Internal constants
-  struct Detail {
-    static int const kLanesInQuad = 4;
-    static int const kRowsPerQuad = 4;
-    static int const kColumnsPerQuad = 8;
-    static int const kAccessesPerQuad = kColumnsPerQuad / Policy::kElementsPerAccess;
-    static int const kAccessQuadDelta = 16;
-  };
-
-  /// Padding quantity
-  using Padding = MatrixShape<
-    0,
-    Policy::kElementsPerAccess>;
+  using ThreadMap = ThreadMap_;
 
-private:
+  /// Element type per access
+  using AccessType = Array<Element, ThreadMap::kElementsPerAccess>;
 
+ private:
   //
   // Data members
   //
 
-  /// Internal pointer to memory
+  /// Stride value
+  StrideIndex stride_;
+
+  /// Internal pointer to first access of tile
   AccessType *pointer_;
 
-  /// Internal layout object
-  Layout layout_;
+  /// Internal byte offset
+  Index byte_offset_;
 
-public:
+  /// Iteration in the contiguous dimension
+  int iteration_contiguous_;
 
-  /// Default constructor
-  CUTLASS_HOST_DEVICE
-  TileIteratorVoltaTensorOp(): pointer_(nullptr) { }
+  /// Iteration in the strided dimension
+  int iteration_strided_;
 
-  /// Constructor from TensorRef
-  CUTLASS_DEVICE
-  TileIteratorVoltaTensorOp(
-    TensorRef const &ref,
-    unsigned lane_id
-  ):
-    pointer_(reinterpret_cast<AccessType *>(ref.data())),
-    layout_(ref.stride()[0] / Policy::kElementsPerAccess) { 
-
-    int quad_id = lane_id / Detail::kLanesInQuad;
-    int lane_in_quad = (lane_id % Detail::kLanesInQuad);
+ public:
+  /// Construct a TileIterator with zero threadblock offset
+  CUTLASS_HOST_DEVICE
+  RegularTileAccessIterator(TensorRef ref,  ///< Pointer to start of tensor
+                            int thread_id   ///< ID of each participating thread
+                            )
+      : stride_(ref.stride(0) / ThreadMap::kElementsPerAccess),
+        byte_offset_(0) {
 
-    int quad_row_idx = ((quad_id & 4) >> 1) + (quad_id & 1);
-    int quad_col_idx = ((quad_id & 2) >> 1);
+    layout::PitchLinearCoord thread_offset_base = ThreadMap::initial_offset(thread_id);
 
-    int row = quad_row_idx * Detail::kRowsPerQuad + lane_in_quad;
-    int column = quad_col_idx * Detail::kColumnsPerQuad;
+    // initialize pointer
+    pointer_ = reinterpret_cast<AccessType *>(ref.data() + ref.offset(thread_offset_base));
 
-    pointer_ += layout_({row, column / kElementsPerAccess});
+    set_iteration_index(0);
   }
 
-  /// Adds a pointer offset
+  /// Overrides the internal iteration index
   CUTLASS_HOST_DEVICE
-  TileIteratorVoltaTensorOp & add_pointer_offset(Index pointer_offset) {
-    pointer_ += pointer_offset / Policy::kElementsPerAccess;
-    return *this;
+  void set_iteration_index(int index) {
+    iteration_contiguous_ = index % ThreadMap::Iterations::kContiguous;
+    iteration_strided_ = index / ThreadMap::Iterations::kContiguous;
   }
 
-  ///< advances in units of whole tiles along the logical coordinate space of the tensor
+  /// Adds a pointer offset in units of Element
   CUTLASS_HOST_DEVICE
-  TileIteratorVoltaTensorOp & add_tile_offset(TensorCoord const &tile_offset) {
-
-    pointer_ += layout_({
-      tile_offset.row() * Shape::kRow, 
-      tile_offset.column() * Shape::kColumn / Policy::kElementsPerAccess});
-
-    return *this;
-  }
-
-  ///< advances in units of whole tiles along the logical coordinate space of the tensor
-  CUTLASS_HOST_DEVICE
-  TileIteratorVoltaTensorOp & operator+=(TensorCoord const &tile_offset) {
-    add_tile_offset(tile_offset);
-    return *this;
+  void add_pointer_offset(LongIndex pointer_offset) {
+    byte_offset_ += pointer_offset * sizeof(Element);
   }
 
-  /// Store
+  /// Returns a pointer
   CUTLASS_DEVICE
-  void store_with_pointer_offset(Fragment const &frag, Index pointer_offset) {
-
-    AccessType const *frag_ptr = reinterpret_cast<AccessType const *>(&frag);
+  AccessType *get() const {
 
-    CUTLASS_PRAGMA_UNROLL
-    for (int tile_idx = 0; tile_idx < Policy::TileIterations::kColumn; ++tile_idx) {
+    AccessType *access_ptr = pointer_;
 
-      CUTLASS_PRAGMA_UNROLL
-      for (int access_idx = 0; access_idx < Policy::kAccessesPerInterleavedTile; ++access_idx) {
+    int access_offset = iteration_strided_ * ThreadMap::Delta::kStrided * stride_ +
+                        iteration_contiguous_ * ThreadMap::Delta::kContiguous /
+                            ThreadMap::kElementsPerAccess;
 
-        int access_quad = access_idx / 2;
-        int access = access_idx % 2;
+    char *access_byte_ptr =
+        reinterpret_cast<char *>(access_ptr + access_offset);
 
-        int ptr_offset = tile_idx * InterleavedTileShape::kN / Policy::kElementsPerAccess +
-          access_quad * Detail::kAccessQuadDelta / Policy::kElementsPerAccess + 
-          access + pointer_offset / Policy::kElementsPerAccess;
-
-        int frag_idx = tile_idx * Policy::kAccessesPerInterleavedTile + access_idx;
-
-        AccessType access_vector = frag_ptr[frag_idx];
-
-        pointer_[ptr_offset] = access_vector;
-      }
-    }
+    return reinterpret_cast<AccessType *>(access_byte_ptr + byte_offset_);
   }
 
-  /// Store
+  /// Advances to the next tile in memory.
   CUTLASS_HOST_DEVICE
-  void store(Fragment const &frag) {
-    store_with_pointer_offset(frag, 0);
-  }
-
-  /// Load
-  CUTLASS_HOST_DEVICE
-  void load_with_pointer_offset(Fragment const &frag, Index pointer_offset) {
-
-    AccessType *frag_ptr = reinterpret_cast<AccessType *>(&frag);
+  RegularTileAccessIterator &operator++() {
+    ++iteration_contiguous_;
 
-    CUTLASS_PRAGMA_UNROLL
-    for (int tile_idx = 0; tile_idx < Policy::TileIterations::kColumn; ++tile_idx) {
+    if (iteration_contiguous_ < ThreadMap::Iterations::kContiguous)
+      return *this;
 
-      CUTLASS_PRAGMA_UNROLL
-      for (int access_idx = 0; access_idx < Policy::kAccessesPerInterleavedTile; ++access_idx) {
+    // Enter here only if (iteration_contiguous_ ==
+    // ThreadMap::Iteration::kContiguous)
+    iteration_contiguous_ = 0;
+    ++iteration_strided_;
 
-        int access_quad = access_idx / 2;
-        int access = access_idx % 2;
-
-        int ptr_offset = tile_idx * Detail::kTileDelta + access_quad * Detail::kAccessQuadDelta + 
-          access + pointer_offset / Policy::kElementsPerAccess;
+    if (iteration_strided_ < ThreadMap::Iterations::kStrided) {
+      return *this;
+    }
 
-        int frag_idx = tile_idx * Policy::kAccessesPerInterleavedTile + access_idx;
+    // Enter here only if (iteration_stride_ == ThreadMap::Iteration::kStrided)
+    // which means we enter the next tile.
+    iteration_strided_ = 0;
 
-        frag_ptr[frag_idx] = pointer_[ptr_offset];
-      }
-    }
+    return *this;
   }
 
-  /// Load
+  /// Advances to the next tile in memory.
   CUTLASS_HOST_DEVICE
-  void load(Fragment const &frag) {
-    load_with_pointer_offset(frag, 0);
+  RegularTileAccessIterator operator++(int) {
+    RegularTileAccessIterator prev(*this);
+    this->operator++();
+
+    return prev;
+  }
+
+  /// Adds a tile offset in the unit of tile.
+  /// In GEMM/Conv implementation, this is used to move in the k dimension in the shared memory.
+  /// Below layouts are the shared memory layouts.  Current SM50 SIMT kernels only use col major A and row major B.
+  ///   For row major A operand, k dimension is contiguous dimension;
+  ///   For col major A operand, k dimension is strided dimension;
+  ///   For row major B operand, k dimension is strided dimension;
+  ///   For col major B operand, k dimension is contiguous dimension.
+  /// Below two classes map col/row major to the pitch linear coordinates used
+  /// in this base class.
+  CUTLASS_DEVICE
+  void add_tile_offset(TensorCoord const &coord) {
+    add_pointer_offset(coord.contiguous() * Shape::kContiguous +
+                       coord.strided() * Shape::kStrided * stride_ *
+                           ThreadMap::kElementsPerAccess);
   }
 };
 
-/////////////////////////////////////////////////////////////////////////////////////////////////
+////////////////////////////////////////////////////////////////////////////////
 
-/// Template for reading and writing tiles of accumulators to shared memory
-template <
-  typename WarpShape_         ///< shape of warp-level GEMM (concept: MatrixShape)
->
-struct TileIteratorVoltaTensorOp<WarpShape_, gemm::GemmShape<32, 32, 4>, float, layout::RowMajor> {
-public:
-
-  using WarpShape = WarpShape_;
-  using InterleavedTileShape = gemm::GemmShape<32, 32, 4>;
-  using Element = float;
-  using Layout = layout::RowMajor;
+/// Tile iterator specialized for column major layouts
+///
+///
+/// Satisfies: ForwardTileIteratorConcept |
+///            ReadableContiguousTileIteratorConcept |
+///            WriteableContiguousTileIteratorConcept
+///
+template <typename Shape_, typename Element_, int AdvanceRank,
+          typename ThreadMap_, int Alignment>
+class RegularTileAccessIterator<
+    Shape_, Element_,
+    layout::ColumnMajor,
+    AdvanceRank, ThreadMap_, Alignment> {
+ public:
+  static_assert(
+      AdvanceRank == 0 || AdvanceRank == 1,
+      "Specialization for pitch-linear iterator may along advance along the "
+      "contiguous(rank=0) or strided(rank=1) dimension.");
 
-  using TensorRef = TensorRef<Element, Layout>;         ///< Tensor Reference object
-  using TensorCoord = MatrixCoord;                      ///< Logical coordinate in referenced tensor
-  using Index = typename TensorRef::Index;
-  using LongIndex = typename TensorRef::LongIndex;
-
-  using Policy = VoltaTensorOpPolicy<WarpShape, InterleavedTileShape, Element, Layout>;
-
-  /// Shape of the tile in memory
-  using Shape = MatrixShape<
-    Policy::kRowsPerIteration,
-    WarpShape::kN
-  >;
-
-  /// Array type for aligned memory accesses
-  using AccessType = typename Policy::AccessType;
-  
-  /// This is the fragment size produced by one access of the iterator.
-  using Fragment = typename Policy::Fragment;
-
-  /// This is the complete warp-level accumulator tile.
-  using AccumulatorTile = typename Policy::AccumulatorTile;
-
-  /// Number of times this iterator can be incremented
-  static int const kIterations = Policy::kIterations;
-
-  /// Number of elements per access
-  static int const kElementsPerAccess = Policy::kElementsPerAccess;
-
-  // Internal constants
-  struct Detail {
-    static int const kLanesInQuad = 4;
-    static int const kRowsPerQuad = 4;
-    static int const kColumnsPerQuad = 8;
-    static int const kAccessesPerQuad = kColumnsPerQuad / Policy::kElementsPerAccess;
-    static int const kAccessQuadDelta = 16;
-  };
-
-  /// Padding quantity
-  using Padding = MatrixShape<
-    0,
-    Policy::kElementsPerAccess>;
+  using Shape = Shape_;
+  using Element = Element_;
+  using Layout = layout::ColumnMajor;
+  static int const kAdvanceRank = AdvanceRank;
+  static int const kAlignment = Alignment;
 
-private:
+  using Index = typename Layout::Index;
+  using LongIndex = typename Layout::LongIndex;
 
-  //
-  // Data members
-  //
-
-  /// Internal pointer to memory
-  AccessType *pointer_;
+  using TensorRef = TensorRef<Element, Layout>;
+  using TensorCoord = typename Layout::TensorCoord;
 
-  /// Internal layout object
-  Layout layout_;
+  using ThreadMap = ThreadMap_;
 
-public:
+  /// Underlying iterator type
+  using UnderlyingIterator = RegularTileAccessIterator<
+      layout::PitchLinearShape<Shape::kRow, Shape::kColumn>, Element,
+      layout::PitchLinear,
+      (kAdvanceRank == 0 ? 0 : 1), 
+      ThreadMap_>;
 
-  /// Default constructor
-  CUTLASS_HOST_DEVICE
-  TileIteratorVoltaTensorOp(): pointer_(nullptr) { }
+  using AccessType = typename UnderlyingIterator::AccessType;
 
-  /// Constructor from TensorRef
-  CUTLASS_DEVICE
-  TileIteratorVoltaTensorOp(
-    TensorRef const &ref,
-    unsigned lane_id
-  ):
-    pointer_(reinterpret_cast<AccessType *>(ref.data())),
-    layout_(ref.stride()[0] / Policy::kElementsPerAccess) { 
+ private:
 
-    int quad_id = lane_id / Detail::kLanesInQuad;
-    int lane_in_quad = (lane_id % Detail::kLanesInQuad);
+  /// Underlying iterator
+  UnderlyingIterator iterator_;
 
-    int const kQuadRowDelta = 4;
-    int const kQuadColumnDelta = 2 * Policy::MmaIterations::kColumn;
+ public:
+  /// Construct a TileIterator with zero threadblock offset
+  CUTLASS_HOST_DEVICE
+  RegularTileAccessIterator(TensorRef ref,  ///< Pointer to start of tensor
+                            int thread_id   ///< ID of each participating thread
+                            )
+      : iterator_({ref.data(), ref.stride()}, thread_id) {}
 
-    int quad_row_offset = ((quad_id & 4) / 2 + (quad_id & 1)) * kQuadRowDelta;
-    int quad_column_offset = (quad_id & 2) / 2 * kQuadColumnDelta;
+  /// Overrides the internal iteration index
+  CUTLASS_HOST_DEVICE
+  void set_iteration_index(int index) { iterator_.set_iteration_index(index); }
 
-    int thread_row_offset = (lane_in_quad & 1);
-    int thread_column_offset = (lane_in_quad & 2) / 2;
+  /// Adds a pointer offset in units of Element
+  CUTLASS_HOST_DEVICE
+  void add_pointer_offset(LongIndex pointer_offset) {
+    iterator_.add_pointer_offset(pointer_offset);
+  }
 
-    int row = quad_row_offset + thread_row_offset;
-    int column = quad_column_offset + thread_column_offset;
+  /// Returns a pointer
+  CUTLASS_HOST_DEVICE
+  AccessType *get() const {
+    return reinterpret_cast<AccessType *>(iterator_.get());
+  }
 
-    pointer_ += layout_({row, column});
+  /// Adds a tile offset
+  CUTLASS_DEVICE
+  void add_tile_offset(TensorCoord const &coord) {
+    iterator_.add_tile_offset({coord.row(), coord.column()});
   }
 
-  /// Adds a pointer offset
+  /// Advances to the next tile in memory.
   CUTLASS_HOST_DEVICE
-  TileIteratorVoltaTensorOp & add_pointer_offset(Index pointer_offset) {
-    pointer_ += pointer_offset / Policy::kElementsPerAccess;
+  RegularTileAccessIterator &operator++() {
+    ++iterator_;
     return *this;
   }
 
-  ///< advances in units of whole tiles along the logical coordinate space of the tensor
+  /// Advances to the next tile in memory.
   CUTLASS_HOST_DEVICE
-  TileIteratorVoltaTensorOp & add_tile_offset(TensorCoord const &tile_offset) {
+  RegularTileAccessIterator operator++(int) {
+    RegularTileAccessIterator prev(*this);
+    ++iterator_;
 
-    pointer_ += layout_({
-      tile_offset.row() * Shape::kRow, 
-      tile_offset.column() * Shape::kColumn / Policy::kElementsPerAccess});
-
-    return *this;
+    return prev;
   }
+};
 
-  ///< advances in units of whole tiles along the logical coordinate space of the tensor
-  CUTLASS_HOST_DEVICE
-  TileIteratorVoltaTensorOp & operator+=(TensorCoord const &tile_offset) {
-    add_tile_offset(tile_offset);
-    return *this;
-  }
 
-  /// Store
-  CUTLASS_DEVICE
-  void store_with_pointer_offset(Fragment const &frag, Index pointer_offset) {
+////////////////////////////////////////////////////////////////////////////////
+
+/// Tile iterator specialized for row major layouts
+///
+///
+/// Satisfies: ForwardTileIteratorConcept |
+///            ReadableContiguousTileIteratorConcept |
+///            WriteableContiguousTileIteratorConcept
+///
+template <typename Shape_, typename Element_, int AdvanceRank,
+          typename ThreadMap_, int Alignment>
+class RegularTileAccessIterator<
+    Shape_, Element_,
+    layout::RowMajor,
+    AdvanceRank, ThreadMap_, Alignment> {
+ public:
+  static_assert(
+      AdvanceRank == 0 || AdvanceRank == 1,
+      "Specialization for pitch-linear iterator may along advance along the "
+      "contiguous(rank=0) or strided(rank=1) dimension.");
 
-    AccessType const *frag_ptr = reinterpret_cast<AccessType const *>(&frag);
+  using Shape = Shape_;
+  using Element = Element_;
+  using Layout = layout::RowMajor;
+  static int const kAdvanceRank = AdvanceRank;
+  static int const kAlignment = Alignment;
 
-    int const kAccessesPerRow = Policy::TileIterations::kColumn * Policy::MmaIterations::kColumn * 2;
+  using Index = typename Layout::Index;
+  using LongIndex = typename Layout::LongIndex;
 
-    CUTLASS_PRAGMA_UNROLL
-    for (int row_idx = 0; row_idx < Policy::kRowsPerMmaTile; ++row_idx) {
+  using TensorRef = TensorRef<Element, Layout>;
+  using TensorCoord = typename Layout::TensorCoord;
 
-      CUTLASS_PRAGMA_UNROLL
-      for (int access_idx = 0; access_idx < kAccessesPerRow; ++access_idx) {
+  using ThreadMap = ThreadMap_;
 
-        int frag_idx = row_idx * kAccessesPerRow + access_idx;
+  /// Underlying iterator type
+  using UnderlyingIterator = RegularTileAccessIterator<
+      layout::PitchLinearShape<Shape::kColumn, Shape::kRow>, Element,
+      layout::PitchLinear,
+      (kAdvanceRank == 0 ? 1 : 0), 
+      ThreadMap_>;
 
-        int ptr_column_offset = (access_idx & 1) * 2 + 
-          (access_idx & 2) * Policy::MmaIterations::kColumn * 2 + 
-          (access_idx & 4) * Policy::MmaIterations::kColumn * 2;
+  using AccessType = typename UnderlyingIterator::AccessType;
 
-        int ptr_row_offset = row_idx * 2;
+ private:
 
-        int ptr_offset = layout_({ptr_row_offset, ptr_column_offset}) + pointer_offset / Policy::kElementsPerAccess;
+  /// Underlying iterator
+  UnderlyingIterator iterator_;
 
-        pointer_[ptr_offset] = frag_ptr[frag_idx];
-      }
-    }
-  }
+ public:
+  /// Construct a TileIterator with zero threadblock offset
+  CUTLASS_HOST_DEVICE
+  RegularTileAccessIterator(TensorRef ref,  ///< Pointer to start of tensor
+                            int thread_id   ///< ID of each participating thread
+                            )
+      : iterator_({ref.data(), ref.stride()}, thread_id) {}
 
-  /// Store
+  /// Overrides the internal iteration index
   CUTLASS_HOST_DEVICE
-  void store(Fragment const &frag) {
-    store_with_pointer_offset(frag, 0);
+  void set_iteration_index(int index) { iterator_.set_iteration_index(index); }
+
+  /// Adds a pointer offset in units of Element
+  CUTLASS_HOST_DEVICE
+  void add_pointer_offset(LongIndex pointer_offset) {
+    iterator_.add_pointer_offset(pointer_offset);
   }
 
-  /// Load
+  /// Returns a pointer
   CUTLASS_HOST_DEVICE
-  void load_with_pointer_offset(Fragment const &frag, Index pointer_offset) {
+  AccessType *get() const {
+    return reinterpret_cast<AccessType *>(iterator_.get());
+  }
 
-    AccessType *frag_ptr = reinterpret_cast<AccessType *>(&frag);
+  /// Adds a tile offset
+  CUTLASS_DEVICE
+  void add_tile_offset(TensorCoord const &coord) {
+    iterator_.add_tile_offset({coord.column(), coord.row()});
+  }
 
-    assert(0); // TODO
+  /// Advances to the next tile in memory.
+  CUTLASS_HOST_DEVICE
+  RegularTileAccessIterator &operator++() {
+    ++iterator_;
+    return *this;
   }
 
-  /// Load
+  /// Advances to the next tile in memory.
   CUTLASS_HOST_DEVICE
-  void load(Fragment const &frag) {
-    load_with_pointer_offset(frag, 0);
+  RegularTileAccessIterator operator++(int) {
+    RegularTileAccessIterator prev(*this);
+    ++iterator_;
+
+    return prev;
   }
 };
 
-/////////////////////////////////////////////////////////////////////////////////////////////////
+////////////////////////////////////////////////////////////////////////////////
 
-} // namespace warp
-} // namespace epilogue
-} // namespace cutlass
+}  // namespace threadblock
+}  // namespace transform
+}  // namespace cutlass
 
-/////////////////////////////////////////////////////////////////////////////////////////////////
+////////////////////////////////////////////////////////////////////////////////
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/epilogue/warp/tile_iterator_wmma_tensor_op.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/epilogue/warp/tile_iterator_wmma_tensor_op.h`

 * *Files 3% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -203,14 +203,20 @@
   }
 
   /// Load
   CUTLASS_HOST_DEVICE
   void load(Fragment &frag) const {
     load_with_pointer_offset(frag, 0);
   }
+
+  
+  /// Set smem base address
+  CUTLASS_HOST_DEVICE
+  void set_smem_base_address(Index address) {
+  }
 };
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
 } // namespace warp
 } // namespace epilogue
 } // namespace cutlass
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/epilogue/warp/volta_tensor_op_policy.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/epilogue/warp/volta_tensor_op_policy.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/epilogue/warp/wmma_tensor_op_policy.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/epilogue/warp/wmma_tensor_op_policy.h`

 * *Files 2% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/fast_math.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/fast_math.h`

 * *Files 2% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -276,60 +276,113 @@
 ///
 struct FastDivmod {
 
   int divisor;
   unsigned int multiplier;
   unsigned int shift_right;
 
+  /// Find quotient and remainder using device-side intrinsics
+  CUTLASS_HOST_DEVICE
+  void fast_divmod(int& quotient, int& remainder, int dividend) const {
+
+#if defined(__CUDA_ARCH__)
+    // Use IMUL.HI if divisor != 1, else simply copy the source.
+    quotient = (divisor != 1) ? __umulhi(dividend, multiplier) >> shift_right : dividend;
+#else
+    quotient = int((divisor != 1) ? int(((int64_t)dividend * multiplier) >> 32) >> shift_right : dividend);
+#endif
+
+    // The remainder.
+    remainder = dividend - (quotient * divisor);
+  }
+
+  /// For long int input
+  CUTLASS_HOST_DEVICE
+  void fast_divmod(int& quotient, int64_t& remainder, int64_t dividend) const {
+
+#if defined(__CUDA_ARCH__)
+    // Use IMUL.HI if divisor != 1, else simply copy the source.
+    quotient = (divisor != 1) ? __umulhi(dividend, multiplier) >> shift_right : dividend;
+#else
+    quotient = int((divisor != 1) ? ((dividend * multiplier) >> 32) >> shift_right : dividend);
+#endif
+    // The remainder.
+    remainder = dividend - (quotient * divisor);
+  }
+
+
   /// Construct the FastDivmod object, in host code ideally.
   ///
   /// This precomputes some values based on the divisor and is computationally expensive.
 
   CUTLASS_HOST_DEVICE
   FastDivmod(): divisor(0), multiplier(0), shift_right(0) { }
 
   CUTLASS_HOST_DEVICE
-  FastDivmod(int divisor_): divisor(divisor_) {
-    find_divisor(multiplier, shift_right, divisor);
+  FastDivmod(int divisor): divisor(divisor) {
+
+    if (divisor != 1) {
+      unsigned int p = 31 + find_log2(divisor);
+      unsigned m = unsigned(((1ull << p) + unsigned(divisor) - 1) / unsigned(divisor));
+
+      multiplier = m;
+      shift_right = p - 32;
+    } else {
+      multiplier = 0;
+      shift_right = 0;
+    }
   }
 
   /// Computes integer division and modulus using precomputed values. This is computationally
   /// inexpensive.
   CUTLASS_HOST_DEVICE
   void operator()(int &quotient, int &remainder, int dividend) const {
-    fast_divmod(quotient, remainder, dividend, divisor, multiplier, shift_right);
+    fast_divmod(quotient, remainder, dividend);
   }
 
+  /// Computes integer division using precomputed values. This is computationally
+  /// inexpensive.
+  CUTLASS_HOST_DEVICE
+  int div(int dividend) const {
+    int quotient, remainder;
+    fast_divmod(quotient, remainder, dividend);
+    return quotient;
+  }
 
   /// Computes integer division and modulus using precomputed values. This is computationally
   /// inexpensive.
   ///
   /// Simply returns the quotient
   CUTLASS_HOST_DEVICE
   int divmod(int &remainder, int dividend) const {
     int quotient;
-    fast_divmod(quotient, remainder, dividend, divisor, multiplier, shift_right);
+    fast_divmod(quotient, remainder, dividend);
     return quotient;
   }
 
   /// Computes integer division and modulus using precomputed values. This is computationally
   /// inexpensive.
   CUTLASS_HOST_DEVICE
   void operator()(int &quotient, int64_t &remainder, int64_t dividend) const {
-    fast_divmod(quotient, remainder, dividend, divisor, multiplier, shift_right);
+    fast_divmod(quotient, remainder, dividend);
   }
 
   /// Computes integer division and modulus using precomputed values. This is computationally
   /// inexpensive.
   CUTLASS_HOST_DEVICE
   int divmod(int64_t &remainder, int64_t dividend) const {
     int quotient;
-    fast_divmod(quotient, remainder, dividend, divisor, multiplier, shift_right);
+    fast_divmod(quotient, remainder, dividend);
     return quotient;
   }
+
+  /// Returns the divisor when cast to integer
+  CUTLASS_HOST_DEVICE
+  operator int() const { return divisor; }
+
 };
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
 /// Object to encapsulate the fast division+modulus operation for 64b integer division.
 ///
 /// This object precomputes two values used to accelerate the computation and is best used
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/float8.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/float8.h`

 * *Files 0% similar despite different names*

```diff
@@ -395,41 +395,41 @@
     #if defined(CUDA_PTX_FP8_CVT_ENABLED)
         uint16_t tmp = 0;
         uint32_t bits = reinterpret_cast<uint16_t const &>(flt);
         asm volatile("cvt.rn.satfinite.e4m3x2.f16x2 %0, %1;" : "=h"(tmp) : "r"(bits));
 
         return *reinterpret_cast<float_e4m3_t *>(&tmp);
     #else
-        return bitcast(Base::convert_float_to_fp8(__half2float(flt)));
+        return bitcast(Base::convert_float_to_fp8(float(flt)));
     #endif
     }
 
     // E4M3 -> half
     CUTLASS_HOST_DEVICE
     static half to_half(float_e4m3_t const& x) {
     #if defined(CUDA_PTX_FP8_CVT_ENABLED)
         uint16_t bits = x.storage;
         uint32_t packed;
         asm volatile("cvt.rn.f16x2.e4m3x2 %0, %1;\n" : "=r"(packed) : "h"(bits));
 
         return reinterpret_cast<half2 const &>(packed).x;
     #else
-        return __float2half(Base::convert_fp8_to_float(x.storage));
+        return half(Base::convert_fp8_to_float(x.storage));
     #endif
     }
 
     // E4M3 -> Float
     CUTLASS_HOST_DEVICE
     static float to_float(float_e4m3_t const& x) {
     #if defined(CUDA_PTX_FP8_CVT_ENABLED)
         uint16_t bits = x.storage;
         uint32_t packed;
         asm volatile("cvt.rn.f16x2.e4m3x2 %0, %1;\n" : "=r"(packed) : "h"(bits));
 
-        return __half2float(reinterpret_cast<half2 const &>(packed).x);
+        return float(reinterpret_cast<half2 const &>(packed).x);
     #else
         return Base::convert_fp8_to_float(x.storage);
     #endif
     }
 
     //
     // Methods
@@ -605,41 +605,41 @@
     #if defined(CUDA_PTX_FP8_CVT_ENABLED)
         uint16_t tmp = 0;
         uint32_t bits = reinterpret_cast<uint16_t const &>(flt);
         asm volatile("cvt.rn.satfinite.e5m2x2.f16x2 %0, %1;" : "=h"(tmp) : "r"(bits));
 
         return *reinterpret_cast<float_e5m2_t *>(&tmp);
     #else
-        return bitcast(Base::convert_float_to_fp8(__half2float(flt)));
+        return bitcast(Base::convert_float_to_fp8(float(flt)));
     #endif
     }
 
     // E5M2 -> half
     CUTLASS_HOST_DEVICE
     static half to_half(float_e5m2_t const& x) {
     #if defined(CUDA_PTX_FP8_CVT_ENABLED)
         uint16_t bits = x.storage;
         uint32_t packed;
         asm volatile("cvt.rn.f16x2.e5m2x2 %0, %1;\n" : "=r"(packed) : "h"(bits));
 
         return reinterpret_cast<half2 const &>(packed).x;
     #else
-        return __float2half(Base::convert_fp8_to_float(x.storage));
+        return half(Base::convert_fp8_to_float(x.storage));
     #endif
     }
 
     // E5M2 -> Float
     CUTLASS_HOST_DEVICE
     static float to_float(float_e5m2_t const& x) {
     #if defined(CUDA_PTX_FP8_CVT_ENABLED)
         uint16_t bits = x.storage;
         uint32_t packed;
         asm volatile("cvt.rn.f16x2.e5m2x2 %0, %1;\n" : "=r"(packed) : "h"(bits));
 
-        return __half2float(reinterpret_cast<half2 const &>(packed).x);
+        return float(reinterpret_cast<half2 const &>(packed).x);
     #else
         return Base::convert_fp8_to_float(x.storage);
     #endif
     }
 
     //
     // Methods
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/floating_point_nvrtc.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/floating_point_nvrtc.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/functional.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/array.h`

 * *Files 12% similar despite different names*

```diff
@@ -1,9 +1,9 @@
-  /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+/***************************************************************************************************
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -25,556 +25,546 @@
  * SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER
  * CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,
  * OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
  * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
  *
  **************************************************************************************************/
 /*! \file
-    \brief Define basic numeric operators with specializations for Array<T, N>. SIMD-ize where possible.
-
-    This is inspired by the Standard Library's <functional> header.
+    \brief Statically sized array of elements that accommodates all CUTLASS-supported numeric types
+           and is safe to use in a union.
 */
 
 #pragma once
-
 #include "cutlass/cutlass.h"
+#include "cutlass/functional.h"
 #include "cutlass/numeric_types.h"
-#include "cutlass/complex.h"
-#include "cutlass/quaternion.h"
-#include "cutlass/array.h"
 #include "cutlass/half.h"
 
 namespace cutlass {
 
-/////////////////////////////////////////////////////////////////////////////////////////////////
+////////////////////////////////////////////////////////////////////////////////////////////////////
 
-template <typename T>
-struct absolute_value_op {
-  CUTLASS_HOST_DEVICE
-  T operator()(T lhs) const {
-    return abs(lhs);
-  }
-};
+/// Statically sized array for any data type
+template <
+  typename T,
+  int N,
+  bool RegisterSized = sizeof_bits<T>::value >= 32
+>
+class Array;
 
-template <typename T>
-struct plus {
-  CUTLASS_HOST_DEVICE
-  T operator()(T lhs, T const &rhs) const {
-    lhs += rhs;
-    return lhs;
-  }
-};
+////////////////////////////////////////////////////////////////////////////////////////////////////
 
-template <typename T>
-struct minus {
-  CUTLASS_HOST_DEVICE
-  T operator()(T lhs, T const &rhs) const {
-    lhs -= rhs;
-    return lhs;
-  }
+/// Defines the size of an Array<> in bits
+template <typename T, int N, bool RegisterSized>
+struct sizeof_bits<Array<T, N, RegisterSized> > {
+  static int const value =
+    int(sizeof(typename Array<T, N, RegisterSized>::Storage)) * 8 * int(Array<T, N, RegisterSized>::kStorageElements);
 };
 
-template <typename T>
-struct multiplies {
-  CUTLASS_HOST_DEVICE
-  T operator()(T lhs, T const &rhs) const {
-    lhs *= rhs;
-    return lhs;
-  }
-};
+////////////////////////////////////////////////////////////////////////////////////////////////////
 
-template <typename T>
-struct multiplies<Quaternion<T>> {
-  CUTLASS_HOST_DEVICE
-  Quaternion<T> operator()(Quaternion<T> lhs, Quaternion<T> const &rhs) const {
-    lhs = lhs * rhs;
-    return lhs;
-  }
-};
+/// Returns true if the argument is a power of 2
+CUTLASS_HOST_DEVICE
+constexpr bool ispow2(unsigned x) {
+  return x && (!(x & (x - 1)));
+}
 
-/// Squares with optional conversion
-template <typename T, typename Output = T>
-struct square {
-  CUTLASS_HOST_DEVICE
-  Output operator()(T lhs) const {
-    multiplies<Output> mul_op;
+////////////////////////////////////////////////////////////////////////////////////////////////////
 
-    Output y = Output(lhs);
-    return mul_op(y, y);
-  }
-};
+/// Returns the largest power of two not greater than the argument.
+CUTLASS_HOST_DEVICE
+constexpr unsigned floor_pow_2(unsigned x) {
+  return (x == 0 || ispow2(x)) ? x : ((floor_pow_2(x >> 1)) << 1);
+}
 
-/// Returns the magnitude squared of an element.
-template <typename T, typename Output = T>
-struct magnitude_squared {
-  CUTLASS_HOST_DEVICE
-  Output operator()(T lhs) const {
-    multiplies<Output> mul_op;
+////////////////////////////////////////////////////////////////////////////////////////////////////
 
-    Output y = Output(lhs);
-    return mul_op(y, y);
-  }
-};
+/// Statically sized array for any data type
+template <
+  typename T,
+  int N
+>
+class Array<T, N, true> {
+public:
 
-/// Squares with optional conversion
-template <typename T, typename Output>
-struct magnitude_squared<complex<T>, Output> {
-  CUTLASS_HOST_DEVICE
-  Output operator()(complex<T> lhs) const {
-    multiplies<Output> mul_op;
+  /// Storage type
+  using Storage = T;
 
-    Output y_r = Output(lhs.real());
-    Output y_i = Output(lhs.imag());
+  /// Element type
+  using Element = T;
 
-    return mul_op(y_r, y_r) + mul_op(y_i, y_i);
-  }
-};
+  /// Number of storage elements
+  //static std::size_t const kStorageElements = N;
+  static size_t const kStorageElements = N;
 
-/// Squares with optional conversion
-template <typename T, typename Output>
-struct magnitude_squared<Quaternion<T>, Output> {
-  CUTLASS_HOST_DEVICE
-  Output operator()(Quaternion<T> lhs) const {
-    multiplies<Output> mul_op;
+  /// Number of logical elements
+  static size_t const kElements = N;
 
-    Output y_w = Output(lhs.w());
-    Output y_x = Output(lhs.x());
-    Output y_y = Output(lhs.y());
-    Output y_z = Output(lhs.z());
+  //
+  // C++ standard members
+  //
 
-    return mul_op(y_w, y_w) + mul_op(y_x, y_x) + mul_op(y_y, y_y) + \
-           mul_op(y_z, y_z);
-  }
-};
+  typedef T value_type;
+  typedef size_t size_type;
+  typedef ptrdiff_t difference_type;
+  typedef value_type &reference;
+  typedef value_type const & const_reference;
+  typedef value_type *pointer;
+  typedef value_type const * const_pointer;
 
-/// Computes the square of a difference with optional conversion
-template <typename T, typename Output = T>
-struct square_difference {
-  CUTLASS_HOST_DEVICE
-  Output operator()(T lhs, T rhs) const {
-    multiplies<Output> mul_op;
+  //
+  // Iterators
+  //
 
-    Output y = Output(lhs) - Output(rhs);
-    return mul_op(y, y);
-  }
-};
+  /// Bidirectional iterator over elements
+  class iterator {
 
-/// Computes the square of a difference with optional conversion
-template <typename T, typename Output = T>
-struct magnitude_squared_difference {
-  CUTLASS_HOST_DEVICE
-  Output operator()(T lhs, T rhs) const {
-    multiplies<Output> mul_op;
+    /// Pointer to object
+    T *ptr_;
 
-    Output y = Output(lhs) - Output(rhs);
-    return mul_op(y, y);
-  }
-};
+  public:
 
-/// Computes the square of a difference with optional conversion
-template <typename T, typename Output>
-struct magnitude_squared_difference<complex<T>, Output> {
-  CUTLASS_HOST_DEVICE
-  Output operator()(complex<T> lhs, complex<T> rhs) const {
-    multiplies<Output> mul_op;
+    CUTLASS_HOST_DEVICE
+    iterator(): ptr_(nullptr) { }
 
-    Output y_r = Output(lhs.real()) - Output(rhs.real());
-    Output y_i = Output(lhs.imag()) - Output(rhs.imag());
+    CUTLASS_HOST_DEVICE
+    iterator(T *_ptr): ptr_(_ptr) { }
 
-    return mul_op(y_r, y_r) + mul_op(y_i, y_i);
-  }
-};
+    CUTLASS_HOST_DEVICE
+    iterator &operator++() {
+      ++ptr_;
+      return *this;
+    }
 
-template <typename T>
-struct divides {
-  CUTLASS_HOST_DEVICE
-  T operator()(T lhs, T const &rhs) const {
-    lhs /= rhs;
-    return lhs;
-  }
-};
+    CUTLASS_HOST_DEVICE
+    iterator &operator--() {
+      --ptr_;
+      return *this;
+    }
+
+    CUTLASS_HOST_DEVICE
+    iterator operator++(int) {
+      iterator ret(*this);
+      ++ptr_;
+      return ret;
+    }
+
+    CUTLASS_HOST_DEVICE
+    iterator operator--(int) {
+      iterator ret(*this);
+      --ptr_;
+      return ret;
+    }
+
+    CUTLASS_HOST_DEVICE
+    T &operator*() const {
+      return *ptr_;
+    }
+
+    CUTLASS_HOST_DEVICE
+    bool operator==(iterator const &other) const {
+      return ptr_ == other.ptr_;
+    }
+
+    CUTLASS_HOST_DEVICE
+    bool operator!=(iterator const &other) const {
+      return ptr_ != other.ptr_;
+    }
+  };
+
+  /// Bidirectional constant iterator over elements
+  class const_iterator {
+
+    /// Pointer to object
+    const T *ptr_;
+
+  public:
+
+    CUTLASS_HOST_DEVICE
+    const_iterator(): ptr_(nullptr) { }
+
+    CUTLASS_HOST_DEVICE
+    const_iterator(T const *_ptr): ptr_(_ptr) { }
+
+    CUTLASS_HOST_DEVICE
+    const_iterator &operator++() {
+      ++ptr_;
+      return *this;
+    }
+
+    CUTLASS_HOST_DEVICE
+    const_iterator &operator--() {
+      --ptr_;
+      return *this;
+    }
+
+    CUTLASS_HOST_DEVICE
+    const_iterator operator++(int) {
+      const_iterator ret(*this);
+      ++ptr_;
+      return ret;
+    }
+
+    CUTLASS_HOST_DEVICE
+    const_iterator operator--(int) {
+      const_iterator ret(*this);
+      --ptr_;
+      return ret;
+    }
+
+    CUTLASS_HOST_DEVICE
+    T const &operator*() const {
+      return *ptr_;
+    }
+
+    CUTLASS_HOST_DEVICE
+    bool operator==(const_iterator const &other) const {
+      return ptr_ == other.ptr_;
+    }
+
+    CUTLASS_HOST_DEVICE
+    bool operator!=(const_iterator const &other) const {
+      return ptr_ != other.ptr_;
+    }
+  };
+
+  /// Bidirectional iterator over elements
+  class reverse_iterator {
+
+    /// Pointer to object
+    T *ptr_;
+
+  public:
+
+    CUTLASS_HOST_DEVICE
+    reverse_iterator(): ptr_(nullptr) { }
+
+    CUTLASS_HOST_DEVICE
+    reverse_iterator(T *_ptr): ptr_(_ptr) { }
+
+    CUTLASS_HOST_DEVICE
+    reverse_iterator &operator++() {
+      --ptr_;
+      return *this;
+    }
+
+    CUTLASS_HOST_DEVICE
+    reverse_iterator &operator--() {
+      ++ptr_;
+      return *this;
+    }
+
+    CUTLASS_HOST_DEVICE
+    reverse_iterator operator++(int) {
+      iterator ret(*this);
+      --ptr_;
+      return ret;
+    }
+
+    CUTLASS_HOST_DEVICE
+    reverse_iterator operator--(int) {
+      iterator ret(*this);
+      ++ptr_;
+      return ret;
+    }
+
+    CUTLASS_HOST_DEVICE
+    T &operator*() const {
+      return *(ptr_ - 1);
+    }
+
+    CUTLASS_HOST_DEVICE
+    bool operator==(reverse_iterator const &other) const {
+      return ptr_ == other.ptr_;
+    }
+
+    CUTLASS_HOST_DEVICE
+    bool operator!=(reverse_iterator const &other) const {
+      return ptr_ != other.ptr_;
+    }
+  };
+
+  /// Bidirectional constant iterator over elements
+  class const_reverse_iterator {
+
+    /// Pointer to object
+    T const *ptr_;
+
+  public:
+
+    CUTLASS_HOST_DEVICE
+    const_reverse_iterator(): ptr_(nullptr) { }
+
+    CUTLASS_HOST_DEVICE
+    const_reverse_iterator(T const *_ptr): ptr_(_ptr) { }
+
+    CUTLASS_HOST_DEVICE
+    const_reverse_iterator &operator++() {
+      --ptr_;
+      return *this;
+    }
+
+    CUTLASS_HOST_DEVICE
+    const_reverse_iterator &operator--() {
+      ++ptr_;
+      return *this;
+    }
+
+    CUTLASS_HOST_DEVICE
+    const_reverse_iterator operator++(int) {
+      const_reverse_iterator ret(*this);
+      --ptr_;
+      return ret;
+    }
+
+    CUTLASS_HOST_DEVICE
+    const_reverse_iterator operator--(int) {
+      const_reverse_iterator ret(*this);
+      ++ptr_;
+      return ret;
+    }
+
+    CUTLASS_HOST_DEVICE
+    T const &operator*() const {
+      return *(ptr_ - 1);
+    }
+
+    CUTLASS_HOST_DEVICE
+    bool operator==(const_iterator const &other) const {
+      return ptr_ == other.ptr_;
+    }
+
+    CUTLASS_HOST_DEVICE
+    bool operator!=(const_iterator const &other) const {
+      return ptr_ != other.ptr_;
+    }
+  };
+
+private:
+
+  /// Internal storage
+  Storage storage[kElements];
 
+public:
 
-template <typename T>
-struct negate {
+  #if 0
   CUTLASS_HOST_DEVICE
-  T operator()(T lhs) const {
-    return -lhs;
-  }
-};
+  Array() { }
 
-/// Greater equal 
-template <typename T>
-struct greater_equal {
   CUTLASS_HOST_DEVICE
-  bool operator()(T const &lhs, T const &rhs) const {
-    return (lhs >= rhs);
+  Array(Array const &x) {
+    CUTLASS_PRAGMA_UNROLL
+    for (int i = 0; i < kElements; ++i) {
+      storage[i] = x.storage[i];
+    }
   }
-};
+  #endif
 
-/// Greater  
-template <typename T>
-struct greater {
+  /// Efficient clear method
   CUTLASS_HOST_DEVICE
-  bool operator()(T const &lhs, T const &rhs) const {
-    return (lhs > rhs);
+  void clear() {
+    fill(T(0));
   }
-};
 
-/// Less equal 
-template <typename T>
-struct less_equal {
   CUTLASS_HOST_DEVICE
-  bool operator()(T const &lhs, T const &rhs) const {
-    return (lhs <= rhs);
+  reference at(size_type pos) {
+    return reinterpret_cast<reference>(storage[pos]);
   }
-};
 
-/// Less  
-template <typename T>
-struct less {
   CUTLASS_HOST_DEVICE
-  bool operator()(T const &lhs, T const &rhs) const {
-    return (lhs < rhs);
+  const_reference at(size_type pos) const {
+    return reinterpret_cast<const_reference>(storage[pos]);
   }
-};
-
-template <typename T>
-struct maximum {
 
   CUTLASS_HOST_DEVICE
-  T operator()(T const &lhs, T const &rhs) const {
-    return (lhs < rhs ? rhs : lhs);
+  reference operator[](size_type pos) {
+    return reinterpret_cast<reference>(storage[pos]);
   }
-};
 
-template <>
-struct maximum<float> {
   CUTLASS_HOST_DEVICE
-  float operator()(float const &lhs, float const &rhs) const {
-    return fmaxf(lhs, rhs);
+  const_reference operator[](size_type pos) const {
+    return reinterpret_cast<const_reference>(storage[pos]);
   }
-};
-
-template <typename T>
-struct minimum {
 
   CUTLASS_HOST_DEVICE
-  T operator()(T const &lhs, T const &rhs) const {
-    return (rhs < lhs ? rhs : lhs);
+  reference front() {
+    return reinterpret_cast<reference>(storage[0]);
   }
-};
 
-template <>
-struct minimum<float> {
   CUTLASS_HOST_DEVICE
-  float operator()(float const &lhs, float const &rhs) const {
-    return fminf(lhs, rhs);
+  const_reference front() const {
+    return reinterpret_cast<const_reference>(storage[0]);
   }
-};
 
-/// Fused multiply-add
-template <typename A, typename B = A, typename C = A>
-struct multiply_add {
   CUTLASS_HOST_DEVICE
-  C operator()(A const &a, B const &b, C const &c) const {
-    return C(a) * C(b) + c;
+  reference back() {
+    return reinterpret_cast<reference>(storage[kStorageElements - 1]);
   }
-};
 
-/// Fused multiply-add
-template <typename A, typename B = A, typename C = A>
-struct multiply_add_relu0 {
   CUTLASS_HOST_DEVICE
-  C operator()(A const &a, B const &b, C const &c) const {
-    maximum<C> mx;
-    return mx(C(a) * C(b) + c, C(0));
+  const_reference back() const {
+    return reinterpret_cast<const_reference>(storage[kStorageElements - 1]);
   }
-};
 
-/// Fused multiply-add
-template <typename T>
-struct and_add {
   CUTLASS_HOST_DEVICE
-  T operator()(T const &a, T const &b, T const &c) const {
-    return ((a & b) + c);
+  pointer data() {
+    return reinterpret_cast<pointer>(storage);
   }
-};
 
-
-/// Fused multiply-add
-template <typename T>
-struct xor_add {
   CUTLASS_HOST_DEVICE
-  T operator()(T const &a, T const &b, T const &c) const {
-    return ((a ^ b) + c);
+  const_pointer data() const {
+    return reinterpret_cast<const_pointer>(storage);
+  }
+  
+  CUTLASS_HOST_DEVICE
+  pointer raw_data() {
+    return reinterpret_cast<pointer>(storage);
   }
-};
 
-template <typename T>
-struct conjugate {
   CUTLASS_HOST_DEVICE
-  T operator()(T const &a) const {
-    return a;
+  const_pointer raw_data() const {
+    return reinterpret_cast<const_pointer>(storage);
   }
-};
 
-/////////////////////////////////////////////////////////////////////////////////////////////////
 
-template <typename T>
-struct logical_and {
   CUTLASS_HOST_DEVICE
-  T operator()(T const &a, T const &b) const {
-    return ((a && b) ? T(1) : T());
+  constexpr bool empty() const {
+    return !kElements;
   }
-};
 
-template <typename T>
-struct logical_or {
   CUTLASS_HOST_DEVICE
-  T operator()(T const &a, T const &b) const {
-    return ((a || b) ? T(1) : T());
+  constexpr size_type size() const {
+    return kElements;
   }
-};
 
-template <typename T>
-struct logical_not {
   CUTLASS_HOST_DEVICE
-  T operator()(T const &a) const {
-    return T(!(a));
+  constexpr size_type max_size() const {
+    return kElements;
   }
-};
 
-/////////////////////////////////////////////////////////////////////////////////////////////////
-
-template <typename T>
-struct bit_and {
   CUTLASS_HOST_DEVICE
-  T operator()(T const &a, T const &b) const {
-    return a & b;
+  void fill(T const &value) {
+    CUTLASS_PRAGMA_UNROLL
+    for (int i = 0; i < kElements; ++i) {
+      storage[i] = static_cast<Storage>(value);
+    }
   }
-};
 
-template <typename T>
-struct bit_or {
   CUTLASS_HOST_DEVICE
-  T operator()(T const &a, T const &b) const {
-    return a | b;
+  iterator begin() {
+    return iterator(storage);
   }
-};
 
-template <typename T>
-struct bit_not {
   CUTLASS_HOST_DEVICE
-  T operator()(T const &a) const {
-    return ~a;
+  const_iterator begin() const {
+    return cbegin();
   }
-};
 
-template <typename T>
-struct bit_xor {
   CUTLASS_HOST_DEVICE
-  T operator()(T const &a, T const &b) const {
-    return a ^ b;
+  const_iterator cbegin() const {
+    return const_iterator(storage);
   }
-};
 
-/////////////////////////////////////////////////////////////////////////////////////////////////
-
-// Partial specializations for Arrays
-template <int N>
-struct bit_and<Array<uint1b_t, N>> {
   CUTLASS_HOST_DEVICE
-  Array<uint1b_t, N> operator()(Array<uint1b_t, N> const &a, Array<uint1b_t, N> const &b) const {
-    using ArrayType = Array<uint1b_t, N>;
-    using Storage = typename ArrayType::Storage;
-    ArrayType result;
-
-    Storage *result_data = result.raw_data();
-    Storage const *a_data = a.raw_data();
-    Storage const *b_data = b.raw_data();
-
-    CUTLASS_PRAGMA_UNROLL
-    for (int i = 0; i < ArrayType::kStorageElements; ++i) {
-      result_data[i] = (a_data[i] & b_data[i]);
-    }
-
-    return result;
+  iterator end() {
+    return iterator(reinterpret_cast<pointer>(storage + kStorageElements));
   }
-};
 
-// Partial specializations for Arrays
-template <int N>
-struct bit_or<Array<uint1b_t, N>> {
   CUTLASS_HOST_DEVICE
-  Array<uint1b_t, N> operator()(Array<uint1b_t, N> const &a, Array<uint1b_t, N> const &b) const {
-    using ArrayType = Array<uint1b_t, N>;
-    using Storage = typename ArrayType::Storage;
-    ArrayType result;
-
-    Storage *result_data = result.raw_data();
-    Storage const *a_data = a.raw_data();
-    Storage const *b_data = b.raw_data();
-
-    CUTLASS_PRAGMA_UNROLL
-    for (int i = 0; i < ArrayType::kStorageElements; ++i) {
-      result_data[i] = (a_data[i] | b_data[i]);
-    }
-
-    return result;
+  const_iterator end() const {
+    return cend();
   }
-};
 
-// Partial specializations for Arrays
-template <int N>
-struct bit_not<Array<uint1b_t, N>> {
   CUTLASS_HOST_DEVICE
-  Array<uint1b_t, N> operator()(Array<uint1b_t, N> const &a) const {
-    using ArrayType = Array<uint1b_t, N>;
-    using Storage = typename ArrayType::Storage;
-    ArrayType result;
-
-    Storage *result_data = result.raw_data();
-    Storage const *a_data = a.raw_data();
-
-    CUTLASS_PRAGMA_UNROLL
-    for (int i = 0; i < ArrayType::kStorageElements; ++i) {
-      result_data[i] = (~a_data[i]);
-    }
-
-    return result;
+  const_iterator cend() const {
+    return const_iterator(reinterpret_cast<const_pointer>(storage + kStorageElements));
   }
-};
 
-// Partial specializations for Arrays
-template <int N>
-struct bit_xor<Array<uint1b_t, N>> {
   CUTLASS_HOST_DEVICE
-  Array<uint1b_t, N> operator()(Array<uint1b_t, N> const &a, Array<uint1b_t, N> const &b) const {
-    using ArrayType = Array<uint1b_t, N>;
-    using Storage = typename ArrayType::Storage;
-    ArrayType result;
-
-    Storage *result_data = result.raw_data();
-    Storage const *a_data = a.raw_data();
-    Storage const *b_data = b.raw_data();
-
-    CUTLASS_PRAGMA_UNROLL
-    for (int i = 0; i < ArrayType::kStorageElements; ++i) {
-      result_data[i] = (a_data[i] ^ b_data[i]);
-    }
-
-    return result;
+  reverse_iterator rbegin() {
+    return reverse_iterator(reinterpret_cast<pointer>(storage + kStorageElements));
   }
-};
 
-/////////////////////////////////////////////////////////////////////////////////////////////////
-
-template <typename T>
-struct conjugate<complex<T>>  {
   CUTLASS_HOST_DEVICE
-  complex<T> operator()(complex<T> const &a) const {
-    return conj(a);
+  const_reverse_iterator rbegin() const {
+    return crbegin();
   }
-};
 
-template <typename T, int N>
-struct conjugate<Array<T, N> >  {
   CUTLASS_HOST_DEVICE
-  Array<T, N> operator()(Array<T, N> const &a) const {
-
-    conjugate<T> conj_op;
-
-    Array<T, N> ca;
-    CUTLASS_PRAGMA_UNROLL
-    for (int i = 0; i < N; ++i) {
-      ca[i] = conj_op(a[i]);
-    }
-    return ca;
+  const_reverse_iterator crbegin() const {
+    return const_reverse_iterator(reinterpret_cast<const_pointer>(storage + kStorageElements));
   }
-};
 
-/////////////////////////////////////////////////////////////////////////////////////////////////
-//
-// Partial specialization for complex<T> to target four scalar fused multiply-adds.
-//
-/////////////////////////////////////////////////////////////////////////////////////////////////
-
-/// Fused multiply-add
-template <typename T>
-struct multiply_add<complex<T>, complex<T>, complex<T>> {
   CUTLASS_HOST_DEVICE
-  complex<T> operator()(
-    complex<T> const &a, 
-    complex<T> const &b, 
-    complex<T> const &c) const {
-
-    T real = c.real();
-    T imag = c.imag();
-
-    real += a.real() * b.real();
-    real += -a.imag() * b.imag();
-    imag += a.real() * b.imag();
-    imag += a.imag () * b.real();
-
-    return complex<T>{
-      real,
-      imag
-    };
+  reverse_iterator rend() {
+    return reverse_iterator(reinterpret_cast<pointer>(storage));
   }
-};
 
-/// Fused multiply-add
-template <typename T>
-struct multiply_add<complex<T>, T, complex<T>> {
   CUTLASS_HOST_DEVICE
-  complex<T> operator()(
-    complex<T> const &a, 
-    T const &b, 
-    complex<T> const &c) const {
-
-    T real = c.real();
-    T imag = c.imag();
-
-    real += a.real() * b;
-    imag += a.imag () * b;
-
-    return complex<T>{
-      real,
-      imag
-    };
+  const_reverse_iterator rend() const {
+    return crend();
   }
-};
 
-/// Fused multiply-add
-template <typename T>
-struct multiply_add<T, complex<T>, complex<T>> {
   CUTLASS_HOST_DEVICE
-  complex<T> operator()(
-    T const &a, 
-    complex<T> const &b, 
-    complex<T> const &c) const {
-
-    T real = c.real();
-    T imag = c.imag();
-
-    real += a * b.real();
-    imag += a * b.imag();
-
-    return complex<T>{
-      real,
-      imag
-    };
+  const_reverse_iterator crend() const {
+    return const_reverse_iterator(reinterpret_cast<const_pointer>(storage));
   }
+
+  //
+  // Comparison operators
+  //
+
 };
 
+
+////////////////////////////////////////////////////////////////////////////////////////////////////
+// Factories
+////////////////////////////////////////////////////////////////////////////////////////////////////
+
+template <typename Element>
+CUTLASS_HOST_DEVICE
+Array<Element, 1> make_Array(Element x) {
+  Array<Element, 1> m;
+  m[0] = x;
+  return m;
+}
+
+template <typename Element>
+CUTLASS_HOST_DEVICE
+Array<Element, 2> make_Array(Element x, Element y) {
+  Array<Element, 2> m;
+  m[0] = x;
+  m[1] = y;
+  return m;
+}
+
+template <typename Element>
+CUTLASS_HOST_DEVICE
+Array<Element, 3> make_Array(Element x, Element y, Element z) {
+  Array<Element, 3> m;
+  m[0] = x;
+  m[1] = y;
+  m[2] = z;
+  return m;
+}
+
+template <typename Element>
+CUTLASS_HOST_DEVICE
+Array<Element, 4> make_Array(Element x, Element y, Element z, Element w) {
+  Array<Element, 4> m;
+  m[0] = x;
+  m[1] = y;
+  m[2] = z;
+  m[3] = w;
+  return m;
+}
+
+
 /////////////////////////////////////////////////////////////////////////////////////////////////
-//
-// Partial specializations for Array<T, N>
-//
+// functional.h numeric specializations
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
 template <typename T, int N>
 struct absolute_value_op< Array<T, N> > {
 
   CUTLASS_HOST_DEVICE
   Array<T, N> operator()(Array<T, N> const &lhs) const {
@@ -591,43 +581,43 @@
   }
 };
 
 template <typename T, int N>
 struct plus<Array<T, N>> {
   CUTLASS_HOST_DEVICE
   Array<T, N> operator()(Array<T, N> const &lhs, Array<T, N> const &rhs) const {
-    
+
     Array<T, N> result;
     plus<T> scalar_op;
 
     CUTLASS_PRAGMA_UNROLL
     for (int i = 0; i < N; ++i) {
       result[i] = scalar_op(lhs[i], rhs[i]);
     }
 
     return result;
   }
 
   CUTLASS_HOST_DEVICE
   Array<T, N> operator()(Array<T, N> const &lhs, T const &scalar) const {
-    
+
     Array<T, N> result;
     plus<T> scalar_op;
 
     CUTLASS_PRAGMA_UNROLL
     for (int i = 0; i < N; ++i) {
       result[i] = scalar_op(lhs[i], scalar);
     }
 
     return result;
   }
 
   CUTLASS_HOST_DEVICE
   Array<T, N> operator()( T const &scalar, Array<T, N> const &rhs) const {
-    
+
     Array<T, N> result;
     plus<T> scalar_op;
 
     CUTLASS_PRAGMA_UNROLL
     for (int i = 0; i < N; ++i) {
       result[i] = scalar_op(scalar, rhs[i]);
     }
@@ -636,43 +626,43 @@
   }
 };
 template <typename T, int N>
 struct minus<Array<T, N>> {
 
   CUTLASS_HOST_DEVICE
   Array<T, N> operator()(Array<T, N> const &lhs, Array<T, N> const &rhs) const {
-    
+
     Array<T, N> result;
     minus<T> scalar_op;
 
     CUTLASS_PRAGMA_UNROLL
     for (int i = 0; i < N; ++i) {
       result[i] = scalar_op(lhs[i], rhs[i]);
     }
 
     return result;
   }
 
   CUTLASS_HOST_DEVICE
   Array<T, N> operator()(Array<T, N> const &lhs, T const &scalar) const {
-    
+
     Array<T, N> result;
     minus<T> scalar_op;
 
     CUTLASS_PRAGMA_UNROLL
     for (int i = 0; i < N; ++i) {
       result[i] = scalar_op(lhs[i], scalar);
     }
 
     return result;
   }
 
   CUTLASS_HOST_DEVICE
   Array<T, N> operator()( T const &scalar, Array<T, N> const &rhs) const {
-    
+
     Array<T, N> result;
     minus<T> scalar_op;
 
     CUTLASS_PRAGMA_UNROLL
     for (int i = 0; i < N; ++i) {
       result[i] = scalar_op(scalar, rhs[i]);
     }
@@ -682,43 +672,43 @@
 };
 
 template <typename T, int N>
 struct multiplies<Array<T, N>> {
 
   CUTLASS_HOST_DEVICE
   Array<T, N> operator()(Array<T, N> const &lhs, Array<T, N> const &rhs) const {
-    
+
     Array<T, N> result;
     multiplies<T> scalar_op;
 
     CUTLASS_PRAGMA_UNROLL
     for (int i = 0; i < N; ++i) {
       result[i] = scalar_op(lhs[i], rhs[i]);
     }
 
     return result;
   }
 
   CUTLASS_HOST_DEVICE
   Array<T, N> operator()(Array<T, N> const &lhs, T const &scalar) const {
-    
+
     Array<T, N> result;
     multiplies<T> scalar_op;
 
     CUTLASS_PRAGMA_UNROLL
     for (int i = 0; i < N; ++i) {
       result[i] = scalar_op(lhs[i], scalar);
     }
 
     return result;
   }
 
   CUTLASS_HOST_DEVICE
   Array<T, N> operator()( T const &scalar, Array<T, N> const &rhs) const {
-    
+
     Array<T, N> result;
     multiplies<T> scalar_op;
 
     CUTLASS_PRAGMA_UNROLL
     for (int i = 0; i < N; ++i) {
       result[i] = scalar_op(scalar, rhs[i]);
     }
@@ -728,43 +718,43 @@
 };
 
 template <typename T, int N>
 struct divides<Array<T, N>> {
 
   CUTLASS_HOST_DEVICE
   Array<T, N> operator()(Array<T, N> const &lhs, Array<T, N> const &rhs) const {
-    
+
     Array<T, N> result;
     divides<T> scalar_op;
 
     CUTLASS_PRAGMA_UNROLL
     for (int i = 0; i < N; ++i) {
       result[i] = scalar_op(lhs[i], rhs[i]);
     }
 
     return result;
   }
 
   CUTLASS_HOST_DEVICE
   Array<T, N> operator()(Array<T, N> const &lhs, T const &scalar) const {
-    
+
     Array<T, N> result;
     divides<T> scalar_op;
 
     CUTLASS_PRAGMA_UNROLL
     for (int i = 0; i < N; ++i) {
       result[i] = scalar_op(lhs[i], scalar);
     }
 
     return result;
   }
 
   CUTLASS_HOST_DEVICE
   Array<T, N> operator()( T const &scalar, Array<T, N> const &rhs) const {
-    
+
     Array<T, N> result;
     divides<T> scalar_op;
 
     CUTLASS_PRAGMA_UNROLL
     for (int i = 0; i < N; ++i) {
       result[i] = scalar_op(scalar, rhs[i]);
     }
@@ -774,43 +764,43 @@
 };
 
 template <typename T, int N>
 struct maximum<Array<T, N>> {
 
   CUTLASS_HOST_DEVICE
   Array<T, N> operator()(Array<T, N> const &lhs, Array<T, N> const &rhs) const {
-    
+
     Array<T, N> result;
     maximum<T> scalar_op;
 
     CUTLASS_PRAGMA_UNROLL
     for (int i = 0; i < N; ++i) {
       result[i] = scalar_op(lhs[i], rhs[i]);
     }
 
     return result;
   }
 
   CUTLASS_HOST_DEVICE
   Array<T, N> operator()(Array<T, N> const &lhs, T const &scalar) const {
-    
+
     Array<T, N> result;
     maximum<T> scalar_op;
 
     CUTLASS_PRAGMA_UNROLL
     for (int i = 0; i < N; ++i) {
       result[i] = scalar_op(lhs[i], scalar);
     }
 
     return result;
   }
 
   CUTLASS_HOST_DEVICE
   Array<T, N> operator()( T const &scalar, Array<T, N> const &rhs) const {
-    
+
     Array<T, N> result;
     maximum<T> scalar_op;
 
     CUTLASS_PRAGMA_UNROLL
     for (int i = 0; i < N; ++i) {
       result[i] = scalar_op(scalar, rhs[i]);
     }
@@ -825,43 +815,43 @@
   CUTLASS_HOST_DEVICE
   static T scalar_op(T const &lhs, T const &rhs) {
     return (rhs < lhs ? rhs : lhs);
   }
 
   CUTLASS_HOST_DEVICE
   Array<T, N> operator()(Array<T, N> const &lhs, Array<T, N> const &rhs) const {
-    
+
     Array<T, N> result;
     minimum<T> scalar_op;
 
     CUTLASS_PRAGMA_UNROLL
     for (int i = 0; i < N; ++i) {
       result[i] = scalar_op(lhs[i], rhs[i]);
     }
 
     return result;
   }
 
   CUTLASS_HOST_DEVICE
   Array<T, N> operator()(Array<T, N> const &lhs, T const &scalar) const {
-    
+
     Array<T, N> result;
     minimum<T> scalar_op;
 
     CUTLASS_PRAGMA_UNROLL
     for (int i = 0; i < N; ++i) {
       result[i] = scalar_op(lhs[i], scalar);
     }
 
     return result;
   }
 
   CUTLASS_HOST_DEVICE
   Array<T, N> operator()( T const &scalar, Array<T, N> const &rhs) const {
-    
+
     Array<T, N> result;
     minimum<T> scalar_op;
 
     CUTLASS_PRAGMA_UNROLL
     for (int i = 0; i < N; ++i) {
       result[i] = scalar_op(scalar, rhs[i]);
     }
@@ -871,15 +861,15 @@
 };
 
 template <typename T, int N>
 struct negate<Array<T, N>> {
 
   CUTLASS_HOST_DEVICE
   Array<T, N> operator()(Array<T, N> const &lhs) const {
-    
+
     Array<T, N> result;
     negate<T> scalar_op;
 
     CUTLASS_PRAGMA_UNROLL
     for (int i = 0; i < N; ++i) {
       result[i] = scalar_op(lhs[i]);
     }
@@ -890,43 +880,43 @@
 
 /// Fused multiply-add
 template <typename T, int N>
 struct multiply_add<Array<T, N>, Array<T, N>, Array<T, N>> {
 
   CUTLASS_HOST_DEVICE
   Array<T, N> operator()(Array<T, N> const &a, Array<T, N> const &b, Array<T, N> const &c) const {
-    
+
     Array<T, N> result;
     multiply_add<T> scalar_op;
 
     CUTLASS_PRAGMA_UNROLL
     for (int i = 0; i < N; ++i) {
       result[i] = scalar_op(a[i], b[i], c[i]);
     }
 
     return result;
   }
 
   CUTLASS_HOST_DEVICE
   Array<T, N> operator()(Array<T, N> const &a, T const &scalar, Array<T, N> const &c) const {
-    
+
     Array<T, N> result;
     multiply_add<T> scalar_op;
 
     CUTLASS_PRAGMA_UNROLL
     for (int i = 0; i < N; ++i) {
       result[i] = scalar_op(a[i], scalar, c[i]);
     }
 
     return result;
   }
 
   CUTLASS_HOST_DEVICE
   Array<T, N> operator()(T const &scalar, Array<T, N> const &b, Array<T, N> const &c) const {
-    
+
     Array<T, N> result;
     multiply_add<T> scalar_op;
 
     CUTLASS_PRAGMA_UNROLL
     for (int i = 0; i < N; ++i) {
       result[i] = scalar_op(scalar, b[i], c[i]);
     }
@@ -937,62 +927,78 @@
 
 /// Fused multiply-add-relu0
 template <typename T, int N>
 struct multiply_add_relu0<Array<T, N>, Array<T, N>, Array<T, N>> {
 
   CUTLASS_HOST_DEVICE
   Array<T, N> operator()(Array<T, N> const &a, Array<T, N> const &b, Array<T, N> const &c) const {
-    
+
     Array<T, N> result;
     multiply_add<T> scalar_op;
     maximum<T> mx;
 
     CUTLASS_PRAGMA_UNROLL
     for (int i = 0; i < N; ++i) {
       result[i] = mx(scalar_op(a[i], b[i], c[i]), T(0));
     }
 
     return result;
   }
 
   CUTLASS_HOST_DEVICE
   Array<T, N> operator()(Array<T, N> const &a, T const &scalar, Array<T, N> const &c) const {
-    
+
     Array<T, N> result;
     multiply_add<T> scalar_op;
     maximum<T> mx;
 
     CUTLASS_PRAGMA_UNROLL
     for (int i = 0; i < N; ++i) {
       result[i] = mx(scalar_op(a[i], scalar, c[i]), T(0));
     }
 
     return result;
   }
 
   CUTLASS_HOST_DEVICE
   Array<T, N> operator()(T const &scalar, Array<T, N> const &b, Array<T, N> const &c) const {
-    
+
     Array<T, N> result;
     multiply_add<T> scalar_op;
     maximum<T> mx;
 
     CUTLASS_PRAGMA_UNROLL
     for (int i = 0; i < N; ++i) {
       result[i] = mx(scalar_op(scalar, b[i], c[i]), T(0));
     }
 
     return result;
   }
 };
 
+
+template <typename T, int N>
+struct conjugate<Array<T, N> >  {
+  CUTLASS_HOST_DEVICE
+  Array<T, N> operator()(Array<T, N> const &a) const {
+
+    conjugate<T> conj_op;
+
+    Array<T, N> ca;
+    CUTLASS_PRAGMA_UNROLL
+    for (int i = 0; i < N; ++i) {
+      ca[i] = conj_op(a[i]);
+    }
+    return ca;
+  }
+};
+
+
 /////////////////////////////////////////////////////////////////////////////////////////////////
-//
-// Partial specializations for Array<half_t, N> targeting SIMD instructions in device code.
-//
+// functional.h numeric specializations targeting SIMD instructions in device code.
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
 template <int N>
 struct plus<Array<half_t, N>> {
   CUTLASS_HOST_DEVICE
   Array<half_t, N> operator()(Array<half_t, N> const & lhs, Array<half_t, N> const &rhs) const {
     Array<half_t, N> result;
@@ -1240,15 +1246,15 @@
       result_ptr[i] = __hmul2(lhs_pair, rhs_ptr[i]);
     }
 
     if (N % 2) {
       __half const *b_residual_ptr = reinterpret_cast<__half const *>(&rhs);
 
       __half d_residual = __hmul(
-        reinterpret_cast<__half const &>(lhs), 
+        reinterpret_cast<__half const &>(lhs),
         b_residual_ptr[N - 1]);
 
       result[N - 1] = reinterpret_cast<half_t const &>(d_residual);
     }
 
     #else
 
@@ -1275,15 +1281,15 @@
       result_ptr[i] = __hmul2(lhs_ptr[i], rhs_pair);
     }
 
     if (N % 2) {
       __half const *a_residual_ptr = reinterpret_cast<__half const *>(&lhs);
 
       __half d_residual = __hmul(
-        a_residual_ptr[N - 1], 
+        a_residual_ptr[N - 1],
         reinterpret_cast<__half const &>(rhs));
 
       result[N - 1] = reinterpret_cast<half_t const &>(d_residual);
     }
 
     #else
 
@@ -1314,15 +1320,15 @@
     }
 
     if (N % 2) {
       __half const *a_residual_ptr = reinterpret_cast<__half const *>(&lhs);
       __half const *b_residual_ptr = reinterpret_cast<__half const *>(&rhs);
 
       __half d_residual = __hdiv(
-        a_residual_ptr[N - 1], 
+        a_residual_ptr[N - 1],
         b_residual_ptr[N - 1]);
 
       result[N - 1] = reinterpret_cast<half_t const &>(d_residual);
     }
 
     #else
 
@@ -1349,15 +1355,15 @@
       result_ptr[i] = __h2div(lhs_pair, rhs_ptr[i]);
     }
 
     if (N % 2) {
       __half const *b_residual_ptr = reinterpret_cast<__half const *>(&rhs);
 
       __half d_residual = __hdiv(
-        reinterpret_cast<__half const &>(lhs), 
+        reinterpret_cast<__half const &>(lhs),
         b_residual_ptr[N - 1]);
 
       result[N - 1] = reinterpret_cast<half_t const &>(d_residual);
     }
 
     #else
 
@@ -1384,15 +1390,15 @@
       result_ptr[i] = __h2div(lhs_ptr[i], rhs_pair);
     }
 
     if (N % 2) {
       __half const *a_residual_ptr = reinterpret_cast<__half const *>(&lhs);
 
       __half d_residual = __hdiv(
-        a_residual_ptr[N - 1], 
+        a_residual_ptr[N - 1],
         reinterpret_cast<__half const &>(rhs));
 
       result[N - 1] = reinterpret_cast<half_t const &>(d_residual);
     }
 
     #else
 
@@ -1441,18 +1447,18 @@
 
 /// Fused multiply-add
 template <int N>
 struct multiply_add<Array<half_t, N>, Array<half_t, N>, Array<half_t, N>> {
 
   CUTLASS_HOST_DEVICE
   Array<half_t, N> operator()(
-    Array<half_t, N> const &a, 
-    Array<half_t, N> const &b, 
+    Array<half_t, N> const &a,
+    Array<half_t, N> const &b,
     Array<half_t, N> const &c) const {
-    
+
     Array<half_t, N> result;
     #if defined(__CUDA_ARCH__) && (__CUDA_ARCH__ >= 530)
 
     __half2 *result_ptr = reinterpret_cast<__half2 *>(&result);
     __half2 const *a_ptr = reinterpret_cast<__half2 const *>(&a);
     __half2 const *b_ptr = reinterpret_cast<__half2 const *>(&b);
     __half2 const *c_ptr = reinterpret_cast<__half2 const *>(&c);
@@ -1465,16 +1471,16 @@
     if (N % 2) {
 
       __half const *a_residual_ptr = reinterpret_cast<__half const *>(&a);
       __half const *b_residual_ptr = reinterpret_cast<__half const *>(&b);
       __half const *c_residual_ptr = reinterpret_cast<__half const *>(&c);
 
       __half d_residual = __hfma(
-        a_residual_ptr[N - 1], 
-        b_residual_ptr[N - 1], 
+        a_residual_ptr[N - 1],
+        b_residual_ptr[N - 1],
         c_residual_ptr[N - 1]);
 
       result[N - 1] = reinterpret_cast<half_t const &>(d_residual);
     }
 
     #else
 
@@ -1487,18 +1493,18 @@
     #endif
 
     return result;
   }
 
   CUTLASS_HOST_DEVICE
   Array<half_t, N> operator()(
-    half_t const &a, 
-    Array<half_t, N> const &b, 
+    half_t const &a,
+    Array<half_t, N> const &b,
     Array<half_t, N> const &c) const {
-    
+
     Array<half_t, N> result;
     #if defined(__CUDA_ARCH__) && (__CUDA_ARCH__ >= 530)
 
     __half2 *result_ptr = reinterpret_cast<__half2 *>(&result);
     __half2 a_pair = __half2half2(reinterpret_cast<__half const &>(a));
     __half2 const *b_ptr = reinterpret_cast<__half2 const *>(&b);
     __half2 const *c_ptr = reinterpret_cast<__half2 const *>(&c);
@@ -1509,16 +1515,16 @@
     }
 
     if (N % 2) {
 
       __half const *b_residual_ptr = reinterpret_cast<__half const *>(&b);
       __half const *c_residual_ptr = reinterpret_cast<__half const *>(&c);
       __half d_residual = __hfma(
-        reinterpret_cast<__half const &>(a), 
-        b_residual_ptr[N - 1], 
+        reinterpret_cast<__half const &>(a),
+        b_residual_ptr[N - 1],
         c_residual_ptr[N - 1]);
 
       result[N - 1] = reinterpret_cast<half_t const &>(d_residual);
     }
 
     #else
 
@@ -1531,18 +1537,18 @@
     #endif
 
     return result;
   }
 
   CUTLASS_HOST_DEVICE
   Array<half_t, N> operator()(
-    Array<half_t, N> const &a, 
-    half_t const &b, 
+    Array<half_t, N> const &a,
+    half_t const &b,
     Array<half_t, N> const &c) const {
-    
+
     Array<half_t, N> result;
     #if defined(__CUDA_ARCH__) && (__CUDA_ARCH__ >= 530)
 
     __half2 *result_ptr = reinterpret_cast<__half2 *>(&result);
     __half2 const *a_ptr = reinterpret_cast<__half2 const *>(&a);
     __half2 b_pair = __half2half2(reinterpret_cast<__half const &>(b));
     __half2 const *c_ptr = reinterpret_cast<__half2 const *>(&c);
@@ -1554,16 +1560,16 @@
 
     if (N % 2) {
 
       __half const *a_residual_ptr = reinterpret_cast<__half const *>(&a);
       __half const *c_residual_ptr = reinterpret_cast<__half const *>(&c);
 
       __half d_residual = __hfma(
-        a_residual_ptr[N - 1], 
-        reinterpret_cast<__half const &>(b), 
+        a_residual_ptr[N - 1],
+        reinterpret_cast<__half const &>(b),
         c_residual_ptr[N - 1]);
 
       result[N - 1] = reinterpret_cast<half_t const &>(d_residual);
     }
 
     #else
 
@@ -1576,18 +1582,18 @@
     #endif
 
     return result;
   }
 
   CUTLASS_HOST_DEVICE
   Array<half_t, N> operator()(
-    Array<half_t, N> const &a, 
-    Array<half_t, N> const &b, 
+    Array<half_t, N> const &a,
+    Array<half_t, N> const &b,
     half_t const &c) const {
-    
+
     Array<half_t, N> result;
     #if defined(__CUDA_ARCH__) && (__CUDA_ARCH__ >= 530)
 
     __half2 *result_ptr = reinterpret_cast<__half2 *>(&result);
     __half2 const *a_ptr = reinterpret_cast<__half2 const *>(&a);
     __half2 const *b_ptr = reinterpret_cast<__half2 const *>(&b);
     __half2 c_pair = __half2half2(reinterpret_cast<__half const &>(c));
@@ -1599,16 +1605,16 @@
 
     if (N % 2) {
 
       __half const *a_residual_ptr = reinterpret_cast<__half const *>(&a);
       __half const *b_residual_ptr = reinterpret_cast<__half const *>(&b);
 
       __half d_residual = __hfma(
-        a_residual_ptr[N - 1], 
-        b_residual_ptr[N - 1], 
+        a_residual_ptr[N - 1],
+        b_residual_ptr[N - 1],
         reinterpret_cast<__half const &>(c));
 
       result[N - 1] = reinterpret_cast<half_t const &>(d_residual);
     }
 
     #else
 
@@ -1626,18 +1632,18 @@
 
 /// Fused multiply-add-relu0
 template <int N>
 struct multiply_add_relu0<Array<half_t, N>, Array<half_t, N>, Array<half_t, N>> {
 
   CUTLASS_HOST_DEVICE
   Array<half_t, N> operator()(
-    Array<half_t, N> const &a, 
-    Array<half_t, N> const &b, 
+    Array<half_t, N> const &a,
+    Array<half_t, N> const &b,
     Array<half_t, N> const &c) const {
-    
+
     Array<half_t, N> result;
     #if defined(__CUDA_ARCH__) && (__CUDA_ARCH__ >= 800)
 
     __half2 *result_ptr = reinterpret_cast<__half2 *>(&result);
     __half2 const *a_ptr = reinterpret_cast<__half2 const *>(&a);
     __half2 const *b_ptr = reinterpret_cast<__half2 const *>(&b);
     __half2 const *c_ptr = reinterpret_cast<__half2 const *>(&c);
@@ -1650,16 +1656,16 @@
     if (N % 2) {
 
       __half const *a_residual_ptr = reinterpret_cast<__half const *>(&a);
       __half const *b_residual_ptr = reinterpret_cast<__half const *>(&b);
       __half const *c_residual_ptr = reinterpret_cast<__half const *>(&c);
 
       __half d_residual = __hfma_relu(
-        a_residual_ptr[N - 1], 
-        b_residual_ptr[N - 1], 
+        a_residual_ptr[N - 1],
+        b_residual_ptr[N - 1],
         c_residual_ptr[N - 1]);
 
       result[N - 1] = reinterpret_cast<half_t const &>(d_residual);
     }
 
     #else
 
@@ -1673,18 +1679,18 @@
     #endif
 
     return result;
   }
 
   CUTLASS_HOST_DEVICE
   Array<half_t, N> operator()(
-    half_t const &a, 
-    Array<half_t, N> const &b, 
+    half_t const &a,
+    Array<half_t, N> const &b,
     Array<half_t, N> const &c) const {
-    
+
     Array<half_t, N> result;
     #if defined(__CUDA_ARCH__) && (__CUDA_ARCH__ >= 800)
 
     __half2 *result_ptr = reinterpret_cast<__half2 *>(&result);
     __half2 a_pair = __half2half2(reinterpret_cast<__half const &>(a));
     __half2 const *b_ptr = reinterpret_cast<__half2 const *>(&b);
     __half2 const *c_ptr = reinterpret_cast<__half2 const *>(&c);
@@ -1695,16 +1701,16 @@
     }
 
     if (N % 2) {
 
       __half const *b_residual_ptr = reinterpret_cast<__half const *>(&b);
       __half const *c_residual_ptr = reinterpret_cast<__half const *>(&c);
       __half d_residual = __hfma_relu(
-        reinterpret_cast<__half const &>(a), 
-        b_residual_ptr[N - 1], 
+        reinterpret_cast<__half const &>(a),
+        b_residual_ptr[N - 1],
         c_residual_ptr[N - 1]);
 
       result[N - 1] = reinterpret_cast<half_t const &>(d_residual);
     }
 
     #else
 
@@ -1718,18 +1724,18 @@
     #endif
 
     return result;
   }
 
   CUTLASS_HOST_DEVICE
   Array<half_t, N> operator()(
-    Array<half_t, N> const &a, 
-    half_t const &b, 
+    Array<half_t, N> const &a,
+    half_t const &b,
     Array<half_t, N> const &c) const {
-    
+
     Array<half_t, N> result;
     #if defined(__CUDA_ARCH__) && (__CUDA_ARCH__ >= 800)
 
     __half2 *result_ptr = reinterpret_cast<__half2 *>(&result);
     __half2 const *a_ptr = reinterpret_cast<__half2 const *>(&a);
     __half2 b_pair = __half2half2(reinterpret_cast<__half const &>(b));
     __half2 const *c_ptr = reinterpret_cast<__half2 const *>(&c);
@@ -1741,16 +1747,16 @@
 
     if (N % 2) {
 
       __half const *a_residual_ptr = reinterpret_cast<__half const *>(&a);
       __half const *c_residual_ptr = reinterpret_cast<__half const *>(&c);
 
       __half d_residual = __hfma_relu(
-        a_residual_ptr[N - 1], 
-        reinterpret_cast<__half const &>(b), 
+        a_residual_ptr[N - 1],
+        reinterpret_cast<__half const &>(b),
         c_residual_ptr[N - 1]);
 
       result[N - 1] = reinterpret_cast<half_t const &>(d_residual);
     }
 
     #else
 
@@ -1764,18 +1770,18 @@
     #endif
 
     return result;
   }
 
   CUTLASS_HOST_DEVICE
   Array<half_t, N> operator()(
-    Array<half_t, N> const &a, 
-    Array<half_t, N> const &b, 
+    Array<half_t, N> const &a,
+    Array<half_t, N> const &b,
     half_t const &c) const {
-    
+
     Array<half_t, N> result;
     #if defined(__CUDA_ARCH__) && (__CUDA_ARCH__ >= 800)
 
     __half2 *result_ptr = reinterpret_cast<__half2 *>(&result);
     __half2 const *a_ptr = reinterpret_cast<__half2 const *>(&a);
     __half2 const *b_ptr = reinterpret_cast<__half2 const *>(&b);
     __half2 c_pair = __half2half2(reinterpret_cast<__half const &>(c));
@@ -1787,16 +1793,16 @@
 
     if (N % 2) {
 
       __half const *a_residual_ptr = reinterpret_cast<__half const *>(&a);
       __half const *b_residual_ptr = reinterpret_cast<__half const *>(&b);
 
       __half d_residual = __hfma_relu(
-        a_residual_ptr[N - 1], 
-        b_residual_ptr[N - 1], 
+        a_residual_ptr[N - 1],
+        b_residual_ptr[N - 1],
         reinterpret_cast<__half const &>(c));
 
       result[N - 1] = reinterpret_cast<half_t const &>(d_residual);
     }
 
     #else
 
@@ -1830,15 +1836,15 @@
     }
 
     if (N % 2) {
       __half const *a_residual_ptr = reinterpret_cast<__half const *>(&lhs);
       __half const *b_residual_ptr = reinterpret_cast<__half const *>(&rhs);
 
       __half d_residual = __hmin(
-        a_residual_ptr[N - 1], 
+        a_residual_ptr[N - 1],
         b_residual_ptr[N - 1]);
 
       result[N - 1] = reinterpret_cast<half_t const &>(d_residual);
     }
 
     #else
 
@@ -1865,15 +1871,15 @@
       result_ptr[i] = __hmin2(lhs_pair, rhs_ptr[i]);
     }
 
     if (N % 2) {
       __half const *b_residual_ptr = reinterpret_cast<__half const *>(&rhs);
 
       __half d_residual = __hmin(
-        reinterpret_cast<__half const &>(lhs), 
+        reinterpret_cast<__half const &>(lhs),
         b_residual_ptr[N - 1]);
 
       result[N - 1] = reinterpret_cast<half_t const &>(d_residual);
     }
 
     #else
 
@@ -1900,15 +1906,15 @@
       result_ptr[i] = __hmin2(lhs_ptr[i], rhs_pair);
     }
 
     if (N % 2) {
       __half const *a_residual_ptr = reinterpret_cast<__half const *>(&lhs);
 
       __half d_residual = __hmin(
-        a_residual_ptr[N - 1], 
+        a_residual_ptr[N - 1],
         reinterpret_cast<__half const &>(rhs));
 
       result[N - 1] = reinterpret_cast<half_t const &>(d_residual);
     }
 
     #else
 
@@ -1939,15 +1945,15 @@
     }
 
     if (N % 2) {
       __half const *a_residual_ptr = reinterpret_cast<__half const *>(&lhs);
       __half const *b_residual_ptr = reinterpret_cast<__half const *>(&rhs);
 
       __half d_residual = __hmax(
-        a_residual_ptr[N - 1], 
+        a_residual_ptr[N - 1],
         b_residual_ptr[N - 1]);
 
       result[N - 1] = reinterpret_cast<half_t const &>(d_residual);
     }
 
     #else
 
@@ -1974,15 +1980,15 @@
       result_ptr[i] = __hmax2(lhs_pair, rhs_ptr[i]);
     }
 
     if (N % 2) {
       __half const *b_residual_ptr = reinterpret_cast<__half const *>(&rhs);
 
       __half d_residual = __hmax(
-        reinterpret_cast<__half const &>(lhs), 
+        reinterpret_cast<__half const &>(lhs),
         b_residual_ptr[N - 1]);
 
       result[N - 1] = reinterpret_cast<half_t const &>(d_residual);
     }
 
     #else
 
@@ -2009,15 +2015,15 @@
       result_ptr[i] = __hmax2(lhs_ptr[i], rhs_pair);
     }
 
     if (N % 2) {
       __half const *a_residual_ptr = reinterpret_cast<__half const *>(&lhs);
 
       __half d_residual = __hmax(
-        a_residual_ptr[N - 1], 
+        a_residual_ptr[N - 1],
         reinterpret_cast<__half const &>(rhs));
 
       result[N - 1] = reinterpret_cast<half_t const &>(d_residual);
     }
 
     #else
 
@@ -2027,51 +2033,49 @@
     }
     #endif
 
     return result;
   }
 };
 
-/////////////////////////////////////////////////////////////////////////////////////////////////
-
 /// Fused multiply-add
 template <int N>
 struct multiply_add<Array<bfloat16_t, N>, Array<bfloat16_t, N>, Array<bfloat16_t, N>> {
 
   CUTLASS_HOST_DEVICE
   Array<bfloat16_t, N> operator()(
-    Array<bfloat16_t, N> const &a, 
-    Array<bfloat16_t, N> const &b, 
+    Array<bfloat16_t, N> const &a,
+    Array<bfloat16_t, N> const &b,
     Array<bfloat16_t, N> const &c) const {
-    
+
     Array<bfloat16_t, N> result;
     #if defined(__CUDA_ARCH__) && (__CUDA_ARCH__ >= 800)
 
     unsigned *result_ptr = reinterpret_cast<unsigned *>(&result);
     unsigned const *a_ptr = reinterpret_cast<unsigned const *>(&a);
     unsigned const *b_ptr = reinterpret_cast<unsigned const *>(&b);
     unsigned const *c_ptr = reinterpret_cast<unsigned const *>(&c);
 
     CUTLASS_PRAGMA_UNROLL
     for (int i = 0; i < N / 2; ++i) {
-      asm ("fma.rn.bf16x2 %0, %1, %2, %3;\n" 
-        : "=r"(result_ptr[i]) 
+      asm ("fma.rn.bf16x2 %0, %1, %2, %3;\n"
+        : "=r"(result_ptr[i])
         : "r"(a_ptr[i]), "r"(b_ptr[i]), "r"(c_ptr[i])
       );
     }
 
     if (N % 2) {
 
       uint16_t *result_ptr = reinterpret_cast<uint16_t *>(&result);
       uint16_t const *a_residual_ptr = reinterpret_cast<uint16_t const *>(&a);
       uint16_t const *b_residual_ptr = reinterpret_cast<uint16_t const *>(&b);
       uint16_t const *c_residual_ptr = reinterpret_cast<uint16_t const *>(&c);
 
-      asm ("fma.rn.bf16 %0, %1, %2, %3;\n" 
-        : "=h"(result_ptr[N - 1]) 
+      asm ("fma.rn.bf16 %0, %1, %2, %3;\n"
+        : "=h"(result_ptr[N - 1])
         : "h"(a_residual_ptr[N - 1]), "h"(b_residual_ptr[N - 1]), "h"(c_residual_ptr[N - 1])
       );
     }
 
     #else
 
     multiply_add<bfloat16_t> op;
@@ -2083,46 +2087,46 @@
     #endif
 
     return result;
   }
 
   CUTLASS_HOST_DEVICE
   Array<bfloat16_t, N> operator()(
-    bfloat16_t const &a, 
-    Array<bfloat16_t, N> const &b, 
+    bfloat16_t const &a,
+    Array<bfloat16_t, N> const &b,
     Array<bfloat16_t, N> const &c) const {
-    
+
     Array<bfloat16_t, N> result;
     #if defined(__CUDA_ARCH__) && (__CUDA_ARCH__ >= 800)
 
     unsigned *result_ptr = reinterpret_cast<unsigned *>(&result);
 
     unsigned const *b_ptr = reinterpret_cast<unsigned const *>(&b);
     unsigned const *c_ptr = reinterpret_cast<unsigned const *>(&c);
 
     unsigned a_packed = static_cast<unsigned>(a.raw());
     a_packed = (a_packed | (a_packed << 16));
 
     CUTLASS_PRAGMA_UNROLL
     for (int i = 0; i < N / 2; ++i) {
-      asm ("fma.rn.bf16x2 %0, %1, %2, %3;\n" 
-        : "=r"(result_ptr[i]) 
+      asm ("fma.rn.bf16x2 %0, %1, %2, %3;\n"
+        : "=r"(result_ptr[i])
         : "r"(a_packed), "r"(b_ptr[i]), "r"(c_ptr[i])
       );
     }
 
     if (N % 2) {
 
       uint16_t *result_ptr = reinterpret_cast<uint16_t *>(&result);
       uint16_t const *a_residual_ptr = reinterpret_cast<uint16_t const *>(&a);
       uint16_t const *b_residual_ptr = reinterpret_cast<uint16_t const *>(&b);
       uint16_t const *c_residual_ptr = reinterpret_cast<uint16_t const *>(&c);
 
-      asm ("fma.rn.bf16 %0, %1, %2, %3;\n" 
-        : "=h"(result_ptr[N - 1]) 
+      asm ("fma.rn.bf16 %0, %1, %2, %3;\n"
+        : "=h"(result_ptr[N - 1])
         : "h"(a_residual_ptr[0]), "h"(b_residual_ptr[N - 1]), "h"(c_residual_ptr[N - 1])
       );
     }
 
     #else
 
     multiply_add<bfloat16_t> op;
@@ -2134,46 +2138,46 @@
     #endif
 
     return result;
   }
 
   CUTLASS_HOST_DEVICE
   Array<bfloat16_t, N> operator()(
-    Array<bfloat16_t, N> const &a, 
-    bfloat16_t const &b, 
+    Array<bfloat16_t, N> const &a,
+    bfloat16_t const &b,
     Array<bfloat16_t, N> const &c) const {
-    
+
     Array<bfloat16_t, N> result;
     #if defined(__CUDA_ARCH__) && (__CUDA_ARCH__ >= 800)
 
     unsigned *result_ptr = reinterpret_cast<unsigned *>(&result);
-    
+
     unsigned const *a_ptr = reinterpret_cast<unsigned const *>(&a);
     unsigned const *c_ptr = reinterpret_cast<unsigned const *>(&c);
 
     unsigned b_packed = static_cast<unsigned>(b.raw());
     b_packed = (b_packed | (b_packed << 16));
 
     CUTLASS_PRAGMA_UNROLL
     for (int i = 0; i < N / 2; ++i) {
-      asm ("fma.rn.bf16x2 %0, %1, %2, %3;\n" 
-        : "=r"(result_ptr[i]) 
+      asm ("fma.rn.bf16x2 %0, %1, %2, %3;\n"
+        : "=r"(result_ptr[i])
         : "r"(a_ptr[i]), "r"(b_packed), "r"(c_ptr[i])
       );
     }
 
     if (N % 2) {
 
       uint16_t *result_ptr = reinterpret_cast<uint16_t *>(&result);
       uint16_t const *a_residual_ptr = reinterpret_cast<uint16_t const *>(&a);
       uint16_t const *b_residual_ptr = reinterpret_cast<uint16_t const *>(&b);
       uint16_t const *c_residual_ptr = reinterpret_cast<uint16_t const *>(&c);
 
-      asm ("fma.rn.bf16 %0, %1, %2, %3;\n" 
-        : "=h"(result_ptr[N - 1]) 
+      asm ("fma.rn.bf16 %0, %1, %2, %3;\n"
+        : "=h"(result_ptr[N - 1])
         : "h"(a_residual_ptr[N - 1]), "h"(b_residual_ptr[0]), "h"(c_residual_ptr[N - 1])
       );
     }
 
     #else
 
     multiply_add<bfloat16_t> op;
@@ -2185,46 +2189,46 @@
     #endif
 
     return result;
   }
 
   CUTLASS_HOST_DEVICE
   Array<bfloat16_t, N> operator()(
-    Array<bfloat16_t, N> const &a, 
-    Array<bfloat16_t, N> const &b, 
+    Array<bfloat16_t, N> const &a,
+    Array<bfloat16_t, N> const &b,
     bfloat16_t const &c) const {
-    
+
     Array<bfloat16_t, N> result;
     #if defined(__CUDA_ARCH__) && (__CUDA_ARCH__ >= 800)
 
     unsigned *result_ptr = reinterpret_cast<unsigned *>(&result);
-    
+
     unsigned const *a_ptr = reinterpret_cast<unsigned const *>(&a);
     unsigned const *b_ptr = reinterpret_cast<unsigned const *>(&b);
 
     unsigned c_packed = static_cast<unsigned>(c.raw());
     c_packed = (c_packed | (c_packed << 16));
 
     CUTLASS_PRAGMA_UNROLL
     for (int i = 0; i < N / 2; ++i) {
-      asm ("fma.rn.bf16x2 %0, %1, %2, %3;\n" 
-        : "=r"(result_ptr[i]) 
+      asm ("fma.rn.bf16x2 %0, %1, %2, %3;\n"
+        : "=r"(result_ptr[i])
         : "r"(a_ptr[i]), "r"(b_ptr[i]), "r"(c_packed)
       );
     }
 
     if (N % 2) {
 
       uint16_t *result_ptr = reinterpret_cast<uint16_t *>(&result);
       uint16_t const *a_residual_ptr = reinterpret_cast<uint16_t const *>(&a);
       uint16_t const *b_residual_ptr = reinterpret_cast<uint16_t const *>(&b);
       uint16_t const *c_residual_ptr = reinterpret_cast<uint16_t const *>(&c);
 
-      asm ("fma.rn.bf16 %0, %1, %2, %3;\n" 
-        : "=h"(result_ptr[N - 1]) 
+      asm ("fma.rn.bf16 %0, %1, %2, %3;\n"
+        : "=h"(result_ptr[N - 1])
         : "h"(a_residual_ptr[N - 1]), "h"(b_residual_ptr[N - 1]), "h"(c_residual_ptr[0])
       );
     }
 
     #else
 
     multiply_add<bfloat16_t> op;
@@ -2235,17 +2239,110 @@
     }
     #endif
 
     return result;
   }
 };
 
-/////////////////////////////////////////////////////////////////////////////////////////////////
+
+/// bit_and
+template <int N>
+struct bit_and<Array<uint1b_t, N>> {
+  CUTLASS_HOST_DEVICE
+  Array<uint1b_t, N> operator()(Array<uint1b_t, N> const &a, Array<uint1b_t, N> const &b) const {
+    using ArrayType = Array<uint1b_t, N>;
+    using Storage = typename ArrayType::Storage;
+    ArrayType result;
+
+    Storage *result_data = result.raw_data();
+    Storage const *a_data = a.raw_data();
+    Storage const *b_data = b.raw_data();
+
+    CUTLASS_PRAGMA_UNROLL
+    for (int i = 0; i < ArrayType::kStorageElements; ++i) {
+      result_data[i] = (a_data[i] & b_data[i]);
+    }
+
+    return result;
+  }
+};
+
+
+/// bit_or
+template <int N>
+struct bit_or<Array<uint1b_t, N>> {
+  CUTLASS_HOST_DEVICE
+  Array<uint1b_t, N> operator()(Array<uint1b_t, N> const &a, Array<uint1b_t, N> const &b) const {
+    using ArrayType = Array<uint1b_t, N>;
+    using Storage = typename ArrayType::Storage;
+    ArrayType result;
+
+    Storage *result_data = result.raw_data();
+    Storage const *a_data = a.raw_data();
+    Storage const *b_data = b.raw_data();
+
+    CUTLASS_PRAGMA_UNROLL
+    for (int i = 0; i < ArrayType::kStorageElements; ++i) {
+      result_data[i] = (a_data[i] | b_data[i]);
+    }
+
+    return result;
+  }
+};
+
+
+/// bit_not
+template <int N>
+struct bit_not<Array<uint1b_t, N>> {
+  CUTLASS_HOST_DEVICE
+  Array<uint1b_t, N> operator()(Array<uint1b_t, N> const &a) const {
+    using ArrayType = Array<uint1b_t, N>;
+    using Storage = typename ArrayType::Storage;
+    ArrayType result;
+
+    Storage *result_data = result.raw_data();
+    Storage const *a_data = a.raw_data();
+
+    CUTLASS_PRAGMA_UNROLL
+    for (int i = 0; i < ArrayType::kStorageElements; ++i) {
+      result_data[i] = (~a_data[i]);
+    }
+
+    return result;
+  }
+};
+
+
+/// bit_xor
+template <int N>
+struct bit_xor<Array<uint1b_t, N>> {
+  CUTLASS_HOST_DEVICE
+  Array<uint1b_t, N> operator()(Array<uint1b_t, N> const &a, Array<uint1b_t, N> const &b) const {
+    using ArrayType = Array<uint1b_t, N>;
+    using Storage = typename ArrayType::Storage;
+    ArrayType result;
+
+    Storage *result_data = result.raw_data();
+    Storage const *a_data = a.raw_data();
+    Storage const *b_data = b.raw_data();
+
+    CUTLASS_PRAGMA_UNROLL
+    for (int i = 0; i < ArrayType::kStorageElements; ++i) {
+      result_data[i] = (a_data[i] ^ b_data[i]);
+    }
+
+    return result;
+  }
+};
 
 
+/////////////////////////////////////////////////////////////////////////////////////////////////
+// Operator overloads
+/////////////////////////////////////////////////////////////////////////////////////////////////
+
 template <typename T, int N>
 CUTLASS_HOST_DEVICE
 Array<T, N> operator+(Array<T, N> const &lhs, Array<T, N> const &rhs) {
   plus<Array<T, N>> op;
   return op(lhs, rhs);
 }
 
@@ -2316,57 +2413,45 @@
 CUTLASS_HOST_DEVICE
 Array<T, N> fma(Array<T, N> const &a, Array<T, N> const &b, T c) {
   multiply_add<Array<T, N>> op;
   return op(a, b, c);
 }
 
 
-/////////////////////////////////////////////////////////////////////////////////////////////////
-//
-// Partial specializations for Quaternion<T> fused multiply-add
-//
-/////////////////////////////////////////////////////////////////////////////////////////////////
+////////////////////////////////////////////////////////////////////////////////////////////////////
 
-template <typename T>
-struct multiply_add<Quaternion<T>, Quaternion<T>, Quaternion<T>> {
-  CUTLASS_HOST_DEVICE
-  Quaternion<T> operator()(
-    Quaternion<T> const &a,
-    Quaternion<T> const &b,
-    Quaternion<T> const &c) const {
-
-    T x = c.x();
-    T y = c.y();
-    T z = c.z();
-    T w = c.w();
-
-    x += a.w() * b.x();
-    x += b.w() * a.x();
-    x += a.y() * b.z();
-    x += -a.z() * b.y(),
-
-    y += a.w() * b.y();
-    y += b.w() * a.y();
-    y += a.z() * b.x();
-    y += -a.x() * b.z();
-
-    z += a.w() * b.z();
-    z += b.w() * a.z();
-    z += a.x() * b.y();
-    z += -a.y() * b.x();
-
-    w += a.w() * b.w();
-    w += -a.x() * b.x();
-    w += -a.y() * b.y();
-    w += -a.z() * b.z();
-    
-    return cutlass::make_Quaternion(x, y, z, w);
 
-  }
-};
 
+} // namespace cutlass
+
+////////////////////////////////////////////////////////////////////////////////////////////////////
 
-/////////////////////////////////////////////////////////////////////////////////////////////////
+#include "cutlass/array_subbyte.h"
+
+////////////////////////////////////////////////////////////////////////////////////////////////////
+
+namespace cutlass {
+
+////////////////////////////////////////////////////////////////////////////////////////////////////
+// AlignedArray
+////////////////////////////////////////////////////////////////////////////////////////////////////
+
+/// Aligned array type
+template <
+  /// Element type
+  typename T,
+  /// Number of elements in the array
+  int N,
+  /// Alignment requirement in bytes
+  int Alignment = sizeof_bits<T>::value * N / 8
+>
+class alignas(Alignment) AlignedArray: public Array<T, N> {
+public:
+
+};
+
+////////////////////////////////////////////////////////////////////////////////////////////////////
 
 } // namespace cutlass
 
-/////////////////////////////////////////////////////////////////////////////////////////////////
+////////////////////////////////////////////////////////////////////////////////////////////////////
+
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/device/base_grouped.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/gemm/device/base_grouped.h`

 * *Files 2% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -207,15 +207,15 @@
 
     return dim3(args.threadblock_count, 1, 1);
   }
 
   /// Computes the maximum number of active blocks per multiprocessor
   static int maximum_active_blocks(int smem_capacity = -1) {
 
-    CUTLASS_TRACE_HOST("GemmUniversalBase::maximum_active_blocks()");
+    CUTLASS_TRACE_HOST("BaseGrouped::maximum_active_blocks()");
 
     int smem_size = int(sizeof(typename BaseKernel::SharedStorage));
 
     CUTLASS_TRACE_HOST("  smem_size: " << smem_size << " bytes");
 
     cudaError_t result;
     if (smem_size > (48 << 10)) {
@@ -345,15 +345,15 @@
     return min(total_tiles, occupancy_based_block_count);
   }
 
 
   /// Initializes GEMM state from arguments.
   Status initialize(Arguments const &args, void *workspace = nullptr, cudaStream_t stream = nullptr) {
 
-    CUTLASS_TRACE_HOST("GemmUniversalBase::initialize() - workspace "
+    CUTLASS_TRACE_HOST("BaseGrouped::initialize() - workspace "
       << workspace << ", stream: " << (stream ? "non-null" : "null"));
 
     // Workspace
     size_t workspace_bytes = get_workspace_size(args);
 
     if (workspace_bytes && !workspace) {
       return Status::kErrorWorkspaceNull;
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/device/default_gemm_configuration.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/gemm/device/default_gemm_configuration.h`

 * *Files 2% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -761,12 +761,58 @@
 
   using Operator = arch::OpMultiplyAdd;
 };
 
 ////////////////////////////////////////////////////////////////////////////////
 
 ////////////////////////////////////////////////////////////////////////////////
+
+template <typename ElementC,
+          typename ElementAccumulator>
+struct DefaultGemmConfiguration<arch::OpClassTensorOp, arch::Sm90, double,
+                                double, ElementC, ElementAccumulator> {
+
+  static int const kAlignmentA = 1;
+  static int const kAlignmentB = 1;
+  
+  using ThreadblockShape = GemmShape<128, 256, 64>;
+  using WarpShape = GemmShape<64, 64, 64>;
+  using InstructionShape = GemmShape<16, 8, 4>;
+  static int const kStages = 3;
+
+  using EpilogueOutputOp = epilogue::thread::LinearCombination<
+      ElementC, 128 / sizeof_bits<ElementC>::value, ElementAccumulator,
+      ElementAccumulator>;
+
+  using Operator = arch::OpMultiplyAdd;
+};
+
+template <>
+struct DefaultGemmConfiguration<
+    arch::OpClassTensorOp, 
+    arch::Sm90, 
+    complex<double>,
+    complex<double>, 
+    complex<double>,
+    complex<double>
+  > {
+
+  static int const kAlignmentA = 1;
+  static int const kAlignmentB = 1;
+  
+  using ThreadblockShape = GemmShape<64, 64, 16>;
+  using WarpShape = GemmShape<32, 32, 16>;
+  using InstructionShape = GemmShape<16, 8, 4>;
+  static int const kStages = 3;
+
+  using EpilogueOutputOp = epilogue::thread::LinearCombination<
+      complex<double>, 1, complex<double>,
+      complex<double>>;
+
+  using Operator = arch::OpMultiplyAddComplex;
+};
+
 } // namespace device
 } // namespace gemm
 } // namespace cutlass
 
 ////////////////////////////////////////////////////////////////////////////////
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/device/ell_gemm.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/gemm/device/ell_gemm.h`

 * *Files 0% similar despite different names*

```diff
@@ -543,15 +543,15 @@
 
     return status;
   }
 };
 
 ////////////////////////////////////////////////////////////////////////////////
 
-/// Partial specialization for column-major output exchanges problem size and operand.
+/// Parital specialization for column-major output exchanges problem size and operand.
 template <
     /// Element type for A matrix operand
     typename ElementA_,
     /// Layout type for A matrix operand
     typename LayoutA_,
     /// Element type for B matrix operand
     typename ElementB_,
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/device/gemm.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/gemm/device/gemm.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -505,15 +505,15 @@
 
   /// Runs the kernel using initialized state.
   Status operator()(
     Arguments const &args, 
     void *workspace = nullptr, 
     cudaStream_t stream = nullptr) {
     
-    Status status = initialize(args, workspace);
+    Status status = initialize(args, workspace, stream);
     
     if (status == Status::kSuccess) {
       status = run(stream);
     }
 
     return status;
   }
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/device/gemm_array.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/gemm/device/gemm_array.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/device/gemm_batched.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/gemm/device/gemm_batched.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -25,15 +25,15 @@
  * SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER
  * CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,
  * OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
  * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
  *
  **************************************************************************************************/
 /*! \file
-    \brief Template for a pipelined GEMM kernel. Does not compute batching or support split-K.
+    \brief Template for a pipelined batch GEMM kernel.
 */
 
 #pragma once
 
 #include "cutlass/cutlass.h"
 #include "cutlass/numeric_types.h"
 #include "cutlass/arch/arch.h"
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/device/gemm_complex.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/gemm/device/gemm_complex.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/device/gemm_grouped.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/gemm/device/gemm_grouped.h`

 * *Files 5% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/device/gemm_layernorm_mainloop_fusion.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/gemm/device/gemm_layernorm_mainloop_fusion.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/device/gemm_sparse.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/gemm/device/gemm_sparse.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/device/gemm_splitk_parallel.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/gemm/device/gemm_splitk_parallel.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/device/gemm_universal.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/gemm/device/gemm_universal.h`

 * *Files 2% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -54,14 +54,19 @@
 namespace cutlass {
 namespace gemm {
 namespace device {
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
 /*! 
+  GemmUniversal is a stateful, reusable GEMM handle.  Once initialized for a given GEMM computation
+  (problem geometry and data references), it can be reused across different GEMM problems having the
+  geometry.  (Once initialized, details regarding problem geometry and references to workspace memory
+  cannot be updated.)
+
   The universal GEMM accommodates serial reductions, parallel reductions, batched strided, and 
   batched array variants.
 */
 template <
     /// Element type for A matrix operand
     typename ElementA_,
     /// Layout type for A matrix operand
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/device/gemm_universal_adapter.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/gemm/device/gemm_universal_adapter.h`

 * *Files 2% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -105,15 +105,14 @@
   using TensorRefD = TensorRef<ElementC, LayoutC>;
 
   static int const kStages = GemmKernel::Mma::kStages;
 
   using EpilogueOutputOp = typename GemmKernel::EpilogueOutputOp;
   using ElementAccumulator = typename EpilogueOutputOp::ElementAccumulator;
   using ThreadblockSwizzle = typename GemmKernel::ThreadblockSwizzle;
-
   using UnderlyingOperator = GemmUniversalBase<GemmKernel>;
   using Arguments = typename UnderlyingOperator::Arguments;
 
 private:
 
   UnderlyingOperator underlying_operator_;
 
@@ -156,18 +155,19 @@
 
   /// Initializes GEMM state from arguments.
   Status initialize(Arguments const &args, void *workspace = nullptr, cudaStream_t stream = nullptr) {
 
     return underlying_operator_.initialize(to_underlying_arguments(args), workspace, stream);
   }
 
-  /// Lightweight update given a subset of arguments
-  Status update(Arguments const &args, void *workspace = nullptr) {
+  /// Lightweight update given a subset of arguments.  Problem geometry is assumed to
+  /// remain the same.
+  Status update(Arguments const &args) {
 
-    return underlying_operator_.update(to_underlying_arguments(args), workspace);
+    return underlying_operator_.update(to_underlying_arguments(args));
   }
 
   /// Runs the kernel using initialized state.
   Status run(cudaStream_t stream = nullptr) {
 
     return underlying_operator_.run(stream);
   }
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/device/gemm_universal_base.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/gemm/device/gemm_universal_base.h`

 * *Files 18% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -24,54 +24,53 @@
  * DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR
  * SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER
  * CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,
  * OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
  * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
  *
  **************************************************************************************************/
-/*! 
+/*!
   \file
-  \brief The universal GEMM accommodates serial reductions, parallel reductions, batched strided, and 
-    batched array variants.
+  \brief The universal GEMM accommodates streamk, batched strided, and batched array variants.
 */
 
+
 #pragma once
 
-//#include <limits>
+#include <limits>
 
 #include "cutlass/cutlass.h"
 #include "cutlass/numeric_types.h"
 #include "cutlass/arch/arch.h"
 #include "cutlass/device_kernel.h"
 
 #include "cutlass/gemm/gemm.h"
-#include "cutlass/gemm/threadblock/threadblock_swizzle.h"
 #include "cutlass/gemm/kernel/gemm_universal.h"
 
 #include "cutlass/gemm/kernel/default_gemm_universal.h"
 #include "cutlass/gemm/device/default_gemm_configuration.h"
 
 #include "cutlass/trace.h"
 
-////////////////////////////////////////////////////////////////////////////////
+/////////////////////////////////////////////////////////////////////////////////////////////////
 
 namespace cutlass {
 namespace gemm {
 namespace device {
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
 
 template <typename GemmKernel_>
 class GemmUniversalBase {
 public:
 
   using GemmKernel = GemmKernel_;
   using ThreadblockShape = typename GemmKernel::Mma::Shape;
-  
+
   using ElementA = typename GemmKernel::ElementA;
   using LayoutA = typename GemmKernel::LayoutA;
   using TensorRefA = TensorRef<ElementA const, LayoutA>;
   static ComplexTransform const kTransformA = GemmKernel::kTransformA;
 
   using ElementB = typename GemmKernel::ElementB;
   using LayoutB = typename GemmKernel::LayoutB;
@@ -79,343 +78,339 @@
   static ComplexTransform const kTransformB = GemmKernel::kTransformB;
 
   using ElementC = typename GemmKernel::ElementC;
   using LayoutC = typename GemmKernel::LayoutC;
   using TensorRefC = TensorRef<ElementC const, LayoutC>;
   using TensorRefD = TensorRef<ElementC, LayoutC>;
 
-  using ElementAccumulator = typename GemmKernel::Mma::Policy::Operator::ElementC;
+  /// Numerical accumulation element type
+  using ElementAccumulator = typename GemmKernel::Mma::ElementC;
 
   using EpilogueOutputOp = typename GemmKernel::EpilogueOutputOp;
   using ThreadblockSwizzle = typename GemmKernel::ThreadblockSwizzle;
   using Operator = typename GemmKernel::Operator;
 
   /// Argument structure
   using Arguments = typename GemmKernel::Arguments;
 
 protected:
 
-  /// Kernel parameters object
-  typename GemmKernel::Params params_;
+  //
+  // Device properties (uniform across all instances of the current thread)
+  //
+
+  // Device ordinal
+  thread_local static int device_ordinal_;
+
+  /// Device SM count
+  thread_local static int device_sms_;
+
+  /// Kernel SM occupancy (in thread blocks)
+  thread_local static int sm_occupancy_;
+
+  /// Kernel dynamic shared memory allocation requirement
+  thread_local static int smem_size_;
+
+  /// Initialize static thread-local members for the thread's current device,
+  /// if necessary.
+  static Status init_device_props()
+  {
+    CUTLASS_TRACE_HOST("GemmUniversalBase::init_device_props()");
+
+    cudaError_t cudart_result;
+
+    // Get current device ordinal
+    int current_ordinal;
+    cudart_result = cudaGetDevice(&current_ordinal);
+    if (cudart_result != cudaSuccess) {
+      CUTLASS_TRACE_HOST("  cudaGetDevice() returned error " << cudaGetErrorString(cudart_result));
+      return Status::kErrorInternal;
+    }
 
-protected:
+    // Done if matches the current static member
+    if (current_ordinal == device_ordinal_) {
+      // Already initialized
+      return Status::kSuccess;
+    }
+
+    // Update SM count member
+    cudart_result = cudaDeviceGetAttribute (&device_sms_, cudaDevAttrMultiProcessorCount, current_ordinal);
+    if (cudart_result != cudaSuccess) {
+      CUTLASS_TRACE_HOST("  cudaDeviceGetAttribute() returned error " << cudaGetErrorString(cudart_result));
+      return Status::kErrorInternal;
+    }
 
-  /// Private helper to obtain the grid dimensions with fix-up for split-K
-  static void get_grid_shape_(gemm::GemmCoord &grid_tiled_shape, int &gemm_k_size, Arguments const &args) {
+    // Update the kernel function's shared memory configuration for the current device
+    smem_size_ = int(sizeof(typename GemmKernel::SharedStorage));
 
-    // Determine grid shape
-    ThreadblockSwizzle threadblock_swizzle;
+    // If requires more than 48KB: configure for extended, dynamic shared memory
+    if (smem_size_ >= (48 << 10))
+    {
+      cudart_result = cudaFuncSetAttribute(
+        Kernel2<GemmKernel>,
+        cudaFuncAttributeMaxDynamicSharedMemorySize,
+        smem_size_);
+      if (cudart_result != cudaSuccess) {
+        CUTLASS_TRACE_HOST("  cudaFuncSetAttribute() returned error " << cudaGetErrorString(cudart_result));
+        return Status::kErrorInternal;
+      }
 
-    grid_tiled_shape = threadblock_swizzle.get_tiled_shape(
-      args.problem_size, 
-      {ThreadblockShape::kM, ThreadblockShape::kN, ThreadblockShape::kK},
-      args.batch_count);
-    
-    gemm_k_size = args.problem_size.k();
-
-    if (args.mode == GemmUniversalMode::kGemm || args.mode == GemmUniversalMode::kGemmSplitKParallel) {
-
-      int const kAlignK = const_max(const_max(128 / sizeof_bits<ElementA>::value, 128 / sizeof_bits<ElementB>::value), 1);
-
-      gemm_k_size = round_up(ceil_div(args.problem_size.k(), args.batch_count), kAlignK);
-      
-      if (gemm_k_size) {
-        grid_tiled_shape.k() = ceil_div(args.problem_size.k(), gemm_k_size);
+      cudart_result = cudaFuncSetAttribute(
+          Kernel2<GemmKernel>,
+          cudaFuncAttributePreferredSharedMemoryCarveout, 100); // 100% shared memory
+      if (cudart_result != cudaSuccess) {
+        CUTLASS_TRACE_HOST("  cudaFuncSetAttribute() returned error " << cudaGetErrorString(cudart_result));
+        return Status::kErrorInternal;
       }
     }
+
+    // Update SM occupancy member
+    cudart_result = cudaOccupancyMaxActiveBlocksPerMultiprocessorWithFlags(
+      &sm_occupancy_,
+      Kernel2<GemmKernel>,
+      GemmKernel::kThreadCount,
+      smem_size_,
+      cudaOccupancyDisableCachingOverride);
+    if (cudart_result != cudaSuccess) {
+      CUTLASS_TRACE_HOST("  cudaOccupancyMaxActiveBlocksPerMultiprocessorWithFlags() returned error " << cudaGetErrorString(cudart_result));
+      return Status::kErrorInternal;
+    }
+
+    // Update device ordinal member on success
+    device_ordinal_ = current_ordinal;
+
+    CUTLASS_TRACE_HOST("  "
+      "device_ordinal: (" << device_ordinal_ << "), "
+      "device_sms: (" << device_sms_ << "), "
+      "sm_occupancy: (" << sm_occupancy_ << ") "
+      "smem_size: (" << smem_size_ << ") "
+      "GemmKernel::kThreadCount: (" << GemmKernel::kThreadCount << ")");
+
+    return Status::kSuccess;
+  }
+
+
+protected:
+
+  //
+  // Instance data members
+  //
+
+  /// Kernel parameters
+  typename GemmKernel::Params params_;
+
+
+  /// Initialize params member
+  Status init_params(Arguments const &args)
+  {
+    // Initialize static device properties, if necessary
+    Status result = init_device_props();
+    if (result != Status::kSuccess) {
+      return result;
+    }
+
+    // Initialize params member
+    params_ = typename GemmKernel::Params(args, device_sms_, sm_occupancy_);
+    return Status::kSuccess;
   }
 
 public:
 
-  /// Constructs the GEMM.
-  GemmUniversalBase() { }
+  //---------------------------------------------------------------------------------------------
+  // Stateless API
+  //---------------------------------------------------------------------------------------------
 
   /// Determines whether the GEMM can execute the given problem.
-  static Status can_implement(Arguments const &args) {
-    
-    // Determine grid shape
-    cutlass::gemm::GemmCoord grid_tiled_shape;
-    int gemm_k_size = 0;
-    
-    get_grid_shape_(grid_tiled_shape, gemm_k_size, args);
-    
-    ThreadblockSwizzle threadblock_swizzle;
-    dim3 grid = threadblock_swizzle.get_grid_shape(grid_tiled_shape);
-
-    uint32_t const kGridYZMax = ((1 << (sizeof(uint16_t) * 8)) - 1);
-  
-    if (!(grid.y <= kGridYZMax && grid.z <= kGridYZMax)) {
+  static Status can_implement(Arguments const &args)
+  {
+    CUTLASS_TRACE_HOST("GemmUniversalBase::can_implement()");
+
+    // Initialize static kernel and device properties, if necessary.
+    Status result = init_device_props();
+    if (result != Status::kSuccess) {
+      return result;
+    }
+
+    dim3 grid = get_grid_shape(args);
 
+    if (!(grid.y <= std::numeric_limits<uint16_t>::max() &&
+          grid.z <= std::numeric_limits<uint16_t>::max()))
+    {
       return Status::kErrorInvalidProblem;
-    } 
+    }
 
     return GemmKernel::can_implement(args);
   }
 
-  /// Gets the workspace size
-  static size_t get_workspace_size(Arguments const &args) {
 
+  /// Returns the workspace size (in bytes) needed for the problem
+  /// geometry expressed by these arguments
+  static size_t get_workspace_size(Arguments const &args)
+  {
     CUTLASS_TRACE_HOST("GemmUniversalBase::get_workspace_size()");
- 
-    size_t workspace_bytes = 0;
 
-    // Determine grid shape
-    cutlass::gemm::GemmCoord grid_tiled_shape;
-    int gemm_k_size = 0;
-    
-    get_grid_shape_(grid_tiled_shape, gemm_k_size, args);
-    
-    if (args.mode == GemmUniversalMode::kGemmSplitKParallel) {
-
-      // Split-K parallel always requires a temporary workspace
-      workspace_bytes = 
-        sizeof(ElementC) *
-        size_t(args.batch_stride_D) *
-        size_t(grid_tiled_shape.k());
-    }
-    else if (args.mode == GemmUniversalMode::kGemm && grid_tiled_shape.k() > 1) {
-
-      // Serial split-K only requires a temporary workspace if the number of partitions along the
-      // GEMM K dimension is greater than one.
-      workspace_bytes = sizeof(int) * size_t(grid_tiled_shape.m()) * size_t(grid_tiled_shape.n());
+    // Initialize parameters from args
+    GemmUniversalBase base;
+    if (base.init_params(args) != Status::kSuccess) {
+      return 0;
     }
 
-    CUTLASS_TRACE_HOST("  workspace_bytes: " << workspace_bytes);
+    // Get size from parameters
+    size_t workspace_bytes = base.params_.get_workspace_size();
 
-    workspace_bytes += GemmKernel::get_extra_workspace_size(args, grid_tiled_shape);
- 
+    CUTLASS_TRACE_HOST("  workspace_bytes: " << workspace_bytes);
     return workspace_bytes;
   }
 
-  /// Computes the grid shape
-  static dim3 get_grid_shape(Arguments const &args) {
 
+  /// Returns the grid extents in thread blocks to launch
+  static dim3 get_grid_shape(Arguments const &args)
+  {
     CUTLASS_TRACE_HOST("GemmUniversalBase::get_grid_shape()");
 
-    ThreadblockSwizzle threadblock_swizzle;
+    // Initialize parameters from args
+    GemmUniversalBase base;
+    if (base.init_params(args) != Status::kSuccess) {
+      return dim3(0,0,0);
+    }
 
-    cutlass::gemm::GemmCoord grid_tiled_shape;
-    int gemm_k_size = 0;
+    // Get dims from parameters
+    dim3 grid_dims = base.params_.get_grid_dims();
 
-    get_grid_shape_(grid_tiled_shape, gemm_k_size, args);
-    dim3 result = threadblock_swizzle.get_grid_shape(grid_tiled_shape);    
-    
     CUTLASS_TRACE_HOST(
-         "  grid_tiled_shape: " << grid_tiled_shape  << "\n"
-      << "  result = {" << result << "}");
+         "  tiled_shape: " << base.params_.get_tiled_shape()  << "\n"
+      << "  grid_dims: {" << grid_dims << "}");
 
-    return result;
+    return grid_dims;
   }
 
-  /// Computes the maximum number of active blocks per multiprocessor
-  static int maximum_active_blocks(int smem_capacity = -1) {
 
+  /// Returns the maximum number of active thread blocks per multiprocessor
+  static int maximum_active_blocks()
+  {
     CUTLASS_TRACE_HOST("GemmUniversalBase::maximum_active_blocks()");
 
-    int max_active_blocks = -1;
-    int smem_size = int(sizeof(typename GemmKernel::SharedStorage));
-
-    CUTLASS_TRACE_HOST("  smem_size: " << smem_size << " bytes");
-
-    if (smem_size <= (48 << 10)) {
-
-      cudaError_t result = cudaOccupancyMaxActiveBlocksPerMultiprocessor(
-        &max_active_blocks,
-        Kernel<GemmKernel>,
-        GemmKernel::kThreadCount,
-        smem_size);
-
-      if (result == cudaSuccess) {
-        CUTLASS_TRACE_HOST("  max_active_blocks: " << max_active_blocks);
-        return max_active_blocks;
-      }
-    }
-    else {
-
-      // Query assuming zero shared memory then compute occupancy limit based on SMEM
-      cudaError_t result = cudaOccupancyMaxActiveBlocksPerMultiprocessor(
-        &max_active_blocks,
-        Kernel<GemmKernel>,
-        GemmKernel::kThreadCount,
-        0);
-
-      if (result != cudaSuccess) {
-
-        CUTLASS_TRACE_HOST(
-          "  cudaOccupancyMaxActiveBlocksPerMultiprocessor() returned error "
-          << cudaGetErrorString(result));
-
-        return -1;
-      }
-
-      if (smem_capacity < 0) {
-        int device_idx = 0;
-        result = cudaGetDevice(&device_idx);
-
-        if (result != cudaSuccess) {
-          return -1;
-        }
-
-        cudaDeviceProp properties;
-        result = cudaGetDeviceProperties(&properties, device_idx);
-
-        if (result != cudaSuccess) {
-          return -1;
-        }
-
-        smem_capacity = static_cast<int>(properties.sharedMemPerMultiprocessor);
-      }
-
-      int occupancy = std::min(max_active_blocks, smem_capacity / smem_size);
-
-      CUTLASS_TRACE_HOST("  occupancy: " << occupancy);
-
-      return occupancy;
+    // Initialize static device properties, if necessary
+    if (init_device_props() != Status::kSuccess) {
+      return -1;
     }
 
-    CUTLASS_TRACE_HOST("  returning internal error");
-
-    return -1;
+    CUTLASS_TRACE_HOST("  max_active_blocks: " << sm_occupancy_);
+    return sm_occupancy_;
   }
 
-  /// Initializes GEMM state from arguments.
-  Status initialize(Arguments const &args, void *workspace = nullptr, cudaStream_t stream = nullptr) {
 
-    CUTLASS_TRACE_HOST("GemmUniversalBase::initialize() - workspace " 
+  //---------------------------------------------------------------------------------------------
+  // Stateful API
+  //---------------------------------------------------------------------------------------------
+
+  /// Initializes GEMM state from arguments and workspace memory
+  Status initialize(
+    Arguments const &args,
+    void *workspace,
+    cudaStream_t stream = nullptr)
+  {
+    CUTLASS_TRACE_HOST("GemmUniversalBase::initialize() - workspace "
       << workspace << ", stream: " << (stream ? "non-null" : "null"));
 
-    size_t workspace_bytes = get_workspace_size(args);
-
-    CUTLASS_TRACE_HOST("  workspace_bytes: " << workspace_bytes);
-
-    if (workspace_bytes) {
-      
-      if (!workspace) {
-        CUTLASS_TRACE_HOST("  error: device workspace must not be null");
-
-        return Status::kErrorWorkspaceNull;
-      }
-
-      if (args.mode == GemmUniversalMode::kGemm) {
-        CUTLASS_TRACE_HOST("  clearing device workspace");
-        cudaError_t result = cudaMemsetAsync(workspace, 0, workspace_bytes, stream);
-
-        if (result != cudaSuccess) {
-          CUTLASS_TRACE_HOST("  cudaMemsetAsync() returned error " << cudaGetErrorString(result));
-
-          return Status::kErrorInternal;
-        }
-      }
-    }
-
-    // Get CUDA grid shape
-    cutlass::gemm::GemmCoord grid_tiled_shape;
-    int gemm_k_size = 0;
-
-    get_grid_shape_(grid_tiled_shape, gemm_k_size, args);
-
-    // Initialize the Params structure
-    params_ = typename GemmKernel::Params(
-      args,
-      grid_tiled_shape,
-      gemm_k_size,
-      static_cast<int *>(workspace)
-    );
-   
-    // Specify shared memory capacity for kernel. 
-    int smem_size = int(sizeof(typename GemmKernel::SharedStorage));
-
-    if (smem_size >= (48 << 10)) {
-      cudaError_t result = cudaFuncSetAttribute(Kernel<GemmKernel>,
-                                    cudaFuncAttributeMaxDynamicSharedMemorySize,
-                                    smem_size);
-
-      if (result != cudaSuccess) {
-        return Status::kErrorInternal;
-      }
+    // Initialize parameters from args
+    Status result = init_params(args);
+    if (result != Status::kSuccess) {
+      return result;
     }
 
-    return Status::kSuccess;
+    // Assign and prepare workspace memory
+    return params_.init_workspace(workspace, stream);
   }
 
-  /// Lightweight update given a subset of arguments
-  Status update(Arguments const &args, void *workspace = nullptr) {
 
-    CUTLASS_TRACE_HOST("GemmUniversalBase()::update() - workspace: " << workspace);
-
-    size_t workspace_bytes = get_workspace_size(args);
-
-    if (workspace_bytes && !workspace) {
-      return Status::kErrorWorkspaceNull;
-    }
-    
-    params_.update(args, workspace);
-    
+  /// Lightweight update given a subset of arguments.  Problem geometry is assumed to
+  /// remain the same.
+  Status update(Arguments const &args)
+  {
+    CUTLASS_TRACE_HOST("GemmUniversalBase()::update()");
+    params_.update(args);
     return Status::kSuccess;
   }
 
+
   /// Runs the kernel using initialized state.
-  Status run(cudaStream_t stream = nullptr) {
+  Status run(cudaStream_t stream = nullptr)
+  {
     CUTLASS_TRACE_HOST("GemmUniversalBase::run()");
 
-    //
     // Configure grid and block dimensions
-    //
-
-    ThreadblockSwizzle threadblock_swizzle;
-
-    dim3 grid = threadblock_swizzle.get_grid_shape(params_.grid_tiled_shape);
     dim3 block(GemmKernel::kThreadCount, 1, 1);
+    dim3 grid = params_.get_grid_dims();
 
-    int smem_size = int(sizeof(typename GemmKernel::SharedStorage));
-
-    //
     // Launch kernel
-    //
-
-    CUTLASS_TRACE_HOST("  grid: (" << grid << "),  block: (" << block 
-      << "),  SMEM: " << smem_size << " bytes");
+    CUTLASS_TRACE_HOST("  "
+      "grid: (" << grid << "), "
+      "block: (" << block << "), "
+      "SMEM: (" << smem_size_ << ")");
 
-    // Launch
-    cutlass::Kernel<GemmKernel><<<grid, block, smem_size, stream>>>(params_);
+    Kernel2<GemmKernel><<<grid, block, smem_size_, stream>>>(params_);
 
-    //
     // Query for errors
-    //
     cudaError_t result = cudaGetLastError();
-
     if (result != cudaSuccess) {
       CUTLASS_TRACE_HOST("  grid launch failed with error " << cudaGetErrorString(result));
       return Status::kErrorInternal;
     }
-  
+
     return Status::kSuccess;
   }
 
+
   /// Runs the kernel using initialized state.
-  Status operator()(cudaStream_t stream = nullptr) {
+  Status operator()(cudaStream_t stream = nullptr)
+  {
     return run(stream);
   }
 
+
   /// Runs the kernel using initialized state.
   Status operator()(
     Arguments const &args, 
     void *workspace = nullptr, 
-    cudaStream_t stream = nullptr) {
-    
+    cudaStream_t stream = nullptr)
+  {
     Status status = initialize(args, workspace, stream);
-    
+
     if (status == Status::kSuccess) {
       status = run(stream);
     }
 
     return status;
   }
 };
 
+
+/////////////////////////////////////////////////////////////////////////////////////////////////
+/// Static initializers
+/////////////////////////////////////////////////////////////////////////////////////////////////
+
+/// Device ordinal
+template <typename GemmKernel_>
+thread_local int GemmUniversalBase<GemmKernel_>::device_ordinal_ = -1;
+
+/// Device SM count
+template <typename GemmKernel_>
+thread_local int GemmUniversalBase<GemmKernel_>::device_sms_ = -1;
+
+/// Kernel SM occupancy (in thread blocks)
+template <typename GemmKernel_>
+thread_local int GemmUniversalBase<GemmKernel_>::sm_occupancy_ = -1;
+
+/// Kernel dynamic shared memory allocation requirement
+template <typename GemmKernel_>
+thread_local int GemmUniversalBase<GemmKernel_>::smem_size_ = -1;
+
+
+
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
 } // namespace device
 } // namespace gemm
 } // namespace cutlass
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/device/gemm_universal_with_broadcast.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/gemm/device/gemm_universal_with_broadcast.h`

 * *Files 0% similar despite different names*

```diff
@@ -194,15 +194,15 @@
 
   using Arguments = typename Base::Arguments;
   using GemmKernel = typename Base::GemmKernel;
 };
 
 ////////////////////////////////////////////////////////////////////////////////
 
-/// Partial specialization for column-major output exchanges problem size and operand.
+/// Parital specialization for column-major output exchanges problem size and operand.
 template <
     /// Element type for A matrix operand
     typename ElementA_,
     /// Layout type for A matrix operand
     typename LayoutA_,
     /// Element type for B matrix operand
     typename ElementB_,
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/device/gemm_with_k_reduction.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/gemm/device/gemm_with_k_reduction.h`

 * *Files 1% similar despite different names*

```diff
@@ -207,15 +207,15 @@
 
   using Arguments = typename Base::Arguments;
   using GemmKernel = typename Base::GemmKernel;
 };
 
 ////////////////////////////////////////////////////////////////////////////////
 
-/// Partial specialization for column-major output exchanges problem size and operand.
+/// Parital specialization for column-major output exchanges problem size and operand.
 template <
     /// Element type for A matrix operand
     typename ElementA_,
     /// Layout type for A matrix operand
     typename LayoutA_,
     /// Element type for B matrix operand
     typename ElementB_,
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/device/gemv.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/gemm/device/gemv.h`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/device/rank_2k.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/gemm/device/rank_2k.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/device/rank_2k_grouped.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/gemm/device/rank_2k_grouped.h`

 * *Files 8% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/device/rank_k.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/gemm/device/rank_k.h`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/device/symm.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/gemm/device/symm.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/device/trmm.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/gemm/device/trmm.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/gemm.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/gemm/gemm.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/default_ell_gemm.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/default_ell_gemm.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/default_gemm.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/default_gemm.h`

 * *Files 2% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -131,14 +131,85 @@
     typename PermuteDLayout = layout::NoPermute,
     ///
     typename Enable = void
 >
 struct DefaultGemm;
 
 ////////////////////////////////////////////////////////////////////////////////
+
+/// Partial specialization for Hopper Architecture
+template <
+    /// Element type for A matrix operand
+    typename ElementA,
+    /// Layout type for A matrix operand
+    typename LayoutA,
+    /// Access granularity of A matrix in units of elements
+    int kAlignmentA,
+    /// Element type for B matrix operand
+    typename ElementB,
+    /// Layout type for B matrix operand
+    typename LayoutB,
+    /// Access granularity of A matrix in units of elements
+    int kAlignmentB,
+    /// Element type for C and D matrix operands
+    typename ElementC,
+    /// Element type for internal accumulation
+    typename ElementAccumulator,
+    /// Threadblock-level tile size (concept: GemmShape)
+    typename ThreadblockShape,
+    /// Warp-level tile size (concept: GemmShape)
+    typename WarpShape,
+    /// Warp-level tile size (concept: GemmShape)
+    typename InstructionShape,
+    /// Epilogue output operator
+    typename EpilogueOutputOp,
+    /// Threadblock-level swizzling operator
+    typename ThreadblockSwizzle,
+    /// Number of stages used in the pipelined mainloop
+    int Stages,
+    /// If true, kernel is configured to support serial reduction in the
+    /// epilogue
+    bool SplitKSerial,
+    /// Operation performed by GEMM
+    typename Operator,
+    /// Use zfill or predicate for out-of-bound cp.async
+    SharedMemoryClearOption SharedMemoryClear,
+    /// Gather operand A by using an index array
+    bool GatherA,
+    /// Gather operand B by using an index array
+    bool GatherB,
+    /// Scatter result D by using an index array
+    bool ScatterD,
+    /// Permute result D
+    typename PermuteDLayout
+>
+struct DefaultGemm<ElementA, LayoutA, kAlignmentA, ElementB, LayoutB, kAlignmentB, ElementC,
+                   layout::RowMajor, ElementAccumulator, arch::OpClassTensorOp,
+                   arch::Sm90, ThreadblockShape, WarpShape, InstructionShape,
+                   EpilogueOutputOp, ThreadblockSwizzle, Stages, SplitKSerial,
+                   Operator, SharedMemoryClear, GatherA, GatherB, ScatterD, PermuteDLayout> {
+  /// Define the threadblock-scoped matrix multiply-accumulate
+  using Mma = typename cutlass::gemm::threadblock::DefaultMma<
+      ElementA, LayoutA, kAlignmentA, ElementB, LayoutB, kAlignmentB,
+      ElementAccumulator, layout::RowMajor, arch::OpClassTensorOp, arch::Sm90,
+      ThreadblockShape, WarpShape, InstructionShape, Stages,
+      Operator, false, SharedMemoryClear, GatherA, GatherB>::ThreadblockMma;
+
+  static const int kPartitionsK = ThreadblockShape::kK / WarpShape::kK;
+
+  /// Define the epilogue
+  using Epilogue =
+      typename cutlass::epilogue::threadblock::DefaultEpilogueTensorOp<
+          ThreadblockShape, typename Mma::Operator, kPartitionsK, EpilogueOutputOp,
+          EpilogueOutputOp::kCount, ScatterD, PermuteDLayout>::Epilogue;
+
+  /// Define the kernel-level GEMM operator.
+  using GemmKernel = kernel::Gemm<Mma, Epilogue, ThreadblockSwizzle, SplitKSerial>;
+};
+
 ////////////////////////////////////////////////////////////////////////////////
 
 /// Partial specialization for Ampere Architecture
 template <
     /// Element type for A matrix operand
     typename ElementA,
     /// Layout type for A matrix operand
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/default_gemm_complex.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/default_gemm_complex.h`

 * *Files 9% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -115,14 +115,74 @@
   /// If true, kernel is configured to support serial reduction in the epilogue
   bool SplitKSerial
 >
 struct DefaultGemmComplex;
 
 ////////////////////////////////////////////////////////////////////////////////
 
+/// Partial specialization for Hopper Architecture
+template <
+    /// Element type for A matrix operand
+    typename ElementA,
+    /// Layout type for A matrix operand
+    typename LayoutA,
+    /// Element type for B matrix operand
+    typename ElementB,
+    /// Layout type for B matrix operand
+    typename LayoutB,
+    /// Element type for C and D matrix operands
+    typename ElementC,
+    /// Element type for internal accumulation
+    typename ElementAccumulator,
+    /// Threadblock-level tile size (concept: GemmShape)
+    typename ThreadblockShape,
+    /// Warp-level tile size (concept: GemmShape)
+    typename WarpShape,
+    /// Warp-level tile size (concept: GemmShape)
+    typename InstructionShape,
+    /// Epilogue output operator
+    typename EpilogueOutputOp,
+    /// Threadblock-level swizzling operator
+    typename ThreadblockSwizzle,
+    /// Number of stages used in the pipelined mainloop
+    int Stages,
+    /// Complex elementwise transformation on A operand
+    ComplexTransform TransformA,
+    /// Complex elementwise transformation on B operand
+    ComplexTransform TransformB,
+    /// Multiply-add operator 
+    // (arch::OpMultiplyAddComplex, arch::OpMultiplyGaussianComplex)
+    typename Operator,
+    /// If true, kernel is configured to support serial reduction in the epilogue
+    bool SplitKSerial
+  >
+struct DefaultGemmComplex<
+  ElementA, LayoutA, ElementB, LayoutB, ElementC,
+  layout::RowMajor, ElementAccumulator, arch::OpClassTensorOp,
+  arch::Sm90, ThreadblockShape, WarpShape, InstructionShape,
+  EpilogueOutputOp, ThreadblockSwizzle, Stages, TransformA, TransformB, Operator, SplitKSerial> {
+
+  /// Define the threadblock-scoped matrix multiply-accumulate
+  using Mma = typename cutlass::gemm::threadblock::DefaultMultistageMmaComplex<
+      ElementA, LayoutA, ElementB, LayoutB, ElementAccumulator,
+      layout::RowMajor, arch::OpClassTensorOp, arch::Sm90, ThreadblockShape,
+      WarpShape, InstructionShape, Stages, TransformA, TransformB, Operator>::ThreadblockMma;
+
+  /// Define the epilogue
+  using Epilogue =
+      typename cutlass::epilogue::threadblock::DefaultEpilogueComplexTensorOp<
+          ThreadblockShape, typename Mma::Operator, 1, EpilogueOutputOp,
+          EpilogueOutputOp::kCount, Operator>::Epilogue;
+
+  /// Define the kernel-level GEMM operator.
+  using GemmKernel = kernel::Gemm<Mma, Epilogue, ThreadblockSwizzle, SplitKSerial>;
+};
+
+////////////////////////////////////////////////////////////////////////////////
+
 /// Partial specialization for Ampere Architecture
 template <
     /// Element type for A matrix operand
     typename ElementA,
     /// Layout type for A matrix operand
     typename LayoutA,
     /// Element type for B matrix operand
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/default_gemm_grouped.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/default_gemm_grouped.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/default_gemm_grouped_softmax_mainloop_fusion.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/default_gemm_grouped_softmax_mainloop_fusion.h`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/default_gemm_layernorm_mainloop_fusion.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/default_gemm_layernorm_mainloop_fusion.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/default_gemm_planar_complex_universal.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/default_gemm_planar_complex_universal.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/default_gemm_sparse.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/default_gemm_sparse.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/default_gemm_splitk_parallel.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/default_gemm_splitk_parallel.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/default_gemm_universal.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/default_gemm_universal.h`

 * *Files 8% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -45,14 +45,15 @@
 #include "cutlass/cutlass.h"
 
 #include "cutlass/complex.h"
 #include "cutlass/layout/matrix.h"
 #include "cutlass/numeric_types.h"
 
 #include "cutlass/gemm/kernel/gemm_universal.h"
+#include "cutlass/gemm/kernel/gemm_universal_streamk.h"
 #include "cutlass/gemm/kernel/default_gemm.h"
 #include "cutlass/gemm/kernel/default_gemm_complex.h"
 
 #include "cutlass/layout/permute.h"
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
@@ -223,20 +224,34 @@
     SharedMemoryClear,
     GatherA,
     GatherB,
     ScatterD,
     PermuteDLayout
   >::GemmKernel;
 
-    /// Define the kernel in terms of the default kernel
-  using GemmKernel = kernel::GemmUniversal<
-    typename DefaultGemmKernel::Mma,
-    typename DefaultGemmKernel::Epilogue,
-    ThreadblockSwizzle
-  >;
+  /// Universal kernel without StreamkFeature member type
+  template <class SwizzleT, class Enable = void>
+  class SelectBase :
+    public kernel::GemmUniversal<
+      typename DefaultGemmKernel::Mma,
+      typename DefaultGemmKernel::Epilogue,
+      SwizzleT>
+  {};
+
+  /// Universal kernel with StreamkFeature member type
+  template <class SwizzleT>
+  class SelectBase<SwizzleT, typename SwizzleT::StreamkFeature> :
+    public kernel::GemmUniversalStreamk<
+      typename DefaultGemmKernel::Mma,
+      typename DefaultGemmKernel::Epilogue,
+      SwizzleT>
+  {};
+
+  /// Select kernel by ThreadblockSwizzle's support for StreamkFeature
+  using GemmKernel = SelectBase<ThreadblockSwizzle>;
 };
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
 //
 // Complex-valued GEMM kernels
 //
@@ -332,20 +347,34 @@
     Stages,
     TransformA,
     TransformB,
     Operator,
     false
   >::GemmKernel;
 
-  /// Define the kernel in terms of the default kernel
-  using GemmKernel = kernel::GemmUniversal<
-    typename DefaultGemmKernel::Mma,
-    typename DefaultGemmKernel::Epilogue, 
-    ThreadblockSwizzle
-  >;
+  /// Universal kernel without StreamkFeature member type
+  template <class SwizzleT, class Enable = void>
+  class SelectBase :
+    public kernel::GemmUniversal<
+      typename DefaultGemmKernel::Mma,
+      typename DefaultGemmKernel::Epilogue,
+      SwizzleT>
+  {};
+
+  /// Universal kernel with StreamkFeature member type
+  template <class SwizzleT>
+  class SelectBase<SwizzleT, typename SwizzleT::StreamkFeature> :
+    public kernel::GemmUniversalStreamk<
+      typename DefaultGemmKernel::Mma,
+      typename DefaultGemmKernel::Epilogue,
+      SwizzleT>
+  {};
+
+  /// Select kernel by ThreadblockSwizzle's support for StreamkFeature
+  using GemmKernel = SelectBase<ThreadblockSwizzle>;
 };
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
 }  // namespace kernel
 }  // namespace gemm
 }  // namespace cutlass
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/default_gemm_with_broadcast.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/default_gemm_with_broadcast.h`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/default_gemm_with_k_reduction.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/default_gemm_with_k_reduction.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -87,15 +87,15 @@
     typename ElementC,
     /// Layout type for C and D matrix operands
     typename LayoutC,
     /// Element type for internal accumulation
     typename ElementAccumulator,
     /// Operator class tag
     typename OperatorClass,
-    ///
+    /// Reduce A or B along the K dimension
     bool ReduceKForA_,
     /// Tag indicating architecture to tune for
     typename ArchTag,
     /// Threadblock-level tile size (concept: GemmShape)
     typename ThreadblockShape,
     /// Warp-level tile size (concept: GemmShape)
     typename WarpShape,
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/default_gemm_with_reduction.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/default_gemm_with_reduction.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/default_gemv.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/threadblock/mma_multistage_slicedk.cu`

 * *Files 22% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -25,108 +25,87 @@
  * SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER
  * CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,
  * OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
  * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
  *
  **************************************************************************************************/
 
-#pragma once
+/*! \file
+    \brief Unit tests for CTA-level GEMM specifically for sliced-k kernels (SM_61 and SM_75)
+*/
 
-#include "cutlass/gemm/threadblock/gemv.h"
-#include "cutlass/gemm/threadblock/default_gemv_core.h"
-#include "cutlass/gemm/threadblock/threadblock_swizzle.h"
-
-namespace cutlass {
-namespace gemm {
-namespace kernel {
+#include "mma_multistage_testbed_slicedk.h"
 
+#if defined(CUTLASS_ARCH_MMA_SM80_SUPPORTED)
+
+/////////////////////////////////////////////////////////////////////////////////////////////////
+// Tensor Op GEMM for SM_80
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
-template <
-    /// Size of the ThreadBlock tile - concept: gemm::GemmShape<>
-    typename ThreadBlockShape_,
-    /// Size of the per-thread shape - concept: gemm::GemmShape<>
-    typename ThreadShape_,
-    /// Data type of A elements
-    typename ElementA_,
-    /// Layout of A matrix (concept: MatrixLayout)
-    typename LayoutA_,
-    /// Data type of B elements
-    typename ElementB_,
-    /// Layout of B matrix (concept: MatrixLayout)
-    typename LayoutB_,
-    /// Element type of C/D matrix
-    typename ElementCD_,
-    /// Layout of C/D matrix (concept: MatrixLayout)
-    typename LayoutCD_,
-    ///  Data type of the accumulator
-    typename ElementAccumulator_ = ElementCD_>
-struct DefaultGemv {
-
-  /// Shape of Threadblock-level matrix operation (concept: GemmShape)
-  using ThreadBlockShape = ThreadBlockShape_;
-
-  /// Shape of warp-level matrix operation (concept: GemmShape)
-  using ThreadShape = ThreadShape_;
-
-  /// Data type of multiplicand A
-  using ElementA = ElementA_;
-
-  /// Layout of multiplicand A
-  using LayoutA = LayoutA_;
-
-  /// Data type of multiplicand B
-  using ElementB = ElementB_;
-
-  /// Layout of multiplicand B
-  using LayoutB = LayoutB_;
-
-  /// Data type of accumulators
-  using ElementAccumulator = ElementAccumulator_;
-
-  /// Data type of accumulators (same as C/D)
-  using LayoutAccumulator = LayoutCD_;
-
-  /// Data type of input/output matrix C/D
-  using ElementCD = ElementCD_;
-
-  /// Layout of input/output matrix C/D
-  using LayoutCD = LayoutCD_;
-
-  // Define the core components
-  using Core = typename cutlass::gemm::threadblock::DefaultGemvCore<
-      ThreadBlockShape, ThreadShape, ElementA, LayoutA, ElementB, LayoutB,
-      ElementAccumulator, LayoutAccumulator>;
-
-  // Define the threadblock-scoped gemv
-  using ThreadBlockGemv = cutlass::gemm::threadblock::Gemv<Core>;
-
-  // Iterator for multiplicand A
-  using IteratorA = typename ThreadBlockGemv::IteratorA;
-
-  // Iterator for multiplicand B
-  using IteratorB = typename ThreadBlockGemv::IteratorB;
-
-  /// Policy for the iterator that reads/writes C/D
-  using IteratorPolicyCD = typename platform::conditional<
-        platform::is_same<LayoutCD, layout::RowMajor>::value,
-        cutlass::transform::PitchLinearTilePolicyStripminedThreadContiguous<
-          layout::PitchLinearShape<ThreadBlockShape::kN, ThreadBlockShape::kM>, Core::kThreadsPerN, ThreadShape::kN>,
-        cutlass::transform::PitchLinearTilePolicyStripminedThreadStrided<
-          layout::PitchLinearShape<ThreadBlockShape::kM, ThreadBlockShape::kN>, Core::kThreadsPerN, ThreadShape::kM>>::type;
-
-  /// Iterator that reads/writes C/D
-  using IteratorCD = cutlass::transform::threadblock::PredicatedTileIterator<
-   cutlass::MatrixShape<ThreadBlockShape::kM, ThreadBlockShape::kN>, ElementCD, LayoutCD, 0, IteratorPolicyCD>;
-
-  /// Fragment storage for C/D
-  using FragmentCD = typename IteratorCD::Fragment;
-
-  // Define the threadblock swizzle
-  using ThreadBlockSwizzle = cutlass::gemm::threadblock::GemvBatchedStridedThreadblockDefaultSwizzle;
-};
+TEST(SM80_gemm_threadblock_congruous_sliced, tensor_op_128x64x256_tb128x64x64_warp64x64x32_16x8x16) {
 
-/////////////////////////////////////////////////////////////////////////////////////////////////
+  using ElementA = cutlass::half_t;
+  using LayoutA = cutlass::layout::ColumnMajor;
+  using ElementB = cutlass::half_t;
+  using LayoutB = cutlass::layout::RowMajor;
+  using ElementC = float;
+  using LayoutC = cutlass::layout::ColumnMajor;
+
+  cutlass::gemm::GemmCoord problem_size(128, 64, 256);
+
+  using ThreadblockShape = cutlass::gemm::GemmShape<128, 64, 64>;
+  using WarpShape = cutlass::gemm::GemmShape<64, 64, 32>;
+  using InstructionShape = cutlass::gemm::GemmShape<16, 8, 16>;
+
+  float alpha = 1.f;
+  float beta = 0.0f;
+  int const Stages = 3;
+
+  // Define the MmaCore components
+  using MmaCore = typename cutlass::gemm::threadblock::DefaultMmaCore<
+      ThreadblockShape, WarpShape, InstructionShape, ElementA, LayoutA,
+      ElementB, LayoutB, ElementC, LayoutC, cutlass::arch::OpClassTensorOp,
+      Stages>;
+
+  dim3 grid(1, 1);
+  dim3 block(32, 4, 1);
+
+  test::gemm::threadblock::Testbed<MmaCore>(problem_size.m(), problem_size.n(),
+                                            problem_size.k(), alpha, beta)
+      .run(grid, block);
+}
+
+TEST(SM80_gemm_threadblock_crosswise_sliced, tensor_op_128x64x256_tb128x64x64_warp64x64x32_16x8x16) {
+  using ElementA = cutlass::half_t;
+  using LayoutA = cutlass::layout::RowMajor;
+  using ElementB = cutlass::half_t;
+  using LayoutB = cutlass::layout::ColumnMajor;
+  using ElementC = float;
+  using LayoutC = cutlass::layout::ColumnMajor;
+
+  cutlass::gemm::GemmCoord problem_size(128, 64, 256);
+
+  using ThreadblockShape = cutlass::gemm::GemmShape<128, 64, 64>;
+  using WarpShape = cutlass::gemm::GemmShape<64, 64, 32>;
+  using InstructionShape = cutlass::gemm::GemmShape<16, 8, 16>;
+
+  float alpha = 1.f;
+  float beta = 0.0f;
+  int const Stages = 3;
+
+  // Define the MmaCore components
+  using MmaCore = typename cutlass::gemm::threadblock::DefaultMmaCore<
+      ThreadblockShape, WarpShape, InstructionShape, ElementA, LayoutA,
+      ElementB, LayoutB, ElementC, LayoutC, cutlass::arch::OpClassTensorOp,
+      Stages>;
+
+  dim3 grid(1, 1);
+  dim3 block(32, 4, 1);
+
+  test::gemm::threadblock::Testbed<MmaCore>(problem_size.m(), problem_size.n(),
+                                            problem_size.k(), alpha, beta)
+      .run(grid, block);
+}
+
+////////////////////////////////////////////////////////////////////////////////
 
-}  // namespace kernel
-}  // namespace gemm
-}  // namespace cutlass
+#endif
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/default_rank_2k.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/default_rank_k.h`

 * *Files 7% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -27,15 +27,15 @@
  * OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
  * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
  *
  **************************************************************************************************/
 
 /*! \file
     \brief 
-      Default kernel-level Rank2K definitions combine threadblock-scoped matrix multiply-add with
+      Default kernel-level RankK definitions combine threadblock-scoped matrix multiply-add with
       the appropriate threadblock-scoped epilogue.
 
   
 */
 
 #pragma once
 
@@ -44,15 +44,15 @@
 #include "cutlass/layout/matrix.h"
 #include "cutlass/arch/wmma.h"
 
 #include "cutlass/epilogue/threadblock/epilogue.h"
 #include "cutlass/epilogue/thread/linear_combination.h"
 
 #include "cutlass/gemm/gemm.h"
-#include "cutlass/gemm/kernel/rank_2k_universal.h"
+#include "cutlass/gemm/kernel/rank_k_universal.h"
 #include "cutlass/gemm/threadblock/default_mma_core_sm75.h"
 #include "cutlass/gemm/threadblock/default_mma_core_sm70.h"
 #include "cutlass/gemm/threadblock/default_mma_core_sm80.h"
 #include "cutlass/gemm/threadblock/default_mma.h"
 #include "cutlass/gemm/threadblock/default_mma_core_simt.h"
 #include "cutlass/gemm/threadblock/threadblock_swizzle.h"
 
@@ -77,20 +77,14 @@
 template <
     /// Element type for A matrix operand
     typename ElementA_,
     /// Layout type for A matrix operand
     typename LayoutA_,
     /// Access granularity of A matrix in units of elements
     int kAlignmentA,
-    /// Element type for B matrix operand
-    typename ElementB_,
-    /// Layout type for B matrix operand
-    typename LayoutB_,
-    /// Access granularity of B matrix in units of elements
-    int kAlignmentB,
     /// Element type for C and D matrix operands
     typename ElementC_,
     /// Layout type for C and D matrix operands
     typename LayoutC_,
     /// Fill Mode for C (kLower or kUpper)
     FillMode FillModeC_,
     /// Element type for internal accumulation
@@ -114,33 +108,26 @@
     /// If true, kernel is configured to support serial reduction in the
     /// epilogue
     bool SplitKSerial,
     /// Operation performed by GEMM
     typename Operator,
     /// Blas3 computation mode
     BlasMode BlasMode_ = BlasMode::kSymmetric>
-struct DefaultRank2K;
-
+struct DefaultRankK;
 
 ////////////////////////////////////////////////////////////////////////////////
 
-/// Partial specialization for Ampere Architecture
+/// Partial specialization for Hopper Architecture
 template <
     /// Element type for A matrix operand
     typename ElementA,
     /// Layout type for A matrix operand
     typename LayoutA,
     /// Access granularity of A matrix in units of elements
     int kAlignmentA,
-    /// Element type for B matrix operand
-    typename ElementB,
-    /// Layout type for B matrix operand
-    typename LayoutB,
-    /// Access granularity of A matrix in units of elements
-    int kAlignmentB,
     /// Element type for C and D matrix operands
     typename ElementC,
     /// Fill Mode for C (kLower or kUpper)
     FillMode FillModeC,
     /// Element type for internal accumulation
     typename ElementAccumulator,
     /// Threadblock-level tile size (concept: GemmShape)
@@ -156,52 +143,105 @@
     /// Number of stages used in the pipelined mainloop
     int Stages,
     /// If true, kernel is configured to support serial reduction in the
     /// epilogue
     bool SplitKSerial,
     /// Operation performed by GEMM
     typename Operator>
-struct DefaultRank2K<
+struct DefaultRankK<
                     ElementA, LayoutA, kAlignmentA, 
-                    ElementB, LayoutB, kAlignmentB, 
                     ElementC,layout::RowMajor, FillModeC, 
-                    ElementAccumulator, arch::OpClassTensorOp, arch::Sm80, 
+                    ElementAccumulator, arch::OpClassTensorOp, arch::Sm90, 
                     ThreadblockShape, WarpShape, InstructionShape,
                     EpilogueOutputOp, ThreadblockSwizzle, Stages, SplitKSerial,
                     Operator> {
-  /// Define the threadblock-scoped matrix multiply-accumulate (A x BT)
-  using Mma1 = typename cutlass::gemm::threadblock::DefaultMma<
+  /// Define the threadblock-scoped matrix multiply-accumulate (A x AT)
+  using Mma = typename cutlass::gemm::threadblock::DefaultMma<
       ElementA, LayoutA, 
       kAlignmentA, 
-      ElementB, typename layout::LayoutTranspose<LayoutB>::type, 
-      kAlignmentB,
-      ElementAccumulator, layout::RowMajor, arch::OpClassTensorOp, arch::Sm80,
+      ElementA, typename layout::LayoutTranspose<LayoutA>::type, 
+      kAlignmentA,
+      ElementAccumulator, layout::RowMajor, arch::OpClassTensorOp, arch::Sm90,
       ThreadblockShape, WarpShape, InstructionShape, Stages,
       Operator>::ThreadblockMma;
   
-  /// Define the threadblock-scoped matrix multiply-accumulate (B x AT)
-  using Mma2 = typename cutlass::gemm::threadblock::DefaultMma<
-      ElementB, LayoutB, 
-      kAlignmentB, 
+
+  static const int kPartitionsK = ThreadblockShape::kK / WarpShape::kK;
+
+  /// Define the epilogue
+  using Epilogue =
+      typename cutlass::epilogue::threadblock::DefaultEpilogueTensorOpBlas3<
+          ThreadblockShape, typename Mma::Operator, kPartitionsK, EpilogueOutputOp,
+          EpilogueOutputOp::kCount, BlasMode::kSymmetric>::Epilogue;
+
+  /// Define the kernel-level Rank2 operator.
+  using RankKkernel = kernel::RankKUniversal<Mma, Epilogue, ThreadblockSwizzle, FillModeC>;
+};
+
+////////////////////////////////////////////////////////////////////////////////
+
+/// Partial specialization for Ampere Architecture
+template <
+    /// Element type for A matrix operand
+    typename ElementA,
+    /// Layout type for A matrix operand
+    typename LayoutA,
+    /// Access granularity of A matrix in units of elements
+    int kAlignmentA,
+    /// Element type for C and D matrix operands
+    typename ElementC,
+    /// Fill Mode for C (kLower or kUpper)
+    FillMode FillModeC,
+    /// Element type for internal accumulation
+    typename ElementAccumulator,
+    /// Threadblock-level tile size (concept: GemmShape)
+    typename ThreadblockShape,
+    /// Warp-level tile size (concept: GemmShape)
+    typename WarpShape,
+    /// Warp-level tile size (concept: GemmShape)
+    typename InstructionShape,
+    /// Epilogue output operator
+    typename EpilogueOutputOp,
+    /// Threadblock-level swizzling operator
+    typename ThreadblockSwizzle,
+    /// Number of stages used in the pipelined mainloop
+    int Stages,
+    /// If true, kernel is configured to support serial reduction in the
+    /// epilogue
+    bool SplitKSerial,
+    /// Operation performed by GEMM
+    typename Operator>
+struct DefaultRankK<
+                    ElementA, LayoutA, kAlignmentA, 
+                    ElementC,layout::RowMajor, FillModeC, 
+                    ElementAccumulator, arch::OpClassTensorOp, arch::Sm80, 
+                    ThreadblockShape, WarpShape, InstructionShape,
+                    EpilogueOutputOp, ThreadblockSwizzle, Stages, SplitKSerial,
+                    Operator> {
+  /// Define the threadblock-scoped matrix multiply-accumulate (A x AT)
+  using Mma = typename cutlass::gemm::threadblock::DefaultMma<
+      ElementA, LayoutA, 
+      kAlignmentA, 
       ElementA, typename layout::LayoutTranspose<LayoutA>::type, 
       kAlignmentA,
       ElementAccumulator, layout::RowMajor, arch::OpClassTensorOp, arch::Sm80,
       ThreadblockShape, WarpShape, InstructionShape, Stages,
       Operator>::ThreadblockMma;
+  
 
   static const int kPartitionsK = ThreadblockShape::kK / WarpShape::kK;
 
   /// Define the epilogue
   using Epilogue =
       typename cutlass::epilogue::threadblock::DefaultEpilogueTensorOpBlas3<
-          ThreadblockShape, typename Mma1::Operator, kPartitionsK, EpilogueOutputOp,
+          ThreadblockShape, typename Mma::Operator, kPartitionsK, EpilogueOutputOp,
           EpilogueOutputOp::kCount, BlasMode::kSymmetric>::Epilogue;
 
-  /// Define the kernel-level Rank2K operator.
-  using Rank2Kkernel = kernel::Rank2KUniversal<Mma1, Mma2, Epilogue, ThreadblockSwizzle, FillModeC, BlasMode::kSymmetric>;
+  /// Define the kernel-level Rank2 operator.
+  using RankKkernel = kernel::RankKUniversal<Mma, Epilogue, ThreadblockSwizzle, FillModeC>;
 };
 ////////////////////////////////////////////////////////////////////////////////
 
 
 }  // namespace kernel
 }  // namespace gemm
 }  // namespace cutlass
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/default_rank_2k_complex.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/default_rank_k_complex.h`

 * *Files 12% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -27,15 +27,15 @@
  * OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
  * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
  *
  **************************************************************************************************/
 
 /*! \file
     \brief 
-      Default kernel-level Rank2K definitions combine threadblock-scoped matrix multiply-add with
+      Default kernel-level RankK definitions combine threadblock-scoped matrix multiply-add with
       the appropriate threadblock-scoped epilogue.
 
   
 */
 
 #pragma once
 
@@ -44,15 +44,15 @@
 #include "cutlass/layout/matrix.h"
 #include "cutlass/arch/wmma.h"
 
 #include "cutlass/epilogue/threadblock/epilogue.h"
 #include "cutlass/epilogue/thread/linear_combination.h"
 
 #include "cutlass/gemm/gemm.h"
-#include "cutlass/gemm/kernel/rank_2k_universal.h"
+#include "cutlass/gemm/kernel/rank_k_universal.h"
 #include "cutlass/gemm/threadblock/default_mma_core_sm80.h"
 #include "cutlass/gemm/threadblock/default_mma.h"
 #include "cutlass/gemm/threadblock/default_multistage_mma_complex.h"
 #include "cutlass/gemm/threadblock/threadblock_swizzle.h"
 
 #include "cutlass/epilogue/threadblock/default_epilogue_complex_tensor_op_blas3.h"
 #include "cutlass/transform/threadblock/predicated_tile_iterator.h"
@@ -71,18 +71,14 @@
 ////////////////////////////////////////////////////////////////////////////////
 
 template <
     /// Element type for A matrix operand
     typename ElementA_,
     /// Layout type for A matrix operand
     typename LayoutA_,
-    /// Element type for B matrix operand
-    typename ElementB_,
-    /// Layout type for B matrix operand
-    typename LayoutB_,
     /// Element type for C and D matrix operands
     typename ElementC_,
     /// Layout type for C and D matrix operands
     typename LayoutC_,
     /// Fill Mode for C (kLower or kUpper)
     FillMode FillModeC_,
     /// Element type for internal accumulation
@@ -101,84 +97,74 @@
     typename EpilogueOutputOp,
     /// Threadblock-level swizzling operator
     typename ThreadblockSwizzle,
     /// Number of stages used in the pipelined mainloop
     int Stages,
     /// Complex elementwise transformation on A operand
     ComplexTransform TransformA,
-    /// Complex elementwise transformation on B operand
-    ComplexTransform TransformB,
     /// Operation performed by GEMM
     typename Operator,
     /// If true, kernel is configured to support serial reduction in the
     /// epilogue
     bool SplitKSerial,
     /// Blas3 computation mode
     BlasMode BlasMode_ = BlasMode::kSymmetric>
-struct DefaultRank2KComplex;
+struct DefaultRankKComplex;
 
 
 ////////////////////////////////////////////////////////////////////////////////
 namespace detail {
 
 template <
   /// Layout type for A matrix operand
   typename LayoutA_,
-  /// Layout type for B matrix operand
-  typename LayoutB_,
   /// Complex elementwise transformation 
   ComplexTransform TransformA,
-  /// Complex elementwise transformation 
-  ComplexTransform TransformB,
   /// Blas3 computation mode (symmetric/hermitian)
   BlasMode BlasMode_
-  > struct Rank2KTransposedComplexTransform {
+  > struct RankKTransposedComplexTransform {
   
   static ComplexTransform const kTransformA = TransformA;
-  static ComplexTransform const kTransformB = TransformB;
+  static ComplexTransform const kTransformB = TransformA;
 
 };
   
-  // partial specializations for HER2K CUBLAS_OP_N layout (ColumMajor)
+  // partial specializations for HERK CUBLAS_OP_N layout (ColumMajor)
 template <>
-  struct Rank2KTransposedComplexTransform <
-  layout::ColumnMajor, layout::ColumnMajor, 
-  ComplexTransform::kNone, ComplexTransform::kNone,
+  struct RankKTransposedComplexTransform <
+  layout::ColumnMajor, 
+  ComplexTransform::kNone,
   BlasMode::kHermitian> {
 
   static ComplexTransform const kTransformA = ComplexTransform::kConjugate;
   static ComplexTransform const kTransformB = ComplexTransform::kNone;
 
 };
 
-  // partial specializations for HER2K CUBLAS_OP_C layout (RowMajor + Complex conjugate) 
+  // partial specializations for HERK CUBLAS_OP_C layout (RowMajor + Complex conjugate) 
 template <>
-  struct Rank2KTransposedComplexTransform <
-  layout::RowMajor, layout::RowMajor, 
-  ComplexTransform::kConjugate, ComplexTransform::kConjugate,
+  struct RankKTransposedComplexTransform <
+  layout::RowMajor, 
+  ComplexTransform::kConjugate,
   BlasMode::kHermitian> {
 
   static ComplexTransform const kTransformA = ComplexTransform::kNone;
   static ComplexTransform const kTransformB = ComplexTransform::kConjugate;
 
 };
 
 }
 ////////////////////////////////////////////////////////////////////////////////
 
-/// Partial specialization for Ampere Architecture complex datatype (symmetric)
+/// Partial specialization for Hopper Architecture complex datatype (symmetric)
 template <
     /// Element type for A matrix operand
     typename ElementA,
     /// Layout type for A matrix operand
     typename LayoutA,
-    /// Element type for B matrix operand
-    typename ElementB,
-    /// Layout type for B matrix operand
-    typename LayoutB,
     /// Element type for C and D matrix operands
     typename ElementC,
     /// Fill Mode for C (kLower or kUpper)
     FillMode FillModeC,
     /// Element type for internal accumulation
     typename ElementAccumulator,
     /// Threadblock-level tile size (concept: GemmShape)
@@ -191,69 +177,189 @@
     typename EpilogueOutputOp,
     /// Threadblock-level swizzling operator
     typename ThreadblockSwizzle,
     /// Number of stages used in the pipelined mainloop
     int Stages,
     /// Complex elementwise transformation on A operand
     ComplexTransform TransformA,
-    /// Complex elementwise transformation on B operand
-    ComplexTransform TransformB,
     /// Operation performed by GEMM
     typename Operator,
     /// If true, kernel is configured to support serial reduction in the
     /// epilogue
     bool SplitKSerial>
-struct DefaultRank2KComplex<
-  ElementA, LayoutA, ElementB, LayoutB, ElementC, 
+struct DefaultRankKComplex<
+  ElementA, LayoutA, ElementC, 
   layout::RowMajor, FillModeC, ElementAccumulator, arch::OpClassTensorOp,
-  arch::Sm80, ThreadblockShape, WarpShape, InstructionShape, 
+  arch::Sm90, ThreadblockShape, WarpShape, InstructionShape, 
   EpilogueOutputOp, ThreadblockSwizzle, Stages, 
-  TransformA, TransformB, Operator, SplitKSerial, BlasMode::kSymmetric> {
+  TransformA, Operator, SplitKSerial, BlasMode::kSymmetric> {
 
   static BlasMode const kBlasMode = BlasMode::kSymmetric;
   
   /// Define the threadblock-scoped matrix multiply-accumulate (A x B^T)
-  using Mma1 = typename cutlass::gemm::threadblock::DefaultMultistageMmaComplex<
+  using Mma = typename cutlass::gemm::threadblock::DefaultMultistageMmaComplex<
       ElementA, LayoutA, 
-      ElementB, typename layout::LayoutTranspose<LayoutB>::type, 
-      ElementAccumulator, layout::RowMajor, arch::OpClassTensorOp, arch::Sm80, 
+      ElementA, typename layout::LayoutTranspose<LayoutA>::type, 
+      ElementAccumulator, layout::RowMajor, arch::OpClassTensorOp, arch::Sm90, 
       ThreadblockShape, WarpShape, InstructionShape, Stages, 
-      TransformA, TransformB, Operator>::ThreadblockMma;
+      TransformA, TransformA, Operator>::ThreadblockMma;
+
+  /// Define the epilogue
+  using Epilogue =
+      typename cutlass::epilogue::threadblock::DefaultEpilogueComplexTensorOpBlas3<
+          ThreadblockShape, typename Mma::Operator, 1, EpilogueOutputOp,
+          EpilogueOutputOp::kCount, Operator, kBlasMode>::Epilogue;
+
+  /// Define the kernel-level RankK operator.
+  using RankKkernel = kernel::RankKUniversal<Mma, Epilogue, ThreadblockSwizzle, FillModeC>;
+
+};
+
+////////////////////////////////////////////////////////////////////////////////
+
+/// Partial specialization for Hopper Architecture complex datatype (hermitian)
+template <
+    /// Element type for A matrix operand
+    typename ElementA,
+    /// Layout type for A matrix operand
+    typename LayoutA,
+    /// Element type for C and D matrix operands
+    typename ElementC,
+    /// Fill Mode for C (kLower or kUpper)
+    FillMode FillModeC,
+    /// Element type for internal accumulation
+    typename ElementAccumulator,
+    /// Threadblock-level tile size (concept: GemmShape)
+    typename ThreadblockShape,
+    /// Warp-level tile size (concept: GemmShape)
+    typename WarpShape,
+    /// Warp-level tile size (concept: GemmShape)
+    typename InstructionShape,
+    /// Epilogue output operator
+    typename EpilogueOutputOp,
+    /// Threadblock-level swizzling operator
+    typename ThreadblockSwizzle,
+    /// Number of stages used in the pipelined mainloop
+    int Stages,
+    /// Complex elementwise transformation on A operand
+    ComplexTransform TransformA,
+    /// Operation performed by GEMM
+    typename Operator,
+    /// If true, kernel is configured to support serial reduction in the
+    /// epilogue
+    bool SplitKSerial>
+struct DefaultRankKComplex<
+  ElementA, LayoutA, ElementC, 
+  layout::RowMajor, FillModeC, ElementAccumulator, arch::OpClassTensorOp,
+  arch::Sm90, ThreadblockShape, WarpShape, InstructionShape, 
+  EpilogueOutputOp, ThreadblockSwizzle, Stages, 
+  TransformA, Operator, SplitKSerial, BlasMode::kHermitian> {
+
+  static BlasMode const kBlasMode = BlasMode::kHermitian;
+
+  // Complex transform for input A and B matrices (function on input layout)
+  static ComplexTransform const kTransformA = TransformA;
+
+  using TransposedComplexTransform = detail::RankKTransposedComplexTransform<
+                                        LayoutA, 
+                                        TransformA,
+                                        kBlasMode>;
+
+  // Complex transform on operandA and operandB (function of blas3 computation)
+  static ComplexTransform const kTransformOperandA = TransposedComplexTransform::kTransformA;
+  static ComplexTransform const kTransformOperandB = TransposedComplexTransform::kTransformB;
 
-  /// Define the threadblock-scoped matrix multiply-accumulate (B x A^T)
-  using Mma2 = typename cutlass::gemm::threadblock::DefaultMultistageMmaComplex<
-      ElementB, LayoutB, 
+  /// Define the threadblock-scoped matrix multiply-accumulate (A x A^H)
+  using Mma = typename cutlass::gemm::threadblock::DefaultMultistageMmaComplex<
+      ElementA, LayoutA, 
+      ElementA, typename layout::LayoutTranspose<LayoutA>::type, 
+      ElementAccumulator, layout::RowMajor, arch::OpClassTensorOp, arch::Sm90, 
+      ThreadblockShape, WarpShape, InstructionShape, Stages, 
+      kTransformOperandA, kTransformOperandB, Operator>::ThreadblockMma;
+
+  /// Define the epilogue
+  using Epilogue =
+      typename cutlass::epilogue::threadblock::DefaultEpilogueComplexTensorOpBlas3<
+          ThreadblockShape, typename Mma::Operator, 1, EpilogueOutputOp,
+          EpilogueOutputOp::kCount, Operator, kBlasMode>::Epilogue;
+
+  /// Define the kernel-level RankK operator.
+  using RankKkernel = kernel::RankKUniversal<Mma, Epilogue, ThreadblockSwizzle, FillModeC>;
+
+};
+
+////////////////////////////////////////////////////////////////////////////////
+
+/// Partial specialization for Ampere Architecture complex datatype (symmetric)
+template <
+    /// Element type for A matrix operand
+    typename ElementA,
+    /// Layout type for A matrix operand
+    typename LayoutA,
+    /// Element type for C and D matrix operands
+    typename ElementC,
+    /// Fill Mode for C (kLower or kUpper)
+    FillMode FillModeC,
+    /// Element type for internal accumulation
+    typename ElementAccumulator,
+    /// Threadblock-level tile size (concept: GemmShape)
+    typename ThreadblockShape,
+    /// Warp-level tile size (concept: GemmShape)
+    typename WarpShape,
+    /// Warp-level tile size (concept: GemmShape)
+    typename InstructionShape,
+    /// Epilogue output operator
+    typename EpilogueOutputOp,
+    /// Threadblock-level swizzling operator
+    typename ThreadblockSwizzle,
+    /// Number of stages used in the pipelined mainloop
+    int Stages,
+    /// Complex elementwise transformation on A operand
+    ComplexTransform TransformA,
+    /// Operation performed by GEMM
+    typename Operator,
+    /// If true, kernel is configured to support serial reduction in the
+    /// epilogue
+    bool SplitKSerial>
+struct DefaultRankKComplex<
+  ElementA, LayoutA, ElementC, 
+  layout::RowMajor, FillModeC, ElementAccumulator, arch::OpClassTensorOp,
+  arch::Sm80, ThreadblockShape, WarpShape, InstructionShape, 
+  EpilogueOutputOp, ThreadblockSwizzle, Stages, 
+  TransformA, Operator, SplitKSerial, BlasMode::kSymmetric> {
+
+  static BlasMode const kBlasMode = BlasMode::kSymmetric;
+  
+  /// Define the threadblock-scoped matrix multiply-accumulate (A x B^T)
+  using Mma = typename cutlass::gemm::threadblock::DefaultMultistageMmaComplex<
+      ElementA, LayoutA, 
       ElementA, typename layout::LayoutTranspose<LayoutA>::type, 
       ElementAccumulator, layout::RowMajor, arch::OpClassTensorOp, arch::Sm80, 
       ThreadblockShape, WarpShape, InstructionShape, Stages, 
-      TransformA, TransformB, Operator>::ThreadblockMma;
+      TransformA, TransformA, Operator>::ThreadblockMma;
 
   /// Define the epilogue
   using Epilogue =
       typename cutlass::epilogue::threadblock::DefaultEpilogueComplexTensorOpBlas3<
-          ThreadblockShape, typename Mma1::Operator, 1, EpilogueOutputOp,
+          ThreadblockShape, typename Mma::Operator, 1, EpilogueOutputOp,
           EpilogueOutputOp::kCount, Operator, kBlasMode>::Epilogue;
 
-  /// Define the kernel-level Rank2K operator.
-  using Rank2Kkernel = kernel::Rank2KUniversal<Mma1, Mma2, Epilogue, ThreadblockSwizzle, FillModeC, kBlasMode>;
+  /// Define the kernel-level RankK operator.
+  using RankKkernel = kernel::RankKUniversal<Mma, Epilogue, ThreadblockSwizzle, FillModeC>;
 
 };
 
 ////////////////////////////////////////////////////////////////////////////////
 
 /// Partial specialization for Ampere Architecture complex datatype (hermitian)
 template <
     /// Element type for A matrix operand
     typename ElementA,
     /// Layout type for A matrix operand
     typename LayoutA,
-    /// Element type for B matrix operand
-    typename ElementB,
-    /// Layout type for B matrix operand
-    typename LayoutB,
     /// Element type for C and D matrix operands
     typename ElementC,
     /// Fill Mode for C (kLower or kUpper)
     FillMode FillModeC,
     /// Element type for internal accumulation
     typename ElementAccumulator,
     /// Threadblock-level tile size (concept: GemmShape)
@@ -266,67 +372,56 @@
     typename EpilogueOutputOp,
     /// Threadblock-level swizzling operator
     typename ThreadblockSwizzle,
     /// Number of stages used in the pipelined mainloop
     int Stages,
     /// Complex elementwise transformation on A operand
     ComplexTransform TransformA,
-    /// Complex elementwise transformation on B operand
-    ComplexTransform TransformB,
     /// Operation performed by GEMM
     typename Operator,
     /// If true, kernel is configured to support serial reduction in the
     /// epilogue
     bool SplitKSerial>
-struct DefaultRank2KComplex<
-  ElementA, LayoutA, ElementB, LayoutB, ElementC, 
+struct DefaultRankKComplex<
+  ElementA, LayoutA, ElementC, 
   layout::RowMajor, FillModeC, ElementAccumulator, arch::OpClassTensorOp,
   arch::Sm80, ThreadblockShape, WarpShape, InstructionShape, 
   EpilogueOutputOp, ThreadblockSwizzle, Stages, 
-  TransformA, TransformB, Operator, SplitKSerial, BlasMode::kHermitian> {
+  TransformA, Operator, SplitKSerial, BlasMode::kHermitian> {
 
   static BlasMode const kBlasMode = BlasMode::kHermitian;
 
   // Complex transform for input A and B matrices (function on input layout)
   static ComplexTransform const kTransformA = TransformA;
-  static ComplexTransform const kTransformB = TransformB;
 
-  using TransposedComplexTransform = detail::Rank2KTransposedComplexTransform<
-                                        LayoutA, LayoutB, 
-                                        TransformA, TransformB,
+  using TransposedComplexTransform = detail::RankKTransposedComplexTransform<
+                                        LayoutA, 
+                                        TransformA,
                                         kBlasMode>;
 
   // Complex transform on operandA and operandB (function of blas3 computation)
   static ComplexTransform const kTransformOperandA = TransposedComplexTransform::kTransformA;
   static ComplexTransform const kTransformOperandB = TransposedComplexTransform::kTransformB;
 
-  /// Define the threadblock-scoped matrix multiply-accumulate (A x B^H)
-  using Mma1 = typename cutlass::gemm::threadblock::DefaultMultistageMmaComplex<
+  /// Define the threadblock-scoped matrix multiply-accumulate (A x A^H)
+  using Mma = typename cutlass::gemm::threadblock::DefaultMultistageMmaComplex<
       ElementA, LayoutA, 
-      ElementB, typename layout::LayoutTranspose<LayoutB>::type, 
-      ElementAccumulator, layout::RowMajor, arch::OpClassTensorOp, arch::Sm80, 
-      ThreadblockShape, WarpShape, InstructionShape, Stages, 
-      kTransformOperandA, kTransformOperandB, Operator>::ThreadblockMma;
-
-  /// Define the threadblock-scoped matrix multiply-accumulate (B x A^H)
-  using Mma2 = typename cutlass::gemm::threadblock::DefaultMultistageMmaComplex<
-      ElementB, LayoutB, 
       ElementA, typename layout::LayoutTranspose<LayoutA>::type, 
       ElementAccumulator, layout::RowMajor, arch::OpClassTensorOp, arch::Sm80, 
       ThreadblockShape, WarpShape, InstructionShape, Stages, 
       kTransformOperandA, kTransformOperandB, Operator>::ThreadblockMma;
 
   /// Define the epilogue
   using Epilogue =
       typename cutlass::epilogue::threadblock::DefaultEpilogueComplexTensorOpBlas3<
-          ThreadblockShape, typename Mma1::Operator, 1, EpilogueOutputOp,
+          ThreadblockShape, typename Mma::Operator, 1, EpilogueOutputOp,
           EpilogueOutputOp::kCount, Operator, kBlasMode>::Epilogue;
 
-  /// Define the kernel-level Rank2K operator.
-  using Rank2Kkernel = kernel::Rank2KUniversal<Mma1, Mma2, Epilogue, ThreadblockSwizzle, FillModeC, kBlasMode>;
+  /// Define the kernel-level RankK operator.
+  using RankKkernel = kernel::RankKUniversal<Mma, Epilogue, ThreadblockSwizzle, FillModeC>;
 
 };
 
 ////////////////////////////////////////////////////////////////////////////////
 
 
 }  // namespace kernel
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/default_rank_2k_grouped.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/default_rank_2k_grouped.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/default_rank_2k_universal.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/default_rank_2k_universal.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/default_rank_k.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/default_trmm.h`

 * *Files 20% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -25,42 +25,42 @@
  * SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER
  * CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,
  * OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
  * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
  *
  **************************************************************************************************/
 
+// 
 /*! \file
     \brief 
-      Default kernel-level RankK definitions combine threadblock-scoped matrix multiply-add with
+      Default kernel-level TRMM definitions combine threadblock-scoped matrix multiply-add with
       the appropriate threadblock-scoped epilogue.
-
-  
 */
 
 #pragma once
 
 #include "cutlass/blas3.h"
 
 #include "cutlass/layout/matrix.h"
 #include "cutlass/arch/wmma.h"
 
 #include "cutlass/epilogue/threadblock/epilogue.h"
 #include "cutlass/epilogue/thread/linear_combination.h"
 
 #include "cutlass/gemm/gemm.h"
-#include "cutlass/gemm/kernel/rank_k_universal.h"
+#include "cutlass/gemm/kernel/trmm_universal.h"
 #include "cutlass/gemm/threadblock/default_mma_core_sm75.h"
 #include "cutlass/gemm/threadblock/default_mma_core_sm70.h"
 #include "cutlass/gemm/threadblock/default_mma_core_sm80.h"
 #include "cutlass/gemm/threadblock/default_mma.h"
+#include "cutlass/gemm/threadblock/default_trmm.h"
 #include "cutlass/gemm/threadblock/default_mma_core_simt.h"
 #include "cutlass/gemm/threadblock/threadblock_swizzle.h"
 
-#include "cutlass/epilogue/threadblock/default_epilogue_tensor_op_blas3.h"
+#include "cutlass/epilogue/threadblock/default_epilogue_tensor_op.h"
 #include "cutlass/epilogue/threadblock/default_epilogue_volta_tensor_op.h"
 #include "cutlass/epilogue/threadblock/default_epilogue_simt.h"
 #include "cutlass/transform/threadblock/predicated_tile_iterator.h"
 
 #if defined(CUTLASS_ARCH_WMMA_ENABLED)
 #include "cutlass/epilogue/threadblock/default_epilogue_wmma_tensor_op.h"
 #endif //CUTLASS_ARCH_WMMA_ENABLED
@@ -77,20 +77,30 @@
 template <
     /// Element type for A matrix operand
     typename ElementA_,
     /// Layout type for A matrix operand
     typename LayoutA_,
     /// Access granularity of A matrix in units of elements
     int kAlignmentA,
+    /// Element type for B matrix operand
+    typename ElementB_,
+    /// Layout type for B matrix operand
+    typename LayoutB_,
+    /// Access granularity of B matrix in units of elements
+    int kAlignmentB,
+    /// Side Mode for the kernel
+    SideMode SideMode_,
+    /// Fill Mode for the triangular matrix
+    FillMode FillMode_,
+    /// Diag Type for the triangular matrix
+    DiagType DiagType_,
     /// Element type for C and D matrix operands
     typename ElementC_,
     /// Layout type for C and D matrix operands
     typename LayoutC_,
-    /// Fill Mode for C (kLower or kUpper)
-    FillMode FillModeC_,
     /// Element type for internal accumulation
     typename ElementAccumulator,
     /// Operator class tag
     typename OperatorClass,
     /// Tag indicating architecture to tune for
     typename ArchTag,
     /// Threadblock-level tile size (concept: GemmShape)
@@ -105,34 +115,111 @@
     typename ThreadblockSwizzle,
     /// Number of stages used in the pipelined mainloop
     int Stages,
     /// If true, kernel is configured to support serial reduction in the
     /// epilogue
     bool SplitKSerial,
     /// Operation performed by GEMM
-    typename Operator,
-    /// Blas3 computation mode
-    BlasMode BlasMode_ = BlasMode::kSymmetric>
-struct DefaultRankK;
+    typename Operator>
+struct DefaultTrmm;
+
+////////////////////////////////////////////////////////////////////////////////
+
+/// Partial specialization for Hopper Architecture
+template <
+    /// Element type for A matrix operand
+    typename ElementA,
+    /// Layout type for A matrix operand
+    typename LayoutA,
+    /// Access granularity of A matrix in units of elements
+    int kAlignmentA,
+    /// Element type for B matrix operand
+    typename ElementB,
+    /// Layout type for B matrix operand
+    typename LayoutB,
+    /// Access granularity of A matrix in units of elements
+    int kAlignmentB,
+    /// Side Mode for the kernel
+    SideMode kSideMode,
+    /// Fill Mode for the triangular matrix
+    FillMode kFillMode,
+    /// Diag Type for the triangular matrix
+    DiagType kDiagType,
+    /// Element type for C and D matrix operands
+    typename ElementC,
+    /// Element type for internal accumulation
+    typename ElementAccumulator,
+    /// Threadblock-level tile size (concept: GemmShape)
+    typename ThreadblockShape,
+    /// Warp-level tile size (concept: GemmShape)
+    typename WarpShape,
+    /// Warp-level tile size (concept: GemmShape)
+    typename InstructionShape,
+    /// Epilogue output operator
+    typename EpilogueOutputOp,
+    /// Threadblock-level swizzling operator
+    typename ThreadblockSwizzle,
+    /// Number of stages used in the pipelined mainloop
+    int Stages,
+    /// If true, kernel is configured to support serial reduction in the
+    /// epilogue
+    bool SplitKSerial,
+    /// Operation performed by GEMM
+    typename Operator>
+struct DefaultTrmm<ElementA, LayoutA, kAlignmentA, ElementB, LayoutB, kAlignmentB,
+                   kSideMode, kFillMode, kDiagType, ElementC,
+                   layout::RowMajor, ElementAccumulator, arch::OpClassTensorOp,
+                   arch::Sm90, ThreadblockShape, WarpShape, InstructionShape,
+                   EpilogueOutputOp, ThreadblockSwizzle, Stages, SplitKSerial,
+                   Operator> {
+                    
+  /// Define the threadblock-scoped triagular matrix multiply-accumulate
+  using Mma = typename cutlass::gemm::threadblock::DefaultTrmm<
+      ElementA, LayoutA, kAlignmentA, ElementB, LayoutB, kAlignmentB,
+      kSideMode, kFillMode, kDiagType, 
+      ElementAccumulator, layout::RowMajor, arch::OpClassTensorOp, arch::Sm90,
+      ThreadblockShape, WarpShape, InstructionShape, Stages,
+      Operator>::ThreadblockMma;
+
+  static const int kPartitionsK = ThreadblockShape::kK / WarpShape::kK;
 
+  /// Define the epilogue
+  using Epilogue =
+      typename cutlass::epilogue::threadblock::DefaultEpilogueTensorOp<
+          ThreadblockShape, typename Mma::Operator, kPartitionsK, EpilogueOutputOp,
+          EpilogueOutputOp::kCount>::Epilogue;
+
+  /// Define the kernel-level TRMM operator.
+  using TrmmKernel = kernel::TrmmUniversal<Mma, Epilogue, ThreadblockSwizzle, kSideMode, kFillMode, kDiagType>;
+};
 
 ////////////////////////////////////////////////////////////////////////////////
 
 /// Partial specialization for Ampere Architecture
 template <
     /// Element type for A matrix operand
     typename ElementA,
     /// Layout type for A matrix operand
     typename LayoutA,
     /// Access granularity of A matrix in units of elements
     int kAlignmentA,
+    /// Element type for B matrix operand
+    typename ElementB,
+    /// Layout type for B matrix operand
+    typename LayoutB,
+    /// Access granularity of A matrix in units of elements
+    int kAlignmentB,
+    /// Side Mode for the kernel
+    SideMode kSideMode,
+    /// Fill Mode for the triangular matrix
+    FillMode kFillMode,
+    /// Diag Type for the triangular matrix
+    DiagType kDiagType,
     /// Element type for C and D matrix operands
     typename ElementC,
-    /// Fill Mode for C (kLower or kUpper)
-    FillMode FillModeC,
     /// Element type for internal accumulation
     typename ElementAccumulator,
     /// Threadblock-level tile size (concept: GemmShape)
     typename ThreadblockShape,
     /// Warp-level tile size (concept: GemmShape)
     typename WarpShape,
     /// Warp-level tile size (concept: GemmShape)
@@ -144,42 +231,39 @@
     /// Number of stages used in the pipelined mainloop
     int Stages,
     /// If true, kernel is configured to support serial reduction in the
     /// epilogue
     bool SplitKSerial,
     /// Operation performed by GEMM
     typename Operator>
-struct DefaultRankK<
-                    ElementA, LayoutA, kAlignmentA, 
-                    ElementC,layout::RowMajor, FillModeC, 
-                    ElementAccumulator, arch::OpClassTensorOp, arch::Sm80, 
-                    ThreadblockShape, WarpShape, InstructionShape,
-                    EpilogueOutputOp, ThreadblockSwizzle, Stages, SplitKSerial,
-                    Operator> {
-  /// Define the threadblock-scoped matrix multiply-accumulate (A x AT)
-  using Mma = typename cutlass::gemm::threadblock::DefaultMma<
-      ElementA, LayoutA, 
-      kAlignmentA, 
-      ElementA, typename layout::LayoutTranspose<LayoutA>::type, 
-      kAlignmentA,
+struct DefaultTrmm<ElementA, LayoutA, kAlignmentA, ElementB, LayoutB, kAlignmentB,
+                   kSideMode, kFillMode, kDiagType, ElementC,
+                   layout::RowMajor, ElementAccumulator, arch::OpClassTensorOp,
+                   arch::Sm80, ThreadblockShape, WarpShape, InstructionShape,
+                   EpilogueOutputOp, ThreadblockSwizzle, Stages, SplitKSerial,
+                   Operator> {
+                    
+  /// Define the threadblock-scoped triagular matrix multiply-accumulate
+  using Mma = typename cutlass::gemm::threadblock::DefaultTrmm<
+      ElementA, LayoutA, kAlignmentA, ElementB, LayoutB, kAlignmentB,
+      kSideMode, kFillMode, kDiagType, 
       ElementAccumulator, layout::RowMajor, arch::OpClassTensorOp, arch::Sm80,
       ThreadblockShape, WarpShape, InstructionShape, Stages,
       Operator>::ThreadblockMma;
-  
 
   static const int kPartitionsK = ThreadblockShape::kK / WarpShape::kK;
 
   /// Define the epilogue
   using Epilogue =
-      typename cutlass::epilogue::threadblock::DefaultEpilogueTensorOpBlas3<
+      typename cutlass::epilogue::threadblock::DefaultEpilogueTensorOp<
           ThreadblockShape, typename Mma::Operator, kPartitionsK, EpilogueOutputOp,
-          EpilogueOutputOp::kCount, BlasMode::kSymmetric>::Epilogue;
+          EpilogueOutputOp::kCount>::Epilogue;
 
-  /// Define the kernel-level Rank2 operator.
-  using RankKkernel = kernel::RankKUniversal<Mma, Epilogue, ThreadblockSwizzle, FillModeC>;
+  /// Define the kernel-level TRMM operator.
+  using TrmmKernel = kernel::TrmmUniversal<Mma, Epilogue, ThreadblockSwizzle, kSideMode, kFillMode, kDiagType>;
 };
-////////////////////////////////////////////////////////////////////////////////
 
+////////////////////////////////////////////////////////////////////////////////
 
 }  // namespace kernel
 }  // namespace gemm
 }  // namespace cutlass
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/default_rank_k_complex.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/default_rank_2k.h`

 * *Files 20% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -27,15 +27,15 @@
  * OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
  * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
  *
  **************************************************************************************************/
 
 /*! \file
     \brief 
-      Default kernel-level RankK definitions combine threadblock-scoped matrix multiply-add with
+      Default kernel-level Rank2K definitions combine threadblock-scoped matrix multiply-add with
       the appropriate threadblock-scoped epilogue.
 
   
 */
 
 #pragma once
 
@@ -44,21 +44,25 @@
 #include "cutlass/layout/matrix.h"
 #include "cutlass/arch/wmma.h"
 
 #include "cutlass/epilogue/threadblock/epilogue.h"
 #include "cutlass/epilogue/thread/linear_combination.h"
 
 #include "cutlass/gemm/gemm.h"
-#include "cutlass/gemm/kernel/rank_k_universal.h"
+#include "cutlass/gemm/kernel/rank_2k_universal.h"
+#include "cutlass/gemm/threadblock/default_mma_core_sm75.h"
+#include "cutlass/gemm/threadblock/default_mma_core_sm70.h"
 #include "cutlass/gemm/threadblock/default_mma_core_sm80.h"
 #include "cutlass/gemm/threadblock/default_mma.h"
-#include "cutlass/gemm/threadblock/default_multistage_mma_complex.h"
+#include "cutlass/gemm/threadblock/default_mma_core_simt.h"
 #include "cutlass/gemm/threadblock/threadblock_swizzle.h"
 
-#include "cutlass/epilogue/threadblock/default_epilogue_complex_tensor_op_blas3.h"
+#include "cutlass/epilogue/threadblock/default_epilogue_tensor_op_blas3.h"
+#include "cutlass/epilogue/threadblock/default_epilogue_volta_tensor_op.h"
+#include "cutlass/epilogue/threadblock/default_epilogue_simt.h"
 #include "cutlass/transform/threadblock/predicated_tile_iterator.h"
 
 #if defined(CUTLASS_ARCH_WMMA_ENABLED)
 #include "cutlass/epilogue/threadblock/default_epilogue_wmma_tensor_op.h"
 #endif //CUTLASS_ARCH_WMMA_ENABLED
 
 
@@ -71,14 +75,22 @@
 ////////////////////////////////////////////////////////////////////////////////
 
 template <
     /// Element type for A matrix operand
     typename ElementA_,
     /// Layout type for A matrix operand
     typename LayoutA_,
+    /// Access granularity of A matrix in units of elements
+    int kAlignmentA,
+    /// Element type for B matrix operand
+    typename ElementB_,
+    /// Layout type for B matrix operand
+    typename LayoutB_,
+    /// Access granularity of B matrix in units of elements
+    int kAlignmentB,
     /// Element type for C and D matrix operands
     typename ElementC_,
     /// Layout type for C and D matrix operands
     typename LayoutC_,
     /// Fill Mode for C (kLower or kUpper)
     FillMode FillModeC_,
     /// Element type for internal accumulation
@@ -95,76 +107,39 @@
     typename InstructionShape,
     /// Epilogue output operator
     typename EpilogueOutputOp,
     /// Threadblock-level swizzling operator
     typename ThreadblockSwizzle,
     /// Number of stages used in the pipelined mainloop
     int Stages,
-    /// Complex elementwise transformation on A operand
-    ComplexTransform TransformA,
-    /// Operation performed by GEMM
-    typename Operator,
     /// If true, kernel is configured to support serial reduction in the
     /// epilogue
     bool SplitKSerial,
+    /// Operation performed by GEMM
+    typename Operator,
     /// Blas3 computation mode
     BlasMode BlasMode_ = BlasMode::kSymmetric>
-struct DefaultRankKComplex;
-
-
-////////////////////////////////////////////////////////////////////////////////
-namespace detail {
-
-template <
-  /// Layout type for A matrix operand
-  typename LayoutA_,
-  /// Complex elementwise transformation 
-  ComplexTransform TransformA,
-  /// Blas3 computation mode (symmetric/hermitian)
-  BlasMode BlasMode_
-  > struct RankKTransposedComplexTransform {
-  
-  static ComplexTransform const kTransformA = TransformA;
-  static ComplexTransform const kTransformB = TransformA;
-
-};
-  
-  // partial specializations for HERK CUBLAS_OP_N layout (ColumMajor)
-template <>
-  struct RankKTransposedComplexTransform <
-  layout::ColumnMajor, 
-  ComplexTransform::kNone,
-  BlasMode::kHermitian> {
-
-  static ComplexTransform const kTransformA = ComplexTransform::kConjugate;
-  static ComplexTransform const kTransformB = ComplexTransform::kNone;
-
-};
-
-  // partial specializations for HERK CUBLAS_OP_C layout (RowMajor + Complex conjugate) 
-template <>
-  struct RankKTransposedComplexTransform <
-  layout::RowMajor, 
-  ComplexTransform::kConjugate,
-  BlasMode::kHermitian> {
-
-  static ComplexTransform const kTransformA = ComplexTransform::kNone;
-  static ComplexTransform const kTransformB = ComplexTransform::kConjugate;
-
-};
+struct DefaultRank2K;
 
-}
 ////////////////////////////////////////////////////////////////////////////////
 
-/// Partial specialization for Ampere Architecture complex datatype (symmetric)
+/// Partial specialization for Hopper Architecture
 template <
     /// Element type for A matrix operand
     typename ElementA,
     /// Layout type for A matrix operand
     typename LayoutA,
+    /// Access granularity of A matrix in units of elements
+    int kAlignmentA,
+    /// Element type for B matrix operand
+    typename ElementB,
+    /// Layout type for B matrix operand
+    typename LayoutB,
+    /// Access granularity of A matrix in units of elements
+    int kAlignmentB,
     /// Element type for C and D matrix operands
     typename ElementC,
     /// Fill Mode for C (kLower or kUpper)
     FillMode FillModeC,
     /// Element type for internal accumulation
     typename ElementAccumulator,
     /// Threadblock-level tile size (concept: GemmShape)
@@ -175,57 +150,75 @@
     typename InstructionShape,
     /// Epilogue output operator
     typename EpilogueOutputOp,
     /// Threadblock-level swizzling operator
     typename ThreadblockSwizzle,
     /// Number of stages used in the pipelined mainloop
     int Stages,
-    /// Complex elementwise transformation on A operand
-    ComplexTransform TransformA,
-    /// Operation performed by GEMM
-    typename Operator,
     /// If true, kernel is configured to support serial reduction in the
     /// epilogue
-    bool SplitKSerial>
-struct DefaultRankKComplex<
-  ElementA, LayoutA, ElementC, 
-  layout::RowMajor, FillModeC, ElementAccumulator, arch::OpClassTensorOp,
-  arch::Sm80, ThreadblockShape, WarpShape, InstructionShape, 
-  EpilogueOutputOp, ThreadblockSwizzle, Stages, 
-  TransformA, Operator, SplitKSerial, BlasMode::kSymmetric> {
-
-  static BlasMode const kBlasMode = BlasMode::kSymmetric;
-  
-  /// Define the threadblock-scoped matrix multiply-accumulate (A x B^T)
-  using Mma = typename cutlass::gemm::threadblock::DefaultMultistageMmaComplex<
+    bool SplitKSerial,
+    /// Operation performed by GEMM
+    typename Operator>
+struct DefaultRank2K<
+                    ElementA, LayoutA, kAlignmentA, 
+                    ElementB, LayoutB, kAlignmentB, 
+                    ElementC,layout::RowMajor, FillModeC, 
+                    ElementAccumulator, arch::OpClassTensorOp, arch::Sm90, 
+                    ThreadblockShape, WarpShape, InstructionShape,
+                    EpilogueOutputOp, ThreadblockSwizzle, Stages, SplitKSerial,
+                    Operator> {
+  /// Define the threadblock-scoped matrix multiply-accumulate (A x BT)
+  using Mma1 = typename cutlass::gemm::threadblock::DefaultMma<
       ElementA, LayoutA, 
+      kAlignmentA, 
+      ElementB, typename layout::LayoutTranspose<LayoutB>::type, 
+      kAlignmentB,
+      ElementAccumulator, layout::RowMajor, arch::OpClassTensorOp, arch::Sm90,
+      ThreadblockShape, WarpShape, InstructionShape, Stages,
+      Operator>::ThreadblockMma;
+  
+  /// Define the threadblock-scoped matrix multiply-accumulate (B x AT)
+  using Mma2 = typename cutlass::gemm::threadblock::DefaultMma<
+      ElementB, LayoutB, 
+      kAlignmentB, 
       ElementA, typename layout::LayoutTranspose<LayoutA>::type, 
-      ElementAccumulator, layout::RowMajor, arch::OpClassTensorOp, arch::Sm80, 
-      ThreadblockShape, WarpShape, InstructionShape, Stages, 
-      TransformA, TransformA, Operator>::ThreadblockMma;
+      kAlignmentA,
+      ElementAccumulator, layout::RowMajor, arch::OpClassTensorOp, arch::Sm90,
+      ThreadblockShape, WarpShape, InstructionShape, Stages,
+      Operator>::ThreadblockMma;
+
+  static const int kPartitionsK = ThreadblockShape::kK / WarpShape::kK;
 
   /// Define the epilogue
   using Epilogue =
-      typename cutlass::epilogue::threadblock::DefaultEpilogueComplexTensorOpBlas3<
-          ThreadblockShape, typename Mma::Operator, 1, EpilogueOutputOp,
-          EpilogueOutputOp::kCount, Operator, kBlasMode>::Epilogue;
-
-  /// Define the kernel-level RankK operator.
-  using RankKkernel = kernel::RankKUniversal<Mma, Epilogue, ThreadblockSwizzle, FillModeC>;
+      typename cutlass::epilogue::threadblock::DefaultEpilogueTensorOpBlas3<
+          ThreadblockShape, typename Mma1::Operator, kPartitionsK, EpilogueOutputOp,
+          EpilogueOutputOp::kCount, BlasMode::kSymmetric>::Epilogue;
 
+  /// Define the kernel-level Rank2K operator.
+  using Rank2Kkernel = kernel::Rank2KUniversal<Mma1, Mma2, Epilogue, ThreadblockSwizzle, FillModeC, BlasMode::kSymmetric>;
 };
 
 ////////////////////////////////////////////////////////////////////////////////
 
-/// Partial specialization for Ampere Architecture complex datatype (hermitian)
+/// Partial specialization for Ampere Architecture
 template <
     /// Element type for A matrix operand
     typename ElementA,
     /// Layout type for A matrix operand
     typename LayoutA,
+    /// Access granularity of A matrix in units of elements
+    int kAlignmentA,
+    /// Element type for B matrix operand
+    typename ElementB,
+    /// Layout type for B matrix operand
+    typename LayoutB,
+    /// Access granularity of A matrix in units of elements
+    int kAlignmentB,
     /// Element type for C and D matrix operands
     typename ElementC,
     /// Fill Mode for C (kLower or kUpper)
     FillMode FillModeC,
     /// Element type for internal accumulation
     typename ElementAccumulator,
     /// Threadblock-level tile size (concept: GemmShape)
@@ -236,60 +229,57 @@
     typename InstructionShape,
     /// Epilogue output operator
     typename EpilogueOutputOp,
     /// Threadblock-level swizzling operator
     typename ThreadblockSwizzle,
     /// Number of stages used in the pipelined mainloop
     int Stages,
-    /// Complex elementwise transformation on A operand
-    ComplexTransform TransformA,
-    /// Operation performed by GEMM
-    typename Operator,
     /// If true, kernel is configured to support serial reduction in the
     /// epilogue
-    bool SplitKSerial>
-struct DefaultRankKComplex<
-  ElementA, LayoutA, ElementC, 
-  layout::RowMajor, FillModeC, ElementAccumulator, arch::OpClassTensorOp,
-  arch::Sm80, ThreadblockShape, WarpShape, InstructionShape, 
-  EpilogueOutputOp, ThreadblockSwizzle, Stages, 
-  TransformA, Operator, SplitKSerial, BlasMode::kHermitian> {
-
-  static BlasMode const kBlasMode = BlasMode::kHermitian;
-
-  // Complex transform for input A and B matrices (function on input layout)
-  static ComplexTransform const kTransformA = TransformA;
-
-  using TransposedComplexTransform = detail::RankKTransposedComplexTransform<
-                                        LayoutA, 
-                                        TransformA,
-                                        kBlasMode>;
-
-  // Complex transform on operandA and operandB (function of blas3 computation)
-  static ComplexTransform const kTransformOperandA = TransposedComplexTransform::kTransformA;
-  static ComplexTransform const kTransformOperandB = TransposedComplexTransform::kTransformB;
-
-  /// Define the threadblock-scoped matrix multiply-accumulate (A x A^H)
-  using Mma = typename cutlass::gemm::threadblock::DefaultMultistageMmaComplex<
+    bool SplitKSerial,
+    /// Operation performed by GEMM
+    typename Operator>
+struct DefaultRank2K<
+                    ElementA, LayoutA, kAlignmentA, 
+                    ElementB, LayoutB, kAlignmentB, 
+                    ElementC,layout::RowMajor, FillModeC, 
+                    ElementAccumulator, arch::OpClassTensorOp, arch::Sm80, 
+                    ThreadblockShape, WarpShape, InstructionShape,
+                    EpilogueOutputOp, ThreadblockSwizzle, Stages, SplitKSerial,
+                    Operator> {
+  /// Define the threadblock-scoped matrix multiply-accumulate (A x BT)
+  using Mma1 = typename cutlass::gemm::threadblock::DefaultMma<
       ElementA, LayoutA, 
+      kAlignmentA, 
+      ElementB, typename layout::LayoutTranspose<LayoutB>::type, 
+      kAlignmentB,
+      ElementAccumulator, layout::RowMajor, arch::OpClassTensorOp, arch::Sm80,
+      ThreadblockShape, WarpShape, InstructionShape, Stages,
+      Operator>::ThreadblockMma;
+  
+  /// Define the threadblock-scoped matrix multiply-accumulate (B x AT)
+  using Mma2 = typename cutlass::gemm::threadblock::DefaultMma<
+      ElementB, LayoutB, 
+      kAlignmentB, 
       ElementA, typename layout::LayoutTranspose<LayoutA>::type, 
-      ElementAccumulator, layout::RowMajor, arch::OpClassTensorOp, arch::Sm80, 
-      ThreadblockShape, WarpShape, InstructionShape, Stages, 
-      kTransformOperandA, kTransformOperandB, Operator>::ThreadblockMma;
+      kAlignmentA,
+      ElementAccumulator, layout::RowMajor, arch::OpClassTensorOp, arch::Sm80,
+      ThreadblockShape, WarpShape, InstructionShape, Stages,
+      Operator>::ThreadblockMma;
+
+  static const int kPartitionsK = ThreadblockShape::kK / WarpShape::kK;
 
   /// Define the epilogue
   using Epilogue =
-      typename cutlass::epilogue::threadblock::DefaultEpilogueComplexTensorOpBlas3<
-          ThreadblockShape, typename Mma::Operator, 1, EpilogueOutputOp,
-          EpilogueOutputOp::kCount, Operator, kBlasMode>::Epilogue;
-
-  /// Define the kernel-level RankK operator.
-  using RankKkernel = kernel::RankKUniversal<Mma, Epilogue, ThreadblockSwizzle, FillModeC>;
+      typename cutlass::epilogue::threadblock::DefaultEpilogueTensorOpBlas3<
+          ThreadblockShape, typename Mma1::Operator, kPartitionsK, EpilogueOutputOp,
+          EpilogueOutputOp::kCount, BlasMode::kSymmetric>::Epilogue;
 
+  /// Define the kernel-level Rank2K operator.
+  using Rank2Kkernel = kernel::Rank2KUniversal<Mma1, Mma2, Epilogue, ThreadblockSwizzle, FillModeC, BlasMode::kSymmetric>;
 };
-
 ////////////////////////////////////////////////////////////////////////////////
 
 
 }  // namespace kernel
 }  // namespace gemm
 }  // namespace cutlass
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/default_rank_k_universal.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/default_rank_k_universal.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/default_symm.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/default_trmm_complex.h`

 * *Files 11% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -27,127 +27,189 @@
  * OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
  * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
  *
  **************************************************************************************************/
 
 /*! \file
     \brief 
-      Default kernel-level SYMM/HEMM definitions combine threadblock-scoped matrix multiply-add with
+      Default kernel-level TRMM definitions combine threadblock-scoped matrix multiply-add with
       the appropriate threadblock-scoped epilogue.
+  
+      Note, CUTLASS epilogues universally target row-major outputs. Column-major outputs are
+      accommodated by exchanging A and B operands and assuming transposed layouts.
 
   
 */
 
 #pragma once
 
 #include "cutlass/blas3.h"
 
 #include "cutlass/layout/matrix.h"
-#include "cutlass/arch/wmma.h"
 
 #include "cutlass/epilogue/threadblock/epilogue.h"
 #include "cutlass/epilogue/thread/linear_combination.h"
 
 #include "cutlass/gemm/gemm.h"
-#include "cutlass/gemm/kernel/symm_universal.h"
+#include "cutlass/gemm/kernel/trmm_universal.h"
 #include "cutlass/gemm/threadblock/default_mma_core_sm75.h"
 #include "cutlass/gemm/threadblock/default_mma_core_sm70.h"
-#include "cutlass/gemm/threadblock/default_mma_core_sm80.h"
-#include "cutlass/gemm/threadblock/default_trmm.h"
+#include "cutlass/gemm/threadblock/default_multistage_mma_complex_core_sm80.h"
 #include "cutlass/gemm/threadblock/default_mma.h"
+#include "cutlass/gemm/threadblock/default_multistage_trmm_complex.h"
 #include "cutlass/gemm/threadblock/default_mma_core_simt.h"
 #include "cutlass/gemm/threadblock/threadblock_swizzle.h"
-
-#include "cutlass/epilogue/threadblock/default_epilogue_tensor_op.h"
-#include "cutlass/epilogue/threadblock/default_epilogue_volta_tensor_op.h"
+#include "cutlass/epilogue/threadblock/default_epilogue_complex_tensor_op.h"
 #include "cutlass/epilogue/threadblock/default_epilogue_simt.h"
-#include "cutlass/transform/threadblock/predicated_tile_iterator.h"
-
-#if defined(CUTLASS_ARCH_WMMA_ENABLED)
-#include "cutlass/epilogue/threadblock/default_epilogue_wmma_tensor_op.h"
-#endif //CUTLASS_ARCH_WMMA_ENABLED
 
+#include "cutlass/transform/threadblock/predicated_tile_iterator.h"
 
 ////////////////////////////////////////////////////////////////////////////////
 
 namespace cutlass {
 namespace gemm {
 namespace kernel {
 
 ////////////////////////////////////////////////////////////////////////////////
 
 template <
+  /// Element type for A matrix operand
+  typename ElementA_,
+  /// Layout type for A matrix operand
+  typename LayoutA_,
+  /// Element type for B matrix operand
+  typename ElementB_,
+  /// Layout type for B matrix operand
+  typename LayoutB_,
+  /// Side Mode for the kernel
+  SideMode SideMode_,
+  /// Fill Mode for the triangular matrix
+  FillMode FillMode_,
+  /// Diag Type for the triangular matrix
+  DiagType DiagType_,
+  /// Element type for C and D matrix operands
+  typename ElementC_,
+  /// Layout type for C and D matrix operands
+  typename LayoutC_,
+  /// Element type for internal accumulation
+  typename ElementAccumulator,
+  /// Operator class tag
+  typename OperatorClass,
+  /// Tag indicating architecture to tune for
+  typename ArchTag,
+  /// Threadblock-level tile size (concept: GemmShape)
+  typename ThreadblockShape,
+  /// Warp-level tile size (concept: GemmShape)
+  typename WarpShape,
+  /// Warp-level tile size (concept: GemmShape)
+  typename InstructionShape,
+  /// Epilogue output operator
+  typename EpilogueOutputOp,
+  /// Threadblock-level swizzling operator
+  typename ThreadblockSwizzle,
+  /// Number of stages used in the pipelined mainloop
+  int Stages,
+  /// Complex elementwise transformation on A operand
+  ComplexTransform TransformA,
+  /// Complex elementwise transformation on B operand
+  ComplexTransform TransformB,
+  /// Multiply-add operator 
+  // (arch::OpMultiplyAddComplex, arch::OpMultiplyGaussianComplex)
+  typename Operator,
+  /// If true, kernel is configured to support serial reduction in the epilogue
+  bool SplitKSerial
+>
+struct DefaultTrmmComplex;
+
+////////////////////////////////////////////////////////////////////////////////
+
+/// Partial specialization for Hopper Architecture
+template <
     /// Element type for A matrix operand
-    typename ElementA_,
+    typename ElementA,
     /// Layout type for A matrix operand
-    typename LayoutA_,
-    /// Side Mode for A (kLeft or kRight)
-    SideMode kSideModeA,
-    /// Fill Mode for A (kLower or kUpper)
-    FillMode kFillModeA,
-    /// Access granularity of A matrix in units of elements
-    int kAlignmentA,
+    typename LayoutA,
     /// Element type for B matrix operand
-    typename ElementB_,
+    typename ElementB,
     /// Layout type for B matrix operand
-    typename LayoutB_,
-    /// Access granularity of B matrix in units of elements
-    int kAlignmentB,
+    typename LayoutB,
+    /// Side Mode for the kernel
+    SideMode kSideMode,
+    /// Fill Mode for the triangular matrix
+    FillMode kFillMode,
+    /// Diag Type for the triangular matrix
+    DiagType kDiagType,
     /// Element type for C and D matrix operands
-    typename ElementC_,
-    /// Layout type for C and D matrix operands
-    typename LayoutC_,
+    typename ElementC,
     /// Element type for internal accumulation
     typename ElementAccumulator,
-    /// Operator class tag
-    typename OperatorClass,
-    /// Tag indicating architecture to tune for
-    typename ArchTag,
     /// Threadblock-level tile size (concept: GemmShape)
     typename ThreadblockShape,
     /// Warp-level tile size (concept: GemmShape)
     typename WarpShape,
     /// Warp-level tile size (concept: GemmShape)
     typename InstructionShape,
     /// Epilogue output operator
     typename EpilogueOutputOp,
     /// Threadblock-level swizzling operator
     typename ThreadblockSwizzle,
     /// Number of stages used in the pipelined mainloop
     int Stages,
-    /// If true, kernel is configured to support serial reduction in the
-    /// epilogue
-    bool SplitKSerial,
-    /// Operation performed by GEMM
+    /// Complex elementwise transformation on A operand
+    ComplexTransform TransformA,
+    /// Complex elementwise transformation on B operand
+    ComplexTransform TransformB,
+    /// Multiply-add operator 
+    // (arch::OpMultiplyAddComplex, arch::OpMultiplyGaussianComplex)
     typename Operator,
-    /// Blas3 computation mode
-    BlasMode BlasMode_ = BlasMode::kSymmetric>
-struct DefaultSymm;
+    /// If true, kernel is configured to support serial reduction in the epilogue
+    bool SplitKSerial
+  >
+struct DefaultTrmmComplex<
+  ElementA, LayoutA, ElementB, LayoutB, 
+  kSideMode, kFillMode, kDiagType,
+  ElementC, layout::RowMajor, ElementAccumulator, arch::OpClassTensorOp,
+  arch::Sm90, ThreadblockShape, WarpShape, InstructionShape,
+  EpilogueOutputOp, ThreadblockSwizzle, Stages, TransformA, TransformB, Operator, SplitKSerial> {
+
+  /// Define the threadblock-scoped matrix multiply-accumulate
+  using Mma = typename cutlass::gemm::threadblock::DefaultMultistageTrmmComplex<
+      ElementA, LayoutA, ElementB, LayoutB, 
+      kSideMode, kFillMode, kDiagType,
+      ElementAccumulator,layout::RowMajor, arch::OpClassTensorOp, arch::Sm90, ThreadblockShape,
+      WarpShape, InstructionShape, Stages, TransformA, TransformB, Operator>::ThreadblockMma;
+
+  /// Define the epilogue
+  using Epilogue =
+      typename cutlass::epilogue::threadblock::DefaultEpilogueComplexTensorOp<
+          ThreadblockShape, typename Mma::Operator, 1, EpilogueOutputOp,
+          EpilogueOutputOp::kCount, Operator>::Epilogue;
 
+  /// Define the kernel-level TRMM operator.
+  using TrmmKernel = kernel::TrmmUniversal<Mma, Epilogue, ThreadblockSwizzle, kSideMode, kFillMode, kDiagType>;
+};
 
 ////////////////////////////////////////////////////////////////////////////////
 
 /// Partial specialization for Ampere Architecture
 template <
     /// Element type for A matrix operand
     typename ElementA,
     /// Layout type for A matrix operand
     typename LayoutA,
-    /// Side Mode for A (kLeft or kRight)
-    SideMode kSideModeA,
-    /// Fill Mode for A (kLower or kUpper)
-    FillMode kFillModeA,
-    /// Access granularity of A matrix in units of elements
-    int kAlignmentA,
     /// Element type for B matrix operand
     typename ElementB,
     /// Layout type for B matrix operand
     typename LayoutB,
-    /// Access granularity of A matrix in units of elements
-    int kAlignmentB,
+    /// Side Mode for the kernel
+    SideMode kSideMode,
+    /// Fill Mode for the triangular matrix
+    FillMode kFillMode,
+    /// Diag Type for the triangular matrix
+    DiagType kDiagType,
     /// Element type for C and D matrix operands
     typename ElementC,
     /// Element type for internal accumulation
     typename ElementAccumulator,
     /// Threadblock-level tile size (concept: GemmShape)
     typename ThreadblockShape,
     /// Warp-level tile size (concept: GemmShape)
@@ -156,72 +218,48 @@
     typename InstructionShape,
     /// Epilogue output operator
     typename EpilogueOutputOp,
     /// Threadblock-level swizzling operator
     typename ThreadblockSwizzle,
     /// Number of stages used in the pipelined mainloop
     int Stages,
-    /// If true, kernel is configured to support serial reduction in the
-    /// epilogue
-    bool SplitKSerial,
-    /// Operation performed by GEMM
-    typename Operator>
-struct DefaultSymm<
-                    ElementA, LayoutA, kSideModeA, kFillModeA, kAlignmentA, 
-                    ElementB, LayoutB, kAlignmentB, 
-                    ElementC,layout::RowMajor, 
-                    ElementAccumulator, arch::OpClassTensorOp, arch::Sm80, 
-                    ThreadblockShape, WarpShape, InstructionShape,
-                    EpilogueOutputOp, ThreadblockSwizzle, Stages, SplitKSerial,
-                    Operator> {
-
-  /// Define the threadblock-scoped triagular matrix multiply-accumulate
-  /// TRMM - with diagonal: alpha * A * B or alpha * B * A
-	static const DiagType kDiagTypeMma1 = DiagType::kNonUnit;
-  using Mma1 = typename cutlass::gemm::threadblock::DefaultTrmm<
-      ElementA, LayoutA, kAlignmentA, 
-      ElementB, LayoutB, kAlignmentB,
-      kSideModeA, kFillModeA, kDiagTypeMma1, 
-      ElementAccumulator, layout::RowMajor, 
-      arch::OpClassTensorOp, arch::Sm80,
-      ThreadblockShape, WarpShape, InstructionShape,
-      Stages, Operator>::ThreadblockMma;
-
-  /// Define the threadblock-scoped triagular matrix multiply-accumulate 
-  /// TRMM - withOUT diagonal: alpha * AT * B or alpha * B * AT
-	static const DiagType kDiagTypeMma2 = DiagType::kZero;
-  using LayoutAMma2 = typename platform::conditional<
-                                (kSideModeA == SideMode::kLeft), 
-                                typename layout::LayoutTranspose<LayoutA>::type, 
-                                LayoutA
-                              >::type;
-  using LayoutBMma2 = typename platform::conditional<
-                                (kSideModeA == SideMode::kLeft), 
-                                LayoutB, 
-                                typename layout::LayoutTranspose<LayoutB>::type
-                              >::type; 
-	using Mma2 = typename cutlass::gemm::threadblock::DefaultTrmm<
-			ElementA, LayoutAMma2, kAlignmentA, 
-			ElementB, LayoutBMma2, kAlignmentB,
-			kSideModeA, InvertFillMode<kFillModeA>::mode, kDiagTypeMma2, 
-			ElementAccumulator, layout::RowMajor, 
-			arch::OpClassTensorOp, arch::Sm80,
-			ThreadblockShape, WarpShape, InstructionShape,
-			Stages, Operator>::ThreadblockMma;
-
-  static const int kPartitionsK = ThreadblockShape::kK / WarpShape::kK;
+    /// Complex elementwise transformation on A operand
+    ComplexTransform TransformA,
+    /// Complex elementwise transformation on B operand
+    ComplexTransform TransformB,
+    /// Multiply-add operator 
+    // (arch::OpMultiplyAddComplex, arch::OpMultiplyGaussianComplex)
+    typename Operator,
+    /// If true, kernel is configured to support serial reduction in the epilogue
+    bool SplitKSerial
+  >
+struct DefaultTrmmComplex<
+  ElementA, LayoutA, ElementB, LayoutB, 
+  kSideMode, kFillMode, kDiagType,
+  ElementC, layout::RowMajor, ElementAccumulator, arch::OpClassTensorOp,
+  arch::Sm80, ThreadblockShape, WarpShape, InstructionShape,
+  EpilogueOutputOp, ThreadblockSwizzle, Stages, TransformA, TransformB, Operator, SplitKSerial> {
+
+  /// Define the threadblock-scoped matrix multiply-accumulate
+  using Mma = typename cutlass::gemm::threadblock::DefaultMultistageTrmmComplex<
+      ElementA, LayoutA, ElementB, LayoutB, 
+      kSideMode, kFillMode, kDiagType,
+      ElementAccumulator,layout::RowMajor, arch::OpClassTensorOp, arch::Sm80, ThreadblockShape,
+      WarpShape, InstructionShape, Stages, TransformA, TransformB, Operator>::ThreadblockMma;
 
   /// Define the epilogue
   using Epilogue =
-      typename cutlass::epilogue::threadblock::DefaultEpilogueTensorOp<
-          ThreadblockShape, typename Mma1::Operator, kPartitionsK, EpilogueOutputOp,
-          EpilogueOutputOp::kCount>::Epilogue;
+      typename cutlass::epilogue::threadblock::DefaultEpilogueComplexTensorOp<
+          ThreadblockShape, typename Mma::Operator, 1, EpilogueOutputOp,
+          EpilogueOutputOp::kCount, Operator>::Epilogue;
 
-  /// Define the kernel-level SYMM/HEMM operator.
-  using SymmKernel = kernel::SymmUniversal<Mma1, Mma2, Epilogue, ThreadblockSwizzle, kSideModeA, kFillModeA>;
+  /// Define the kernel-level TRMM operator.
+  using TrmmKernel = kernel::TrmmUniversal<Mma, Epilogue, ThreadblockSwizzle, kSideMode, kFillMode, kDiagType>;
 };
-////////////////////////////////////////////////////////////////////////////////
 
+////////////////////////////////////////////////////////////////////////////////
 
 }  // namespace kernel
 }  // namespace gemm
 }  // namespace cutlass
+
+////////////////////////////////////////////////////////////////////////////////
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/default_symm_complex.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/default_symm.h`

 * *Files 15% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -45,21 +45,25 @@
 #include "cutlass/arch/wmma.h"
 
 #include "cutlass/epilogue/threadblock/epilogue.h"
 #include "cutlass/epilogue/thread/linear_combination.h"
 
 #include "cutlass/gemm/gemm.h"
 #include "cutlass/gemm/kernel/symm_universal.h"
+#include "cutlass/gemm/threadblock/default_mma_core_sm75.h"
+#include "cutlass/gemm/threadblock/default_mma_core_sm70.h"
 #include "cutlass/gemm/threadblock/default_mma_core_sm80.h"
+#include "cutlass/gemm/threadblock/default_trmm.h"
 #include "cutlass/gemm/threadblock/default_mma.h"
-#include "cutlass/gemm/threadblock/default_multistage_trmm_complex.h"
-#include "cutlass/gemm/threadblock/default_multistage_mma_complex.h"
+#include "cutlass/gemm/threadblock/default_mma_core_simt.h"
 #include "cutlass/gemm/threadblock/threadblock_swizzle.h"
 
-#include "cutlass/epilogue/threadblock/default_epilogue_complex_tensor_op.h"
+#include "cutlass/epilogue/threadblock/default_epilogue_tensor_op.h"
+#include "cutlass/epilogue/threadblock/default_epilogue_volta_tensor_op.h"
+#include "cutlass/epilogue/threadblock/default_epilogue_simt.h"
 #include "cutlass/transform/threadblock/predicated_tile_iterator.h"
 
 #if defined(CUTLASS_ARCH_WMMA_ENABLED)
 #include "cutlass/epilogue/threadblock/default_epilogue_wmma_tensor_op.h"
 #endif //CUTLASS_ARCH_WMMA_ENABLED
 
 
@@ -76,18 +80,22 @@
     typename ElementA_,
     /// Layout type for A matrix operand
     typename LayoutA_,
     /// Side Mode for A (kLeft or kRight)
     SideMode kSideModeA,
     /// Fill Mode for A (kLower or kUpper)
     FillMode kFillModeA,
+    /// Access granularity of A matrix in units of elements
+    int kAlignmentA,
     /// Element type for B matrix operand
     typename ElementB_,
     /// Layout type for B matrix operand
     typename LayoutB_,
+    /// Access granularity of B matrix in units of elements
+    int kAlignmentB,
     /// Element type for C and D matrix operands
     typename ElementC_,
     /// Layout type for C and D matrix operands
     typename LayoutC_,
     /// Element type for internal accumulation
     typename ElementAccumulator,
     /// Operator class tag
@@ -102,39 +110,43 @@
     typename InstructionShape,
     /// Epilogue output operator
     typename EpilogueOutputOp,
     /// Threadblock-level swizzling operator
     typename ThreadblockSwizzle,
     /// Number of stages used in the pipelined mainloop
     int Stages,
-    /// Operation performed by GEMM
-    typename Operator,
     /// If true, kernel is configured to support serial reduction in the
     /// epilogue
     bool SplitKSerial,
+    /// Operation performed by GEMM
+    typename Operator,
     /// Blas3 computation mode
     BlasMode BlasMode_ = BlasMode::kSymmetric>
-struct DefaultSymmComplex;
+struct DefaultSymm;
 
 ////////////////////////////////////////////////////////////////////////////////
 
-/// Partial specialization for Ampere Architecture complex datatype (symmetric)
+/// Partial specialization for Hopper Architecture
 template <
     /// Element type for A matrix operand
     typename ElementA,
     /// Layout type for A matrix operand
     typename LayoutA,
     /// Side Mode for A (kLeft or kRight)
     SideMode kSideModeA,
     /// Fill Mode for A (kLower or kUpper)
     FillMode kFillModeA,
+    /// Access granularity of A matrix in units of elements
+    int kAlignmentA,
     /// Element type for B matrix operand
     typename ElementB,
     /// Layout type for B matrix operand
     typename LayoutB,
+    /// Access granularity of A matrix in units of elements
+    int kAlignmentB,
     /// Element type for C and D matrix operands
     typename ElementC,
     /// Element type for internal accumulation
     typename ElementAccumulator,
     /// Threadblock-level tile size (concept: GemmShape)
     typename ThreadblockShape,
     /// Warp-level tile size (concept: GemmShape)
@@ -143,92 +155,94 @@
     typename InstructionShape,
     /// Epilogue output operator
     typename EpilogueOutputOp,
     /// Threadblock-level swizzling operator
     typename ThreadblockSwizzle,
     /// Number of stages used in the pipelined mainloop
     int Stages,
-    /// Operation performed by GEMM
-    typename Operator,
     /// If true, kernel is configured to support serial reduction in the
     /// epilogue
-    bool SplitKSerial>
-struct DefaultSymmComplex<
-  ElementA, LayoutA, kSideModeA, kFillModeA, ElementB, LayoutB, ElementC, 
-  layout::RowMajor, ElementAccumulator, arch::OpClassTensorOp,
-  arch::Sm80, ThreadblockShape, WarpShape, InstructionShape, 
-  EpilogueOutputOp, ThreadblockSwizzle, Stages, 
-  Operator, SplitKSerial, BlasMode::kSymmetric> {
-
-  static BlasMode const kBlasMode = BlasMode::kSymmetric;
-  // Complex Transform don't appply to A or B for SYMM
-  static ComplexTransform const TransformA = ComplexTransform::kNone; 
-  static ComplexTransform const TransformB = ComplexTransform::kNone; 
+    bool SplitKSerial,
+    /// Operation performed by GEMM
+    typename Operator>
+struct DefaultSymm<
+                    ElementA, LayoutA, kSideModeA, kFillModeA, kAlignmentA, 
+                    ElementB, LayoutB, kAlignmentB, 
+                    ElementC,layout::RowMajor, 
+                    ElementAccumulator, arch::OpClassTensorOp, arch::Sm90, 
+                    ThreadblockShape, WarpShape, InstructionShape,
+                    EpilogueOutputOp, ThreadblockSwizzle, Stages, SplitKSerial,
+                    Operator> {
 
   /// Define the threadblock-scoped triagular matrix multiply-accumulate
   /// TRMM - with diagonal: alpha * A * B or alpha * B * A
 	static const DiagType kDiagTypeMma1 = DiagType::kNonUnit;
-  using Mma1 = typename cutlass::gemm::threadblock::DefaultMultistageTrmmComplex<
-      ElementA, LayoutA, 
-      ElementB, LayoutB, 
+  using Mma1 = typename cutlass::gemm::threadblock::DefaultTrmm<
+      ElementA, LayoutA, kAlignmentA, 
+      ElementB, LayoutB, kAlignmentB,
       kSideModeA, kFillModeA, kDiagTypeMma1, 
       ElementAccumulator, layout::RowMajor, 
-      arch::OpClassTensorOp, arch::Sm80,
+      arch::OpClassTensorOp, arch::Sm90,
       ThreadblockShape, WarpShape, InstructionShape,
-      Stages, TransformA, TransformB, Operator>::ThreadblockMma;
+      Stages, Operator>::ThreadblockMma;
 
-  /// Define the threadblock-scoped triagular matrix multiply-accumulate
+  /// Define the threadblock-scoped triagular matrix multiply-accumulate 
   /// TRMM - withOUT diagonal: alpha * AT * B or alpha * B * AT
 	static const DiagType kDiagTypeMma2 = DiagType::kZero;
   using LayoutAMma2 = typename platform::conditional<
                                 (kSideModeA == SideMode::kLeft), 
                                 typename layout::LayoutTranspose<LayoutA>::type, 
                                 LayoutA
                               >::type;
   using LayoutBMma2 = typename platform::conditional<
                                 (kSideModeA == SideMode::kLeft), 
                                 LayoutB, 
                                 typename layout::LayoutTranspose<LayoutB>::type
                               >::type; 
-	using Mma2 = typename cutlass::gemm::threadblock::DefaultMultistageTrmmComplex<
-			ElementA, LayoutAMma2, 
-			ElementB, LayoutBMma2, 
+	using Mma2 = typename cutlass::gemm::threadblock::DefaultTrmm<
+			ElementA, LayoutAMma2, kAlignmentA, 
+			ElementB, LayoutBMma2, kAlignmentB,
 			kSideModeA, InvertFillMode<kFillModeA>::mode, kDiagTypeMma2, 
 			ElementAccumulator, layout::RowMajor, 
-			arch::OpClassTensorOp, arch::Sm80,
+			arch::OpClassTensorOp, arch::Sm90,
 			ThreadblockShape, WarpShape, InstructionShape,
-			Stages, TransformA, TransformB, Operator>::ThreadblockMma;
+			Stages, Operator>::ThreadblockMma;
+
+  static const int kPartitionsK = ThreadblockShape::kK / WarpShape::kK;
 
   /// Define the epilogue
   using Epilogue =
-      typename cutlass::epilogue::threadblock::DefaultEpilogueComplexTensorOp<
-          ThreadblockShape, typename Mma1::Operator, 1, EpilogueOutputOp,
-          EpilogueOutputOp::kCount, Operator>::Epilogue;
+      typename cutlass::epilogue::threadblock::DefaultEpilogueTensorOp<
+          ThreadblockShape, typename Mma1::Operator, kPartitionsK, EpilogueOutputOp,
+          EpilogueOutputOp::kCount>::Epilogue;
 
-  /// Define the kernel-level Symm operator.
+  /// Define the kernel-level SYMM/HEMM operator.
   using SymmKernel = kernel::SymmUniversal<Mma1, Mma2, Epilogue, ThreadblockSwizzle, kSideModeA, kFillModeA>;
-
 };
 
 ////////////////////////////////////////////////////////////////////////////////
 
-/// Partial specialization for Ampere Architecture complex datatype (hermitian)
+/// Partial specialization for Ampere Architecture
 template <
     /// Element type for A matrix operand
     typename ElementA,
     /// Layout type for A matrix operand
     typename LayoutA,
     /// Side Mode for A (kLeft or kRight)
     SideMode kSideModeA,
     /// Fill Mode for A (kLower or kUpper)
     FillMode kFillModeA,
+    /// Access granularity of A matrix in units of elements
+    int kAlignmentA,
     /// Element type for B matrix operand
     typename ElementB,
     /// Layout type for B matrix operand
     typename LayoutB,
+    /// Access granularity of A matrix in units of elements
+    int kAlignmentB,
     /// Element type for C and D matrix operands
     typename ElementC,
     /// Element type for internal accumulation
     typename ElementAccumulator,
     /// Threadblock-level tile size (concept: GemmShape)
     typename ThreadblockShape,
     /// Warp-level tile size (concept: GemmShape)
@@ -237,80 +251,71 @@
     typename InstructionShape,
     /// Epilogue output operator
     typename EpilogueOutputOp,
     /// Threadblock-level swizzling operator
     typename ThreadblockSwizzle,
     /// Number of stages used in the pipelined mainloop
     int Stages,
-    /// Operation performed by GEMM
-    typename Operator,
     /// If true, kernel is configured to support serial reduction in the
     /// epilogue
-    bool SplitKSerial>
-struct DefaultSymmComplex<
-  ElementA, LayoutA, kSideModeA, kFillModeA, ElementB, LayoutB, ElementC, 
-  layout::RowMajor, ElementAccumulator, arch::OpClassTensorOp,
-  arch::Sm80, ThreadblockShape, WarpShape, InstructionShape, 
-  EpilogueOutputOp, ThreadblockSwizzle, Stages, 
-  Operator, SplitKSerial, BlasMode::kHermitian> {
-
-  static BlasMode const kBlasMode = BlasMode::kHermitian;
-
+    bool SplitKSerial,
+    /// Operation performed by GEMM
+    typename Operator>
+struct DefaultSymm<
+                    ElementA, LayoutA, kSideModeA, kFillModeA, kAlignmentA, 
+                    ElementB, LayoutB, kAlignmentB, 
+                    ElementC,layout::RowMajor, 
+                    ElementAccumulator, arch::OpClassTensorOp, arch::Sm80, 
+                    ThreadblockShape, WarpShape, InstructionShape,
+                    EpilogueOutputOp, ThreadblockSwizzle, Stages, SplitKSerial,
+                    Operator> {
 
   /// Define the threadblock-scoped triagular matrix multiply-accumulate
   /// TRMM - with diagonal: alpha * A * B or alpha * B * A
 	static const DiagType kDiagTypeMma1 = DiagType::kNonUnit;
-  static ComplexTransform const TransformAMma1 = ComplexTransform::kNone; 
-  static ComplexTransform const TransformBMma1 = ComplexTransform::kNone; 
-  using Mma1 = typename cutlass::gemm::threadblock::DefaultMultistageTrmmComplex<
-      ElementA, LayoutA, 
-      ElementB, LayoutB, 
+  using Mma1 = typename cutlass::gemm::threadblock::DefaultTrmm<
+      ElementA, LayoutA, kAlignmentA, 
+      ElementB, LayoutB, kAlignmentB,
       kSideModeA, kFillModeA, kDiagTypeMma1, 
       ElementAccumulator, layout::RowMajor, 
       arch::OpClassTensorOp, arch::Sm80,
       ThreadblockShape, WarpShape, InstructionShape,
-      Stages, TransformAMma1, TransformBMma1, Operator, BlasMode::kHermitian>::ThreadblockMma;
+      Stages, Operator>::ThreadblockMma;
 
-  /// Define the threadblock-scoped triagular matrix multiply-accumulate
-  /// TRMM - withOUT diagonal - with conjugate transpose: alpha * AT * B or alpha * B * AT
+  /// Define the threadblock-scoped triagular matrix multiply-accumulate 
+  /// TRMM - withOUT diagonal: alpha * AT * B or alpha * B * AT
 	static const DiagType kDiagTypeMma2 = DiagType::kZero;
   using LayoutAMma2 = typename platform::conditional<
                                 (kSideModeA == SideMode::kLeft), 
                                 typename layout::LayoutTranspose<LayoutA>::type, 
                                 LayoutA
                               >::type;
   using LayoutBMma2 = typename platform::conditional<
                                 (kSideModeA == SideMode::kLeft), 
                                 LayoutB, 
                                 typename layout::LayoutTranspose<LayoutB>::type
-                              >::type;
-  static ComplexTransform const TransformAMma2 = (kSideModeA == SideMode::kLeft) ? 
-                                              ComplexTransform::kConjugate : ComplexTransform::kNone;
-  static ComplexTransform const TransformBMma2 = (kSideModeA == SideMode::kLeft) ? 
-                                              ComplexTransform::kNone : ComplexTransform::kConjugate;
-
-	using Mma2 = typename cutlass::gemm::threadblock::DefaultMultistageTrmmComplex<
-			ElementA, LayoutAMma2, 
-			ElementB, LayoutBMma2, 
+                              >::type; 
+	using Mma2 = typename cutlass::gemm::threadblock::DefaultTrmm<
+			ElementA, LayoutAMma2, kAlignmentA, 
+			ElementB, LayoutBMma2, kAlignmentB,
 			kSideModeA, InvertFillMode<kFillModeA>::mode, kDiagTypeMma2, 
 			ElementAccumulator, layout::RowMajor, 
 			arch::OpClassTensorOp, arch::Sm80,
 			ThreadblockShape, WarpShape, InstructionShape,
-			Stages, TransformAMma2, TransformBMma2, Operator>::ThreadblockMma;
+			Stages, Operator>::ThreadblockMma;
+
+  static const int kPartitionsK = ThreadblockShape::kK / WarpShape::kK;
 
   /// Define the epilogue
   using Epilogue =
-      typename cutlass::epilogue::threadblock::DefaultEpilogueComplexTensorOp<
-          ThreadblockShape, typename Mma1::Operator, 1, EpilogueOutputOp,
-          EpilogueOutputOp::kCount, Operator>::Epilogue;
+      typename cutlass::epilogue::threadblock::DefaultEpilogueTensorOp<
+          ThreadblockShape, typename Mma1::Operator, kPartitionsK, EpilogueOutputOp,
+          EpilogueOutputOp::kCount>::Epilogue;
 
-  /// Define the kernel-level Symm operator.
+  /// Define the kernel-level SYMM/HEMM operator.
   using SymmKernel = kernel::SymmUniversal<Mma1, Mma2, Epilogue, ThreadblockSwizzle, kSideModeA, kFillModeA>;
-
 };
-
 ////////////////////////////////////////////////////////////////////////////////
 
-
 }  // namespace kernel
 }  // namespace gemm
 }  // namespace cutlass
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/default_symm_universal.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/default_symm_universal.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/default_trmm.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/default_sparse_mma.h`

 * *Files 22% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -24,57 +24,41 @@
  * DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR
  * SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER
  * CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,
  * OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
  * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
  *
  **************************************************************************************************/
-
-// 
 /*! \file
-    \brief 
-      Default kernel-level TRMM definitions combine threadblock-scoped matrix multiply-add with
-      the appropriate threadblock-scoped epilogue.
+    \brief Template for a pipelined GEMM kernel. Does not compute batching or support split-K.
 */
 
 #pragma once
 
-#include "cutlass/blas3.h"
-
-#include "cutlass/layout/matrix.h"
+#include "cutlass/cutlass.h"
+#include "cutlass/numeric_types.h"
+#include "cutlass/arch/arch.h"
 #include "cutlass/arch/wmma.h"
 
-#include "cutlass/epilogue/threadblock/epilogue.h"
-#include "cutlass/epilogue/thread/linear_combination.h"
-
-#include "cutlass/gemm/gemm.h"
-#include "cutlass/gemm/kernel/trmm_universal.h"
-#include "cutlass/gemm/threadblock/default_mma_core_sm75.h"
+#include "cutlass/layout/matrix.h"
+#include "cutlass/transform/threadblock/predicated_tile_iterator.h"
+#include "cutlass/transform/threadblock/predicated_tile_iterator_2dthreadtile.h"
 #include "cutlass/gemm/threadblock/default_mma_core_sm70.h"
+#include "cutlass/gemm/threadblock/default_mma_core_sm75.h"
 #include "cutlass/gemm/threadblock/default_mma_core_sm80.h"
-#include "cutlass/gemm/threadblock/default_mma.h"
-#include "cutlass/gemm/threadblock/default_trmm.h"
-#include "cutlass/gemm/threadblock/default_mma_core_simt.h"
-#include "cutlass/gemm/threadblock/threadblock_swizzle.h"
-
-#include "cutlass/epilogue/threadblock/default_epilogue_tensor_op.h"
-#include "cutlass/epilogue/threadblock/default_epilogue_volta_tensor_op.h"
-#include "cutlass/epilogue/threadblock/default_epilogue_simt.h"
-#include "cutlass/transform/threadblock/predicated_tile_iterator.h"
-
+#include "cutlass/gemm/threadblock/default_mma_core_sparse_sm80.h"
 #if defined(CUTLASS_ARCH_WMMA_ENABLED)
-#include "cutlass/epilogue/threadblock/default_epilogue_wmma_tensor_op.h"
+#include "cutlass/gemm/threadblock/default_mma_core_wmma.h"
 #endif //CUTLASS_ARCH_WMMA_ENABLED
 
-
 ////////////////////////////////////////////////////////////////////////////////
 
 namespace cutlass {
 namespace gemm {
-namespace kernel {
+namespace threadblock {
 
 ////////////////////////////////////////////////////////////////////////////////
 
 template <
     /// Element type for A matrix operand
     typename ElementA_,
     /// Layout type for A matrix operand
@@ -83,117 +67,130 @@
     int kAlignmentA,
     /// Element type for B matrix operand
     typename ElementB_,
     /// Layout type for B matrix operand
     typename LayoutB_,
     /// Access granularity of B matrix in units of elements
     int kAlignmentB,
-    /// Side Mode for the kernel
-    SideMode SideMode_,
-    /// Fill Mode for the triangular matrix
-    FillMode FillMode_,
-    /// Diag Type for the triangular matrix
-    DiagType DiagType_,
-    /// Element type for C and D matrix operands
-    typename ElementC_,
+    /// Element type for internal accumulation
+    typename ElementAccumulator_,
     /// Layout type for C and D matrix operands
     typename LayoutC_,
-    /// Element type for internal accumulation
-    typename ElementAccumulator,
     /// Operator class tag
-    typename OperatorClass,
+    typename OperatorClass_,
     /// Tag indicating architecture to tune for
-    typename ArchTag,
+    typename ArchTag_,
     /// Threadblock-level tile size (concept: GemmShape)
-    typename ThreadblockShape,
-    /// Warp-level tile size (concept: GemmShape)
-    typename WarpShape,
+    typename ThreadblockShape_,
     /// Warp-level tile size (concept: GemmShape)
-    typename InstructionShape,
-    /// Epilogue output operator
-    typename EpilogueOutputOp,
-    /// Threadblock-level swizzling operator
-    typename ThreadblockSwizzle,
+    typename WarpShape_,
+    /// Instruction-level tile size (concept: GemmShape)
+    typename InstructionShape_,
     /// Number of stages used in the pipelined mainloop
     int Stages,
-    /// If true, kernel is configured to support serial reduction in the
-    /// epilogue
-    bool SplitKSerial,
-    /// Operation performed by GEMM
-    typename Operator>
-struct DefaultTrmm;
+    /// Operation perfomed by GEMM
+    typename Operator,
+    /// Store the accumulators in row major or column major.  Row major is used
+    /// when output layout is interleaved.
+    bool AccumulatorsInRowMajor = false
+    >
+struct DefaultSparseMma;
 
 ////////////////////////////////////////////////////////////////////////////////
 
-/// Partial specialization for Ampere Architecture
+/// Specialization for row-major output (OperatorClass TensorOp)
 template <
     /// Element type for A matrix operand
     typename ElementA,
     /// Layout type for A matrix operand
     typename LayoutA,
     /// Access granularity of A matrix in units of elements
     int kAlignmentA,
     /// Element type for B matrix operand
     typename ElementB,
     /// Layout type for B matrix operand
     typename LayoutB,
-    /// Access granularity of A matrix in units of elements
+    /// Access granularity of B matrix in units of elements
     int kAlignmentB,
-    /// Side Mode for the kernel
-    SideMode kSideMode,
-    /// Fill Mode for the triangular matrix
-    FillMode kFillMode,
-    /// Diag Type for the triangular matrix
-    DiagType kDiagType,
-    /// Element type for C and D matrix operands
-    typename ElementC,
     /// Element type for internal accumulation
     typename ElementAccumulator,
+    /// Tag indicating architecture to tune for
+    typename ArchTag,
     /// Threadblock-level tile size (concept: GemmShape)
     typename ThreadblockShape,
     /// Warp-level tile size (concept: GemmShape)
     typename WarpShape,
-    /// Warp-level tile size (concept: GemmShape)
+    /// Instruction-level tile size (concept: GemmShape)
     typename InstructionShape,
-    /// Epilogue output operator
-    typename EpilogueOutputOp,
-    /// Threadblock-level swizzling operator
-    typename ThreadblockSwizzle,
-    /// Number of stages used in the pipelined mainloop
+    /// Number of stages used in the multistage mainloop
     int Stages,
-    /// If true, kernel is configured to support serial reduction in the
-    /// epilogue
-    bool SplitKSerial,
-    /// Operation performed by GEMM
-    typename Operator>
-struct DefaultTrmm<ElementA, LayoutA, kAlignmentA, ElementB, LayoutB, kAlignmentB,
-                   kSideMode, kFillMode, kDiagType, ElementC,
-                   layout::RowMajor, ElementAccumulator, arch::OpClassTensorOp,
-                   arch::Sm80, ThreadblockShape, WarpShape, InstructionShape,
-                   EpilogueOutputOp, ThreadblockSwizzle, Stages, SplitKSerial,
-                   Operator> {
-                    
-  /// Define the threadblock-scoped triagular matrix multiply-accumulate
-  using Mma = typename cutlass::gemm::threadblock::DefaultTrmm<
-      ElementA, LayoutA, kAlignmentA, ElementB, LayoutB, kAlignmentB,
-      kSideMode, kFillMode, kDiagType, 
-      ElementAccumulator, layout::RowMajor, arch::OpClassTensorOp, arch::Sm80,
-      ThreadblockShape, WarpShape, InstructionShape, Stages,
-      Operator>::ThreadblockMma;
-
-  static const int kPartitionsK = ThreadblockShape::kK / WarpShape::kK;
-
-  /// Define the epilogue
-  using Epilogue =
-      typename cutlass::epilogue::threadblock::DefaultEpilogueTensorOp<
-          ThreadblockShape, typename Mma::Operator, kPartitionsK, EpilogueOutputOp,
-          EpilogueOutputOp::kCount>::Epilogue;
-
-  /// Define the kernel-level TRMM operator.
-  using TrmmKernel = kernel::TrmmUniversal<Mma, Epilogue, ThreadblockSwizzle, kSideMode, kFillMode, kDiagType>;
+    /// Operation perfomed by GEMM
+    typename Operator
+    >
+struct DefaultSparseMma<ElementA, LayoutA, kAlignmentA, ElementB, LayoutB,
+                  kAlignmentB, ElementAccumulator, layout::RowMajor,
+                  arch::OpClassTensorOp, ArchTag, ThreadblockShape, WarpShape,
+                  InstructionShape, Stages, Operator, false> {
+  static cutlass::arch::CacheOperation::Kind const CacheOpA =
+      ((sizeof_bits<ElementA>::value * kAlignmentA) == 128)
+          ? cutlass::arch::CacheOperation::Global
+          : cutlass::arch::CacheOperation::Always;
+
+  static cutlass::arch::CacheOperation::Kind const CacheOpB =
+      ((sizeof_bits<ElementB>::value * kAlignmentB) == 128)
+          ? cutlass::arch::CacheOperation::Global
+          : cutlass::arch::CacheOperation::Always;
+  
+
+  // Define the MmaCore components
+  using MmaCore = typename cutlass::gemm::threadblock::DefaultSparseMmaCore<
+      ThreadblockShape, WarpShape, InstructionShape, ElementA, LayoutA,
+      ElementB, LayoutB, ElementAccumulator, layout::RowMajor, arch::OpClassTensorOp,
+      Stages, Operator, false, CacheOpA, CacheOpB>;
+
+  static int const kSparse = MmaCore::kSparse;
+
+  // Define iterators over tiles from the A operand
+  using ThreadMapA = typename MmaCore::IteratorThreadMapA;
+  using AccessTypeA = cutlass::Array<ElementA, kAlignmentA>;
+  using IteratorA =
+      cutlass::transform::threadblock::PredicatedTileAccessIterator<
+          cutlass::MatrixShape<ThreadblockShape::kM, ThreadblockShape::kK / kSparse>,
+          ElementA, LayoutA, 1, ThreadMapA, AccessTypeA>;
+
+  // Define iterators over tiles from the B operand
+  using ThreadMapB = typename MmaCore::IteratorThreadMapB;
+  using AccessTypeB = cutlass::Array<ElementB, kAlignmentB>;
+  using IteratorB =
+      cutlass::transform::threadblock::PredicatedTileAccessIterator<
+          cutlass::MatrixShape<ThreadblockShape::kK, ThreadblockShape::kN>,
+          ElementB, LayoutB, 0, ThreadMapB, AccessTypeB>;
+
+  // Define iterators over tiles from the E operand
+  using ElementE = typename MmaCore::ElementE;
+  using LayoutE = typename MmaCore::GmemLayoutE;
+  using ThreadMapE = typename MmaCore::IteratorThreadMapE;
+  using AccessTypeE =
+      cutlass::Array<ElementE, 128 / sizeof_bits<ElementE>::value>;
+  using IteratorE =
+      cutlass::transform::threadblock::PredicatedTileAccessIterator<
+          cutlass::MatrixShape<ThreadblockShape::kM,
+                               ThreadblockShape::kK / kSparse /
+                                   MmaCore::kElementsPerElementE>,
+          ElementE, LayoutE, 1, ThreadMapE, AccessTypeE>;
+
+  // Define the threadblock-scoped multistage matrix multiply
+  using ThreadblockMma = cutlass::gemm::threadblock::SparseMmaMultistage<
+      typename MmaCore::Shape, IteratorA, typename MmaCore::SmemIteratorA,
+      MmaCore::kCacheOpA, IteratorB, typename MmaCore::SmemIteratorB,
+      MmaCore::kCacheOpB, ElementAccumulator, layout::RowMajor,
+      IteratorE, typename MmaCore::SmemIteratorE, MmaCore::kCacheOpE,
+      typename MmaCore::MmaPolicy, Stages>;
 };
 
 ////////////////////////////////////////////////////////////////////////////////
 
-}  // namespace kernel
-}  // namespace gemm
-}  // namespace cutlass
+} // namespace threadblock
+} // namespace gemm
+} // namespace cutlass 
+
+////////////////////////////////////////////////////////////////////////////////
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/default_trmm_complex.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/gemm_array.h`

 * *Files 22% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -24,174 +24,241 @@
  * DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR
  * SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER
  * CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,
  * OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
  * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
  *
  **************************************************************************************************/
-
 /*! \file
-    \brief 
-      Default kernel-level TRMM definitions combine threadblock-scoped matrix multiply-add with
-      the appropriate threadblock-scoped epilogue.
-  
-      Note, CUTLASS epilogues universally target row-major outputs. Column-major outputs are
-      accommodated by exchanging A and B operands and assuming transposed layouts.
-
-  
+    \brief Template for a pipelined GEMM kernel. Does not compute batching or support split-K.
 */
 
 #pragma once
 
-#include "cutlass/blas3.h"
-
-#include "cutlass/layout/matrix.h"
-
-#include "cutlass/epilogue/threadblock/epilogue.h"
-#include "cutlass/epilogue/thread/linear_combination.h"
+#include "cutlass/cutlass.h"
 
 #include "cutlass/gemm/gemm.h"
-#include "cutlass/gemm/kernel/trmm_universal.h"
-#include "cutlass/gemm/threadblock/default_mma_core_sm75.h"
-#include "cutlass/gemm/threadblock/default_mma_core_sm70.h"
-#include "cutlass/gemm/threadblock/default_multistage_mma_complex_core_sm80.h"
-#include "cutlass/gemm/threadblock/default_mma.h"
-#include "cutlass/gemm/threadblock/default_multistage_trmm_complex.h"
-#include "cutlass/gemm/threadblock/default_mma_core_simt.h"
-#include "cutlass/gemm/threadblock/threadblock_swizzle.h"
-#include "cutlass/epilogue/threadblock/default_epilogue_complex_tensor_op.h"
-#include "cutlass/epilogue/threadblock/default_epilogue_simt.h"
+#include "cutlass/matrix_coord.h"
 
-#include "cutlass/transform/threadblock/predicated_tile_iterator.h"
-
-////////////////////////////////////////////////////////////////////////////////
+/////////////////////////////////////////////////////////////////////////////////////////////////
 
 namespace cutlass {
 namespace gemm {
 namespace kernel {
 
-////////////////////////////////////////////////////////////////////////////////
+/////////////////////////////////////////////////////////////////////////////////////////////////
 
 template <
-  /// Element type for A matrix operand
-  typename ElementA_,
-  /// Layout type for A matrix operand
-  typename LayoutA_,
-  /// Element type for B matrix operand
-  typename ElementB_,
-  /// Layout type for B matrix operand
-  typename LayoutB_,
-  /// Side Mode for the kernel
-  SideMode SideMode_,
-  /// Fill Mode for the triangular matrix
-  FillMode FillMode_,
-  /// Diag Type for the triangular matrix
-  DiagType DiagType_,
-  /// Element type for C and D matrix operands
-  typename ElementC_,
-  /// Layout type for C and D matrix operands
-  typename LayoutC_,
-  /// Element type for internal accumulation
-  typename ElementAccumulator,
-  /// Operator class tag
-  typename OperatorClass,
-  /// Tag indicating architecture to tune for
-  typename ArchTag,
-  /// Threadblock-level tile size (concept: GemmShape)
-  typename ThreadblockShape,
-  /// Warp-level tile size (concept: GemmShape)
-  typename WarpShape,
-  /// Warp-level tile size (concept: GemmShape)
-  typename InstructionShape,
-  /// Epilogue output operator
-  typename EpilogueOutputOp,
-  /// Threadblock-level swizzling operator
-  typename ThreadblockSwizzle,
-  /// Number of stages used in the pipelined mainloop
-  int Stages,
-  /// Complex elementwise transformation on A operand
-  ComplexTransform TransformA,
-  /// Complex elementwise transformation on B operand
-  ComplexTransform TransformB,
-  /// Multiply-add operator 
-  // (arch::OpMultiplyAddComplex, arch::OpMultiplyGaussianComplex)
-  typename Operator,
-  /// If true, kernel is configured to support serial reduction in the epilogue
-  bool SplitKSerial
+  typename Mma_,                  ///! Threadblock-scoped matrix multiply-accumulate 
+  typename Epilogue_,             ///! Epilogue
+  typename ThreadblockSwizzle_    ///! Threadblock swizzling function
 >
-struct DefaultTrmmComplex;
-
-////////////////////////////////////////////////////////////////////////////////
-
-/// Partial specialization for Ampere Architecture
-template <
-    /// Element type for A matrix operand
-    typename ElementA,
-    /// Layout type for A matrix operand
-    typename LayoutA,
-    /// Element type for B matrix operand
-    typename ElementB,
-    /// Layout type for B matrix operand
-    typename LayoutB,
-    /// Side Mode for the kernel
-    SideMode kSideMode,
-    /// Fill Mode for the triangular matrix
-    FillMode kFillMode,
-    /// Diag Type for the triangular matrix
-    DiagType kDiagType,
-    /// Element type for C and D matrix operands
-    typename ElementC,
-    /// Element type for internal accumulation
-    typename ElementAccumulator,
-    /// Threadblock-level tile size (concept: GemmShape)
-    typename ThreadblockShape,
-    /// Warp-level tile size (concept: GemmShape)
-    typename WarpShape,
-    /// Warp-level tile size (concept: GemmShape)
-    typename InstructionShape,
-    /// Epilogue output operator
-    typename EpilogueOutputOp,
-    /// Threadblock-level swizzling operator
-    typename ThreadblockSwizzle,
-    /// Number of stages used in the pipelined mainloop
-    int Stages,
-    /// Complex elementwise transformation on A operand
-    ComplexTransform TransformA,
-    /// Complex elementwise transformation on B operand
-    ComplexTransform TransformB,
-    /// Multiply-add operator 
-    // (arch::OpMultiplyAddComplex, arch::OpMultiplyGaussianComplex)
-    typename Operator,
-    /// If true, kernel is configured to support serial reduction in the epilogue
-    bool SplitKSerial
-  >
-struct DefaultTrmmComplex<
-  ElementA, LayoutA, ElementB, LayoutB, 
-  kSideMode, kFillMode, kDiagType,
-  ElementC, layout::RowMajor, ElementAccumulator, arch::OpClassTensorOp,
-  arch::Sm80, ThreadblockShape, WarpShape, InstructionShape,
-  EpilogueOutputOp, ThreadblockSwizzle, Stages, TransformA, TransformB, Operator, SplitKSerial> {
-
-  /// Define the threadblock-scoped matrix multiply-accumulate
-  using Mma = typename cutlass::gemm::threadblock::DefaultMultistageTrmmComplex<
-      ElementA, LayoutA, ElementB, LayoutB, 
-      kSideMode, kFillMode, kDiagType,
-      ElementAccumulator,layout::RowMajor, arch::OpClassTensorOp, arch::Sm80, ThreadblockShape,
-      WarpShape, InstructionShape, Stages, TransformA, TransformB, Operator>::ThreadblockMma;
-
-  /// Define the epilogue
-  using Epilogue =
-      typename cutlass::epilogue::threadblock::DefaultEpilogueComplexTensorOp<
-          ThreadblockShape, typename Mma::Operator, 1, EpilogueOutputOp,
-          EpilogueOutputOp::kCount, Operator>::Epilogue;
+struct GemmArray {
 
-  /// Define the kernel-level TRMM operator.
-  using TrmmKernel = kernel::TrmmUniversal<Mma, Epilogue, ThreadblockSwizzle, kSideMode, kFillMode, kDiagType>;
+  using Mma = Mma_;
+  using Epilogue = Epilogue_;
+  using OutputOp = typename Epilogue::OutputOp;
+  using ThreadblockSwizzle = ThreadblockSwizzle_;
+
+  /// Warp count (concept: GemmShape)
+  using WarpCount = typename Mma::WarpCount;
+  static int const kThreadCount = 32 * WarpCount::kCount;
+
+  /// Parameters structure
+  struct Params {
+    cutlass::gemm::GemmCoord problem_size;
+    cutlass::gemm::GemmCoord grid_tiled_shape;
+    int swizzle_log_tile;
+    typename Mma::IteratorA::Params params_A;
+    typename Mma::IteratorA::Element const * const * ptr_A;
+    typename Mma::IteratorB::Params params_B;
+    typename Mma::IteratorB::Element const * const * ptr_B;
+    typename Epilogue::OutputTileIterator::Params params_C;
+    typename Epilogue::OutputTileIterator::Element const * const * ptr_C;
+    typename Epilogue::OutputTileIterator::Params params_D;
+    typename Epilogue::OutputTileIterator::Element * const * ptr_D;
+    int64_t stride_D;
+    typename OutputOp::Params epilogue;
+    int batch_count;
+    int gemm_k_iterations;
+
+    //
+    // Methods
+    //
+
+    CUTLASS_HOST_DEVICE
+    Params() : 
+      swizzle_log_tile(0) { }
+
+    CUTLASS_HOST_DEVICE
+    Params(
+      cutlass::gemm::GemmCoord const & problem_size_,
+      cutlass::gemm::GemmCoord const & grid_tiled_shape_,
+      typename Mma::IteratorA::Element const * const * ptr_A_,
+      typename Mma::IteratorA::Layout layout_A,
+      typename Mma::IteratorB::Element const * const * ptr_B_,
+      typename Mma::IteratorB::Layout layout_B,
+      typename Epilogue::OutputTileIterator::Element const * const * ptr_C_,
+      typename Epilogue::OutputTileIterator::Layout layout_C,
+      typename Epilogue::OutputTileIterator::Element * const * ptr_D_,
+      typename Epilogue::OutputTileIterator::Layout layout_D,
+      typename OutputOp::Params epilogue_,
+      int batch_count_
+    ):
+      problem_size(problem_size_),
+      grid_tiled_shape(grid_tiled_shape_),
+      swizzle_log_tile(ThreadblockSwizzle().get_log_tile(grid_tiled_shape)),
+      params_A(layout_A),
+      ptr_A(ptr_A_),
+      params_B(layout_B),
+      ptr_B(ptr_B_),
+      params_C(layout_C),
+      ptr_C(ptr_C_),
+      params_D(layout_D),
+      ptr_D(ptr_D_),
+      epilogue(epilogue_),
+      batch_count(batch_count_),
+      gemm_k_iterations((problem_size.k() + Mma::Shape::kK - 1) / Mma::Shape::kK) {
+
+    }
+  };
+
+  /// Shared memory storage structure
+  union SharedStorage {
+    typename Mma::SharedStorage main_loop;
+    typename Epilogue::SharedStorage epilogue;
+  };
+
+  //
+  // Methods
+  //
+
+  CUTLASS_HOST_DEVICE
+  GemmArray() { } 
+
+  /// Executes one GEMM
+  CUTLASS_DEVICE
+  void operator()(Params const &params, SharedStorage &shared_storage) {
+
+    // Compute threadblock location
+    ThreadblockSwizzle threadblock_swizzle;
+
+    cutlass::gemm::GemmCoord threadblock_tile_offset =
+        threadblock_swizzle.get_tile_offset(params.swizzle_log_tile);
+
+    // Early exit if CTA is out of range
+    if (params.grid_tiled_shape.m() <= threadblock_tile_offset.m() ||
+      params.grid_tiled_shape.n() <= threadblock_tile_offset.n()) {
+
+      return;
+    }
+
+
+    // Each CTA handles multiple batch indices to accommodate limited range of CUDA grid's Z dimension
+    for (int batch_idx = threadblock_swizzle.get_batch_idx(); 
+      batch_idx < params.batch_count; 
+      batch_idx += gridDim.z) {
+
+      // Compute initial location in logical coordinates
+      cutlass::MatrixCoord tb_offset_A{
+        threadblock_tile_offset.m() * Mma::Shape::kM,
+        0
+      };
+
+      cutlass::MatrixCoord tb_offset_B{
+        0,
+        threadblock_tile_offset.n() * Mma::Shape::kN
+      };
+
+      // Compute position within threadblock
+      int thread_idx = threadIdx.x;
+
+      // Construct iterators to A and B operands
+      typename Mma::IteratorA iterator_A(
+        params.params_A,
+        const_cast<typename Mma::IteratorA::Element *>(params.ptr_A[batch_idx]),
+        params.problem_size.mk(),
+        thread_idx,
+        tb_offset_A);
+
+      typename Mma::IteratorB iterator_B(
+        params.params_B,
+        const_cast<typename Mma::IteratorB::Element *>(params.ptr_B[batch_idx]),
+        params.problem_size.kn(),
+        thread_idx,
+        tb_offset_B);
+
+      //
+      // Main loop
+      //
+      
+      // Broadcast the warp_id computed by lane 0 to ensure dependent code
+      // is compiled as warp-uniform.
+      int warp_idx = __shfl_sync(0xffffffff, threadIdx.x / 32, 0);
+
+      int lane_idx = threadIdx.x % 32;
+      
+      Mma mma(shared_storage.main_loop, thread_idx, warp_idx, lane_idx);
+
+      typename Mma::FragmentC accumulators;
+
+      accumulators.clear();
+
+
+      // Compute threadblock-scoped matrix multiply-add
+      mma(params.gemm_k_iterations, accumulators, iterator_A, iterator_B, accumulators);
+
+      //
+      // Epilogue
+      //
+
+      OutputOp output_op(params.epilogue);
+
+      //
+      // Masked tile iterators constructed from members
+      //
+
+      threadblock_tile_offset =
+          threadblock_swizzle.get_tile_offset(params.swizzle_log_tile);
+
+      //assume identity swizzle
+      MatrixCoord threadblock_offset(
+        threadblock_tile_offset.m() * Mma::Shape::kM,
+        threadblock_tile_offset.n() * Mma::Shape::kN
+      );
+
+      // Tile iterator writing to output tile
+      typename Epilogue::OutputTileIterator iterator_C(
+        params.params_C,
+        const_cast<typename Epilogue::OutputTileIterator::Element *>(params.ptr_C[batch_idx]),
+        params.problem_size.mn(),
+        thread_idx,
+        threadblock_offset
+      );
+
+      // Tile iterator writing to output tile
+      typename Epilogue::OutputTileIterator iterator_D(
+        params.params_D,
+        params.ptr_D[batch_idx],
+        params.problem_size.mn(),
+        thread_idx,
+        threadblock_offset
+      );
+
+      Epilogue epilogue(
+        shared_storage.epilogue, 
+        thread_idx, 
+        warp_idx, 
+        lane_idx);
+
+      // run efficient epilogue
+      epilogue(output_op, iterator_D, accumulators, iterator_C);
+    }
+  }
 };
 
-////////////////////////////////////////////////////////////////////////////////
+/////////////////////////////////////////////////////////////////////////////////////////////////
 
-}  // namespace kernel
-}  // namespace gemm
-}  // namespace cutlass
+} // namespace kernel
+} // namespace gemm
+} // namespace cutlass
 
-////////////////////////////////////////////////////////////////////////////////
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/default_trmm_universal.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/default_trmm_universal.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/ell_gemm.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/ell_gemm.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/gemm.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/gemm.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/gemm_array.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/gemm_batched.h`

 * *Files 14% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -48,15 +48,15 @@
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
 template <
   typename Mma_,                  ///! Threadblock-scoped matrix multiply-accumulate 
   typename Epilogue_,             ///! Epilogue
   typename ThreadblockSwizzle_    ///! Threadblock swizzling function
 >
-struct GemmArray {
+struct GemmBatched {
 
   using Mma = Mma_;
   using Epilogue = Epilogue_;
   using OutputOp = typename Epilogue::OutputOp;
   using ThreadblockSwizzle = ThreadblockSwizzle_;
 
   /// Warp count (concept: GemmShape)
@@ -65,60 +65,66 @@
 
   /// Parameters structure
   struct Params {
     cutlass::gemm::GemmCoord problem_size;
     cutlass::gemm::GemmCoord grid_tiled_shape;
     int swizzle_log_tile;
     typename Mma::IteratorA::Params params_A;
-    typename Mma::IteratorA::Element const * const * ptr_A;
+    typename Mma::IteratorA::TensorRef ref_A;
+    int64_t stride_A;
     typename Mma::IteratorB::Params params_B;
-    typename Mma::IteratorB::Element const * const * ptr_B;
+    typename Mma::IteratorB::TensorRef ref_B;
+    int64_t stride_B;
     typename Epilogue::OutputTileIterator::Params params_C;
-    typename Epilogue::OutputTileIterator::Element const * const * ptr_C;
+    typename Epilogue::OutputTileIterator::TensorRef ref_C;
+    int64_t stride_C;
     typename Epilogue::OutputTileIterator::Params params_D;
-    typename Epilogue::OutputTileIterator::Element * const * ptr_D;
+    typename Epilogue::OutputTileIterator::TensorRef ref_D;
     int64_t stride_D;
     typename OutputOp::Params epilogue;
     int batch_count;
     int gemm_k_iterations;
 
     //
     // Methods
     //
 
     CUTLASS_HOST_DEVICE
-    Params() : 
-      swizzle_log_tile(0) { }
+    Params() : swizzle_log_tile(0) { }
 
     CUTLASS_HOST_DEVICE
     Params(
       cutlass::gemm::GemmCoord const & problem_size_,
       cutlass::gemm::GemmCoord const & grid_tiled_shape_,
-      typename Mma::IteratorA::Element const * const * ptr_A_,
-      typename Mma::IteratorA::Layout layout_A,
-      typename Mma::IteratorB::Element const * const * ptr_B_,
-      typename Mma::IteratorB::Layout layout_B,
-      typename Epilogue::OutputTileIterator::Element const * const * ptr_C_,
-      typename Epilogue::OutputTileIterator::Layout layout_C,
-      typename Epilogue::OutputTileIterator::Element * const * ptr_D_,
-      typename Epilogue::OutputTileIterator::Layout layout_D,
+      typename Mma::IteratorA::TensorRef ref_A_,
+      int64_t stride_A_,
+      typename Mma::IteratorB::TensorRef ref_B_,
+      int64_t stride_B_,
+      typename Epilogue::OutputTileIterator::TensorRef ref_C_,
+      int64_t stride_C_,
+      typename Epilogue::OutputTileIterator::TensorRef ref_D_,
+      int64_t stride_D_,
       typename OutputOp::Params epilogue_,
       int batch_count_
     ):
       problem_size(problem_size_),
       grid_tiled_shape(grid_tiled_shape_),
       swizzle_log_tile(ThreadblockSwizzle().get_log_tile(grid_tiled_shape)),
-      params_A(layout_A),
-      ptr_A(ptr_A_),
-      params_B(layout_B),
-      ptr_B(ptr_B_),
-      params_C(layout_C),
-      ptr_C(ptr_C_),
-      params_D(layout_D),
-      ptr_D(ptr_D_),
+      params_A(ref_A_.layout()),
+      ref_A(ref_A_),
+      stride_A(stride_A_),
+      params_B(ref_B_.layout()),
+      ref_B(ref_B_),
+      stride_B(stride_B_),
+      params_C(ref_C_.layout()),
+      ref_C(ref_C_),
+      stride_C(stride_C_),
+      params_D(ref_D_.layout()),
+      ref_D(ref_D_),
+      stride_D(stride_D_),
       epilogue(epilogue_),
       batch_count(batch_count_),
       gemm_k_iterations((problem_size.k() + Mma::Shape::kK - 1) / Mma::Shape::kK) {
 
     }
   };
 
@@ -129,15 +135,15 @@
   };
 
   //
   // Methods
   //
 
   CUTLASS_HOST_DEVICE
-  GemmArray() { } 
+  GemmBatched() { } 
 
   /// Executes one GEMM
   CUTLASS_DEVICE
   void operator()(Params const &params, SharedStorage &shared_storage) {
 
     // Compute threadblock location
     ThreadblockSwizzle threadblock_swizzle;
@@ -171,30 +177,35 @@
 
       // Compute position within threadblock
       int thread_idx = threadIdx.x;
 
       // Construct iterators to A and B operands
       typename Mma::IteratorA iterator_A(
         params.params_A,
-        const_cast<typename Mma::IteratorA::Element *>(params.ptr_A[batch_idx]),
+        params.ref_A.data(),
         params.problem_size.mk(),
         thread_idx,
         tb_offset_A);
 
+      iterator_A.add_pointer_offset(params.stride_A * batch_idx);
+
       typename Mma::IteratorB iterator_B(
         params.params_B,
-        const_cast<typename Mma::IteratorB::Element *>(params.ptr_B[batch_idx]),
+        params.ref_B.data(),
         params.problem_size.kn(),
         thread_idx,
         tb_offset_B);
 
+      iterator_B.add_pointer_offset(params.stride_B * batch_idx);
+
+
       //
       // Main loop
       //
-      
+
       // Broadcast the warp_id computed by lane 0 to ensure dependent code
       // is compiled as warp-uniform.
       int warp_idx = __shfl_sync(0xffffffff, threadIdx.x / 32, 0);
 
       int lane_idx = threadIdx.x % 32;
       
       Mma mma(shared_storage.main_loop, thread_idx, warp_idx, lane_idx);
@@ -225,29 +236,33 @@
         threadblock_tile_offset.m() * Mma::Shape::kM,
         threadblock_tile_offset.n() * Mma::Shape::kN
       );
 
       // Tile iterator writing to output tile
       typename Epilogue::OutputTileIterator iterator_C(
         params.params_C,
-        const_cast<typename Epilogue::OutputTileIterator::Element *>(params.ptr_C[batch_idx]),
+        params.ref_C.data(),
         params.problem_size.mn(),
         thread_idx,
         threadblock_offset
       );
 
+      iterator_C.add_pointer_offset(params.stride_C * batch_idx);
+
       // Tile iterator writing to output tile
       typename Epilogue::OutputTileIterator iterator_D(
         params.params_D,
-        params.ptr_D[batch_idx],
+        params.ref_D.data(),
         params.problem_size.mn(),
         thread_idx,
         threadblock_offset
       );
 
+      iterator_D.add_pointer_offset(params.stride_D * batch_idx);
+
       Epilogue epilogue(
         shared_storage.epilogue, 
         thread_idx, 
         warp_idx, 
         lane_idx);
 
       // run efficient epilogue
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/gemm_batched.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/gemm_splitk_parallel.h`

 * *Files 9% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -25,15 +25,15 @@
  * SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER
  * CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,
  * OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
  * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
  *
  **************************************************************************************************/
 /*! \file
-    \brief Template for a pipelined GEMM kernel. Does not compute batching or support split-K.
+    \brief Template for GEMM performing a reduction over K partitions in parallel.
 */
 
 #pragma once
 
 #include "cutlass/cutlass.h"
 
 #include "cutlass/gemm/gemm.h"
@@ -48,102 +48,90 @@
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
 template <
   typename Mma_,                  ///! Threadblock-scoped matrix multiply-accumulate 
   typename Epilogue_,             ///! Epilogue
   typename ThreadblockSwizzle_    ///! Threadblock swizzling function
 >
-struct GemmBatched {
+struct GemmSplitKParallel {
 
   using Mma = Mma_;
   using Epilogue = Epilogue_;
   using OutputOp = typename Epilogue::OutputOp;
   using ThreadblockSwizzle = ThreadblockSwizzle_;
 
   /// Warp count (concept: GemmShape)
   using WarpCount = typename Mma::WarpCount;
   static int const kThreadCount = 32 * WarpCount::kCount;
 
+  static int const kAlignmentK = Mma::Operator::Shape::kK;
+
   /// Parameters structure
   struct Params {
     cutlass::gemm::GemmCoord problem_size;
     cutlass::gemm::GemmCoord grid_tiled_shape;
     int swizzle_log_tile;
     typename Mma::IteratorA::Params params_A;
     typename Mma::IteratorA::TensorRef ref_A;
-    int64_t stride_A;
     typename Mma::IteratorB::Params params_B;
     typename Mma::IteratorB::TensorRef ref_B;
-    int64_t stride_B;
-    typename Epilogue::OutputTileIterator::Params params_C;
-    typename Epilogue::OutputTileIterator::TensorRef ref_C;
-    int64_t stride_C;
     typename Epilogue::OutputTileIterator::Params params_D;
     typename Epilogue::OutputTileIterator::TensorRef ref_D;
-    int64_t stride_D;
-    typename OutputOp::Params epilogue;
-    int batch_count;
-    int gemm_k_iterations;
+    typename OutputOp::Params output_op;
+    int64_t splitk_slice_stride;
+    int gemm_k_size;
 
     //
     // Methods
     //
 
     CUTLASS_HOST_DEVICE
-    Params() : swizzle_log_tile(0) { }
+    Params(): swizzle_log_tile(0) { }
 
     CUTLASS_HOST_DEVICE
     Params(
-      cutlass::gemm::GemmCoord const & problem_size_,
-      cutlass::gemm::GemmCoord const & grid_tiled_shape_,
-      typename Mma::IteratorA::TensorRef ref_A_,
-      int64_t stride_A_,
-      typename Mma::IteratorB::TensorRef ref_B_,
-      int64_t stride_B_,
-      typename Epilogue::OutputTileIterator::TensorRef ref_C_,
-      int64_t stride_C_,
-      typename Epilogue::OutputTileIterator::TensorRef ref_D_,
-      int64_t stride_D_,
-      typename OutputOp::Params epilogue_,
-      int batch_count_
+      cutlass::gemm::GemmCoord const & problem_size,
+      cutlass::gemm::GemmCoord const & grid_tiled_shape,
+      typename Mma::IteratorA::TensorRef ref_A,
+      typename Mma::IteratorB::TensorRef ref_B,
+      typename Epilogue::OutputTileIterator::TensorRef ref_D,
+      typename OutputOp::Params output_op,
+      int64_t splitk_slice_stride
     ):
-      problem_size(problem_size_),
-      grid_tiled_shape(grid_tiled_shape_),
+      problem_size(problem_size),
+      grid_tiled_shape(grid_tiled_shape),
       swizzle_log_tile(ThreadblockSwizzle().get_log_tile(grid_tiled_shape)),
-      params_A(ref_A_.layout()),
-      ref_A(ref_A_),
-      stride_A(stride_A_),
-      params_B(ref_B_.layout()),
-      ref_B(ref_B_),
-      stride_B(stride_B_),
-      params_C(ref_C_.layout()),
-      ref_C(ref_C_),
-      stride_C(stride_C_),
-      params_D(ref_D_.layout()),
-      ref_D(ref_D_),
-      stride_D(stride_D_),
-      epilogue(epilogue_),
-      batch_count(batch_count_),
-      gemm_k_iterations((problem_size.k() + Mma::Shape::kK - 1) / Mma::Shape::kK) {
+      params_A(ref_A.layout()),
+      ref_A(ref_A),
+      params_B(ref_B.layout()),
+      ref_B(ref_B),
+      params_D(ref_D.layout()),
+      ref_D(ref_D),
+      output_op(output_op),
+      splitk_slice_stride(splitk_slice_stride) {
+
+      int full_gemm_k_iterations = problem_size.k() / Mma::Shape::kK;
+      int gemm_k_iterations = full_gemm_k_iterations / grid_tiled_shape.k();
 
+      gemm_k_size = gemm_k_iterations * Mma::Shape::kK;
     }
   };
 
   /// Shared memory storage structure
   union SharedStorage {
     typename Mma::SharedStorage main_loop;
     typename Epilogue::SharedStorage epilogue;
   };
 
   //
   // Methods
   //
 
   CUTLASS_HOST_DEVICE
-  GemmBatched() { } 
+  GemmSplitKParallel() { } 
 
   /// Executes one GEMM
   CUTLASS_DEVICE
   void operator()(Params const &params, SharedStorage &shared_storage) {
 
     // Compute threadblock location
     ThreadblockSwizzle threadblock_swizzle;
@@ -154,126 +142,112 @@
     // Early exit if CTA is out of range
     if (params.grid_tiled_shape.m() <= threadblock_tile_offset.m() ||
       params.grid_tiled_shape.n() <= threadblock_tile_offset.n()) {
 
       return;
     }
 
+    // Compute initial location in logical coordinates
+    cutlass::MatrixCoord tb_offset_A{
+      threadblock_tile_offset.m() * Mma::Shape::kM,
+      threadblock_tile_offset.k() * params.gemm_k_size,
+    };
+
+    cutlass::MatrixCoord tb_offset_B{
+      threadblock_tile_offset.k() * params.gemm_k_size,
+      threadblock_tile_offset.n() * Mma::Shape::kN
+    };
+
+    // Problem size is a function of threadblock index in the K dimension
+    int problem_size_k;
+    if (threadblock_tile_offset.k() + 1 == params.grid_tiled_shape.k()) {
+      problem_size_k = params.problem_size.k();
+    }
+    else {
+      problem_size_k = (threadblock_tile_offset.k() + 1) * params.gemm_k_size;
+    }
 
-    // Each CTA handles multiple batch indices to accommodate limited range of CUDA grid's Z dimension
-    for (int batch_idx = threadblock_swizzle.get_batch_idx(); 
-      batch_idx < params.batch_count; 
-      batch_idx += gridDim.z) {
-
-      // Compute initial location in logical coordinates
-      cutlass::MatrixCoord tb_offset_A{
-        threadblock_tile_offset.m() * Mma::Shape::kM,
-        0
-      };
-
-      cutlass::MatrixCoord tb_offset_B{
-        0,
-        threadblock_tile_offset.n() * Mma::Shape::kN
-      };
-
-      // Compute position within threadblock
-      int thread_idx = threadIdx.x;
-
-      // Construct iterators to A and B operands
-      typename Mma::IteratorA iterator_A(
-        params.params_A,
-        params.ref_A.data(),
-        params.problem_size.mk(),
-        thread_idx,
-        tb_offset_A);
-
-      iterator_A.add_pointer_offset(params.stride_A * batch_idx);
-
-      typename Mma::IteratorB iterator_B(
-        params.params_B,
-        params.ref_B.data(),
-        params.problem_size.kn(),
-        thread_idx,
-        tb_offset_B);
-
-      iterator_B.add_pointer_offset(params.stride_B * batch_idx);
-
-
-      //
-      // Main loop
-      //
-
-      // Broadcast the warp_id computed by lane 0 to ensure dependent code
-      // is compiled as warp-uniform.
-      int warp_idx = __shfl_sync(0xffffffff, threadIdx.x / 32, 0);
-
-      int lane_idx = threadIdx.x % 32;
-      
-      Mma mma(shared_storage.main_loop, thread_idx, warp_idx, lane_idx);
-
-      typename Mma::FragmentC accumulators;
-
-      accumulators.clear();
-
-
-      // Compute threadblock-scoped matrix multiply-add
-      mma(params.gemm_k_iterations, accumulators, iterator_A, iterator_B, accumulators);
-
-      //
-      // Epilogue
-      //
-
-      OutputOp output_op(params.epilogue);
-
-      //
-      // Masked tile iterators constructed from members
-      //
-
-      threadblock_tile_offset =
-          threadblock_swizzle.get_tile_offset(params.swizzle_log_tile);
-
-      //assume identity swizzle
-      MatrixCoord threadblock_offset(
-        threadblock_tile_offset.m() * Mma::Shape::kM,
-        threadblock_tile_offset.n() * Mma::Shape::kN
-      );
-
-      // Tile iterator writing to output tile
-      typename Epilogue::OutputTileIterator iterator_C(
-        params.params_C,
-        params.ref_C.data(),
-        params.problem_size.mn(),
-        thread_idx,
-        threadblock_offset
-      );
-
-      iterator_C.add_pointer_offset(params.stride_C * batch_idx);
-
-      // Tile iterator writing to output tile
-      typename Epilogue::OutputTileIterator iterator_D(
-        params.params_D,
-        params.ref_D.data(),
-        params.problem_size.mn(),
-        thread_idx,
-        threadblock_offset
-      );
-
-      iterator_D.add_pointer_offset(params.stride_D * batch_idx);
-
-      Epilogue epilogue(
-        shared_storage.epilogue, 
-        thread_idx, 
-        warp_idx, 
-        lane_idx);
+    // Compute threadblock-scoped matrix multiply-add
+    int gemm_k_iterations = (problem_size_k - tb_offset_A.column() + Mma::Shape::kK - 1) / Mma::Shape::kK;
 
-      // run efficient epilogue
-      epilogue(output_op, iterator_D, accumulators, iterator_C);
-    }
+    // Compute position within threadblock
+    int thread_idx = threadIdx.x;
+
+    // Construct iterators to A and B operands
+    typename Mma::IteratorA iterator_A(
+      params.params_A,
+      params.ref_A.data(),
+      {params.problem_size.m(), problem_size_k},
+      thread_idx,
+      tb_offset_A);
+
+    typename Mma::IteratorB iterator_B(
+      params.params_B,
+      params.ref_B.data(),
+      {problem_size_k, params.problem_size.n()},
+      thread_idx,
+      tb_offset_B);
+
+    int warp_idx = threadIdx.x / 32;
+    int lane_idx = threadIdx.x % 32;
+
+
+    //
+    // Main loop
+    //
+
+    // Construct thread-scoped matrix multiply
+    Mma mma(shared_storage.main_loop, thread_idx, warp_idx, lane_idx);
+
+    typename Mma::FragmentC accumulators;
+
+    accumulators.clear();
+
+    mma(gemm_k_iterations, accumulators, iterator_A, iterator_B, accumulators);
+
+    //
+    // Epilogue
+    //
+
+    OutputOp output_op(params.output_op);
+
+    //
+    // Masked tile iterators constructed from members
+    //
+
+    threadblock_tile_offset =
+        threadblock_swizzle.get_tile_offset(params.swizzle_log_tile);
+
+    //assume identity swizzle
+    MatrixCoord threadblock_offset(
+      threadblock_tile_offset.m() * Mma::Shape::kM,
+      threadblock_tile_offset.n() * Mma::Shape::kN
+    );
+
+    // Tile iterator writing to output tile
+    typename Epilogue::OutputTileIterator iterator_D(
+      params.params_D,
+      params.ref_D.data(),
+      params.problem_size.mn(),
+      thread_idx,
+      threadblock_offset
+    );
+
+    iterator_D.add_pointer_offset(params.splitk_slice_stride * threadblock_tile_offset.k());
+
+    // Execute the epilogue
+    Epilogue epilogue(
+      shared_storage.epilogue, 
+      thread_idx, 
+      warp_idx, 
+      lane_idx);
+
+    // Run efficient epilogue
+    epilogue(output_op, iterator_D, accumulators, iterator_D);
   }
 };
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
 } // namespace kernel
 } // namespace gemm
 } // namespace cutlass
-
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/gemm_grouped.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/gemm_grouped.h`

 * *Files 4% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -311,21 +311,14 @@
   static Status can_implement(cutlass::gemm::GemmCoord const & problem_size) {
     return Status::kSuccess;
   }
 
   static Status can_implement(Arguments const &args) {
     return Status::kSuccess;
   }
-
-  static size_t get_extra_workspace_size(
-    Arguments const &args,
-    cutlass::gemm::GemmCoord const &grid_tiled_shape) {
-
-    return 0;
-  }
  
   /// Executes one GEMM
   CUTLASS_DEVICE
   void operator()(Params const &params, SharedStorage &shared_storage) {
 
     //
     // These types shadow the type-level definitions and support the ability to implement
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/gemm_grouped_problem_visitor.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/gemm_grouped_problem_visitor.h`

 * *Files 12% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -46,20 +46,31 @@
 namespace gemm {
 namespace kernel {
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
 namespace detail {
 // Helper for correctly representing problem sizes in grouped kernels 
-template <bool Transposed>
+template <
+  typename ThreadblockShape,
+  bool Transposed
+>
 struct GemmGroupedProblemSizeHelper {
 
   static bool const kTransposed = Transposed;
 
   CUTLASS_HOST_DEVICE
+  static cutlass::gemm::GemmCoord grid_shape(const cutlass::gemm::GemmCoord& problem) {
+    return cutlass::gemm::GemmCoord(
+      ((problem.m() - 1 + ThreadblockShape::kM) / ThreadblockShape::kM),
+      ((problem.n() - 1 + ThreadblockShape::kN) / ThreadblockShape::kN),
+      1);
+  }
+
+  CUTLASS_HOST_DEVICE
   static void possibly_transpose_problem(cutlass::gemm::GemmCoord& problem) {
     if (kTransposed) {
       swap(problem.m(), problem.n());
     }
   }
 
   CUTLASS_HOST_DEVICE
@@ -73,23 +84,23 @@
 /// Visitor class to abstract away the algorithm for iterating over tiles
 template <typename ThreadblockShape,
           GroupScheduleMode GroupScheduleMode_,
           int PrefetchTileCount,
           int ThreadCount,
           bool Transposed = false>
 struct GemmGroupedProblemVisitor : public GroupedProblemVisitor<
-                                            detail::GemmGroupedProblemSizeHelper<Transposed>,
+                                            detail::GemmGroupedProblemSizeHelper<ThreadblockShape, Transposed>,
                                             ThreadblockShape,
                                             GroupScheduleMode_,
                                             PrefetchTileCount,
                                             ThreadCount> {
 
   static bool const kTransposed = Transposed;
 
-  using ProblemSizeHelper = detail::GemmGroupedProblemSizeHelper<Transposed>;
+  using ProblemSizeHelper = detail::GemmGroupedProblemSizeHelper<ThreadblockShape, Transposed>;
   using Base = GroupedProblemVisitor<ProblemSizeHelper, ThreadblockShape, GroupScheduleMode_, PrefetchTileCount, ThreadCount>;
   using Params = typename Base::Params;
   using SharedStorage = typename Base::SharedStorage;
 
   //
   // Methods
   //
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/gemm_grouped_softmax_mainloop_fusion.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/gemm_grouped_softmax_mainloop_fusion.h`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -331,21 +331,14 @@
     return Status::kSuccess;
   }
 
   static Status can_implement(Arguments const &args) {
     return Status::kSuccess;
   }
 
-  static size_t get_extra_workspace_size(
-    Arguments const &args,
-    cutlass::gemm::GemmCoord const &grid_tiled_shape) {
-
-    return 0;
-  }
-
   /// Executes one GEMM
   CUTLASS_DEVICE
   void operator()(Params const &params, SharedStorage &shared_storage) {
 
     //
     // These types shadow the type-level definitions and support the ability to implement
     // a 'transposed' GEMM that computes the transposed problems.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/gemm_layernorm_mainloop_fusion.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/gemm_layernorm_mainloop_fusion.h`

 * *Files 6% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -37,14 +37,15 @@
 
 #include "cutlass/cutlass.h"
 #include "cutlass/fast_math.h"
 #include "cutlass/gemm/gemm.h"
 #include "cutlass/matrix_coord.h"
 #include "cutlass/complex.h"
 #include "cutlass/semaphore.h"
+#include "cutlass/gemm/kernel/params_universal_base.h"
 
 #include "cutlass/layout/matrix.h"
 
 #include "cutlass/trace.h"
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
@@ -100,24 +101,20 @@
   static int const kSplitKAlignment = const_max(128 / sizeof_bits<ElementA>::value, 128 / sizeof_bits<ElementB>::value);
 
   //
   // Structures
   //
 
   /// Argument structure
-  struct Arguments {
-
+  struct Arguments : UniversalArgumentsBase
+  {
     //
     // Data members
     //
 
-    GemmUniversalMode mode;
-    GemmCoord problem_size;
-    int batch_count;
-
     typename EpilogueOutputOp::Params epilogue;
 
     void const * ptr_A;
     void const * ptr_B;
     void const * ptr_var;
     void const * ptr_mean;
     void const * ptr_gamma;
@@ -128,15 +125,14 @@
     int64_t batch_stride_A;
     int64_t batch_stride_B;
     int64_t batch_stride_var;
     int64_t batch_stride_mean;
     int64_t batch_stride_gamma;
     int64_t batch_stride_beta;
     int64_t batch_stride_C;
-    int64_t batch_stride_D;
 
     typename LayoutA::Stride stride_a;
     typename LayoutB::Stride stride_b;
     typename LayoutScaleBias::Stride stride_var;
     typename LayoutScaleBias::Stride stride_mean;
     typename LayoutScaleBias::Stride stride_gamma;
     typename LayoutScaleBias::Stride stride_beta;
@@ -157,22 +153,21 @@
     int const * ptr_scatter_D_indices;
 
     //
     // Methods
     //
     
     Arguments(): 
-      mode(GemmUniversalMode::kGemm), 
-      batch_count(1), 
       ptr_A(nullptr), ptr_B(nullptr), ptr_C(nullptr), ptr_D(nullptr),
       ptr_var(nullptr), ptr_mean(nullptr),
       ptr_gamma(nullptr), ptr_beta(nullptr),
       ptr_gather_A_indices(nullptr),
       ptr_gather_B_indices(nullptr),
-      ptr_scatter_D_indices(nullptr) {}
+      ptr_scatter_D_indices(nullptr)
+    {}
 
     /// constructs an arguments structure
     Arguments(
       GemmUniversalMode mode,
       GemmCoord problem_size,
       int batch_count,
       typename EpilogueOutputOp::Params epilogue,
@@ -198,39 +193,35 @@
       typename LayoutScaleBias::Stride stride_mean,
       typename LayoutScaleBias::Stride stride_gamma,
       typename LayoutScaleBias::Stride stride_beta,
       typename LayoutC::Stride stride_c,
       typename LayoutC::Stride stride_d,
       int const *ptr_gather_A_indices = nullptr,
       int const *ptr_gather_B_indices = nullptr,
-      int const *ptr_scatter_D_indices = nullptr
-    ):
-      mode(mode), 
-      problem_size(problem_size), 
-      batch_count(batch_count),
+      int const *ptr_scatter_D_indices = nullptr)
+    :
+      UniversalArgumentsBase(mode, problem_size, batch_count, batch_stride_D),
       epilogue(epilogue), 
       ptr_A(ptr_A), ptr_B(ptr_B), ptr_C(ptr_C), ptr_D(ptr_D),
       ptr_var(ptr_var), ptr_mean(ptr_mean), 
       ptr_gamma(ptr_gamma), ptr_beta(ptr_beta), 
-      batch_stride_A(batch_stride_A), batch_stride_B(batch_stride_B), batch_stride_C(batch_stride_C), batch_stride_D(batch_stride_D), 
+      batch_stride_A(batch_stride_A), batch_stride_B(batch_stride_B), batch_stride_C(batch_stride_C),
+      batch_stride_var(batch_stride_var), batch_stride_mean(batch_stride_mean),
+      batch_stride_gamma(batch_stride_gamma), batch_stride_beta(batch_stride_beta),
+      lda(0), ldb(0), ldc(0), ldd(0),
+      ld_var(0), ld_mean(0),
+      ld_gamma(0), ld_beta(0),
       stride_a(stride_a), stride_b(stride_b), stride_c(stride_c), stride_d(stride_d),
       stride_var(stride_var), stride_mean(stride_mean),
       stride_gamma(stride_gamma), stride_beta(stride_beta),
       ptr_gather_A_indices(ptr_gather_A_indices), ptr_gather_B_indices(ptr_gather_B_indices),
-      ptr_scatter_D_indices(ptr_scatter_D_indices) {
-      lda = 0;
-      ldb = 0;
-      ldc = 0;
-      ldd = 0;
-      ld_var = 0;
-      ld_mean = 0;
-      ld_gamma = 0;
-      ld_beta = 0;
+      ptr_scatter_D_indices(ptr_scatter_D_indices)
+    {
       CUTLASS_TRACE_HOST("GemmUniversal::Arguments::Arguments() - problem_size: " << problem_size);
-      }
+    }
 
     /// constructs an arguments structure
     Arguments(
       GemmUniversalMode mode,
       GemmCoord problem_size,
       int batch_count,
       typename EpilogueOutputOp::Params epilogue,
@@ -256,41 +247,40 @@
       typename LayoutScaleBias::Stride::LongIndex ld_mean,
       typename LayoutScaleBias::Stride::LongIndex ld_gamma,
       typename LayoutScaleBias::Stride::LongIndex ld_beta,
       typename LayoutC::Stride::LongIndex ldc,
       typename LayoutC::Stride::LongIndex ldd,
       int const *ptr_gather_A_indices = nullptr,
       int const *ptr_gather_B_indices = nullptr,
-      int const *ptr_scatter_D_indices = nullptr
-    ):
-      mode(mode), 
-      problem_size(problem_size), 
-      batch_count(batch_count),
+      int const *ptr_scatter_D_indices = nullptr)
+    :
+      UniversalArgumentsBase(mode, problem_size, batch_count, batch_stride_D),
       epilogue(epilogue), 
       ptr_A(ptr_A), ptr_B(ptr_B), ptr_C(ptr_C), ptr_D(ptr_D),
       ptr_var(ptr_var), ptr_mean(ptr_mean), 
       ptr_gamma(ptr_gamma), ptr_beta(ptr_beta), 
-      batch_stride_A(batch_stride_A), batch_stride_B(batch_stride_B), batch_stride_C(batch_stride_C), batch_stride_D(batch_stride_D),
+      batch_stride_A(batch_stride_A), batch_stride_B(batch_stride_B), batch_stride_C(batch_stride_C),
       batch_stride_var(batch_stride_var), batch_stride_mean(batch_stride_mean),
       batch_stride_gamma(batch_stride_gamma), batch_stride_beta(batch_stride_beta),
       lda(lda), ldb(ldb), ldc(ldc), ldd(ldd),
       ld_var(ld_var), ld_mean(ld_mean),
       ld_gamma(ld_gamma), ld_beta(ld_beta),
       ptr_gather_A_indices(ptr_gather_A_indices), ptr_gather_B_indices(ptr_gather_B_indices),
-      ptr_scatter_D_indices(ptr_scatter_D_indices) {
+      ptr_scatter_D_indices(ptr_scatter_D_indices)
+    {
       stride_a = make_Coord(lda);
       stride_b = make_Coord(ldb);
       stride_c = make_Coord(ldc);
       stride_d = make_Coord(ldd);
       stride_var = make_Coord(ld_var);
       stride_mean = make_Coord(ld_mean);
       stride_gamma = make_Coord(ld_gamma);
       stride_beta = make_Coord(ld_beta);
       CUTLASS_TRACE_HOST("GemmUniversal::Arguments::Arguments() - problem_size: " << problem_size);
-      }
+    }
 
     /// Returns arguments for the transposed problem
     Arguments transposed_problem() const {
       Arguments args(*this);
       
       std::swap(args.problem_size.m(), args.problem_size.n());
       std::swap(args.ptr_A, args.ptr_B);
@@ -299,36 +289,45 @@
       std::swap(args.batch_stride_A, args.batch_stride_B);
       std::swap(args.ptr_gather_A_indices, args.ptr_gather_B_indices);
 
       return args;
     }
   };
 
+
   //
   // Structure for precomputing values in host memory and passing to kernels
   //
 
   /// Parameters structure
-  struct Params {
+  struct Params : UniversalParamsBase<
+    ThreadblockSwizzle,
+    ThreadblockShape,
+    ElementA,
+    ElementB,
+    ElementC>
+  {
+    using ParamsBase = UniversalParamsBase<
+      ThreadblockSwizzle,
+      ThreadblockShape,
+      ElementA,
+      ElementB,
+      ElementC>;
+
+    //
+    // Data members
+    //
 
-    cutlass::gemm::GemmCoord problem_size;
-    cutlass::gemm::GemmCoord grid_tiled_shape;
-    int swizzle_log_tile;
-    
     typename Mma::IteratorA::Params params_A;
     typename Mma::IteratorB::Params params_B;
     typename Epilogue::OutputTileIterator::Params params_C;
     typename Epilogue::OutputTileIterator::Params params_D;
     
     typename EpilogueOutputOp::Params output_op;
 
-    GemmUniversalMode mode;
-    int batch_count;
-    int gemm_k_size;
-
     void * ptr_A;
     void * ptr_B;
     void * ptr_var;
     void * ptr_mean;
     void * ptr_gamma;
     void * ptr_beta;
     void * ptr_C;
@@ -337,73 +336,38 @@
     int64_t batch_stride_A;
     int64_t batch_stride_B;
     int64_t batch_stride_var;
     int64_t batch_stride_mean;
     int64_t batch_stride_gamma;
     int64_t batch_stride_beta;
     int64_t batch_stride_C;
-    int64_t batch_stride_D;
 
     int * ptr_gather_A_indices;
     int * ptr_gather_B_indices;
     int * ptr_scatter_D_indices;
 
-    int *semaphore;
-
     //
-    // Methods
+    // Host dispatch API
     //
 
-    CUTLASS_HOST_DEVICE
-    Params():
-      swizzle_log_tile(0),
-      params_A(0),
-      params_B(0),
-      params_C(0),
-      params_D(0),
-      batch_count(0),
-      gemm_k_size(0),
-      mode(cutlass::gemm::GemmUniversalMode::kGemm),
-      ptr_A(nullptr),
-      ptr_B(nullptr),
-      ptr_var(nullptr),
-      ptr_mean(nullptr),
-      ptr_gamma(nullptr),
-      ptr_beta(nullptr),
-      ptr_C(nullptr),
-      ptr_D(nullptr),
-      batch_stride_A(0),
-      batch_stride_B(0),
-      batch_stride_var(0),
-      batch_stride_mean(0),
-      batch_stride_C(0),
-      batch_stride_D(0),
-      ptr_gather_A_indices(nullptr),
-      ptr_gather_B_indices(nullptr),
-      ptr_scatter_D_indices(nullptr),
-      semaphore(nullptr) { }
+    /// Default constructor
+    Params() = default;
 
-    CUTLASS_HOST_DEVICE
+    /// Constructor
     Params(
-      Arguments const &args,
-      cutlass::gemm::GemmCoord const & grid_tiled_shape,
-      int gemm_k_size,
-      void *workspace = nullptr
-    ):
-      problem_size(args.problem_size),
-      grid_tiled_shape(grid_tiled_shape),
-      swizzle_log_tile(ThreadblockSwizzle().get_log_tile(grid_tiled_shape)),
+      Arguments const &args,  /// GEMM application arguments
+      int device_sms,         /// Number of SMs on the device
+      int sm_occupancy)       /// Kernel SM occupancy (in thread blocks)
+    :
+      ParamsBase(args, device_sms, sm_occupancy),
       params_A(args.lda ? make_Coord_with_padding<LayoutA::kStrideRank>(args.lda) : args.stride_a),
       params_B(args.ldb ? make_Coord_with_padding<LayoutB::kStrideRank>(args.ldb) : args.stride_b),
       params_C(args.ldc ? make_Coord_with_padding<LayoutC::kStrideRank>(args.ldc) : args.stride_c),
       params_D(args.ldd ? make_Coord_with_padding<LayoutC::kStrideRank>(args.ldd) : args.stride_d),
       output_op(args.epilogue),
-      mode(args.mode),
-      batch_count(args.batch_count),
-      gemm_k_size(gemm_k_size),
       ptr_A(const_cast<void *>(args.ptr_A)),
       ptr_B(const_cast<void *>(args.ptr_B)),
       ptr_var(const_cast<void *>(args.ptr_var)),
       ptr_mean(const_cast<void *>(args.ptr_mean)),
       ptr_gamma(const_cast<void *>(args.ptr_gamma)),
       ptr_beta(const_cast<void *>(args.ptr_beta)),
       ptr_C(const_cast<void *>(args.ptr_C)),
@@ -411,71 +375,55 @@
       batch_stride_A(args.batch_stride_A),
       batch_stride_B(args.batch_stride_B),
       batch_stride_var(args.batch_stride_var),
       batch_stride_mean(args.batch_stride_mean),
       batch_stride_gamma(args.batch_stride_gamma),
       batch_stride_beta(args.batch_stride_beta),
       batch_stride_C(args.batch_stride_C),
-      batch_stride_D(args.batch_stride_D),
       ptr_gather_A_indices(const_cast<int *>(args.ptr_gather_A_indices)),
       ptr_gather_B_indices(const_cast<int *>(args.ptr_gather_B_indices)),
-      ptr_scatter_D_indices(const_cast<int *>(args.ptr_scatter_D_indices)),
-      semaphore(static_cast<int *>(workspace)) {
-
-    }
-
-    CUTLASS_HOST_DEVICE
-    void update(
-      Arguments const &args,
-      void *workspace = nullptr) {
+      ptr_scatter_D_indices(const_cast<int *>(args.ptr_scatter_D_indices))
+    {}
 
+    /// Lightweight update given a subset of arguments.  Problem geometry is assumed
+    /// to remain the same.
+    void update(Arguments const &args)
+    {
       ptr_A = const_cast<void *>(args.ptr_A);
       ptr_B = const_cast<void *>(args.ptr_B);
       ptr_var = const_cast<void *>(args.ptr_var);
       ptr_mean = const_cast<void *>(args.ptr_mean);
       ptr_gamma = const_cast<void *>(args.ptr_gamma);
       ptr_beta = const_cast<void *>(args.ptr_beta);
       ptr_C = const_cast<void *>(args.ptr_C);
       ptr_D = args.ptr_D;
 
       ptr_gather_A_indices = const_cast<int *>(args.ptr_gather_A_indices);
       ptr_gather_B_indices = const_cast<int *>(args.ptr_gather_B_indices);
       ptr_scatter_D_indices = const_cast<int *>(args.ptr_scatter_D_indices);
 
-      batch_stride_A = args.batch_stride_A;
-      batch_stride_B = args.batch_stride_B;
-      batch_stride_var = args.batch_stride_var;
-      batch_stride_mean = args.batch_stride_mean;
-      batch_stride_gamma = args.batch_stride_gamma;
-      batch_stride_beta = args.batch_stride_beta;
-      batch_stride_C = args.batch_stride_C;
-      batch_stride_D = args.batch_stride_D;
-
       output_op = args.epilogue;
       
-      semaphore = static_cast<int *>(workspace);
       CUTLASS_TRACE_HOST("GemmUniversal::Params::update()");
     }
   };
 
+
   /// Shared memory storage structure
   union SharedStorage {
     typename Mma::SharedStorage main_loop;
     typename Epilogue::SharedStorage epilogue;
   };
 
 public:
 
   //
-  // Methods
+  // Host dispatch API
   //
 
-  CUTLASS_DEVICE
-  GemmLayernormMainloopFusion() { } 
-
   /// Determines whether kernel satisfies alignment
   static Status can_implement(
     cutlass::gemm::GemmCoord const & problem_size) {
 
     CUTLASS_TRACE_HOST("GemmUniversal::can_implement()");
 
     static int const kAlignmentA = (platform::is_same<LayoutA,
@@ -551,20 +499,31 @@
     return Status::kSuccess;
   }
 
   static Status can_implement(Arguments const &args) {
     return can_implement(args.problem_size);
   }
 
-  static size_t get_extra_workspace_size(Arguments const &args,
-                                         cutlass::gemm::GemmCoord const &grid_tiled_shape) {
+public:
 
-    return 0;
+  //
+  // Device-only API
+  //
+
+  // Factory invocation
+  CUTLASS_DEVICE
+  static void invoke(
+    Params const &params,
+    SharedStorage &shared_storage)
+  {
+    GemmLayernormMainloopFusion op;
+    op(params, shared_storage);
   }
  
+
   /// Executes one GEMM
   CUTLASS_DEVICE
   void operator()(Params const &params, SharedStorage &shared_storage) {
 
     // Compute threadblock location
     ThreadblockSwizzle threadblock_swizzle;
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/gemm_params.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/gemm_params.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/gemm_pipelined.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/gemm_pipelined.h`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/gemm_planar_complex.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/gemm_planar_complex.h`

 * *Files 6% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -37,14 +37,15 @@
 
 #include "cutlass/cutlass.h"
 #include "cutlass/fast_math.h"
 #include "cutlass/gemm/gemm.h"
 #include "cutlass/matrix_coord.h"
 #include "cutlass/complex.h"
 #include "cutlass/semaphore.h"
+#include "cutlass/gemm/kernel/params_universal_base.h"
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
 namespace cutlass {
 namespace gemm {
 namespace kernel {
 
@@ -101,24 +102,20 @@
   static int const kAlignmentC = Epilogue::OutputTileIterator::kElementsPerAccess;
 
   //
   // Arguments structure
   //
 
   /// Argument structure
-  struct Arguments {
-
+  struct Arguments : UniversalArgumentsBase
+  {
     //
     // Data members
     //
 
-    GemmUniversalMode mode;
-    GemmCoord problem_size;
-    int batch_count;
-
     typename EpilogueOutputOp::Params epilogue;
 
     void const * ptr_A_real;
     void const * ptr_A_imag;
 
     void const * ptr_B_real;
     void const * ptr_B_imag;
@@ -140,34 +137,30 @@
     
     int64_t batch_stride_A;
     int64_t batch_stride_A_imag;
     int64_t batch_stride_B;
     int64_t batch_stride_B_imag;
     int64_t batch_stride_C;
     int64_t batch_stride_C_imag;
-    int64_t batch_stride_D;
     int64_t batch_stride_D_imag;
 
-
     //
     // Methods
     //
     
-    Arguments(): 
-      mode(GemmUniversalMode::kGemm), 
-      batch_count(1), 
+    Arguments() :
       ptr_A_real(nullptr), 
       ptr_A_imag(nullptr), 
       ptr_B_real(nullptr), 
       ptr_B_imag(nullptr), 
       ptr_C_real(nullptr), 
       ptr_C_imag(nullptr), 
       ptr_D_real(nullptr),
       ptr_D_imag(nullptr)
-      { }
+    {}
 
     /// constructs an arguments structure
     Arguments(
       GemmUniversalMode mode,
       GemmCoord problem_size,
       int batch_count,
       typename EpilogueOutputOp::Params epilogue,
@@ -190,19 +183,17 @@
       int64_t batch_stride_A = 0,
       int64_t batch_stride_A_imag = 0,
       int64_t batch_stride_B = 0,
       int64_t batch_stride_B_imag = 0,
       int64_t batch_stride_C = 0,
       int64_t batch_stride_C_imag = 0,
       int64_t batch_stride_D = 0,
-      int64_t batch_stride_D_imag = 0
-    ):
-      mode(mode), 
-      problem_size(problem_size), 
-      batch_count(batch_count),
+      int64_t batch_stride_D_imag = 0)
+    :
+      UniversalArgumentsBase(mode, problem_size, batch_count, batch_stride_D),
       epilogue(epilogue), 
       ptr_A_real(ptr_A_real), 
       ptr_A_imag(ptr_A_imag), 
       ptr_B_real(ptr_B_real),
       ptr_B_imag(ptr_B_imag),
       ptr_C_real(ptr_C_real),
       ptr_C_imag(ptr_C_imag),
@@ -218,18 +209,16 @@
       ldd_imag(ldd_imag),
       batch_stride_A(batch_stride_A),
       batch_stride_A_imag(batch_stride_A_imag),
       batch_stride_B(batch_stride_B),
       batch_stride_B_imag(batch_stride_B_imag),
       batch_stride_C(batch_stride_C),
       batch_stride_C_imag(batch_stride_C_imag),
-      batch_stride_D(batch_stride_D),
-      batch_stride_D_imag(batch_stride_D_imag) {
-
-      }
+      batch_stride_D_imag(batch_stride_D_imag)
+    {}
 
     /// Returns arguments for the transposed problem
     Arguments transposed_problem() const {
       Arguments args(*this);
       
       std::swap(args.problem_size.m(), args.problem_size.n());
       std::swap(args.ptr_A_real, args.ptr_B_real);
@@ -239,178 +228,157 @@
       std::swap(args.batch_stride_A, args.batch_stride_B);
       std::swap(args.batch_stride_A_imag, args.batch_stride_B_imag);
 
       return args;
     }
   };
 
+
   //
   // Structure for precomputing values in host memory and passing to kernels
   //
 
   /// Parameters structure
-  struct Params {
-    cutlass::gemm::GemmCoord problem_size;
-    cutlass::gemm::GemmCoord grid_tiled_shape;
-    int swizzle_log_tile;
-    
+  struct Params : UniversalParamsBase<
+    ThreadblockSwizzle,
+    ThreadblockShape,
+    ElementA,
+    ElementB,
+    ElementC>
+  {
+    using ParamsBase = UniversalParamsBase<
+      ThreadblockSwizzle,
+      ThreadblockShape,
+      ElementA,
+      ElementB,
+      ElementC>;
+
+    //
+    // Data members
+    //
+
     typename Mma::IteratorA::Params params_A_real;
     typename Mma::IteratorA::Params params_A_imag;
     typename Mma::IteratorB::Params params_B_real;
     typename Mma::IteratorB::Params params_B_imag;
     typename Epilogue::OutputTileIterator::Params params_C_real;
     typename Epilogue::OutputTileIterator::Params params_C_imag;
     typename Epilogue::OutputTileIterator::Params params_D_real;
     typename Epilogue::OutputTileIterator::Params params_D_imag;
     
     typename EpilogueOutputOp::Params output_op;
 
-    GemmUniversalMode mode;
-    int batch_count;
-    int gemm_k_size;
-
     void * ptr_A_real;
     void * ptr_A_imag;
     void * ptr_B_real;
     void * ptr_B_imag;
     void * ptr_C_real;
     void * ptr_C_imag;
     void * ptr_D_real;
     void * ptr_D_imag;
 
     int64_t batch_stride_A;
-    int64_t batch_stride_A_imag;
     int64_t batch_stride_B;
-    int64_t batch_stride_B_imag;
     int64_t batch_stride_C;
+
+    int64_t batch_stride_A_imag;
+    int64_t batch_stride_B_imag;
     int64_t batch_stride_C_imag;
-    int64_t batch_stride_D;
     int64_t batch_stride_D_imag;
 
-    int *semaphore;
-
     //
-    // Methods
+    // Host dispatch API
     //
 
-    CUTLASS_HOST_DEVICE
-    Params():
-      batch_count(0),
-      gemm_k_size(0),
-      swizzle_log_tile(0),
-      mode(cutlass::gemm::GemmUniversalMode::kGemm),
-      ptr_A_real(nullptr),
-      ptr_A_imag(nullptr),
-      ptr_B_real(nullptr),
-      ptr_B_imag(nullptr),
-      ptr_C_real(nullptr),
-      ptr_C_imag(nullptr),
-      ptr_D_real(nullptr),
-      ptr_D_imag(nullptr),
-      batch_stride_A(0),
-      batch_stride_A_imag(0),
-      batch_stride_B(0),
-      batch_stride_B_imag(0),
-      batch_stride_C(0),
-      batch_stride_C_imag(0),
-      batch_stride_D(0),
-      batch_stride_D_imag(0),
-      semaphore(nullptr) { }
+    /// Default constructor
+    Params() = default;
 
-    CUTLASS_HOST_DEVICE
+    /// Constructor
     Params(
-      Arguments const &args,
-      cutlass::gemm::GemmCoord const & grid_tiled_shape,
-      int gemm_k_size,
-      void *workspace = nullptr
-    ):
-      problem_size(args.problem_size),
-      grid_tiled_shape(grid_tiled_shape),
-      swizzle_log_tile(ThreadblockSwizzle().get_log_tile(grid_tiled_shape)),
+      Arguments const &args,  /// GEMM application arguments
+      int device_sms,         /// Number of SMs on the device
+      int sm_occupancy)       /// Kernel SM occupancy (in thread blocks)
+    :
+      ParamsBase(args, device_sms, sm_occupancy),
       params_A_real(args.lda_real),
       params_A_imag(args.lda_imag),
       params_B_real(args.ldb_real),
       params_B_imag(args.ldb_imag),
       params_C_real(args.ldc_real),
       params_C_imag(args.ldc_imag),
       params_D_real(args.ldd_real),
       params_D_imag(args.ldd_imag),
       output_op(args.epilogue),
-      mode(args.mode),
-      batch_count(args.batch_count),
-      gemm_k_size(gemm_k_size),
       ptr_A_real(const_cast<void *>(args.ptr_A_real)),
       ptr_A_imag(const_cast<void *>(args.ptr_A_imag)),
       ptr_B_real(const_cast<void *>(args.ptr_B_real)),
       ptr_B_imag(const_cast<void *>(args.ptr_B_imag)),
       ptr_C_real(const_cast<void *>(args.ptr_C_real)),
       ptr_C_imag(const_cast<void *>(args.ptr_C_imag)),
       ptr_D_real(args.ptr_D_real),
       ptr_D_imag(args.ptr_D_imag),
       batch_stride_A(args.batch_stride_A),
-      batch_stride_A_imag(args.batch_stride_A_imag),
       batch_stride_B(args.batch_stride_B),
-      batch_stride_B_imag(args.batch_stride_B_imag),
       batch_stride_C(args.batch_stride_C),
+      batch_stride_A_imag(args.batch_stride_A_imag),
+      batch_stride_B_imag(args.batch_stride_B_imag),
       batch_stride_C_imag(args.batch_stride_C_imag),
-      batch_stride_D(args.batch_stride_D),
-      batch_stride_D_imag(args.batch_stride_D_imag),
-      semaphore(static_cast<int *>(workspace)) {
+      batch_stride_D_imag(args.batch_stride_D_imag)
+    {}
 
-    }
+    /// Returns the workspace size (in bytes) needed for this problem geometry
+    size_t get_workspace_size() const
+    {
+      size_t workspace_bytes = ParamsBase::get_workspace_size();
+      if (this->mode == GemmUniversalMode::kGemmSplitKParallel)
+      {
+        // Double the size returned by the base class because we need to
+        // accumulate two ElementC components
+        workspace_bytes *= 2;
+      }
 
-    void update(
-      Arguments const &args,
-      void *workspace = nullptr) {
+      return workspace_bytes;
+    }
 
+    /// Lightweight update given a subset of arguments.  Problem geometry is assumed
+    /// to remain the same.
+    void update(Arguments const &args)
+    {
       ptr_A_real = const_cast<void *>(args.ptr_A_real);
       ptr_A_imag = const_cast<void *>(args.ptr_A_imag);
 
       ptr_B_real = const_cast<void *>(args.ptr_B_real);
       ptr_B_imag = const_cast<void *>(args.ptr_B_imag);
 
       ptr_C_real = const_cast<void *>(args.ptr_C_real);
       ptr_C_imag = const_cast<void *>(args.ptr_C_imag);
 
       ptr_D_real = const_cast<void *>(args.ptr_D_real);
       ptr_D_imag = const_cast<void *>(args.ptr_D_imag);
 
-      batch_stride_A = args.batch_stride_A;
-      batch_stride_A_imag = args.batch_stride_A_imag;
-      batch_stride_B = args.batch_stride_B;
-      batch_stride_B_imag = args.batch_stride_B_imag;
-      batch_stride_C = args.batch_stride_C;
-      batch_stride_C_imag = args.batch_stride_C_imag;
-      batch_stride_D = args.batch_stride_D;
-      batch_stride_D_imag = args.batch_stride_D_imag;
-
       output_op = args.epilogue;
-      
-      semaphore = static_cast<int *>(workspace);
     }
   };
 
+
   /// Shared memory storage structure
   union SharedStorage {
     typename Mma::SharedStorage main_loop;
     typename Epilogue::SharedStorage epilogue;
   };
 
 public:
 
   //
-  // Methods
+  // Host dispatch API
   //
 
-  CUTLASS_DEVICE
-  GemmPlanarComplex() { } 
-
   /// Determines whether kernel satisfies alignment
-  static Status can_implement(Arguments const &args) {
-
+  static Status can_implement(Arguments const &args)
+  {
     static int const kAlignmentA = Mma::IteratorA::AccessType::kElements;
     static int const kAlignmentB = Mma::IteratorB::AccessType::kElements;
     static int const kAlignmentC = Epilogue::OutputTileIterator::kElementsPerAccess;
 
     bool isAMisaligned = false;
     bool isBMisaligned = false;
     bool isCMisaligned = false;
@@ -436,20 +404,31 @@
     if (isAMisaligned || isBMisaligned || isCMisaligned) {
       return Status::kErrorMisalignedOperand;
     }
 
     return Status::kSuccess;
   }
 
-  static size_t get_extra_workspace_size(Arguments const &args,
-                                         cutlass::gemm::GemmCoord const &grid_tiled_shape) {
+public:
+
+  //
+  // Device-only API
+  //
 
-    return 0;
+  // Factory invocation
+  CUTLASS_DEVICE
+  static void invoke(
+    Params const &params,
+    SharedStorage &shared_storage)
+  {
+    GemmPlanarComplex op;
+    op(params, shared_storage);
   }
 
+
   /// Executes one GEMM
   CUTLASS_DEVICE
   void operator()(Params const &params, SharedStorage &shared_storage) {
 
     // Compute threadblock location
     ThreadblockSwizzle threadblock_swizzle;
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/gemm_planar_complex_array.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/gemm_planar_complex_array.h`

 * *Files 10% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -37,14 +37,15 @@
 
 #include "cutlass/cutlass.h"
 #include "cutlass/fast_math.h"
 #include "cutlass/gemm/gemm.h"
 #include "cutlass/matrix_coord.h"
 #include "cutlass/complex.h"
 #include "cutlass/semaphore.h"
+#include "cutlass/gemm/kernel/params_universal_base.h"
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
 namespace cutlass {
 namespace gemm {
 namespace kernel {
 
@@ -101,24 +102,20 @@
   static int const kAlignmentC = Epilogue::OutputTileIterator::kElementsPerAccess;
 
   //
   // Arguments structure
   //
 
   /// Argument structure
-  struct Arguments {
-
+  struct Arguments : UniversalArgumentsBase
+  {
     //
     // Data members
     //
 
-    GemmUniversalMode mode;
-    GemmCoord problem_size;
-    int batch_count;
-
     typename EpilogueOutputOp::Params epilogue;
 
     int const *ptr_M;
     int const *ptr_N;
     int const *ptr_K;
 
     void const * const * ptr_A_real;
@@ -138,36 +135,31 @@
     typename LayoutB::Stride::Index ldb_real;
     typename LayoutB::Stride::Index ldb_imag;
     typename LayoutC::Stride::Index ldc_real;
     typename LayoutC::Stride::Index ldc_imag;
     typename LayoutC::Stride::Index ldd_real;
     typename LayoutC::Stride::Index ldd_imag;
 
-    int64_t batch_stride_D;    // unused
-
     //
     // Methods
     //
     
     Arguments(): 
-      mode(GemmUniversalMode::kArray),
-      batch_count(1),
       ptr_M(nullptr),
       ptr_N(nullptr),
       ptr_K(nullptr),
       ptr_A_real(nullptr), 
       ptr_A_imag(nullptr), 
       ptr_B_real(nullptr), 
       ptr_B_imag(nullptr), 
       ptr_C_real(nullptr), 
       ptr_C_imag(nullptr), 
       ptr_D_real(nullptr),
-      ptr_D_imag(nullptr),
-      batch_stride_D(0)
-      { }
+      ptr_D_imag(nullptr)
+    {}
 
     /// constructs an arguments structure
     Arguments(
       GemmCoord problem_size,
       int batch_count,
       typename EpilogueOutputOp::Params epilogue,
       int const *ptr_M,
@@ -184,19 +176,17 @@
       typename LayoutA::Stride::Index lda_real,
       typename LayoutA::Stride::Index lda_imag,
       typename LayoutB::Stride::Index ldb_real,
       typename LayoutB::Stride::Index ldb_imag,
       typename LayoutC::Stride::Index ldc_real,
       typename LayoutC::Stride::Index ldc_imag,
       typename LayoutC::Stride::Index ldd_real,
-      typename LayoutC::Stride::Index ldd_imag
-    ):
-      mode(GemmUniversalMode::kArray),
-      problem_size(problem_size), 
-      batch_count(batch_count),
+      typename LayoutC::Stride::Index ldd_imag)
+    :
+      UniversalArgumentsBase(mode, problem_size, batch_count, batch_stride_D),
       epilogue(epilogue),
       ptr_M(ptr_M),
       ptr_N(ptr_N),
       ptr_K(ptr_K),
       ptr_A_real(ptr_A_real), 
       ptr_A_imag(ptr_A_imag), 
       ptr_B_real(ptr_B_real),
@@ -208,18 +198,16 @@
       lda_real(lda_real),
       lda_imag(lda_imag),
       ldb_real(ldb_real),
       ldb_imag(ldb_imag),
       ldc_real(ldc_real),
       ldc_imag(ldc_imag),
       ldd_real(ldd_real),
-      ldd_imag(ldd_imag),
-      batch_stride_D(0) {
-
-      }
+      ldd_imag(ldd_imag)
+    {}
 
     /// Returns arguments for the transposed problem
     Arguments transposed_problem() const {
       Arguments args(*this);
       
       std::swap(args.problem_size.m(), args.problem_size.n());
       std::swap(args.ptr_M, args.ptr_N);
@@ -228,107 +216,102 @@
       std::swap(args.lda_real, args.ldb_real);
       std::swap(args.lda_imag, args.ldb_imag);
 
       return args;
     }
   };
 
+
   //
   // Structure for precomputing values in host memory and passing to kernels
   //
 
   /// Parameters structure
-  struct Params {
-    cutlass::gemm::GemmCoord problem_size;
-    cutlass::gemm::GemmCoord grid_tiled_shape;
-    int swizzle_log_tile;
+  struct Params : UniversalParamsBase<
+    ThreadblockSwizzle,
+    ThreadblockShape,
+    ElementA,
+    ElementB,
+    ElementC>
+  {
+    using ParamsBase = UniversalParamsBase<
+      ThreadblockSwizzle,
+      ThreadblockShape,
+      ElementA,
+      ElementB,
+      ElementC>;
+
+    //
+    // Data members
+    //
+
     typename Mma::IteratorA::Params params_A_real;
     typename Mma::IteratorA::Params params_A_imag;
     typename Mma::IteratorB::Params params_B_real;
     typename Mma::IteratorB::Params params_B_imag;
     typename Epilogue::OutputTileIterator::Params params_C_real;
     typename Epilogue::OutputTileIterator::Params params_C_imag;
     typename Epilogue::OutputTileIterator::Params params_D_real;
     typename Epilogue::OutputTileIterator::Params params_D_imag;
-    
+
     typename EpilogueOutputOp::Params output_op;
 
-    int batch_count;
-    
     int const *ptr_M;
     int const *ptr_N;
     int const *ptr_K;
 
     void const * const * ptr_A_real;
     void const * const * ptr_A_imag;
     void const * const * ptr_B_real;
     void const * const * ptr_B_imag;
     void const * const * ptr_C_real;
     void const * const * ptr_C_imag;
     void * const * ptr_D_real;
     void * const * ptr_D_imag;
 
     //
-    // Methods
+    // Host dispatch API
     //
 
-    CUTLASS_HOST_DEVICE
-    Params():
-      batch_count(0),
-      swizzle_log_tile(0),
-      ptr_M(nullptr),
-      ptr_N(nullptr),
-      ptr_K(nullptr),
-      ptr_A_real(nullptr),
-      ptr_A_imag(nullptr),
-      ptr_B_real(nullptr),
-      ptr_B_imag(nullptr),
-      ptr_C_real(nullptr),
-      ptr_C_imag(nullptr),
-      ptr_D_real(nullptr),
-      ptr_D_imag(nullptr) { }
+    /// Default constructor
+    Params() = default;
 
-    CUTLASS_HOST_DEVICE
+    /// Constructor
     Params(
-      Arguments const &args,
-      cutlass::gemm::GemmCoord const & grid_tiled_shape,
-      int gemm_k_size = 0,                                    // ignored
-      void *workspace = nullptr                               // ignored
-    ):
-      problem_size(args.problem_size),
-      grid_tiled_shape(grid_tiled_shape),
-      swizzle_log_tile(ThreadblockSwizzle().get_log_tile(grid_tiled_shape)),
+      Arguments const &args,  /// GEMM application arguments
+      int device_sms,         /// Number of SMs on the device
+      int sm_occupancy)       /// Kernel SM occupancy (in thread blocks)
+    :
+      ParamsBase(args, device_sms, sm_occupancy),
       ptr_M(args.ptr_M),
       ptr_N(args.ptr_N),
       ptr_K(args.ptr_K),
       params_A_real(args.lda_real),
       params_A_imag(args.lda_imag),
       params_B_real(args.ldb_real),
       params_B_imag(args.ldb_imag),
       params_C_real(args.ldc_real),
       params_C_imag(args.ldc_imag),
       params_D_real(args.ldd_real),
       params_D_imag(args.ldd_imag),
       output_op(args.epilogue),
-      batch_count(args.batch_count),
       ptr_A_real(args.ptr_A_real),
       ptr_A_imag(args.ptr_A_imag),
       ptr_B_real(args.ptr_B_real),
       ptr_B_imag(args.ptr_B_imag),
       ptr_C_real(args.ptr_C_real),
       ptr_C_imag(args.ptr_C_imag),
       ptr_D_real(args.ptr_D_real),
-      ptr_D_imag(args.ptr_D_imag) {
-
-    }
-
-    void update(
-      Arguments const &args,
-      void *workspace = nullptr) {
+      ptr_D_imag(args.ptr_D_imag)
+    {}
 
+    /// Lightweight update given a subset of arguments.  Problem geometry is assumed
+    /// to remain the same.
+    void update(Arguments const &args)
+    {
       ptr_M = args.ptr_M;
       ptr_N = args.ptr_N;
       ptr_K = args.ptr_K;
 
       ptr_A_real = args.ptr_A_real;
       ptr_A_imag = args.ptr_A_imag;
 
@@ -341,29 +324,27 @@
       ptr_D_real = args.ptr_D_real;
       ptr_D_imag = args.ptr_D_imag;
 
       output_op = args.epilogue;
     }
   };
 
+
   /// Shared memory storage structure
   union SharedStorage {
     typename Mma::SharedStorage main_loop;
     typename Epilogue::SharedStorage epilogue;
   };
 
 public:
 
   //
-  // Methods
+  // Host dispatch API
   //
 
-  CUTLASS_DEVICE
-  GemmPlanarComplexArray() { } 
-
   /// Determines whether kernel satisfies alignment
   static Status can_implement(Arguments const &args) {
 
     static int const kAlignmentA = Mma::IteratorA::AccessType::kElements;
     static int const kAlignmentB = Mma::IteratorB::AccessType::kElements;
     static int const kAlignmentC = Epilogue::OutputTileIterator::kElementsPerAccess;
 
@@ -392,20 +373,32 @@
     if (isAMisaligned || isBMisaligned || isCMisaligned) {
       return Status::kErrorMisalignedOperand;
     }
 
     return Status::kSuccess;
   }
 
-  static size_t get_extra_workspace_size(Arguments const &args,
-                                         cutlass::gemm::GemmCoord const &grid_tiled_shape) {
 
-    return 0;
+public:
+
+  //
+  // Device-only API
+  //
+
+  // Factory invocation
+  CUTLASS_DEVICE
+  static void invoke(
+    Params const &params,
+    SharedStorage &shared_storage)
+  {
+    GemmPlanarComplexArray op;
+    op(params, shared_storage);
   }
- 
+
+
   /// Executes one GEMM
   CUTLASS_DEVICE
   void operator()(Params const &params, SharedStorage &shared_storage) {
 
     // Compute threadblock location
     ThreadblockSwizzle threadblock_swizzle;
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/gemm_splitk_parallel.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/reduction/kernel/reduce_split_k.h`

 * *Files 22% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -25,229 +25,224 @@
  * SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER
  * CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,
  * OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
  * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
  *
  **************************************************************************************************/
 /*! \file
-    \brief Template for GEMM performing a reduction over K partitions in parallel.
+  \brief Kernel performing a reduction over densely packed tensors in global memory
 */
 
 #pragma once
 
 #include "cutlass/cutlass.h"
+#include "cutlass/tensor_ref.h"
+#include "cutlass/numeric_types.h"
+#include "cutlass/array.h"
+#include "cutlass/functional.h"
+#include "cutlass/matrix_shape.h"
+#include "cutlass/numeric_conversion.h"
 
-#include "cutlass/gemm/gemm.h"
-#include "cutlass/matrix_coord.h"
+#include "cutlass/layout/matrix.h"
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
 namespace cutlass {
-namespace gemm {
+namespace reduction {
 namespace kernel {
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
 template <
-  typename Mma_,                  ///! Threadblock-scoped matrix multiply-accumulate 
-  typename Epilogue_,             ///! Epilogue
-  typename ThreadblockSwizzle_    ///! Threadblock swizzling function
+  typename Shape_,              ///< shape of CTA        (concept: MatrixShape)
+  typename OutputOp_ ,          ///< output operator     (concept: epilogue::thread operator)
+  typename ReductionOp_,        ///< reduction operator  (concept: ReductionOperator)
+  int PartitionsPerStage = 4    ///< number of partitions to issue 
 >
-struct GemmSplitKParallel {
+class ReduceSplitK {
+public:
 
-  using Mma = Mma_;
-  using Epilogue = Epilogue_;
-  using OutputOp = typename Epilogue::OutputOp;
-  using ThreadblockSwizzle = ThreadblockSwizzle_;
+  using Shape = Shape_;
+  using ReductionOp = ReductionOp_;
+  using OutputOp = OutputOp_;
+  static int const kElementsPerAccess = OutputOp::kCount;
+  static int const kPartitionsPerStage = PartitionsPerStage;
+
+  using ElementWorkspace = typename ReductionOp::Element;
+  using ElementAccumulator = typename ReductionOp::ElementAccumulator;
+  using ElementOutput = typename OutputOp::ElementOutput;
+
+  using WorkspaceTensorRef = TensorRef<ElementWorkspace, layout::RowMajor>;
+  using OutputTensorRef = TensorRef<ElementOutput, layout::RowMajor>;
+  using StrideIndex = typename WorkspaceTensorRef::Layout::Stride::Index;
+
+  using FragmentWorkspace = AlignedArray<ElementWorkspace, kElementsPerAccess>;
+  using FragmentAccumulator = Array<ElementAccumulator, kElementsPerAccess>;
+  using FragmentOutput = AlignedArray<ElementOutput, kElementsPerAccess>;
 
-  /// Warp count (concept: GemmShape)
-  using WarpCount = typename Mma::WarpCount;
-  static int const kThreadCount = 32 * WarpCount::kCount;
-
-  static int const kAlignmentK = Mma::Operator::Shape::kK;
+  //
+  // Types
+  //
 
-  /// Parameters structure
+  /// Params structure
   struct Params {
-    cutlass::gemm::GemmCoord problem_size;
-    cutlass::gemm::GemmCoord grid_tiled_shape;
-    int swizzle_log_tile;
-    typename Mma::IteratorA::Params params_A;
-    typename Mma::IteratorA::TensorRef ref_A;
-    typename Mma::IteratorB::Params params_B;
-    typename Mma::IteratorB::TensorRef ref_B;
-    typename Epilogue::OutputTileIterator::Params params_D;
-    typename Epilogue::OutputTileIterator::TensorRef ref_D;
-    typename OutputOp::Params output_op;
-    int64_t splitk_slice_stride;
-    int gemm_k_size;
+
+    MatrixCoord problem_size;
+    int partitions;
+    size_t partition_stride;
+    WorkspaceTensorRef workspace;
+    OutputTensorRef destination;
+    OutputTensorRef source;
+    typename OutputOp::Params output;
+    typename ReductionOp::Params reduction;
 
     //
     // Methods
     //
 
     CUTLASS_HOST_DEVICE
-    Params(): swizzle_log_tile(0) { }
+    Params() { }
 
     CUTLASS_HOST_DEVICE
     Params(
-      cutlass::gemm::GemmCoord const & problem_size,
-      cutlass::gemm::GemmCoord const & grid_tiled_shape,
-      typename Mma::IteratorA::TensorRef ref_A,
-      typename Mma::IteratorB::TensorRef ref_B,
-      typename Epilogue::OutputTileIterator::TensorRef ref_D,
-      typename OutputOp::Params output_op,
-      int64_t splitk_slice_stride
+      MatrixCoord problem_size_,
+      int partitions_,
+      size_t partition_stride_,
+      WorkspaceTensorRef workspace_,
+      OutputTensorRef destination_,
+      OutputTensorRef source_,
+      typename OutputOp::Params output_ = typename OutputOp::Params(),
+      typename ReductionOp::Params reduction_ = typename ReductionOp::Params()
     ):
-      problem_size(problem_size),
-      grid_tiled_shape(grid_tiled_shape),
-      swizzle_log_tile(ThreadblockSwizzle().get_log_tile(grid_tiled_shape)),
-      params_A(ref_A.layout()),
-      ref_A(ref_A),
-      params_B(ref_B.layout()),
-      ref_B(ref_B),
-      params_D(ref_D.layout()),
-      ref_D(ref_D),
-      output_op(output_op),
-      splitk_slice_stride(splitk_slice_stride) {
-
-      int full_gemm_k_iterations = problem_size.k() / Mma::Shape::kK;
-      int gemm_k_iterations = full_gemm_k_iterations / grid_tiled_shape.k();
+      problem_size(problem_size_),
+      partitions(partitions_),
+      partition_stride(sizeof(FragmentWorkspace) * partition_stride_ / kElementsPerAccess),
+      workspace(workspace_),
+      destination(destination_),
+      source(source_),
+      output(output_),
+      reduction(reduction_) {
 
-      gemm_k_size = gemm_k_iterations * Mma::Shape::kK;
     }
   };
 
-  /// Shared memory storage structure
-  union SharedStorage {
-    typename Mma::SharedStorage main_loop;
-    typename Epilogue::SharedStorage epilogue;
-  };
+  struct SharedStorage { };
 
-  //
-  // Methods
-  //
 
+public:
+
+  /// Computes the grid size given a chosen threadblock shape
   CUTLASS_HOST_DEVICE
-  GemmSplitKParallel() { } 
+  static dim3 grid_shape(
+    cutlass::MatrixCoord problem_size) {
 
-  /// Executes one GEMM
-  CUTLASS_DEVICE
-  void operator()(Params const &params, SharedStorage &shared_storage) {
+    return dim3(
+      (problem_size.row() + Shape::kRow - 1) / Shape::kRow,
+      (problem_size.column() + Shape::kColumn - 1) / Shape::kColumn);
+  }
 
-    // Compute threadblock location
-    ThreadblockSwizzle threadblock_swizzle;
+  /// Determines the threadblock shape
+  CUTLASS_HOST_DEVICE
+  static dim3 block_shape() {
+    return dim3(Shape::kColumn / kElementsPerAccess, Shape::kRow);
+  }
 
-    cutlass::gemm::GemmCoord threadblock_tile_offset =
-        threadblock_swizzle.get_tile_offset(params.swizzle_log_tile);
+  /// Perform a reduction
+  CUTLASS_DEVICE
+  void operator()(Params const &params, SharedStorage &storage) {
 
-    // Early exit if CTA is out of range
-    if (params.grid_tiled_shape.m() <= threadblock_tile_offset.m() ||
-      params.grid_tiled_shape.n() <= threadblock_tile_offset.n()) {
+    // Determine CTA position
+    MatrixCoord thread_offset(
+      MatrixCoord::Index(int(blockIdx.x) * Shape::kRow + threadIdx.y),
+      MatrixCoord::Index(int(blockIdx.y) * Shape::kColumn + threadIdx.x * kElementsPerAccess)
+    );
 
-      return;
-    }
+    // One guard conditional
+    if (!(thread_offset.row() < params.problem_size.row() && 
+          thread_offset.column() < params.problem_size.column())) {
 
-    // Compute initial location in logical coordinates
-    cutlass::MatrixCoord tb_offset_A{
-      threadblock_tile_offset.m() * Mma::Shape::kM,
-      threadblock_tile_offset.k() * params.gemm_k_size,
-    };
-
-    cutlass::MatrixCoord tb_offset_B{
-      threadblock_tile_offset.k() * params.gemm_k_size,
-      threadblock_tile_offset.n() * Mma::Shape::kN
-    };
-
-    // Problem size is a function of threadblock index in the K dimension
-    int problem_size_k;
-    if (threadblock_tile_offset.k() + 1 == params.grid_tiled_shape.k()) {
-      problem_size_k = params.problem_size.k();
-    }
-    else {
-      problem_size_k = (threadblock_tile_offset.k() + 1) * params.gemm_k_size;
+      return;
     }
 
-    // Compute threadblock-scoped matrix multiply-add
-    int gemm_k_iterations = (problem_size_k - tb_offset_A.column() + Mma::Shape::kK - 1) / Mma::Shape::kK;
-
-    // Compute position within threadblock
-    int thread_idx = threadIdx.x;
 
-    // Construct iterators to A and B operands
-    typename Mma::IteratorA iterator_A(
-      params.params_A,
-      params.ref_A.data(),
-      {params.problem_size.m(), problem_size_k},
-      thread_idx,
-      tb_offset_A);
+    ReductionOp reduction_op(params.reduction);
 
-    typename Mma::IteratorB iterator_B(
-      params.params_B,
-      params.ref_B.data(),
-      {problem_size_k, params.problem_size.n()},
-      thread_idx,
-      tb_offset_B);
+    FragmentAccumulator accumulator;
 
-    int warp_idx = threadIdx.x / 32;
-    int lane_idx = threadIdx.x % 32;
+    accumulator.clear();  
+    
+    //
+    // Load the first slice
+    //
 
+    char const *workspace_ptr = 
+      reinterpret_cast<char const *>(
+        params.workspace.data() + params.workspace.offset(thread_offset));
 
+    FragmentWorkspace workspace_frag[kPartitionsPerStage];
+    
     //
-    // Main loop
+    // Construct the output operator
     //
+    
+    OutputOp output_op(params.output);
 
-    // Construct thread-scoped matrix multiply
-    Mma mma(shared_storage.main_loop, thread_idx, warp_idx, lane_idx);
+    //
+    // Load and accumulate with a simple batched loading sequence.
+    //
 
-    typename Mma::FragmentC accumulators;
+    CUTLASS_PRAGMA_NO_UNROLL
+    for (int k = 0; k < params.partitions; k += kPartitionsPerStage) {
 
-    accumulators.clear();
+      CUTLASS_PRAGMA_UNROLL
+      for (int i = 0; i < kPartitionsPerStage; ++i) {
+        if (k + i < params.partitions) {
+          workspace_frag[i] = *reinterpret_cast<FragmentWorkspace const *>(workspace_ptr);
+          workspace_ptr += params.partition_stride;
+        }
+      }   
 
-    mma(gemm_k_iterations, accumulators, iterator_A, iterator_B, accumulators);
+      CUTLASS_PRAGMA_UNROLL
+      for (int i = 0; i < kPartitionsPerStage; ++i) {
+        if (k + i < params.partitions) {
+          accumulator = reduction_op(accumulator, workspace_frag[i]);
+        }
+      }
+    }
 
     //
-    // Epilogue
+    // Conditionally load the source
     //
 
-    OutputOp output_op(params.output_op);
+    FragmentOutput source_frag;
 
-    //
-    // Masked tile iterators constructed from members
-    //
+    source_frag.clear();
 
-    threadblock_tile_offset =
-        threadblock_swizzle.get_tile_offset(params.swizzle_log_tile);
+    FragmentOutput const *source_ptr = reinterpret_cast<FragmentOutput const *>(
+      params.source.data() + params.source.offset(thread_offset));
 
-    //assume identity swizzle
-    MatrixCoord threadblock_offset(
-      threadblock_tile_offset.m() * Mma::Shape::kM,
-      threadblock_tile_offset.n() * Mma::Shape::kN
-    );
+    if (output_op.is_source_needed()) {
+      reinterpret_cast<FragmentOutput &>(source_frag) = *source_ptr;
+    }
+    
+    //
+    // Compute the output
+    //
 
-    // Tile iterator writing to output tile
-    typename Epilogue::OutputTileIterator iterator_D(
-      params.params_D,
-      params.ref_D.data(),
-      params.problem_size.mn(),
-      thread_idx,
-      threadblock_offset
-    );
+    typename OutputOp::FragmentOutput output_frag = output_op(accumulator, source_frag);
 
-    iterator_D.add_pointer_offset(params.splitk_slice_stride * threadblock_tile_offset.k());
+    //
+    // Store
+    //
 
-    // Execute the epilogue
-    Epilogue epilogue(
-      shared_storage.epilogue, 
-      thread_idx, 
-      warp_idx, 
-      lane_idx);
+    FragmentOutput *dest_ptr = reinterpret_cast<FragmentOutput *>(
+      params.destination.data() + params.destination.offset(thread_offset));
 
-    // Run efficient epilogue
-    epilogue(output_op, iterator_D, accumulators, iterator_D);
+    *dest_ptr = reinterpret_cast<FragmentOutput const &>(output_frag);
   }
 };
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
 } // namespace kernel
-} // namespace gemm
+} // namespace reduction
 } // namespace cutlass
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/gemm_transpose_operands.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/gemm_transpose_operands.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/gemm_universal.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/gemm_universal.h`

 * *Files 5% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -32,34 +32,36 @@
 /*! \file
     \brief 
 */
 
 #pragma once
 
 #include "cutlass/cutlass.h"
+
+#include "cutlass/arch/arch.h"
 #include "cutlass/fast_math.h"
-#include "cutlass/gemm/gemm.h"
 #include "cutlass/matrix_coord.h"
 #include "cutlass/complex.h"
 #include "cutlass/semaphore.h"
-
 #include "cutlass/layout/matrix.h"
+#include "cutlass/gemm/gemm.h"
+#include "cutlass/gemm/kernel/params_universal_base.h"
 
 #include "cutlass/trace.h"
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
 namespace cutlass {
 namespace gemm {
 namespace kernel {
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
 template <
-  typename Mma_,                  ///! Threadblock-scoped matrix multiply-accumulate 
+  typename Mma_,                  ///! Threadblock-scoped matrix multiply-accumulate
   typename Epilogue_,             ///! Epilogue
   typename ThreadblockSwizzle_    ///! Threadblock swizzling function
 >
 struct GemmUniversal {
 public:
 
   using Mma = Mma_;
@@ -97,35 +99,30 @@
   static int const kSplitKAlignment = const_max(128 / sizeof_bits<ElementA>::value, 128 / sizeof_bits<ElementB>::value);
 
   //
   // Structures
   //
 
   /// Argument structure
-  struct Arguments {
-
+  struct Arguments : UniversalArgumentsBase
+  {
     //
     // Data members
     //
 
-    GemmUniversalMode mode;
-    GemmCoord problem_size;
-    int batch_count;
-
     typename EpilogueOutputOp::Params epilogue;
 
     void const * ptr_A;
     void const * ptr_B;
     void const * ptr_C;
     void * ptr_D;
 
     int64_t batch_stride_A;
     int64_t batch_stride_B;
     int64_t batch_stride_C;
-    int64_t batch_stride_D;
 
     typename LayoutA::Stride stride_a;
     typename LayoutB::Stride stride_b;
     typename LayoutC::Stride stride_c;
     typename LayoutC::Stride stride_d;
 
     typename LayoutA::Stride::LongIndex lda;
@@ -136,22 +133,21 @@
     int const * ptr_gather_A_indices;
     int const * ptr_gather_B_indices;
     int const * ptr_scatter_D_indices;
 
     //
     // Methods
     //
-    
-    Arguments(): 
-      mode(GemmUniversalMode::kGemm), 
-      batch_count(1), 
+
+    Arguments():
       ptr_A(nullptr), ptr_B(nullptr), ptr_C(nullptr), ptr_D(nullptr),
       ptr_gather_A_indices(nullptr),
       ptr_gather_B_indices(nullptr),
-      ptr_scatter_D_indices(nullptr) {}
+      ptr_scatter_D_indices(nullptr)
+    {}
 
     /// constructs an arguments structure
     Arguments(
       GemmUniversalMode mode,
       GemmCoord problem_size,
       int batch_count,
       typename EpilogueOutputOp::Params epilogue,
@@ -165,31 +161,30 @@
       int64_t batch_stride_D,
       typename LayoutA::Stride stride_a,
       typename LayoutB::Stride stride_b,
       typename LayoutC::Stride stride_c,
       typename LayoutC::Stride stride_d,
       int const *ptr_gather_A_indices = nullptr,
       int const *ptr_gather_B_indices = nullptr,
-      int const *ptr_scatter_D_indices = nullptr
-    ):
-      mode(mode), 
-      problem_size(problem_size), 
-      batch_count(batch_count),
+      int const *ptr_scatter_D_indices = nullptr)
+    :
+      UniversalArgumentsBase(mode, problem_size, batch_count, batch_stride_D),
       epilogue(epilogue), 
       ptr_A(ptr_A), ptr_B(ptr_B), ptr_C(ptr_C), ptr_D(ptr_D), 
-      batch_stride_A(batch_stride_A), batch_stride_B(batch_stride_B), batch_stride_C(batch_stride_C), batch_stride_D(batch_stride_D), 
+      batch_stride_A(batch_stride_A), batch_stride_B(batch_stride_B), batch_stride_C(batch_stride_C),
       stride_a(stride_a), stride_b(stride_b), stride_c(stride_c), stride_d(stride_d),
       ptr_gather_A_indices(ptr_gather_A_indices), ptr_gather_B_indices(ptr_gather_B_indices),
-      ptr_scatter_D_indices(ptr_scatter_D_indices) {
+      ptr_scatter_D_indices(ptr_scatter_D_indices)
+    {
       lda = 0;
       ldb = 0;
       ldc = 0;
       ldd = 0;
       CUTLASS_TRACE_HOST("GemmUniversal::Arguments::Arguments() - problem_size: " << problem_size);
-      }
+    }
 
     /// constructs an arguments structure
     Arguments(
       GemmUniversalMode mode,
       GemmCoord problem_size,
       int batch_count,
       typename EpilogueOutputOp::Params epilogue,
@@ -205,188 +200,158 @@
       typename LayoutB::Stride::LongIndex ldb,
       typename LayoutC::Stride::LongIndex ldc,
       typename LayoutC::Stride::LongIndex ldd,
       int const *ptr_gather_A_indices = nullptr,
       int const *ptr_gather_B_indices = nullptr,
       int const *ptr_scatter_D_indices = nullptr
     ):
-      mode(mode), 
-      problem_size(problem_size), 
-      batch_count(batch_count),
-      epilogue(epilogue), 
-      ptr_A(ptr_A), ptr_B(ptr_B), ptr_C(ptr_C), ptr_D(ptr_D), 
-      batch_stride_A(batch_stride_A), batch_stride_B(batch_stride_B), batch_stride_C(batch_stride_C), batch_stride_D(batch_stride_D),
+      UniversalArgumentsBase(mode, problem_size, batch_count, batch_stride_D),
+      epilogue(epilogue),
+      ptr_A(ptr_A), ptr_B(ptr_B), ptr_C(ptr_C), ptr_D(ptr_D),
+      batch_stride_A(batch_stride_A), batch_stride_B(batch_stride_B), batch_stride_C(batch_stride_C),
       lda(lda), ldb(ldb), ldc(ldc), ldd(ldd),
       ptr_gather_A_indices(ptr_gather_A_indices), ptr_gather_B_indices(ptr_gather_B_indices),
-      ptr_scatter_D_indices(ptr_scatter_D_indices) {
+      ptr_scatter_D_indices(ptr_scatter_D_indices)
+    {
       stride_a = make_Coord(lda);
       stride_b = make_Coord(ldb);
       stride_c = make_Coord(ldc);
       stride_d = make_Coord(ldd);
       CUTLASS_TRACE_HOST("GemmUniversal::Arguments::Arguments() - problem_size: " << problem_size);
-      }
+    }
 
     /// Returns arguments for the transposed problem
-    Arguments transposed_problem() const {
+    Arguments transposed_problem() const
+    {
       Arguments args(*this);
-      
+
       std::swap(args.problem_size.m(), args.problem_size.n());
       std::swap(args.ptr_A, args.ptr_B);
       std::swap(args.lda, args.ldb);
       std::swap(args.stride_a, args.stride_b);
       std::swap(args.batch_stride_A, args.batch_stride_B);
       std::swap(args.ptr_gather_A_indices, args.ptr_gather_B_indices);
 
       return args;
     }
   };
 
+
   //
   // Structure for precomputing values in host memory and passing to kernels
   //
 
   /// Parameters structure
-  struct Params {
+  struct Params : UniversalParamsBase<
+    ThreadblockSwizzle,
+    ThreadblockShape,
+    ElementA,
+    ElementB,
+    ElementC>
+  {
+    using ParamsBase = UniversalParamsBase<
+      ThreadblockSwizzle,
+      ThreadblockShape,
+      ElementA,
+      ElementB,
+      ElementC>;
+
+    //
+    // Data members
+    //
 
-    cutlass::gemm::GemmCoord problem_size;
-    cutlass::gemm::GemmCoord grid_tiled_shape;
-    int swizzle_log_tile;
-    
     typename Mma::IteratorA::Params params_A;
     typename Mma::IteratorB::Params params_B;
     typename Epilogue::OutputTileIterator::Params params_C;
     typename Epilogue::OutputTileIterator::Params params_D;
-    
-    typename EpilogueOutputOp::Params output_op;
 
-    GemmUniversalMode mode;
-    int batch_count;
-    int gemm_k_size;
+    typename EpilogueOutputOp::Params output_op;
 
     void * ptr_A;
     void * ptr_B;
     void * ptr_C;
     void * ptr_D;
 
     int64_t batch_stride_A;
     int64_t batch_stride_B;
     int64_t batch_stride_C;
-    int64_t batch_stride_D;
 
     int * ptr_gather_A_indices;
     int * ptr_gather_B_indices;
     int * ptr_scatter_D_indices;
 
-    int *semaphore;
-
     //
-    // Methods
+    // Host dispatch API
     //
 
-    CUTLASS_HOST_DEVICE
-    Params():
-      swizzle_log_tile(0),
-      params_A(0),
-      params_B(0),
-      params_C(0),
-      params_D(0),
-      batch_count(0),
-      gemm_k_size(0),
-      mode(cutlass::gemm::GemmUniversalMode::kGemm),
-      ptr_A(nullptr),
-      ptr_B(nullptr),
-      ptr_C(nullptr),
-      ptr_D(nullptr),
-      batch_stride_A(0),
-      batch_stride_B(0),
-      batch_stride_C(0),
-      batch_stride_D(0),
-      ptr_gather_A_indices(nullptr),
-      ptr_gather_B_indices(nullptr),
-      ptr_scatter_D_indices(nullptr),
-      semaphore(nullptr) { }
+    /// Default constructor
+    Params() = default;
 
-    CUTLASS_HOST_DEVICE
+    /// Constructor
     Params(
-      Arguments const &args,
-      cutlass::gemm::GemmCoord const & grid_tiled_shape,
-      int gemm_k_size,
-      void *workspace = nullptr
-    ):
-      problem_size(args.problem_size),
-      grid_tiled_shape(grid_tiled_shape),
-      swizzle_log_tile(ThreadblockSwizzle().get_log_tile(grid_tiled_shape)),
+      Arguments const &args,  /// GEMM application arguments
+      int device_sms,         /// Number of SMs on the device
+      int sm_occupancy)       /// Kernel SM occupancy (in thread blocks)
+    :
+      ParamsBase(args, device_sms, sm_occupancy),
       params_A(args.lda ? make_Coord_with_padding<LayoutA::kStrideRank>(args.lda) : args.stride_a),
       params_B(args.ldb ? make_Coord_with_padding<LayoutB::kStrideRank>(args.ldb) : args.stride_b),
       params_C(args.ldc ? make_Coord_with_padding<LayoutC::kStrideRank>(args.ldc) : args.stride_c),
       params_D(args.ldd ? make_Coord_with_padding<LayoutC::kStrideRank>(args.ldd) : args.stride_d),
       output_op(args.epilogue),
-      mode(args.mode),
-      batch_count(args.batch_count),
-      gemm_k_size(gemm_k_size),
       ptr_A(const_cast<void *>(args.ptr_A)),
       ptr_B(const_cast<void *>(args.ptr_B)),
       ptr_C(const_cast<void *>(args.ptr_C)),
       ptr_D(args.ptr_D),
       batch_stride_A(args.batch_stride_A),
       batch_stride_B(args.batch_stride_B),
       batch_stride_C(args.batch_stride_C),
-      batch_stride_D(args.batch_stride_D),
       ptr_gather_A_indices(const_cast<int *>(args.ptr_gather_A_indices)),
       ptr_gather_B_indices(const_cast<int *>(args.ptr_gather_B_indices)),
-      ptr_scatter_D_indices(const_cast<int *>(args.ptr_scatter_D_indices)),
-      semaphore(static_cast<int *>(workspace)) {
+      ptr_scatter_D_indices(const_cast<int *>(args.ptr_scatter_D_indices))
+    {}
 
-    }
-
-    CUTLASS_HOST_DEVICE
-    void update(
-      Arguments const &args,
-      void *workspace = nullptr) {
+    /// Lightweight update given a subset of arguments.  Problem geometry is assumed
+    /// to remain the same.
+    void update(Arguments const &args)
+    {
+      CUTLASS_TRACE_HOST("GemmUniversal::Params::update()");
 
+      // Update input/output pointers
       ptr_A = const_cast<void *>(args.ptr_A);
       ptr_B = const_cast<void *>(args.ptr_B);
       ptr_C = const_cast<void *>(args.ptr_C);
       ptr_D = args.ptr_D;
 
       ptr_gather_A_indices = const_cast<int *>(args.ptr_gather_A_indices);
       ptr_gather_B_indices = const_cast<int *>(args.ptr_gather_B_indices);
       ptr_scatter_D_indices = const_cast<int *>(args.ptr_scatter_D_indices);
 
-      batch_stride_A = args.batch_stride_A;
-      batch_stride_B = args.batch_stride_B;
-      batch_stride_C = args.batch_stride_C;
-      batch_stride_D = args.batch_stride_D;
-
       output_op = args.epilogue;
-      
-      semaphore = static_cast<int *>(workspace);
-      CUTLASS_TRACE_HOST("GemmUniversal::Params::update()");
     }
   };
 
+
   /// Shared memory storage structure
   union SharedStorage {
     typename Mma::SharedStorage main_loop;
     typename Epilogue::SharedStorage epilogue;
   };
 
+
 public:
 
   //
-  // Methods
+  // Host dispatch API
   //
 
-  CUTLASS_DEVICE
-  GemmUniversal() { } 
-
   /// Determines whether kernel satisfies alignment
   static Status can_implement(
-    cutlass::gemm::GemmCoord const & problem_size) {
-
+    cutlass::gemm::GemmCoord const & problem_size)
+  {
     CUTLASS_TRACE_HOST("GemmUniversal::can_implement()");
 
     static int const kAlignmentA = (platform::is_same<LayoutA,
                                                       layout::ColumnMajorInterleaved<32>>::value)
                                    ? 32
                                    : (platform::is_same<LayoutA,
                                                         layout::ColumnMajorInterleaved<64>>::value)
@@ -458,23 +423,38 @@
     return Status::kSuccess;
   }
 
   static Status can_implement(Arguments const &args) {
     return can_implement(args.problem_size);
   }
 
-  static size_t get_extra_workspace_size(Arguments const &args,
-                                         cutlass::gemm::GemmCoord const &grid_tiled_shape) {
 
-    return 0;
+public:
+
+  //
+  // Device-only API
+  //
+
+  // Factory invocation
+  CUTLASS_DEVICE
+  static void invoke(
+    Params const &params,
+    SharedStorage &shared_storage)
+  {
+    GemmUniversal op;
+    op(params, shared_storage);
   }
- 
+
+
   /// Executes one GEMM
   CUTLASS_DEVICE
-  void operator()(Params const &params, SharedStorage &shared_storage) {
+  void operator()(
+    Params const &params,
+    SharedStorage &shared_storage)
+  {
 
     // Compute threadblock location
     ThreadblockSwizzle threadblock_swizzle;
 
     cutlass::gemm::GemmCoord threadblock_tile_offset =
         threadblock_swizzle.get_tile_offset(params.swizzle_log_tile);
 
@@ -673,15 +653,15 @@
       accumulators, 
       iterator_C); 
     
     //
     // Release the semaphore
     //
 
-    if (params.mode == GemmUniversalMode::kGemm && params.grid_tiled_shape.k() > 1) { 
+    if (params.mode == GemmUniversalMode::kGemm && params.grid_tiled_shape.k() > 1) {
 
       int lock = 0;
       if (params.grid_tiled_shape.k() == threadblock_tile_offset.k() + 1) {
 
         // The final threadblock resets the semaphore for subsequent grids.
         lock = 0;
       }
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/gemm_universal_streamk.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/gemm_universal_streamk.h`

 * *Files 4% similar despite different names*

```diff
@@ -266,14 +266,16 @@
     int64_t batch_stride_A;
     int64_t batch_stride_B;
 
     GemmUniversalMode mode;
 
     ThreadblockSwizzle block_mapping;
 
+    bool quick_dp;
+
     void *barrier_workspace;
     void *partials_workspace;
 
     typename EpilogueOutputOp::Params output_op;
 
     void * ptr_D;
     void * ptr_C;
@@ -361,14 +363,21 @@
         args.mode,
         args.problem_size,
         {ThreadblockShape::kM, ThreadblockShape::kN, ThreadblockShape::kK},
         args.batch_count,
         sm_occupancy,
         device_sms,
         avail_sms);
+
+      quick_dp =
+        (block_mapping.sk_waves == 0) &&
+        (mode == GemmUniversalMode::kGemm) &&
+        !block_mapping.cohort_raster &&
+        !EpilogueOutputOp(output_op).is_source_needed();
+
     }
 
 
     /// Returns the workspace size (in bytes) needed for these parameters
     size_t get_workspace_size() const
     {
       return
@@ -861,15 +870,15 @@
         params.params_D,
         ptr_D,
         params.block_mapping.problem_size.mn(),
         thread_idx,
         threadblock_item_begin);
 
     // Execute the epilogue operator to update the destination tensor.
-    epilogue(
+    epilogue.unified(
         EpilogueOutputOp(params.output_op),
         iterator_D,
         accumulator_tile,
         iterator_C);
   }
 
 
@@ -948,22 +957,21 @@
     typename Mma::IteratorA iterator_A = init_iterator_A(tile_work, params.mode);
     typename Mma::IteratorB iterator_B = init_iterator_B(tile_work, params.mode);
 
     // Initialize accumulators
     AccumulatorTile accumulator_tile;
     accumulator_tile.clear();
 
-    // Initialize MMA abstraction
+    // Perform this tile's range of multiply-accumulate (MAC) iterations
     Mma mma(
       shared_storage.main_loop,
       thread_idx,
       warp_idx,
       lane_idx);
 
-    // Perform this tile's range of multiply-accumulate (MAC) iterations
     mma(tile_work.k_iters_remaining, accumulator_tile, iterator_A, iterator_B, accumulator_tile);
 
     if ((ThreadblockSwizzle::kReductionStrategy == ThreadblockSwizzle::kAtomic) ||
         (params.block_mapping.reduction_blocks == 0) ||
         (block_idx >= dp_start_block_idx))
     {
       //
@@ -1008,35 +1016,37 @@
 
 
   /// Executes one GEMM
   CUTLASS_DEVICE
   void gemm()
   {
     // Initialize block's iteration range
-    int tile_idx = 0;
-    int block_iter_begin = 0;
-    int block_iters_remaining = 0;
-
-    int block_idx = params.block_mapping.get_block_idx();
+    int tile_idx, block_iter_begin, block_iters_remaining;
 
     int sk_padding_start_block_idx =  params.block_mapping.sk_regions() * params.block_mapping.sk_blocks_per_region();
     int dp_start_block_idx = params.block_mapping.sk_waves * params.block_mapping.avail_sms;
     int reduce_start_block_idx = dp_start_block_idx + params.block_mapping.dp_blocks;
     int grid_padding_start_block_idx = reduce_start_block_idx + params.block_mapping.reduction_blocks;
 
-    // Initialize tile work descriptor
-    TileWorkDesc tile_work;
-
-    bool dp_block = (block_idx >= dp_start_block_idx) && (block_idx < reduce_start_block_idx);
-    bool sk_block = (block_idx < sk_padding_start_block_idx);
-    bool reduce_block = (block_idx >= reduce_start_block_idx) &&
-            (block_idx < grid_padding_start_block_idx) &&
-            (ThreadblockSwizzle::kReductionStrategy == ThreadblockSwizzle::kMixed);
+    int block_idx = params.block_mapping.get_block_idx();
+    if (block_idx < sk_padding_start_block_idx)
+    {
+      // This is a SK block
+      int block_iter_end;
+      params.block_mapping.get_iter_extents(block_idx, block_iter_begin, block_iter_end);
+      block_iters_remaining = block_iter_end - block_iter_begin;
 
-    if (dp_block)
+      tile_idx = params.block_mapping.get_sk_tile_idx(block_iter_end - 1);
+    }
+    else if (block_idx < dp_start_block_idx)
+    {
+      // This is a filler block
+      return;
+    }
+    else if (block_idx < reduce_start_block_idx)
     {
       // This is a DP block
       int dp_block_idx = block_idx - dp_start_block_idx;
       int first_dp_tile = (params.block_mapping.cohort_raster) ? 0 : params.block_mapping.sk_tiles;
 
       // Blocks in first DP wave get configured number of tiles
       tile_idx = first_dp_tile + dp_block_idx;
@@ -1044,86 +1054,140 @@
 
       // Blocks in subsequent DP waves get 1 tile
       if (dp_block_idx >= params.block_mapping.avail_sms) {
           tile_allottment = 1;
           tile_idx += (params.block_mapping.dp_first_wave_tiles - 1) * params.block_mapping.avail_sms;
       }
 
+      block_iter_begin = 0;
       block_iters_remaining = params.block_mapping.iters_per_tile() * tile_allottment;
-
-      init_dp_tile_work(tile_work, tile_idx);
-
-      // DP blocks exit if out of bounds or overlap an SK tile (only possible during cohort rasterization, where dp_first_wave_tiles must be 1)
-      if ((tile_idx < params.block_mapping.sk_tiles) ||
-          (tile_work.tiled_coord.m() >= params.block_mapping.tiled_shape().m()) ||
-          (tile_work.tiled_coord.n() >= params.block_mapping.tiled_shape().n()))
-      {
-        return;
-      }
     }
-    else if (sk_block)
-    {
-      // This is a SK block
-      int block_iter_end;
-      params.block_mapping.get_iter_extents(block_idx, block_iter_begin, block_iter_end);
-      block_iters_remaining = block_iter_end - block_iter_begin;
 
-      tile_idx = params.block_mapping.get_sk_tile_idx(block_iter_end - 1);
-      init_sk_tile_work(tile_work, tile_idx, block_iter_begin, block_iter_begin + block_iters_remaining);
+    else if ((ThreadblockSwizzle::kReductionStrategy == ThreadblockSwizzle::kMixed) &&
+             (block_idx < grid_padding_start_block_idx))
+    {
+      // This is a reduction threadblock
+      int reduce_block_idx = block_idx - reduce_start_block_idx;
+      separate_reduction(reduce_block_idx);
+      return;
     }
     else
     {
-      if (reduce_block)
-      {
-        // This is a reduction threadblock
-        int reduce_block_idx = block_idx - reduce_start_block_idx;
-        separate_reduction(reduce_block_idx);
-      }
-
+      // This is a filler block
       return;
     }
 
     // Iteration-processing loop body
     CUTLASS_PRAGMA_NO_UNROLL
     while (true)
     {
+      // Initialize tile work descriptor
+      TileWorkDesc tile_work;
+      if (block_idx >= dp_start_block_idx)
+      {
+        init_dp_tile_work(tile_work, tile_idx);
+
+        // DP blocks exit if out of bounds or overlap an SK tile (only possible during cohort rasterization, where dp_first_wave_tiles must be 1)
+        if ((tile_idx < params.block_mapping.sk_tiles) ||
+          (tile_work.tiled_coord.m() >= params.block_mapping.tiled_shape().m()) ||
+          (tile_work.tiled_coord.n() >= params.block_mapping.tiled_shape().n()))
+        {
+          break;
+        }
+      }
+      else
+      {
+        init_sk_tile_work(tile_work, tile_idx, block_iter_begin, block_iter_begin + block_iters_remaining);
+      }
+
       // Perform this block's share of work for this tile
-      process_tile(
-        tile_work,
-        block_idx,
-        dp_start_block_idx,
-        block_iter_begin);
+      process_tile(tile_work, block_idx, dp_start_block_idx, block_iter_begin);
 
+      // Update remaining work for this block
       block_iters_remaining -= tile_work.k_iters_remaining;
-
-      if (block_iters_remaining == 0)
-      {
+      if (block_iters_remaining == 0) {
+        // Done
         break;
       }
 
       // Continue to next tile
       __syncthreads();
 
       if (block_idx >= dp_start_block_idx)
       {
         // DP block consume their tiles at stride
         tile_idx += params.block_mapping.avail_sms;
-        init_dp_tile_work(tile_work, tile_idx);
       }
       else
       {
         // SK blocks consume their tiles in backwards order
         tile_idx--;
-        init_sk_tile_work(tile_work, tile_idx, block_iter_begin, block_iter_begin + block_iters_remaining);
       }
     }
 
   }
 
 
+  /// Executes one DP-only GEMM
+  CUTLASS_DEVICE
+  void gemm_dp()
+  {
+    int block_idx = blockIdx.x;
+    int tile_idx = block_idx;
+
+    TileWorkDesc tile_work;
+    tile_work.tile_idx = tile_idx;
+    tile_work.iter_begin = tile_idx * params.block_mapping.iters_per_tile();
+    tile_work.k_iters_remaining = params.block_mapping.iters_per_tile();
+    tile_work.k_begin = 0;
+    tile_work.k_end = params.block_mapping.problem_size.k();
+    tile_work.tiled_coord = params.block_mapping.get_tile_offset_row_major(tile_work.tile_idx);
+
+    // Initialize input iterators
+    typename Mma::IteratorA iterator_A = init_iterator_A(tile_work, params.mode);
+    typename Mma::IteratorB iterator_B = init_iterator_B(tile_work, params.mode);
+
+    // Initialize accumulators
+    AccumulatorTile accumulator_tile;
+    accumulator_tile.clear();
+
+    // Perform this tile's range of multiply-accumulate (MAC) iterations
+    Mma mma(
+      shared_storage.main_loop,
+      thread_idx,
+      warp_idx,
+      lane_idx);
+
+    mma(tile_work.k_iters_remaining, accumulator_tile, iterator_A, iterator_B, accumulator_tile);
+
+    ElementC *ptr_D = static_cast<ElementC *>(params.ptr_D);
+
+    // Location of this tile in item-coords
+    MatrixCoord threadblock_item_begin(
+      tile_work.tiled_coord.m() * Mma::Shape::kM,
+      tile_work.tiled_coord.n() * Mma::Shape::kN
+    );
+
+    // Tile iterator writing to destination tensor.
+    typename Epilogue::OutputTileIterator iterator_D(
+        params.params_D,
+        ptr_D,
+        params.block_mapping.problem_size.mn(),
+        thread_idx,
+        threadblock_item_begin);
+
+    // Execute the epilogue operator to update the destination tensor.
+    epilogue(
+        EpilogueOutputOp(params.output_op),
+        iterator_D,
+        accumulator_tile);
+  }
+
+
+
 public:
 
   //
   // Device-only API
   //
 
   // Factory invocation
@@ -1156,14 +1220,24 @@
   {}
 
 
   /// Executes one GEMM
   CUTLASS_DEVICE
   void operator()()
   {
+#if (__CUDACC_VER_MAJOR__ > 10)
+    if (params.quick_dp)
+    {
+      // Simple (low-bootstrap latency) GEMM code path for data-parallel only.  (kBatched and kArray
+      // modes will only be launched using a data-parallel configurations)
+      gemm_dp();
+      return;
+    }
+#endif
+
     // Generic SK code path
     gemm();
 
   }
 };
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/gemm_with_fused_epilogue.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/include/gemm/gemm_universal_with_visitor.h`

 * *Files 12% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -24,361 +24,360 @@
  * DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR
  * SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER
  * CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,
  * OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
  * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
  *
  **************************************************************************************************/
+
 /*! \file
-    \brief Gemm kernel with fused reduction operation.
+    \brief 
 */
 
 #pragma once
 
 #include "cutlass/cutlass.h"
 #include "cutlass/fast_math.h"
 #include "cutlass/gemm/gemm.h"
+#include "cutlass/gemm/kernel/params_universal_base.h"
 #include "cutlass/matrix_coord.h"
 #include "cutlass/complex.h"
 #include "cutlass/semaphore.h"
 
+#include "cutlass/layout/matrix.h"
+
 #include "cutlass/trace.h"
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
 namespace cutlass {
 namespace gemm {
 namespace kernel {
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
 template <
-  typename Mma_,                  ///! Threadblock-scoped matrix multiply-accumulate 
+  typename Mma_,                  ///! Threadblock-scoped matrix multiply-accumulate
   typename Epilogue_,             ///! Epilogue
   typename ThreadblockSwizzle_    ///! Threadblock swizzling function
 >
-struct GemmWithFusedEpilogue {
+struct GemmUniversalwithEpilogueVisitor {
 public:
 
   using Mma = Mma_;
   using Epilogue = Epilogue_;
-  using EpilogueOutputOp = typename Epilogue::OutputOp;
+  using EpilogueVisitor = typename Epilogue::Visitor;
   using ThreadblockSwizzle = ThreadblockSwizzle_;
 
   using ElementA = typename Mma::IteratorA::Element;
   using LayoutA = typename Mma::IteratorA::Layout;
   using ElementB = typename Mma::IteratorB::Element;
   using LayoutB = typename Mma::IteratorB::Layout;
-  using ElementC = typename Epilogue::OutputTileIterator::Element;
-  using LayoutC = typename Epilogue::OutputTileIterator::Layout;
+  using ElementC = typename EpilogueVisitor::ElementOutput;
+  using LayoutC = typename EpilogueVisitor::OutputTileIterator::Layout;
 
   static ComplexTransform const kTransformA = Mma::kTransformA;
   static ComplexTransform const kTransformB = Mma::kTransformB;
   using Operator = typename Mma::Operator;
 
   using OperatorClass = typename Mma::Operator::OperatorClass;
   using ThreadblockShape = typename Mma::Shape;
   using WarpShape = typename Mma::Operator::Shape;
   using InstructionShape = typename Mma::Policy::Operator::InstructionShape;
   using ArchTag = typename Mma::ArchTag;
 
   static int const kStages = Mma::kStages;
   static int const kAlignmentA = Mma::IteratorA::AccessType::kElements;
   static int const kAlignmentB = Mma::IteratorB::AccessType::kElements;
-  static int const kAlignmentC = Epilogue::OutputTileIterator::kElementsPerAccess;
+  static int const kAlignmentC = EpilogueVisitor::kElementsPerAccess;
 
   /// Warp count (concept: GemmShape)
   using WarpCount = typename Mma::WarpCount;
   static int const kThreadCount = 32 * WarpCount::kCount;
 
   /// Split-K preserves splits that are 128b aligned
   static int const kSplitKAlignment = const_max(
-    128 / sizeof_bits<ElementA>::value, 
+    128 / sizeof_bits<ElementA>::value,
     128 / sizeof_bits<ElementB>::value
   );
 
   //
   // Structures
   //
 
   /// Argument structure
-  struct Arguments {
+  struct Arguments : UniversalArgumentsBase {
 
     //
     // Data members
     //
 
-    GemmUniversalMode mode;
-    GemmCoord problem_size;
-    int batch_count;
-
-    typename EpilogueOutputOp::Params epilogue;
+    typename EpilogueVisitor::Arguments epilogue_visitor;
 
     void const * ptr_A;
     void const * ptr_B;
     void const * ptr_C;
     void * ptr_D;
 
-    void * ptr_Vector;
-    void * ptr_Tensor;
-
     int64_t batch_stride_A;
     int64_t batch_stride_B;
     int64_t batch_stride_C;
-    int64_t batch_stride_D;
-    int64_t batch_stride_Vector;
-    int64_t batch_stride_Tensor;
-
-    typename LayoutA::Stride::Index lda;
-    typename LayoutB::Stride::Index ldb;
-    typename LayoutC::Stride::Index ldc;
-    typename LayoutC::Stride::Index ldd;
-    typename LayoutC::Stride::Index ldr;
-    typename LayoutC::Stride::Index ldt;
+
+    typename LayoutA::Stride stride_a;
+    typename LayoutB::Stride stride_b;
+    typename LayoutC::Stride stride_c;
+    typename LayoutC::Stride stride_d;
+
+    typename LayoutA::Stride::LongIndex lda;
+    typename LayoutB::Stride::LongIndex ldb;
+    typename LayoutC::Stride::LongIndex ldc;
+    typename LayoutC::Stride::LongIndex ldd;
+
+    int const * ptr_gather_A_indices;
+    int const * ptr_gather_B_indices;
+    int const * ptr_scatter_D_indices;
 
     //
     // Methods
     //
     
     Arguments(): 
-      mode(GemmUniversalMode::kGemm), 
-      batch_count(1), 
-      ptr_A(nullptr), ptr_B(nullptr), ptr_C(nullptr), ptr_D(nullptr) { }
+      ptr_A(nullptr), ptr_B(nullptr), ptr_C(nullptr), ptr_D(nullptr),
+      ptr_gather_A_indices(nullptr),
+      ptr_gather_B_indices(nullptr),
+      ptr_scatter_D_indices(nullptr) {}
 
     /// constructs an arguments structure
     Arguments(
       GemmUniversalMode mode,
       GemmCoord problem_size,
       int batch_count,
-      typename EpilogueOutputOp::Params epilogue,
+      typename EpilogueVisitor::Arguments epilogue_visitor,
       void const * ptr_A,
       void const * ptr_B,
       void const * ptr_C,
       void * ptr_D,
-      void * ptr_Vector,
-      void * ptr_Tensor,
       int64_t batch_stride_A,
       int64_t batch_stride_B,
       int64_t batch_stride_C,
       int64_t batch_stride_D,
-      int64_t batch_stride_Vector,
-      int64_t batch_stride_Tensor,
-      typename LayoutA::Stride::Index lda,
-      typename LayoutB::Stride::Index ldb,
-      typename LayoutC::Stride::Index ldc,
-      typename LayoutC::Stride::Index ldd,
-      typename LayoutC::Stride::Index ldr,
-      typename LayoutC::Stride::Index ldt
+      typename LayoutA::Stride stride_a,
+      typename LayoutB::Stride stride_b,
+      typename LayoutC::Stride stride_c,
+      typename LayoutC::Stride stride_d,
+      int const *ptr_gather_A_indices = nullptr,
+      int const *ptr_gather_B_indices = nullptr,
+      int const *ptr_scatter_D_indices = nullptr
     ):
-      mode(mode), 
-      problem_size(problem_size), 
-      batch_count(batch_count),
-      epilogue(epilogue), 
+      UniversalArgumentsBase(mode, problem_size, batch_count, batch_stride_D),
+      epilogue_visitor(epilogue_visitor), 
       ptr_A(ptr_A), ptr_B(ptr_B), ptr_C(ptr_C), ptr_D(ptr_D), 
-      ptr_Vector(ptr_Vector), 
-      ptr_Tensor(ptr_Tensor),
-      batch_stride_A(batch_stride_A), 
-      batch_stride_B(batch_stride_B), 
-      batch_stride_C(batch_stride_C), 
-      batch_stride_D(batch_stride_D), 
-      batch_stride_Vector(batch_stride_Vector),
-      batch_stride_Tensor(batch_stride_Tensor),
-      lda(lda), ldb(ldb), ldc(ldc), ldd(ldd), ldr(ldr), ldt(ldt)
-    {
-      CUTLASS_TRACE_HOST("GemmWithFusedEpilogue::Arguments::Arguments() - problem_size: " << problem_size);
-      CUTLASS_TRACE_HOST("  ptr_Reduction: " << (void *)this->ptr_Reduction);
-      CUTLASS_TRACE_HOST("  ptr_Tensor: " << (void *)this->ptr_Tensor);
-      CUTLASS_TRACE_HOST("  ldr: " << this->ldr);
-      CUTLASS_TRACE_HOST("  ldt: " << this->ldt);
-    }
+      batch_stride_A(batch_stride_A), batch_stride_B(batch_stride_B), batch_stride_C(batch_stride_C),
+      stride_a(stride_a), stride_b(stride_b), stride_c(stride_c), stride_d(stride_d),
+      ptr_gather_A_indices(ptr_gather_A_indices), ptr_gather_B_indices(ptr_gather_B_indices),
+      ptr_scatter_D_indices(ptr_scatter_D_indices) {
+      lda = 0;
+      ldb = 0;
+      ldc = 0;
+      ldd = 0;
+      CUTLASS_TRACE_HOST("GemmUniversal::Arguments::Arguments() - problem_size: " << problem_size);
+      }
+
+    /// constructs an arguments structure
+    Arguments(
+      GemmUniversalMode mode,
+      GemmCoord problem_size,
+      int batch_count,
+      typename EpilogueVisitor::Arguments epilogue_visitor,
+      void const * ptr_A,
+      void const * ptr_B,
+      void const * ptr_C,
+      void * ptr_D,
+      int64_t batch_stride_A,
+      int64_t batch_stride_B,
+      int64_t batch_stride_C,
+      int64_t batch_stride_D,
+      typename LayoutA::Stride::LongIndex lda,
+      typename LayoutB::Stride::LongIndex ldb,
+      typename LayoutC::Stride::LongIndex ldc,
+      typename LayoutC::Stride::LongIndex ldd,
+      int const *ptr_gather_A_indices = nullptr,
+      int const *ptr_gather_B_indices = nullptr,
+      int const *ptr_scatter_D_indices = nullptr
+    ):
+      UniversalArgumentsBase(mode, problem_size, batch_count, batch_stride_D),
+      epilogue_visitor(epilogue_visitor), 
+      ptr_A(ptr_A), ptr_B(ptr_B), ptr_C(ptr_C), ptr_D(ptr_D), 
+      batch_stride_A(batch_stride_A), batch_stride_B(batch_stride_B), batch_stride_C(batch_stride_C),
+      lda(lda), ldb(ldb), ldc(ldc), ldd(ldd),
+      ptr_gather_A_indices(ptr_gather_A_indices), ptr_gather_B_indices(ptr_gather_B_indices),
+      ptr_scatter_D_indices(ptr_scatter_D_indices) {
+      stride_a = make_Coord(lda);
+      stride_b = make_Coord(ldb);
+      stride_c = make_Coord(ldc);
+      stride_d = make_Coord(ldd);
+      CUTLASS_TRACE_HOST("GemmUniversal::Arguments::Arguments() - problem_size: " << problem_size);
+      }
 
     /// Returns arguments for the transposed problem
     Arguments transposed_problem() const {
       Arguments args(*this);
       
       std::swap(args.problem_size.m(), args.problem_size.n());
       std::swap(args.ptr_A, args.ptr_B);
       std::swap(args.lda, args.ldb);
+      std::swap(args.stride_a, args.stride_b);
       std::swap(args.batch_stride_A, args.batch_stride_B);
+      std::swap(args.ptr_gather_A_indices, args.ptr_gather_B_indices);
 
       return args;
     }
   };
 
   //
   // Structure for precomputing values in host memory and passing to kernels
   //
 
   /// Parameters structure
-  struct Params {
-
-    cutlass::gemm::GemmCoord problem_size;
-    cutlass::gemm::GemmCoord grid_tiled_shape;
-    int swizzle_log_tile;
+  struct Params : UniversalParamsBase<
+    ThreadblockSwizzle,
+    ThreadblockShape,
+    ElementA,
+    ElementB,
+    ElementC> {
+
+    using ParamsBase = UniversalParamsBase<
+      ThreadblockSwizzle,
+      ThreadblockShape,
+      ElementA,
+      ElementB,
+      ElementC>;
 
     typename Mma::IteratorA::Params params_A;
     typename Mma::IteratorB::Params params_B;
-    typename Epilogue::OutputTileIterator::Params params_C;
-    typename Epilogue::OutputTileIterator::Params params_D;
-    typename Epilogue::TensorTileIterator::Params params_Tensor;
+    typename EpilogueVisitor::OutputTileIterator::Params params_C;
+    typename EpilogueVisitor::OutputTileIterator::Params params_D;
     
-    typename EpilogueOutputOp::Params output_op;
-
-
-    GemmUniversalMode mode;
-    int batch_count;
-    int gemm_k_size;
+    typename EpilogueVisitor::Params epilogue_visitor;
 
     void * ptr_A;
     void * ptr_B;
     void * ptr_C;
     void * ptr_D;
-    
-    void * ptr_Vector;
-    typename LayoutC::Stride::Index ldr;
-
-    void * ptr_Tensor;
 
     int64_t batch_stride_A;
     int64_t batch_stride_B;
     int64_t batch_stride_C;
-    int64_t batch_stride_D;
-    int64_t batch_stride_Vector;
-    int64_t batch_stride_Tensor;
+
+    int * ptr_gather_A_indices;
+    int * ptr_gather_B_indices;
+    int * ptr_scatter_D_indices;
 
     int *semaphore;
 
     //
     // Methods
     //
 
-    CUTLASS_HOST_DEVICE
-    Params():
-      swizzle_log_tile(0),
-      params_A(0),
-      params_B(0),
-      params_C(0),
-      params_D(0),
-      batch_count(0),
-      gemm_k_size(0),
-      mode(cutlass::gemm::GemmUniversalMode::kGemm),
-      ptr_A(nullptr),
-      ptr_B(nullptr),
-      ptr_C(nullptr),
-      ptr_D(nullptr),
-      ptr_Vector(nullptr),
-      ldr(0),
-      ptr_Tensor(nullptr),
-      batch_stride_A(0),
-      batch_stride_B(0),
-      batch_stride_C(0),
-      batch_stride_D(0),
-      batch_stride_Vector(0),
-      batch_stride_Tensor(0),
-      semaphore(nullptr) { }
+    /// Default constructor
+    Params() = default;
 
     CUTLASS_HOST_DEVICE
     Params(
       Arguments const &args,
-      cutlass::gemm::GemmCoord const & grid_tiled_shape,
-      int gemm_k_size,
-      void *workspace = nullptr
+      int device_sms,
+      int sm_occupancy
     ):
-      problem_size(args.problem_size),
-      grid_tiled_shape(grid_tiled_shape),
-      swizzle_log_tile(ThreadblockSwizzle().get_log_tile(grid_tiled_shape)),
-      params_A(args.lda),
-      params_B(args.ldb),
-      params_C(args.ldc),
-      params_D(args.ldd),
-      params_Tensor(args.ldt),
-      output_op(args.epilogue),
-      mode(args.mode),
-      batch_count(args.batch_count),
-      gemm_k_size(gemm_k_size),
+      ParamsBase(args, device_sms, sm_occupancy),
+      params_A(args.lda ? make_Coord_with_padding<LayoutA::kStrideRank>(args.lda) : args.stride_a),
+      params_B(args.ldb ? make_Coord_with_padding<LayoutB::kStrideRank>(args.ldb) : args.stride_b),
+      params_C(args.ldc ? make_Coord_with_padding<LayoutC::kStrideRank>(args.ldc) : args.stride_c),
+      params_D(args.ldd ? make_Coord_with_padding<LayoutC::kStrideRank>(args.ldd) : args.stride_d),
+      epilogue_visitor(args.epilogue_visitor),
       ptr_A(const_cast<void *>(args.ptr_A)),
       ptr_B(const_cast<void *>(args.ptr_B)),
       ptr_C(const_cast<void *>(args.ptr_C)),
       ptr_D(args.ptr_D),
-      ptr_Vector(args.ptr_Vector), 
-      ldr(args.ldr),
-      ptr_Tensor(args.ptr_Tensor),
-
       batch_stride_A(args.batch_stride_A),
       batch_stride_B(args.batch_stride_B),
       batch_stride_C(args.batch_stride_C),
-      batch_stride_D(args.batch_stride_D),
-      batch_stride_Vector(args.batch_stride_Vector),
-      batch_stride_Tensor(args.batch_stride_Tensor),
-
-      semaphore(static_cast<int *>(workspace)) {
-
-      CUTLASS_TRACE_HOST("GemmWithFusedEpilogue::Params::Params() - problem_size: " << problem_size);
-      CUTLASS_TRACE_HOST("  ptr_Reduction: " << (void *)this->ptr_Reduction);
-      CUTLASS_TRACE_HOST("  ptr_Tensor: " << (void *)this->ptr_Tensor);
-      CUTLASS_TRACE_HOST("  ldr: " << this->ldr);
-      CUTLASS_TRACE_HOST("  ldt: " << args.ldt);
+      ptr_gather_A_indices(const_cast<int *>(args.ptr_gather_A_indices)),
+      ptr_gather_B_indices(const_cast<int *>(args.ptr_gather_B_indices)),
+      ptr_scatter_D_indices(const_cast<int *>(args.ptr_scatter_D_indices)) {
+
     }
 
     CUTLASS_HOST_DEVICE
     void update(
       Arguments const &args,
       void *workspace = nullptr) {
 
       ptr_A = const_cast<void *>(args.ptr_A);
       ptr_B = const_cast<void *>(args.ptr_B);
       ptr_C = const_cast<void *>(args.ptr_C);
       ptr_D = args.ptr_D;
 
-      ptr_Vector = args.ptr_Vector;
-      ldr = args.ldr;
-      ptr_Tensor = args.ptr_Tensor;
+      ptr_gather_A_indices = const_cast<int *>(args.ptr_gather_A_indices);
+      ptr_gather_B_indices = const_cast<int *>(args.ptr_gather_B_indices);
+      ptr_scatter_D_indices = const_cast<int *>(args.ptr_scatter_D_indices);
 
       batch_stride_A = args.batch_stride_A;
       batch_stride_B = args.batch_stride_B;
       batch_stride_C = args.batch_stride_C;
-      batch_stride_D = args.batch_stride_D;
-      batch_stride_Vector = args.batch_stride_Vector;
-      batch_stride_Tensor = args.batch_stride_Tensor;
-
-      output_op = args.epilogue;
 
+      epilogue_visitor = args.epilogue_visitor;
+      
       semaphore = static_cast<int *>(workspace);
-
-      CUTLASS_TRACE_HOST("GemmWithFusedEpilogue::Params::update()");
-      CUTLASS_TRACE_HOST("  ptr_Reduction: " << (void *)this->ptr_Reduction);
-      CUTLASS_TRACE_HOST("  ptr_Tensor: " << (void *)this->ptr_Tensor);
-      CUTLASS_TRACE_HOST("  ldr: " << this->ldr);
+      CUTLASS_TRACE_HOST("GemmUniversal::Params::update()");
     }
   };
 
   /// Shared memory storage structure
   union SharedStorage {
     typename Mma::SharedStorage main_loop;
     typename Epilogue::SharedStorage epilogue;
+    typename EpilogueVisitor::SharedStorage visitor;
   };
 
 public:
 
   //
   // Methods
   //
 
   CUTLASS_DEVICE
-  GemmWithFusedEpilogue() { } 
+  GemmUniversalwithEpilogueVisitor() { } 
 
   /// Determines whether kernel satisfies alignment
   static Status can_implement(
     cutlass::gemm::GemmCoord const & problem_size) {
 
-    CUTLASS_TRACE_HOST("GemmWithFusedEpilogue::can_implement()");
+    CUTLASS_TRACE_HOST("GemmUniversalwithEpilogueVisitor::can_implement()");
 
-    static int const kAlignmentA = Mma::IteratorA::AccessType::kElements;
-    static int const kAlignmentB = Mma::IteratorB::AccessType::kElements;
-    static int const kAlignmentC = Epilogue::OutputTileIterator::kElementsPerAccess;
+    static int const kAlignmentA = (platform::is_same<LayoutA,
+                                                      layout::ColumnMajorInterleaved<32>>::value)
+                                   ? 32
+                                   : (platform::is_same<LayoutA,
+                                                        layout::ColumnMajorInterleaved<64>>::value)
+                                     ? 64
+                                     : Mma::IteratorA::AccessType::kElements;
+    static int const kAlignmentB = (platform::is_same<LayoutB,
+                                                      layout::RowMajorInterleaved<32>>::value)
+                                   ? 32
+                                   : (platform::is_same<LayoutB,
+                                                        layout::RowMajorInterleaved<64>>::value)
+                                     ? 64
+                                     : Mma::IteratorB::AccessType::kElements;
+    static int const kAlignmentC = (platform::is_same<LayoutC,
+                                                      layout::ColumnMajorInterleaved<32>>::value)
+                                   ? 32
+                                   : (platform::is_same<LayoutC,
+                                                        layout::ColumnMajorInterleaved<64>>::value)
+                                     ? 64
+                                     : Epilogue::OutputTileIterator::kElementsPerAccess;
 
     bool isAMisaligned = false;
     bool isBMisaligned = false;
     bool isCMisaligned = false;
 
     if (platform::is_same<LayoutA, layout::RowMajor>::value) {
       isAMisaligned = problem_size.k() % kAlignmentA;
@@ -427,22 +426,14 @@
     return Status::kSuccess;
   }
 
   static Status can_implement(Arguments const &args) {
     return can_implement(args.problem_size);
   }
 
-  static size_t get_extra_workspace_size(Arguments const &args,
-                                         cutlass::gemm::GemmCoord const &grid_tiled_shape) {
-
-    return 0;
-  }
-
-  #define SPLIT_K_ENABLED 1
-
   /// Executes one GEMM
   CUTLASS_DEVICE
   void operator()(Params const &params, SharedStorage &shared_storage) {
 
     // Compute threadblock location
     ThreadblockSwizzle threadblock_swizzle;
 
@@ -454,19 +445,17 @@
 
       return;
     }
 
     int offset_k = 0;
     int problem_size_k = params.problem_size.k();
 
-    ElementA *ptr_A = static_cast<ElementA *>(params.ptr_A); 
+    ElementA *ptr_A = static_cast<ElementA *>(params.ptr_A);
     ElementB *ptr_B = static_cast<ElementB *>(params.ptr_B);
 
-
-    #if SPLIT_K_ENABLED
     //
     // Fetch pointers based on mode.
     //
     if (params.mode == GemmUniversalMode::kGemm || 
       params.mode == GemmUniversalMode::kGemmSplitKParallel) {
 
       if (threadblock_tile_offset.k() + 1 < params.grid_tiled_shape.k()) {
@@ -480,15 +469,16 @@
       ptr_A += threadblock_tile_offset.k() * params.batch_stride_A;
       ptr_B += threadblock_tile_offset.k() * params.batch_stride_B;
     }
     else if (params.mode == GemmUniversalMode::kArray) {
       ptr_A = static_cast<ElementA * const *>(params.ptr_A)[threadblock_tile_offset.k()];
       ptr_B = static_cast<ElementB * const *>(params.ptr_B)[threadblock_tile_offset.k()];
     }
-    #endif
+
+    __syncthreads();
 
     // Compute initial location in logical coordinates
     cutlass::MatrixCoord tb_offset_A{
       threadblock_tile_offset.m() * Mma::Shape::kM,
       offset_k,
     };
 
@@ -502,22 +492,24 @@
 
     // Construct iterators to A and B operands
     typename Mma::IteratorA iterator_A(
       params.params_A,
       ptr_A,
       {params.problem_size.m(), problem_size_k},
       thread_idx,
-      tb_offset_A);
+      tb_offset_A,
+      params.ptr_gather_A_indices);
 
     typename Mma::IteratorB iterator_B(
       params.params_B,
       ptr_B,
       {problem_size_k, params.problem_size.n()},
       thread_idx,
-      tb_offset_B);
+      tb_offset_B,
+      params.ptr_gather_B_indices);
 
     // Broadcast the warp_id computed by lane 0 to ensure dependent code
     // is compiled as warp-uniform.
     int warp_idx = __shfl_sync(0xffffffff, threadIdx.x / 32, 0);
 
     int lane_idx = threadIdx.x % 32;
 
@@ -543,15 +535,15 @@
       iterator_B, 
       accumulators);
 
     //
     // Epilogue
     //
 
-    EpilogueOutputOp output_op(params.output_op);
+    // EpilogueOutputOp output_op(params.output_op);
 
     //
     // Masked tile iterators constructed from members
     //
 
     threadblock_tile_offset = threadblock_swizzle.get_tile_offset(params.swizzle_log_tile);
 
@@ -561,219 +553,95 @@
       threadblock_tile_offset.n() * Mma::Shape::kN
     );
 
     int block_idx = threadblock_tile_offset.m() + threadblock_tile_offset.n() * params.grid_tiled_shape.m();
 
     ElementC *ptr_C = static_cast<ElementC *>(params.ptr_C); 
     ElementC *ptr_D = static_cast<ElementC *>(params.ptr_D);
-    typename Epilogue::ElementTensor *ptr_Tensor = static_cast<typename Epilogue::ElementTensor *>(params.ptr_Tensor);
-
-    // Define the reduction output pointer and move to the appropriate place
-    typename Epilogue::ElementVector *ptr_Vector = 
-      static_cast<typename Epilogue::ElementVector *>(params.ptr_Vector);
 
     //
     // Fetch pointers based on mode.
     //
     
-    //
-    // Special path when split-K not enabled.
-    // 
-
-    if (params.mode == GemmUniversalMode::kGemm && params.grid_tiled_shape.k() == 1) {
-
-      // Tile iterator loading from source tensor.
-      typename Epilogue::OutputTileIterator iterator_C(
-        params.params_C,
-        ptr_C,
-        params.problem_size.mn(),
-        thread_idx,
-        threadblock_offset
-      );
-
-      // Tile iterator writing to destination tensor.
-      typename Epilogue::OutputTileIterator iterator_D(
-        params.params_D,
-        ptr_D,
-        params.problem_size.mn(),
-        thread_idx,
-        threadblock_offset
-      );
-
-      // Additional tensor to load from
-      typename Epilogue::TensorTileIterator tensor_iterator(
-          params.params_Tensor,
-          // Only the final block outputs Tensor
-          ptr_Tensor,
-          params.problem_size.mn(),
-          thread_idx,
-          threadblock_offset);
-
-      // Construct the epilogue
-      Epilogue epilogue(
-        shared_storage.epilogue, 
-        thread_idx, 
-        warp_idx, 
-        lane_idx);
-
-      // Move to appropriate location for this output tile
-      if (ptr_Vector) {
-        ptr_Vector += threadblock_offset.column() + threadblock_tile_offset.m() * params.ldr;
-      }
-
-      // Execute the epilogue operator to update the destination tensor.
-      epilogue(output_op,
-               ptr_Vector,
-               iterator_D,
-               accumulators,
-               iterator_C,
-               tensor_iterator,
-               params.problem_size.mn(),
-               threadblock_offset);
-
-      return;
-    }
-
-    //
-    // Slower path when split-K or batching is needed
-    //
-
-      
-    #if SPLIT_K_ENABLED
     // Construct the semaphore.
     Semaphore semaphore(params.semaphore + block_idx, thread_idx);
 
-    if (params.mode == GemmUniversalMode::kGemm) {
+    // if (params.mode == GemmUniversalMode::kGemm) {
 
-      // If performing a reduction via split-K, fetch the initial synchronization
-      if (params.grid_tiled_shape.k() > 1) {
+    //   // TODO: fix this order
+    //   // If performing a reduction via split-K, fetch the initial synchronization
+    //   if (params.grid_tiled_shape.k() > 1) {
         
-        // Fetch the synchronization lock initially but do not block.
-        semaphore.fetch();
-
-        // Indicate which position in a serial reduction the output operator is currently updating
-        output_op.set_k_partition(threadblock_tile_offset.k(), params.grid_tiled_shape.k());
-      }
-    }
-    else if (params.mode == GemmUniversalMode::kGemmSplitKParallel) {
-      ptr_D += threadblock_tile_offset.k() * params.batch_stride_D;
-    }
-    else if (params.mode == GemmUniversalMode::kBatched) {
-      ptr_C += threadblock_tile_offset.k() * params.batch_stride_C;
-      ptr_D += threadblock_tile_offset.k() * params.batch_stride_D;
-      if (ptr_Tensor) {
-        ptr_Tensor += threadblock_tile_offset.k() * params.batch_stride_Tensor;
-      }
-      if (ptr_Vector) {
-        ptr_Vector += threadblock_tile_offset.k() * params.batch_stride_Vector;
-      }
-    }
-    else if (params.mode == GemmUniversalMode::kArray) {
-      ptr_C = static_cast<ElementC * const *>(params.ptr_C)[threadblock_tile_offset.k()];
-      ptr_D = static_cast<ElementC * const *>(params.ptr_D)[threadblock_tile_offset.k()];
-      if (ptr_Tensor) {
-        ptr_Tensor = static_cast<typename Epilogue::ElementTensor * const *>(params.ptr_Tensor)[threadblock_tile_offset.k()];
-      }
-      if (ptr_Vector) {
-        ptr_Vector = static_cast<typename Epilogue::ElementVector * const *>(params.ptr_Vector)[threadblock_tile_offset.k()];
-      }
-    }
-    #endif
+    //     // Fetch the synchronization lock initially but do not block.
+    //     semaphore.fetch();
 
+    //     // Indicate which position in a serial reduction the output operator is currently updating
+    //     output_op.set_k_partition(threadblock_tile_offset.k(), params.grid_tiled_shape.k());
+    //   }
+    // }
+    
     // Tile iterator loading from source tensor.
-    typename Epilogue::OutputTileIterator iterator_C(
-      params.params_C,
-      ptr_C,
-      params.problem_size.mn(),
-      thread_idx,
-      threadblock_offset
-    );
 
-    // Tile iterator writing to destination tensor.
-    typename Epilogue::OutputTileIterator iterator_D(
-      params.params_D,
-      ptr_D,
-      params.problem_size.mn(),
-      thread_idx,
-      threadblock_offset
+    EpilogueVisitor epilogue_visitor(
+        params.epilogue_visitor,
+        shared_storage.visitor,
+        threadblock_offset,
+        threadblock_tile_offset,
+        thread_idx,
+        params.problem_size.mn()
     );
 
-    // Additional tensor to load from
-    typename Epilogue::TensorTileIterator tensor_iterator(
-        params.params_Tensor,
-        // Only the final block outputs Tensor
-        ((params.mode == GemmUniversalMode::kGemm && params.grid_tiled_shape.k() > 1) &&
-         (params.grid_tiled_shape.k() != threadblock_tile_offset.k() + 1))
-            ? nullptr
-            : ptr_Tensor,
-        params.problem_size.mn(),
-        thread_idx,
-        threadblock_offset);
+    // if (params.mode == GemmUniversalMode::kGemmSplitKParallel) {
+    //   ptr_D += threadblock_tile_offset.k() * params.batch_stride_D;
+    // }
+    if (params.mode == GemmUniversalMode::kBatched || params.mode == GemmUniversalMode::kArray) {
+      epilogue_visitor.set_batch_index(threadblock_tile_offset.k());
+    }
 
-    // Construct the epilogue
     Epilogue epilogue(
-      shared_storage.epilogue, 
-      thread_idx, 
-      warp_idx, 
+      shared_storage.epilogue,
+      thread_idx,
+      warp_idx,
       lane_idx);
 
-    #if SPLIT_K_ENABLED
     // Wait on the semaphore - this latency may have been covered by iterator construction
-    if ((params.mode == GemmUniversalMode::kGemm) && params.grid_tiled_shape.k() > 1) {
+    if (params.mode == GemmUniversalMode::kGemm && params.grid_tiled_shape.k() > 1) {
         
       // For subsequent threadblocks, the source matrix is held in the 'D' tensor.
-      if (threadblock_tile_offset.k()) {
-        iterator_C = iterator_D;
-      }
+      // TODO: ???
+      // if (threadblock_tile_offset.k()) {
+      //   iterator_C = iterator_D;
+      // }
 
       semaphore.wait(threadblock_tile_offset.k());
-
     }
-    #endif
 
-    // Move to appropriate location for this output tile
-    if (ptr_Vector) {
-      ptr_Vector += threadblock_offset.column() + threadblock_tile_offset.m() * params.ldr;
-    }
 
     // Execute the epilogue operator to update the destination tensor.
-    epilogue(output_op,
-             // Only the final block uses Vector
-             ((params.mode == GemmUniversalMode::kGemm && params.grid_tiled_shape.k() > 1) &&
-              (params.grid_tiled_shape.k() != threadblock_tile_offset.k() + 1))
-                 ? nullptr
-                 : ptr_Vector,
-             iterator_D,
-             accumulators,
-             iterator_C,
-             tensor_iterator,
-             params.problem_size.mn(),
-             threadblock_offset);
-
+    epilogue(epilogue_visitor, accumulators); 
+    
     //
     // Release the semaphore
     //
 
-    #if SPLIT_K_ENABLED
-    if ((params.mode == GemmUniversalMode::kGemm)  && params.grid_tiled_shape.k() > 1) { 
+    if (params.mode == GemmUniversalMode::kGemm && params.grid_tiled_shape.k() > 1) { 
 
       int lock = 0;
       if (params.grid_tiled_shape.k() == threadblock_tile_offset.k() + 1) {
 
         // The final threadblock resets the semaphore for subsequent grids.
         lock = 0;
       }
       else {
         // Otherwise, the semaphore is incremented
         lock = threadblock_tile_offset.k() + 1;
       }
       
       semaphore.release(lock);
     }
-    #endif
   }
 };
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
 } // namespace kernel
 } // namespace gemm
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/gemm_with_k_reduction.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/gemm_with_k_reduction.h`

 * *Files 4% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -37,14 +37,16 @@
 
 #include "cutlass/cutlass.h"
 #include "cutlass/fast_math.h"
 #include "cutlass/gemm/gemm.h"
 #include "cutlass/matrix_coord.h"
 #include "cutlass/complex.h"
 #include "cutlass/semaphore.h"
+#include "cutlass/layout/pitch_linear.h"
+#include "cutlass/gemm/kernel/params_universal_base.h"
 
 #include "cutlass/trace.h"
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
 namespace cutlass {
 namespace gemm {
@@ -100,52 +102,50 @@
   static int const kReduceKForA = Mma::kReduceKForA;
 
   //
   // Structures
   //
 
   /// Argument structure
-  struct Arguments {
-
+  struct Arguments : UniversalArgumentsBase
+  {
     //
     // Data members
     //
 
-    GemmUniversalMode mode;
-    GemmCoord problem_size;
-    int batch_count;
-
     typename EpilogueOutputOp::Params epilogue;
 
     void const * ptr_A;
     void const * ptr_B;
     void const * ptr_C;
     void * ptr_D;
     void * ptr_gemm_k_reduction;
 
     int64_t batch_stride_A;
     int64_t batch_stride_B;
     int64_t batch_stride_C;
-    int64_t batch_stride_D;
     int64_t batch_stride_gemm_k_reduction;
 
     typename LayoutA::Stride::Index lda;
     typename LayoutB::Stride::Index ldb;
     typename LayoutC::Stride::Index ldc;
     typename LayoutC::Stride::Index ldd;
     typename LayoutGemmKReduction::Stride::Index ld_gemm_k_reduction;
 
     //
     // Methods
     //
-    
-    Arguments(): 
-      mode(GemmUniversalMode::kGemm), 
-      batch_count(1), 
-      ptr_A(nullptr), ptr_B(nullptr), ptr_C(nullptr), ptr_D(nullptr), ptr_gemm_k_reduction(nullptr) { }
+
+    Arguments() :
+      ptr_A(nullptr),
+      ptr_B(nullptr),
+      ptr_C(nullptr),
+      ptr_D(nullptr),
+      ptr_gemm_k_reduction(nullptr)
+    {}
 
     /// constructs an arguments structure
     Arguments(
       GemmUniversalMode mode,
       GemmCoord problem_size,
       int batch_count,
       typename EpilogueOutputOp::Params epilogue,
@@ -159,181 +159,175 @@
       int64_t batch_stride_C,
       int64_t batch_stride_D,
       int64_t batch_stride_gemm_k_reduction,
       typename LayoutA::Stride::Index lda,
       typename LayoutB::Stride::Index ldb,
       typename LayoutC::Stride::Index ldc,
       typename LayoutC::Stride::Index ldd,
-      typename LayoutGemmKReduction::Stride::Index ld_gemm_k_reduction
-    ):
-      mode(mode), 
-      problem_size(problem_size), 
-      batch_count(batch_count),
-      epilogue(epilogue), 
-      ptr_A(ptr_A), ptr_B(ptr_B), ptr_C(ptr_C), ptr_D(ptr_D), ptr_gemm_k_reduction(ptr_gemm_k_reduction), 
-      batch_stride_A(batch_stride_A), batch_stride_B(batch_stride_B), batch_stride_C(batch_stride_C), batch_stride_D(batch_stride_D), batch_stride_gemm_k_reduction(batch_stride_gemm_k_reduction),
-      lda(lda), ldb(ldb), ldc(ldc), ldd(ldd), ld_gemm_k_reduction(ld_gemm_k_reduction) {
-
+      typename LayoutGemmKReduction::Stride::Index ld_gemm_k_reduction)
+    :
+      UniversalArgumentsBase(mode, problem_size, batch_count, batch_stride_D),
+      epilogue(epilogue),
+      ptr_A(ptr_A), ptr_B(ptr_B), ptr_C(ptr_C), ptr_D(ptr_D), ptr_gemm_k_reduction(ptr_gemm_k_reduction),
+      batch_stride_A(batch_stride_A), batch_stride_B(batch_stride_B), batch_stride_C(batch_stride_C), batch_stride_gemm_k_reduction(batch_stride_gemm_k_reduction),
+      lda(lda), ldb(ldb), ldc(ldc), ldd(ldd), ld_gemm_k_reduction(ld_gemm_k_reduction)
+    {
       CUTLASS_TRACE_HOST("GemmUniversal::Arguments::Arguments() - problem_size: " << problem_size);
-      }
+    }
 
     /// Returns arguments for the transposed problem
     Arguments transposed_problem() const {
       Arguments args(*this);
-      
+
       std::swap(args.problem_size.m(), args.problem_size.n());
       std::swap(args.ptr_A, args.ptr_B);
       std::swap(args.lda, args.ldb);
       std::swap(args.batch_stride_A, args.batch_stride_B);
 
       return args;
     }
   };
 
+
   //
   // Structure for precomputing values in host memory and passing to kernels
   //
 
   /// Parameters structure
-  struct Params {
+  struct Params : UniversalParamsBase<
+    ThreadblockSwizzle,
+    ThreadblockShape,
+    ElementA,
+    ElementB,
+    ElementC>
+  {
+    using ParamsBase = UniversalParamsBase<
+      ThreadblockSwizzle,
+      ThreadblockShape,
+      ElementA,
+      ElementB,
+      ElementC>;
 
-    cutlass::gemm::GemmCoord problem_size;
-    cutlass::gemm::GemmCoord grid_tiled_shape;
-    int swizzle_log_tile;
+    //
+    // Data members
+    //
     
     typename Mma::IteratorA::Params params_A;
     typename Mma::IteratorB::Params params_B;
     typename Epilogue::OutputTileIterator::Params params_C;
     typename Epilogue::OutputTileIterator::Params params_D;
     
     typename EpilogueOutputOp::Params output_op;
 
-    GemmUniversalMode mode;
-    int batch_count;
-    int gemm_k_size;
-
     void * ptr_A;
     void * ptr_B;
     void * ptr_C;
     void * ptr_D;
     void * ptr_gemm_k_reduction;
 
     int64_t batch_stride_A;
     int64_t batch_stride_B;
     int64_t batch_stride_C;
-    int64_t batch_stride_D;
     int64_t batch_stride_gemm_k_reduction;
 
-    int *semaphore;
-
     //
-    // Methods
+    // Host dispatch API
     //
 
-    CUTLASS_HOST_DEVICE
-    Params():
-      swizzle_log_tile(0),
-      params_A(0),
-      params_B(0),
-      params_C(0),
-      params_D(0),
-      batch_count(0),
-      gemm_k_size(0),
-      mode(cutlass::gemm::GemmUniversalMode::kGemm),
-      ptr_A(nullptr),
-      ptr_B(nullptr),
-      ptr_C(nullptr),
-      ptr_D(nullptr),
-      ptr_gemm_k_reduction(nullptr),
-      batch_stride_A(0),
-      batch_stride_B(0),
-      batch_stride_C(0),
-      batch_stride_D(0),
-      batch_stride_gemm_k_reduction(0),
-      semaphore(nullptr) { }
+    /// Default constructor
+    Params() = default;
 
-    CUTLASS_HOST_DEVICE
+    /// Constructor
     Params(
-      Arguments const &args,
-      cutlass::gemm::GemmCoord const & grid_tiled_shape,
-      int gemm_k_size,
-      void *workspace = nullptr
-    ):
-      problem_size(args.problem_size),
-      grid_tiled_shape(grid_tiled_shape),
-      swizzle_log_tile(ThreadblockSwizzle().get_log_tile(grid_tiled_shape)),
+      Arguments const &args,  /// GEMM application arguments
+      int device_sms,         /// Number of SMs on the device
+      int sm_occupancy)       /// Kernel SM occupancy (in thread blocks)
+    :
+      ParamsBase(args, device_sms, sm_occupancy),
       params_A(args.lda),
       params_B(args.ldb),
       params_C(args.ldc),
       params_D(args.ldd),
       output_op(args.epilogue),
-      mode(args.mode),
-      batch_count(args.batch_count),
-      gemm_k_size(gemm_k_size),
       ptr_A(const_cast<void *>(args.ptr_A)),
       ptr_B(const_cast<void *>(args.ptr_B)),
       ptr_C(const_cast<void *>(args.ptr_C)),
       batch_stride_A(args.batch_stride_A),
       batch_stride_B(args.batch_stride_B),
       batch_stride_C(args.batch_stride_C),
-      batch_stride_D(args.batch_stride_D),
       batch_stride_gemm_k_reduction(args.batch_stride_gemm_k_reduction),
-      semaphore(static_cast<int *>(workspace)) {
-
-      CUTLASS_TRACE_HOST("GemmUniversal::Params::Params() - problem_size: " << problem_size);
+      ptr_D(args.ptr_D),
+      ptr_gemm_k_reduction(args.ptr_gemm_k_reduction)
+    {}
+
+    /// Assign and initialize the specified workspace buffer.  Assumes
+    /// the memory allocated to workspace is at least as large as get_workspace_size().
+    Status init_workspace(
+      void *workspace,
+      cudaStream_t stream = nullptr)
+    {
+      CUTLASS_TRACE_HOST("GemmUniversal::Params::Params() - problem_size: " << this->problem_size);
 
-      if (args.mode == GemmUniversalMode::kGemmSplitKParallel) {
+      if (this->mode == GemmUniversalMode::kGemmSplitKParallel) {
         ptr_D = workspace;
         ptr_gemm_k_reduction = static_cast<uint8_t *>(workspace)
-                 + sizeof(ElementC) * size_t(args.batch_stride_D) * size_t(grid_tiled_shape.k());
-      } else {
-        ptr_D = args.ptr_D;
-        ptr_gemm_k_reduction = args.ptr_gemm_k_reduction;
+                 + sizeof(ElementC) * size_t(this->batch_stride_D) * size_t(this->grid_tiled_shape.k());
+
+        return Status::kSuccess;
       }
+
+      return ParamsBase::init_workspace(workspace, stream);
     }
 
-    CUTLASS_HOST_DEVICE
-    void update(
-      Arguments const &args,
-      void *workspace = nullptr) {
+    /// Returns the workspace size (in bytes) needed for this problem geometry
+    size_t get_workspace_size() const
+    {
+      size_t workspace_bytes = ParamsBase::get_workspace_size();
+
+      if (this->mode == GemmUniversalMode::kGemmSplitKParallel)
+      {
+        // Split-K parallel always requires a temporary workspace
+        workspace_bytes +=
+          sizeof(ElementC) *
+          size_t(batch_stride_gemm_k_reduction) *
+          size_t(this->grid_tiled_shape.k());
+      }
 
+      return workspace_bytes;
+    }
+
+    /// Lightweight update given a subset of arguments.  Problem geometry is assumed
+    /// to remain the same.
+    void update(Arguments const &args)
+    {
       ptr_A = const_cast<void *>(args.ptr_A);
       ptr_B = const_cast<void *>(args.ptr_B);
       ptr_C = const_cast<void *>(args.ptr_C);
       ptr_D = args.ptr_D;
       ptr_gemm_k_reduction = args.ptr_gemm_k_reduction;
 
-      batch_stride_A = args.batch_stride_A;
-      batch_stride_B = args.batch_stride_B;
-      batch_stride_C = args.batch_stride_C;
-      batch_stride_D = args.batch_stride_D;
-      batch_stride_gemm_k_reduction = args.batch_stride_gemm_k_reduction;
-
       output_op = args.epilogue;
 
-      semaphore = static_cast<int *>(workspace);
       CUTLASS_TRACE_HOST("GemmUniversal::Params::update()");
     }
   };
 
   /// Shared memory storage structure
   union SharedStorage {
     typename Mma::SharedStorage main_loop;
     typename Epilogue::SharedStorage epilogue;
   };
 
+
 public:
 
   //
-  // Methods
+  // Host dispatch API
   //
 
-  CUTLASS_DEVICE
-  GemmWithKReduction() { } 
-
   /// Determines whether kernel satisfies alignment
   static Status can_implement(
     cutlass::gemm::GemmCoord const & problem_size) {
 
     CUTLASS_TRACE_HOST("GemmUniversal::can_implement()");
 
     static int const kAlignmentA = (platform::is_same<typename Mma::IteratorA::Layout,
@@ -405,34 +399,37 @@
     }
 
     CUTLASS_TRACE_HOST("  returning kSuccess");
 
     return Status::kSuccess;
   }
 
+
   static Status can_implement(Arguments const &args) {
     return can_implement(args.problem_size);
   }
 
-  static size_t get_extra_workspace_size(Arguments const &args,
-                                         cutlass::gemm::GemmCoord const &grid_tiled_shape) {
-    size_t workspace_bytes = 0;
-
-    if (args.mode == GemmUniversalMode::kGemmSplitKParallel) {                                        
-      
-      // Split-K parallel always requires a temporary workspace                                       
-      workspace_bytes =  
-        sizeof(ElementC) *
-        size_t(args.batch_stride_gemm_k_reduction) * 
-        size_t(grid_tiled_shape.k());                                                                 
-    }
 
-    return workspace_bytes;
+public:
+
+  //
+  // Device-only API
+  //
+
+  // Factory invocation
+  CUTLASS_DEVICE
+  static void invoke(
+    Params const &params,
+    SharedStorage &shared_storage)
+  {
+    GemmWithKReduction op;
+    op(params, shared_storage);
   }
-  
+
+
   /// Executes one GEMM
   CUTLASS_DEVICE
   void operator()(Params const &params, SharedStorage &shared_storage) {
 
     // Compute threadblock location
     ThreadblockSwizzle threadblock_swizzle;
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/gemv.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/gemv.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/gemv_batched_strided.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/gemv_batched_strided.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/grouped_problem_visitor.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/grouped_problem_visitor.h`

 * *Files 2% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -122,19 +122,15 @@
   problem_tile_start(0),
   problem_idx(0)
   {}
 
   /// Get the grid shape
   CUTLASS_HOST_DEVICE
   static cutlass::gemm::GemmCoord grid_shape(const cutlass::gemm::GemmCoord& problem) {
-
-    return cutlass::gemm::GemmCoord(
-      ((problem.m() - 1 + ThreadblockShape::kM) / ThreadblockShape::kM),
-      ((problem.n() - 1 + ThreadblockShape::kN) / ThreadblockShape::kN),
-      1);
+    return ProblemSizeHelper::grid_shape(problem);
   }
 
   /// Gets the global tile index
   CUTLASS_HOST_DEVICE
   int32_t tile_index() const {
     return tile_idx;
   }
@@ -342,15 +338,15 @@
           int ThreadCount>
 struct GroupedProblemVisitor<ProblemSizeHelper,
                              ThreadblockShape,
                              GroupScheduleMode::kHostPrecompute,
                              PrefetchTileCount,
                              ThreadCount> : public BaseGroupedProblemVisitor<ProblemSizeHelper, ThreadblockShape> {
   static_assert(PrefetchTileCount > 0,
-                "GroupedProblemVisitor with GroupScheduleMode `kHost` currently requires prefetching to shared memory");
+                "GroupedProblemVisitor with GroupScheduleMode `kHostPrecompute` currently requires prefetching to shared memory");
 
   using Base = BaseGroupedProblemVisitor<ProblemSizeHelper, ThreadblockShape>;
   using Params = typename Base::Params;
   using ProblemInfo = typename Base::ProblemInfo;
   static bool const kRequiresPrecomputation = true;
 
   static int const kPrefetchTileCount = PrefetchTileCount;
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/params_universal_base.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/params_universal_base.h`

 * *Files 2% similar despite different names*

```diff
@@ -185,34 +185,34 @@
 
   /// Assign and initialize the specified workspace buffer.  Assumes
   /// the memory allocated to workspace is at least as large as get_workspace_size().
   Status init_workspace(
     void *workspace,
     cudaStream_t stream = nullptr)
   {
-    semaphore = static_cast<int *>(workspace);
     // Zero-initialize entire workspace
-    if (semaphore)
+    if (workspace)
     {
       size_t workspace_bytes = get_workspace_size();
 
       CUTLASS_TRACE_HOST("  Initialize " << workspace_bytes << " workspace bytes");
 
       cudaError_t result = cudaMemsetAsync(
-        semaphore,
+        workspace,
         0,
         workspace_bytes,
         stream);
 
       if (result != cudaSuccess) {
         CUTLASS_TRACE_HOST("  cudaMemsetAsync() returned error " << cudaGetErrorString(result));
         return Status::kErrorInternal;
       }
     }
 
+    semaphore = static_cast<int *>(workspace);
     return Status::kSuccess;
   }
 
 
   /// Returns the GEMM volume in thread block tiles
   GemmCoord get_tiled_shape() const
   {
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/rank_2k_grouped.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/rank_2k_grouped.h`

 * *Files 2% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -380,21 +380,14 @@
     return Status::kSuccess;
   }
 
   static Status can_implement(Arguments const &args) {
     return Status::kSuccess;
   }
 
-  static size_t get_extra_workspace_size(
-    Arguments const &args,
-    cutlass::gemm::GemmCoord const &grid_tiled_shape) {
-
-    return 0;
-  }
-
   /// Executes one GEMM
   CUTLASS_DEVICE
   void operator()(Params const &params, SharedStorage &shared_storage) {
 
     //
     // Problem visitor.
     //
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/rank_2k_grouped_problem_visitor.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/rank_2k_grouped_problem_visitor.h`

 * *Files 2% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -277,14 +277,22 @@
 
 // Helper for correctly representing problem sizes in grouped kernels 
 template <typename ThreadblockShape>
 struct Rank2KGroupedProblemSizeHelper {
   using OffsetHelper = Rank2KGroupedProblemVisitorOffsetHelper<ThreadblockShape>;
 
   CUTLASS_HOST_DEVICE
+  static cutlass::gemm::GemmCoord grid_shape(const cutlass::gemm::GemmCoord& problem) {
+    return cutlass::gemm::GemmCoord(
+      ((problem.m() - 1 + ThreadblockShape::kM) / ThreadblockShape::kM),
+      ((problem.n() - 1 + ThreadblockShape::kN) / ThreadblockShape::kN),
+      1);
+  }
+
+  CUTLASS_HOST_DEVICE
   static int32_t tile_count(const cutlass::gemm::GemmCoord& grid) {
     // Return the number of tiles at or below the diagonal (or at and above
     // for mode kUpper). We do this by first calculating this value assuming
     // we have a square matrix of tiles of size `dim x dim` where `dim` is the
     // minimum among {grid.m(), grid.n()}. We then multiply the resulting value
     // by OffsetHelper::kThreadblockSkewRatio to account for cases in which there
     // are more tiles in one dimension than the other.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/rank_2k_transpose_operands.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/rank_2k_transpose_operands.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/rank_2k_universal.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/rank_2k_universal.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/rank_k_universal.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/rank_k_universal.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/sparse_gemm.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/sparse_gemm.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/symm_universal.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/symm_universal.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/trmm_universal.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/trmm_universal.h`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/thread/mma.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/gemm/thread/mma.h`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/thread/mma_sm50.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/gemm/thread/mma_sm50.h`

 * *Files 17% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -140,15 +140,18 @@
     // Copy accumulators
     D = C;
 
     // Compute matrix product
     CUTLASS_PRAGMA_UNROLL
     for (int k = 0; k < Shape::kK; ++k) {
       #if defined(__CUDA_ARCH__) && (__CUDA_ARCH__ >= 860)
-      if (kMultipleOf2 && platform::is_same<ElementA, float>::value && platform::is_same<ElementB, float>::value && platform::is_same<ElementC, float>::value) {
+      if (kMultipleOf2 &&
+        platform::is_same<ElementA, float>::value &&
+        platform::is_same<ElementB, float>::value &&
+        platform::is_same<ElementC, float>::value) {
 
         //2x2 zigzag - m and n loops to increment by 2. Inner loop to process 4 multiply-adds in a 2x2 tile.
         CUTLASS_PRAGMA_UNROLL
         for (int n = 0; n < Shape::kN; n+=2) {
   
           CUTLASS_PRAGMA_UNROLL
           for (int m = 0; m < Shape::kM; m+=2) {
@@ -248,14 +251,192 @@
     }
   }
 };
 
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
+namespace detail {
+
+/// Matrix multiply-add operation - assumes operand B is not changing
+struct MmaComplexF32_Column {
+
+  using Shape = gemm::GemmShape<1, 1, 1>;
+  using ElementC = complex<float>;
+
+  CUTLASS_HOST_DEVICE
+  void operator()(
+    Array<complex<float>, 1> &d,
+    Array<complex<float>, 1> const &a,
+    Array<complex<float>, 1> const &b,
+    Array<complex<float>, 1> const &c
+  ) {
+
+    d[0].real() =  a[0].real() * b[0].real() + c[0].real();
+    d[0].imag() =  a[0].real() * b[0].imag() + d[0].imag();
+    d[0].real() = -a[0].imag() * b[0].imag() + d[0].real();
+    d[0].imag() =  a[0].imag() * b[0].real() + c[0].imag();
+  }
+};
+
+/// Matrix multiply-add operation - assumes operand A is not changing
+struct MmaComplexF32_Corner {
+
+  using Shape = gemm::GemmShape<1, 1, 1>;
+  using ElementC = complex<float>;
+
+  CUTLASS_HOST_DEVICE
+  void operator()(
+    Array<complex<float>, 1> &d,
+    Array<complex<float>, 1> const &a,
+    Array<complex<float>, 1> const &b,
+    Array<complex<float>, 1> const &c
+  ) {
+
+    d[0].real() = -a[0].imag() * b[0].imag() + d[0].real();
+    d[0].imag() =  a[0].real() * b[0].imag() + d[0].imag();
+    d[0].real() =  a[0].real() * b[0].real() + c[0].real();
+    d[0].imag() =  a[0].imag() * b[0].real() + c[0].imag();
+  }
+};
+
+} // namespace detail
+
+/////////////////////////////////////////////////////////////////////////////////////////////////
+
+/// Gemplate that handles all packed matrix layouts
+template <
+  /// Size of the Gemm problem - concept: gemm::GemmShape<>
+  typename Shape_,
+  /// Layout of A matrix (concept: layout::MapFunc)
+  typename LayoutA_,
+  /// Layout of B matrix (concept: layout::MapFunc)
+  typename LayoutB_,
+  /// Layout of C matrix (concept: layout::MapFunc)
+  typename LayoutC_
+>
+struct MmaGeneric<
+  Shape_,
+  complex<float>,
+  LayoutA_,
+  complex<float>,
+  LayoutB_,
+  complex<float>,
+  LayoutC_,
+  arch::OpMultiplyAdd> {
+
+  /// Size of the Gemm problem - concept: gemm::GemmShape<>
+  using Shape = Shape_;
+
+  /// Data type of operand A
+  using ElementA = complex<float>;
+
+  /// Layout of A matrix (concept: layout::MapFunc)
+  using LayoutA = LayoutA_;
+
+  /// Data type of operand B
+  using ElementB = complex<float>;
+
+  /// Layout of B matrix (concept: layout::MapFunc)
+  using LayoutB = LayoutB_;
+
+  /// Element type of operand C
+  using ElementC = complex<float>;
+
+  /// Layout of C matrix (concept: layout::MapFunc)
+  using LayoutC = LayoutC_;
+
+  /// Underlying mathematical operator
+  using Operator = arch::OpMultiplyAdd;
+
+  /// A operand storage
+  using FragmentA = Array<ElementA, Shape::kMK>;
+
+  /// B operand storage
+  using FragmentB = Array<ElementB, Shape::kKN>;
+
+  /// C operand storage
+  using FragmentC = Array<ElementC, Shape::kMN>;
+
+  /// Instruction
+  using MmaOp = arch::Mma<
+    gemm::GemmShape<1,1,1>,
+    1,
+    ElementA, LayoutA,
+    ElementB, LayoutB,
+    ElementC, LayoutC,
+    Operator>;
+
+  //
+  // Methods
+  //
+
+  /// Computes a matrix product D = A * B + C
+  CUTLASS_HOST_DEVICE
+  void operator()(
+    FragmentC & D,
+    FragmentA const & A,
+    FragmentB const & B,
+    FragmentC const & C) {
+
+    TensorRef<ElementA const, LayoutA> a_ref(
+      reinterpret_cast<ElementA const *>(&A), LayoutA::packed({Shape::kM, Shape::kK}));
+
+    TensorRef<ElementB const, LayoutB> b_ref(
+      reinterpret_cast<ElementB const *>(&B), LayoutB::packed({Shape::kK, Shape::kN}));
+
+    TensorRef<ElementC, LayoutC> d_ref(
+      reinterpret_cast<ElementC *>(&D), LayoutC::packed(make_Coord(Shape::kM, Shape::kN)));
+
+    detail::MmaComplexF32_Column mma_column;
+    detail::MmaComplexF32_Corner mma_corner;
+
+    // Copy accumulators
+    D = C;
+
+    // Compute matrix product
+    CUTLASS_PRAGMA_UNROLL
+    for (int k = 0; k < Shape::kK; ++k) {
+
+      CUTLASS_PRAGMA_UNROLL
+      for (int n = 0; n < Shape::kN; ++n) {
+
+        CUTLASS_PRAGMA_UNROLL
+        for (int m = 0; m < Shape::kM; ++m) {
+
+          int m_serpentine = (n % 2) ? (Shape::kM - 1 - m) : m;
+
+          MatrixCoord mn(m_serpentine, n);
+          MatrixCoord mk(m_serpentine, k);
+          MatrixCoord kn(k, n);
+
+          Array<ElementC, 1> d;
+          Array<ElementA, 1> a;
+          Array<ElementB, 1> b;
+
+          d[0] = d_ref.at(mn);
+          a[0] = a_ref.at(mk);
+          b[0] = b_ref.at(kn);
+
+          if ((m == 0 && n) || m == Shape::kM - 1) {
+            mma_corner(d, a, b, d);
+          }
+          else {
+            mma_column(d, a, b, d);
+          }
+
+          d_ref.at(mn) = d[0];
+        }
+      }
+    }
+  }
+};
+
+/////////////////////////////////////////////////////////////////////////////////////////////////
+
 /// Gemplate that handles conventional layouts for FFMA and DFMA GEMM
 template <
   /// Size of the Gemm problem - concept: gemm::GemmShape<>
   typename Shape_,
   /// Data type of A elements
   typename ElementA_,
   /// Layout of A matrix (concept: layout::MapFunc)
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/thread/mma_sm60.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/gemm/thread/mma_sm60.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/thread/mma_sm61.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/gemm/thread/mma_sm61.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/default_ell_mma.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/default_ell_mma.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/default_gemv_core.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/default_gemv_core.h`

 * *Files 2% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/default_mma.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/default_mma.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/default_mma_core.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/default_mma_core.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/default_mma_core_simt.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/default_mma_core_simt.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/default_mma_core_sm70.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/default_mma_core_sm70.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/default_mma_core_sm75.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/default_mma_core_sm75.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/default_mma_core_sm80.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/default_mma_core_sm80.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -32,14 +32,17 @@
 /*! \file
     \brief Defines basic properties needed by CTA-level GEMMs assuming
    expectations about data layout of the global memory fragments, data types,
    and internal tile sizes.
 
       Partial specializations for threadblock::Mma operations targeting TensorOp
    instructions.
+
+      SM80 Multi stage kernel expects stage number to be larger or equal to 3
+   to use asyncronous copy.
 */
 
 #pragma once
 
 #include "cutlass/array.h"
 #include "cutlass/cutlass.h"
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/default_mma_core_sparse_sm80.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/default_mma_core_sparse_sm80.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/default_mma_core_with_access_size.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/conv/warp/mma_depthwise_simt.h`

 * *Files 24% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -25,304 +25,357 @@
  * SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER
  * CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,
  * OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
  * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
  *
  **************************************************************************************************/
 /*! \file
-    \brief Defines basic properties needed by CTA-level GEMMs assuming expectations about data
-      layout of the global memory fragments, data types, and internal tile sizes.
-
-      Partial specializations for threadblock::Mma operations targeting simt instructions.
+    \brief Templates implementing warp-level matrix multiply-accumulate operations.
 */
 
 #pragma once
 
 #include "cutlass/cutlass.h"
 #include "cutlass/array.h"
-
 #include "cutlass/numeric_types.h"
 #include "cutlass/matrix_shape.h"
-
+#include "cutlass/gemm/gemm.h"
 #include "cutlass/gemm/warp/mma.h"
-#include "cutlass/gemm/threadblock/mma_pipelined.h"
-#include "cutlass/gemm/threadblock/mma_singlestage.h"
-#include "cutlass/arch/cache_operation.h" 
 
-/////////////////////////////////////////////////////////////////////////////////////////////////
+#include "cutlass/gemm/thread/mma.h"
+#include "cutlass/conv/convolution.h"
+#include "cutlass/conv/thread/depthwise_mma.h"
 
-namespace cutlass {
-namespace gemm {
-namespace threadblock {
-
-template <
-    /// Shape of threadblock-scoped matrix multiply operator
-    typename Shape,
-    /// Shape of warp-level matrix multiply operator
-    typename WarpShape,
-    /// Shape of one matrix production operation (concept: GemmShape)
-    typename InstructionShape,
-    /// Element data type of A operand
-    typename ElementA,
-    /// Layout of operand A
-    typename LayoutA,
-    /// Element data type of B operand
-    typename ElementB,
-    /// Layout of operand B
-    typename LayoutB,
-    /// Data type of accumulator
-    typename ElementC,
-    /// Layout of accumulator
-    typename LayoutC,
-    /// Indicates type of math operator (arch::OpClassSimt or arch::OpClassTensorOp)
-    typename OperatorClass,
-    /// Size of a threadblock-scoped access
-    int kAccessSizeInBits = -1, // -1 denoting the default
-    /// Number of stages
-    int Stages = 2,
-    /// Operation performed by MMA
-    typename Operator = typename platform::conditional<
-        (platform::is_same<OperatorClass,
-                           cutlass::arch::OpClassTensorOp>::value) &&
-            (platform::is_same<ElementA, int8_t>::value ||
-             platform::is_same<ElementA, int4b_t>::value ||
-             platform::is_same<ElementA, uint8_t>::value ||
-             platform::is_same<ElementA, uint4b_t>::value),
-        cutlass::arch::OpMultiplyAddSaturate,
-        cutlass::arch::OpMultiplyAdd>::type,
-    /// Store the accumulators in row major or column major.  Row major is used
-    /// when output layout is interleaved.
-    bool AccumulatorsInRowMajor = false,
-    /// Cache operation of operand A
-    cutlass::arch::CacheOperation::Kind CacheOpA =
-        cutlass::arch::CacheOperation::Global,
-    /// Cache operation of operand B
-    cutlass::arch::CacheOperation::Kind CacheOpB =
-        cutlass::arch::CacheOperation::Global,
-    /// per-element transformation for elements of A
-    ComplexTransform TransformA = ComplexTransform::kNone,
-    /// per-element transformation for elements of B
-    ComplexTransform TransformB = ComplexTransform::kNone,
-    bool IsComplex = false // (is_complex<ElementA>::value || is_complex<ElementB>::value)
->
-struct DefaultMmaCoreWithAccessSize;
 
-template <
-    /// Shape of threadblock-scoped matrix multiply operator
-    typename Shape,
-    /// Shape of warp-level matrix multiply operator
-    typename WarpShape,
-    /// Shape of one matrix production operation (concept: GemmShape)
-    typename InstructionShape,
-    /// Element data type of A operand
-    typename ElementA,
-    /// Layout of operand A
-    typename LayoutA,
-    /// Element data type of B operand
-    typename ElementB,
-    /// Layout of operand B
-    typename LayoutB,
-    /// Data type of accumulator
-    typename ElementC,
-    /// Layout of accumulator
-    typename LayoutC,
-    /// Indicates type of math operator (arch::OpClassSimt or arch::OpClassTensorOp)
-    typename OperatorClass,
-    /// Number of stages
-    int Stages,
-    /// Operation performed by MMA
-    typename Operator,
-    /// Store the accumulators in row major or column major.  Row major is used
-    /// when output layout is interleaved.
-    bool AccumulatorsInRowMajor,
-    /// Cache operation of operand A
-    cutlass::arch::CacheOperation::Kind CacheOpA,
-    /// Cache operation of operand B
-    cutlass::arch::CacheOperation::Kind CacheOpB,
-    /// per-element transformation for elements of A
-    ComplexTransform TransformA,
-    /// per-element transformation for elements of B
-    ComplexTransform TransformB,
-    bool IsComplex
->
-struct DefaultMmaCoreWithAccessSize<
-    Shape, WarpShape, InstructionShape,
-    ElementA, LayoutA, ElementB, LayoutB, ElementC, LayoutC,
-    OperatorClass, -1, Stages, Operator, AccumulatorsInRowMajor,
-    CacheOpA, CacheOpB, TransformA, TransformB, IsComplex
-> : DefaultMmaCore<
-    Shape, WarpShape, InstructionShape,
-    ElementA, LayoutA, ElementB, LayoutB, ElementC, LayoutC,
-    OperatorClass, Stages, Operator, AccumulatorsInRowMajor,
-    CacheOpA, CacheOpB, TransformA, TransformB, IsComplex
-> {};
+#include "cutlass/gemm/warp/mma_simt_tile_iterator.h"
+#include "cutlass/gemm/warp/mma_simt_policy.h"
 
+#include "cutlass/gemm/warp/mma_simt.h"
+#include "cutlass/conv/warp/mma_depthwise_simt_tile_iterator.h"
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
+namespace cutlass {
+namespace conv {
+namespace warp {
+
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
-/// Partial specialization:
-///
-///   A: column-major
-///   B: row-major
-///   Operator: simt class
-///
-/// This uses the default warp-level operator given tile sizes
+/// Structure to compute the matrix product targeting CUDA cores and SIMT math instructions.
 template <
-    /// Shape of threadblock-scoped matrix multiply operator (concept:
-    /// GemmShape)
+    /// Size of the Gemm problem - concept: gemm::GemmShape<>
     typename Shape_,
-    /// Shape of warp-level matrix multiply operator (concept: GemmShape)
-    typename WarpShape_,
-    /// Data type of A operand
+    /// Data type of A elements
     typename ElementA_,
-    /// Data type of B operand
+    /// Layout of A matrix (concept: MatrixLayout)
+    typename LayoutA_,
+    /// Data type of B elements
     typename ElementB_,
-    /// Data type of accumulator
+    /// Layout of B matrix (concept: MatrixLayout)
+    typename LayoutB_,
+    /// Element type of C matrix
     typename ElementC_,
-    /// Layout of accumulator
+    /// Layout of C matrix (concept: MatrixLayout)
     typename LayoutC_,
-    /// Size of a threadblock-scoped access (a value of -1 indicates the default)
-    int kAccessSizeInBits_,
-    /// Operation performed by GEMM
-    typename Operator_>
-struct DefaultMmaCoreWithAccessSize<Shape_, WarpShape_, typename std::enable_if<kAccessSizeInBits_ != -1, GemmShape<1, 1, 1>>::type, ElementA_,
-                      layout::ColumnMajor, ElementB_, layout::RowMajor,
-                      ElementC_, LayoutC_, arch::OpClassSimt, kAccessSizeInBits_, 2, Operator_
-                     > {
+    /// Shape of the warp in units of thread (concept: MmaSimtPolicy)
+    typename Policy_,
+    /// Number of partitions along K dimension
+    int PartitionsK = 1,
+    /// Complex transformation on operand A
+    ComplexTransform TransformA = ComplexTransform::kNone,
+    /// Complex transformation on operand B
+    ComplexTransform TransformB = ComplexTransform::kNone,
+    /// Used for partial specialization
+    typename Enable = bool>
+class MmaDepthwiseSimt
+    : public cutlass::gemm::warp::
+          MmaSimt<Shape_, ElementA_, LayoutA_, ElementB_, LayoutB_, ElementC_, LayoutC_, Policy_> {
+  using Base = cutlass::gemm::warp::
+      MmaSimt<Shape_, ElementA_, LayoutA_, ElementB_, LayoutB_, ElementC_, LayoutC_, Policy_>;
+      
+public:
+  /// Shape of warp-level matrix operation (concept: GemmShape)
   using Shape = Shape_;
-  using WarpShape = WarpShape_;
-  using InstructionShape = GemmShape<1, 1, 1>;
+
+  /// Data type of multiplicand A
   using ElementA = ElementA_;
-  using LayoutA = layout::ColumnMajor;
+
+  /// Layout of multiplicand A
+  using LayoutA = LayoutA_;
+
+  /// Data type of multiplicand B
   using ElementB = ElementB_;
-  using LayoutB = layout::RowMajor;
+
+  /// Layout of multiplicand B
+  using LayoutB = LayoutB_;
+
+  /// Data type of accumulator matrix C
   using ElementC = ElementC_;
+
+  /// Layout of accumulator matrix C
   using LayoutC = LayoutC_;
+
+  /// Shape of the warp in units of thread (concept: MmaLanePolicySimt)
+  using Policy = Policy_;
+
+  /// Indicates class of matrix operator
   using OperatorClass = arch::OpClassSimt;
-  static int const PartitionsK = Shape::kK / WarpShape::kK;
 
-  /// Default Operator
-  using Operator = Operator_;
+  /// Hard-coded for now
+  using ArchTag = arch::Sm50;
+
+  /// Complex transform on A operand
+  static ComplexTransform const kTransformA = TransformA;
 
-  /// Number of warps present
-  using WarpCount = GemmShape<
-    Shape::kM / WarpShape::kM,
-    Shape::kN / WarpShape::kN,
-    PartitionsK
+  /// Complex transform on B operand
+  static ComplexTransform const kTransformB = TransformB;
+
+public:
+
+  /// Iterates over the B operand in memory
+  using IteratorB = cutlass::conv::warp::DepthwiseMmaSimtTileIterator<
+    MatrixShape<Policy::LaneMmaShape::kK, Shape::kN>,
+    cutlass::gemm::Operand::kB,
+    ElementB,
+    LayoutB,
+    Policy,
+    PartitionsK,
+    Shape::kK
   >;
 
-  // Divisility requirements
-  static_assert(
-    !(Shape::kM % WarpShape::kM) &&
-    !(Shape::kN % WarpShape::kN),
-    "Threadblock-scoped GEMM should be divisible by warp-scoped GEMM size."
-  );
-
-  /// Number of threads per warp
-  static int const kWarpSize = warp::WarpSize<arch::OpClassSimt>::value;
-
-  /// Number of threads total
-  static int const kThreads = WarpCount::kCount * kWarpSize;
-
-  static int const kElementsPerAccessDefault = 1;
-  static_assert(kAccessSizeInBits_ == -1 ||
-          sizeof_bits<ElementA>::value == sizeof_bits<ElementB>::value ||
-          kAccessSizeInBits_ / sizeof_bits<ElementA>::value == kElementsPerAccessDefault,
-          "Non-default value for kAccessSizeInBits_ is only allowed if size(elementA) == sizeof(elementB)");
-  static int const kElementsPerAccess = (kAccessSizeInBits_ != -1) ? kAccessSizeInBits_ / sizeof_bits<ElementA>::value : kElementsPerAccessDefault;
+  /// Storage for B tile
+  using FragmentB = typename IteratorB::Fragment;
 
-  //
-  // Shared memory layouts
-  //
+  /// Storage for transformed A tile
+  using TransformedFragmentB = FragmentB;
 
-  using SmemLayoutA = layout::ColumnMajor;
-  using SmemLayoutB = layout::RowMajor;
+public:
 
   //
-  // Iterators to write to shared memory
+  // Methods
   //
 
-  /// ThreadMap of iterator A
-  using IteratorThreadMapA = transform::PitchLinearStripminedThreadMap<
-    layout::PitchLinearShape<Shape::kM, Shape::kK>,
-    kThreads,
-    kElementsPerAccess
+  /// Ctor
+  CUTLASS_DEVICE
+  MmaDepthwiseSimt():Base() {}
+};
+
+/// Structure to compute the matrix product targeting CUDA cores and SIMT math instructions.
+template <
+    /// Size of the Gemm problem - concept: gemm::GemmShape<>
+    typename Shape_,
+    /// Shape of filter shape per threadblock - concept: gemm::GemmShape<Depth, Height, Width>
+    typename FilterShape_,
+    /// Shape of the output tile computed by thread- concept: conv::TensorNHWCShape<>
+    typename ThreadOutputShape_,
+    /// Shape of the output tile computed by threadblock - concept: conv::TensorNHWCShape<>
+    typename ThreadBlockOutputShape_,
+    /// Data type of A elements
+    typename ElementA_,
+    /// Layout of A matrix (concept: MatrixLayout)
+    typename LayoutA_,
+    /// Data type of B elements
+    typename ElementB_,
+    /// Layout of B matrix (concept: MatrixLayout)
+    typename LayoutB_,
+    /// Element type of C matrix
+    typename ElementC_,
+    /// Layout of C matrix (concept: MatrixLayout)
+    typename LayoutC_,
+    /// Shape of the warp in units of thread (concept: MmaSimtPolicy)
+    typename Policy_,
+    /// Iterator algo type
+    conv::IteratorAlgorithm IteratorAlgorithm_ = IteratorAlgorithm::kAnalytic,
+    /// Stride ( MatrixShape<Height, Width> )
+    typename StrideShape_ = cutlass::MatrixShape<-1, -1>,   
+    /// Dilation ( MatrixShape<Height, Width> )
+    typename DilationShape_ =  cutlass::MatrixShape<-1, -1>,
+    /// Activation Shape loaded by threadblock
+    typename ActivationShape_ = cutlass::conv::TensorNHWCShape<-1,-1,-1,-1>,
+    /// Number of partitions along K dimension
+    int PartitionsK = 1,
+    /// Complex transformation on operand A
+    ComplexTransform TransformA = ComplexTransform::kNone,
+    /// Complex transformation on operand B
+    ComplexTransform TransformB = ComplexTransform::kNone,
+    /// Used for partial specialization
+    typename Enable = bool>
+class MmaDepthwiseDirectConvSimt {
+ public:
+  /// Shape of warp-level matrix operation (concept: GemmShape)
+  using Shape = Shape_;
+
+  /// Shape of filter shape per threadblock - concept: gemm::GemmShape<Depth, Height, Width>
+  using FilterShape = FilterShape_;
+
+  /// Shape of the output tile computed by thread- concept: conv::TensorNHWCShape<>
+  using ThreadOutputShape = ThreadOutputShape_;
+
+  /// Shape of the output tile computed by threadblock - concept: conv::TensorNHWCShape<>
+  using ThreadBlockOutputShape = ThreadBlockOutputShape_;
+
+  /// Data type of multiplicand A
+  using ElementA = ElementA_;
+
+  /// Layout of multiplicand A
+  using LayoutA = LayoutA_;
+
+  /// Data type of multiplicand B
+  using ElementB = ElementB_;
+
+  /// Layout of multiplicand B
+  using LayoutB = LayoutB_;
+
+  /// Data type of accumulator matrix C
+  using ElementC = ElementC_;
+
+  /// Layout of accumulator matrix C
+  using LayoutC = LayoutC_;
+
+  /// Shape of the warp in units of thread (concept: MmaLanePolicySimt)
+  using Policy = Policy_;
+
+  /// Iterator algo type
+  static conv::IteratorAlgorithm const IteratorAlgorithm = IteratorAlgorithm_;
+
+  /// Stride ( MatrixShape<Height, Width> )
+  using StrideShape = StrideShape_; 
+
+  /// Dilation ( MatrixShape<Height, Width> )
+  using DilationShape = DilationShape_;
+  
+  /// Activation Shape loaded by threadblock
+  using ActivationShape = ActivationShape_;
+
+  /// Indicates class of matrix operator
+  using OperatorClass = arch::OpClassSimt;
+
+  /// Hard-coded for now
+  using ArchTag = arch::Sm50;
+
+  /// Complex transform on A operand
+  static ComplexTransform const kTransformA = TransformA;
+
+  /// Complex transform on B operand
+  static ComplexTransform const kTransformB = TransformB;
+
+  static constexpr bool use_dp4a = (platform::is_same< layout::ColumnMajorInterleaved<4>, LayoutA>::value || 
+                                    platform::is_same< layout::RowMajorInterleaved<4>, LayoutA >::value) && 
+                                    platform::is_same< ElementA, int8_t >::value && 
+                                    platform::is_same< ElementB, int8_t >::value;
+
+  using dp4a_type = typename platform::conditional< use_dp4a , int8_t, bool >::type;
+
+  /// Thread-level matrix multiply accumulate operator
+  using ThreadMma = cutlass::conv::thread::DepthwiseDirectConvElementwiseInnerProduct<
+    cutlass::gemm::GemmShape<
+      Shape::kM / Policy::WarpShape::kRow,    // number of output pixels proccessed per thread
+      Shape::kN / Policy::WarpShape::kColumn, // number of channels proccessed per thread
+      1>,
+    ElementA,
+    ElementB,
+    ElementC,
+    arch::OpMultiplyAdd,
+    dp4a_type
   >;
 
-  /// Shared memory iterator to A operand
-  using SmemIteratorA = transform::threadblock::RegularTileIterator<
-    MatrixShape<Shape::kM, Shape::kK>, 
-    ElementA, 
-    SmemLayoutA,
-    1,
-    IteratorThreadMapA
+  /// Underlying matrix multiply operator (concept: arch::Mma)
+  using ArchMmaOperator = typename ThreadMma::ArchMmaOperator;
+
+  /// Indicates math operator 
+  using MathOperator = typename ArchMmaOperator::Operator;
+  
+  /// Shape of the underlying instruction
+  using InstructionShape = cutlass::gemm::GemmShape<1,1,use_dp4a ? 4 : 1>;
+
+public:
+
+  /// Iterates over the A operand in memory
+  using IteratorA = cutlass::conv::warp::DepthwiseDirect2dConvSimtTileIterator<
+    MatrixShape<Shape::kM, Shape::kN>, // <output tile=(P*Q), output channels> per warp
+    FilterShape,
+    ThreadOutputShape,
+    ThreadBlockOutputShape,
+    cutlass::gemm::Operand::kA,
+    ElementA,
+    Policy,
+    IteratorAlgorithm,
+    StrideShape,
+    DilationShape,
+    ActivationShape,
+    PartitionsK,
+    Shape::kK
   >;
 
-  /// Policy of iterator B
-  using IteratorThreadMapB = transform::PitchLinearStripminedThreadMap<
-    layout::PitchLinearShape<Shape::kN, Shape::kK>,
-    kThreads,
-    kElementsPerAccess
+  /// Storage for A tile
+  using FragmentA = typename IteratorA::Fragment;
+
+  /// Storage for transformed A tile
+  using TransformedFragmentA = FragmentA;
+
+  /// Iterates over the B operand in memory
+  using IteratorB = cutlass::gemm::warp::MmaSimtTileIterator<
+    MatrixShape<1, Shape::kN>,
+    cutlass::gemm::Operand::kB,
+    ElementB,
+    LayoutB,
+    Policy,
+    PartitionsK,
+    Shape::kK
   >;
 
-  /// Shared memory iterator to B operand
-  using SmemIteratorB = transform::threadblock::RegularTileIterator<
-    MatrixShape<Shape::kK, Shape::kN>, 
-    ElementB, 
-    SmemLayoutB,
-    0,
-    IteratorThreadMapB
+  /// Storage for B tile
+  using FragmentB = typename IteratorB::Fragment;
+
+  /// Storage for transformed A tile
+  using TransformedFragmentB = FragmentB;
+
+  /// Iterates over the C operand in memory
+  using IteratorC = cutlass::gemm::warp::MmaSimtTileIterator<
+    MatrixShape<Shape::kM, Shape::kN>,
+    cutlass::gemm::Operand::kC,
+    ElementC,
+    LayoutC,
+    Policy
   >;
 
+  /// Storage for C tile
+  using FragmentC = typename ThreadMma::FragmentC;
+
+public:
+
   //
-  // Warp-level matrix multiply operator
+  // Methods
   //
 
-  // Define the warp-level op
-  static const int WarpNumThreadsM = detail::simt_get_warp_threads_m<WarpShape>();
-  static const int WarpNumThreadsN = kWarpSize / WarpNumThreadsM;
-  static const int ThreadTileM = WarpShape::kM / WarpNumThreadsM;
-  static const int ThreadTileN = WarpShape::kN / WarpNumThreadsN;
-  static_assert(!(WarpShape::kM % WarpNumThreadsM) && !(WarpShape::kN % WarpNumThreadsN),
-      "WarpShape must be divisible by ThreadTile shape.");
-  static const int LaneLayout = ThreadTileM > 4 && ThreadTileN > 4 ? 2 : 1;
-  static const int numElementsA = 128 / sizeof_bits<ElementA>::value;
-  static const int numElementsB = 128 / sizeof_bits<ElementB>::value;
-  static const int LaneM = cutlass::const_min(numElementsA, ThreadTileM);
-  static const int LaneN = cutlass::const_min(numElementsB, ThreadTileN);
-  // these should have max of thread tile also
-  using LaneMmaShape = cutlass::gemm::GemmShape<
-      LaneM,
-      LaneN,
-      1>;
-  using Policy = cutlass::gemm::warp::MmaSimtPolicy<
-      cutlass::MatrixShape<WarpNumThreadsM, WarpNumThreadsN>,   // WarpShape
-      cutlass::layout::RowMajorInterleaved<LaneLayout>,         // LaneLayout
-      LaneMmaShape
-  >;
-
-  using MmaWarpSimt = cutlass::gemm::warp::MmaSimt<
-    WarpShape,    /// Size of the Gemm problem - concept: gemm::GemmShape<> 128, 128, 8
-    ElementA,     /// Data type of A elements
-    SmemLayoutA,  /// Layout of A matrix (concept: MatrixLayout)
-    ElementB,     /// Data type of B elements
-    SmemLayoutB,  /// Layout of B matrix (concept: MatrixLayout)
-    ElementC,     /// Element type of C matrix
-    LayoutC,      /// Layout of C matrix (concept: MatrixLayout)
-    Policy        /// Policy describing warp-level MmaSimtOp (concept: MmaSimtOp policy)
-    >;            /// Used for partial specialization
-
-  /// Policy used to define MmaPipelined
-  using MmaPolicy = MmaPolicy<
-    MmaWarpSimt,
-    MatrixShape<0, 0>,
-    MatrixShape<0, 0>,
-    WarpCount::kK
-  >;
+  /// Ctor
+  CUTLASS_DEVICE
+  MmaDepthwiseDirectConvSimt() {}
+
+  /// Performs a warp-level matrix multiply-accumulate operation
+  CUTLASS_DEVICE
+  void operator()(
+    FragmentC &d, 
+    FragmentA a, 
+    FragmentB b, 
+    FragmentC const &c, int group_idx = 0) const {
+
+    ThreadMma mma;
+
+    mma(d, a, b, c);
+  }
+
+  /// Transform the mma operands to the required types
+  CUTLASS_DEVICE
+  void transform(TransformedFragmentA &dst_A, TransformedFragmentB &dst_B,
+                 FragmentA const &A, FragmentB const &B) const {
+    //TODO: Implement this
+    dst_A = A;
+    dst_B = B;
+  }
 };
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
-} // namespace threadblock
-} // namespace gemm
+
+} // namespace warp
+} // namespace conv
 } // namespace cutlass
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/default_mma_core_with_reduction.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/default_mma_core_with_reduction.h`

 * *Files 2% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -86,15 +86,15 @@
     typename LayoutB,
     /// Data type of accumulator
     typename ElementC,
     /// Layout of accumulator
     typename LayoutC,
     /// Indicates type of math operator (arch::OpClassSimt or arch::OpClassTensorOp)
     typename OperatorClass,
-    ///                                                                                               
+    /// Reduce operand A or B along K dimension
     bool ReduceKForA_,
     /// Number of stages
     int Stages = 2,
     /// Operation performed by MMA
     typename Operator = typename platform::conditional<
         (platform::is_same<OperatorClass,
                            cutlass::arch::OpClassTensorOp>::value) &&
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/default_mma_core_wmma.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/default_mma_core_wmma.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/default_mma_layernorm_mainloop_fusion.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/default_multistage_mma_complex.h`

 * *Files 13% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -24,155 +24,136 @@
  * DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR
  * SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER
  * CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,
  * OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
  * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
  *
  **************************************************************************************************/
+
 /*! \file
-    \brief Template for a pipelined GEMM kernel. Does not compute batching or support split-K.
+    \brief Template for a multistage GEMM kernel. Does not compute batching or support split-K.
 */
 
 #pragma once
 
+#include "cutlass/arch/arch.h"
 #include "cutlass/cutlass.h"
+#include "cutlass/gemm/threadblock/default_mma_core_sm80.h"
 #include "cutlass/numeric_types.h"
-#include "cutlass/arch/arch.h"
-
-#include "cutlass/layout/matrix.h"
-#include "cutlass/gemm/threadblock/default_mma_core.h"
-#include "cutlass/gemm/threadblock/mma_layernorm_mainloop_fusion_multistage.h"
-#include "cutlass/transform/threadblock/predicated_scale_bias_vector_iterator.h"
-#include "cutlass/transform/threadblock/predicated_scale_bias_vector_access_iterator.h"
-#include "cutlass/transform/threadblock/regular_scale_bias_vector_access_iterator.h"
-#include "cutlass/gemm/warp/scale_bias_tile_iterator.h"
 #include "cutlass/transform/threadblock/predicated_tile_iterator.h"
+#include "cutlass/gemm/threadblock/default_multistage_mma_complex_core_sm80.h"
 
 ////////////////////////////////////////////////////////////////////////////////
 
 namespace cutlass {
 namespace gemm {
 namespace threadblock {
 
 ////////////////////////////////////////////////////////////////////////////////
 
 template <
     /// Element type for A matrix operand
+    typename ElementA_,
+    /// Layout type for A matrix operand
+    typename LayoutA_,
+    /// Element type for B matrix operand
+    typename ElementB_,
+    /// Layout type for B matrix operand
+    typename LayoutB_,
+    /// Element type for internal accumulation
+    typename ElementAccumulator_,
+    /// Layout type for C and D matrix operands
+    typename LayoutC_,
+    /// Operator class tag
+    typename OperatorClass_,
+    /// Tag indicating architecture to tune for
+    typename ArchTag_,
+    /// Threadblock-level tile size (concept: GemmShape)
+    typename ThreadblockShape_,
+    /// Warp-level tile size (concept: GemmShape)
+    typename WarpShape_,
+    /// Instruction-level tile size (concept: GemmShape)
+    typename InstructionShape_,
+    /// Number of stages used in the pipelined mainloop
+    int Stages,
+    /// Complex transformation on operand A
+    ComplexTransform TransformA = ComplexTransform::kNone,
+    /// Complex transformation on operand B
+    ComplexTransform TransformB = ComplexTransform::kNone,
+    /// Multiply-add operator (arch::OpMultiplyAddComplex, arch::OpMultiplyGaussianComplex)
+    typename Operator = arch::OpMultiplyAddComplex,
+    /// Store the accumulators in row major or column major.  Row major is used
+    /// when output layout is interleaved.
+    bool AccumulatorsInRowMajor = false>
+struct DefaultMultistageMmaComplex;
+
+////////////////////////////////////////////////////////////////////////////////
+
+/// Specialization for row-major output
+template <
+    /// Element type for A matrix operand
     typename ElementA,
     /// Layout type for A matrix operand
     typename LayoutA,
-    /// Access granularity of A matrix in units of elements
-    int kAlignmentA,
     /// Element type for B matrix operand
     typename ElementB,
     /// Layout type for B matrix operand
     typename LayoutB,
-    /// Access granularity of B matrix in units of elements
-    int kAlignmentB,
-    /// Element type for Scale/Bias vectors
-    typename ElementScaleBias,
-    /// Layout type for Scale/Bias vectors
-    typename LayoutScaleBias,
     /// Element type for internal accumulation
     typename ElementAccumulator,
-    /// Layout type for C and D matrix operands
-    typename LayoutC,
-    /// Operator class tag
+    /// Tag indicating architecture to tune for
     typename OperatorClass,
     /// Tag indicating architecture to tune for
     typename ArchTag,
     /// Threadblock-level tile size (concept: GemmShape)
     typename ThreadblockShape,
     /// Warp-level tile size (concept: GemmShape)
     typename WarpShape,
     /// Instruction-level tile size (concept: GemmShape)
     typename InstructionShape,
-    /// Number of stages used in the pipelined mainloop
+    /// Number of stages used in the multistage mainloop
     int Stages,
-    /// Operation perfomed by GEMM
-    typename Operator,
-    /// Store the accumulators in row major or column major.  Row major is used
-    /// when output layout is interleaved.
-    bool AccumulatorsInRowMajor = false,
-    /// Use zfill or predicate for SM80 out-of-bound cp.async 
-    SharedMemoryClearOption SharedMemoryClear = SharedMemoryClearOption::kNone
-    >
-struct DefaultMmaLayernormMainloopFusion {
-
-  static cutlass::arch::CacheOperation::Kind const CacheOpA =
-      ((sizeof_bits<ElementA>::value * kAlignmentA) == 128)
-          ? cutlass::arch::CacheOperation::Global
-          : cutlass::arch::CacheOperation::Always;
-
-  static cutlass::arch::CacheOperation::Kind const CacheOpB =
-      ((sizeof_bits<ElementB>::value * kAlignmentB) == 128)
-          ? cutlass::arch::CacheOperation::Global
-          : cutlass::arch::CacheOperation::Always;
-
-  static cutlass::arch::CacheOperation::Kind const CacheOpGammaBeta = CacheOpA;
-
+    /// Complex transformation on operand A
+    ComplexTransform TransformA,
+    /// Complex transformation on operand B
+    ComplexTransform TransformB,
+    /// Multiply-add operator (arch::OpMultiplyAddComplex, arch::OpMultiplyGaussianComplex)
+    typename Operator>
+struct DefaultMultistageMmaComplex<ElementA, LayoutA, ElementB, LayoutB,
+                            ElementAccumulator, layout::RowMajor, OperatorClass,
+                            ArchTag, ThreadblockShape, WarpShape,
+                            InstructionShape, Stages, TransformA, TransformB, Operator> {
   // Define the MmaCore components
-  using MmaCore = typename cutlass::gemm::threadblock::DefaultMmaCore<
-      ThreadblockShape, WarpShape, InstructionShape, ElementA, LayoutA,
-      ElementB, LayoutB, ElementAccumulator, layout::RowMajor, arch::OpClassTensorOp,
-      Stages, Operator, false, CacheOpA, CacheOpB>;
+  using MmaCore = typename cutlass::gemm::threadblock::DefaultMultistageMmaComplexCore<
+      ThreadblockShape, WarpShape, InstructionShape, ElementA, LayoutA, 
+      ElementB, LayoutB, ElementAccumulator, layout::RowMajor, OperatorClass,
+      Stages, TransformA, TransformB, Operator>;
 
   // Define iterators over tiles from the A operand
   using ThreadMapA = typename MmaCore::IteratorThreadMapA;
-  using AccessTypeA = cutlass::Array<ElementA, kAlignmentA>;
+  using AccessTypeA = cutlass::Array<ElementA, ThreadMapA::kElementsPerAccess>;
   using IteratorA =
       cutlass::transform::threadblock::PredicatedTileAccessIterator<
           cutlass::MatrixShape<ThreadblockShape::kM, ThreadblockShape::kK>,
           ElementA, LayoutA, 1, ThreadMapA, AccessTypeA>;
 
   // Define iterators over tiles from the B operand
   using ThreadMapB = typename MmaCore::IteratorThreadMapB;
-  using AccessTypeB = cutlass::Array<ElementB, kAlignmentB>;
+  using AccessTypeB = cutlass::Array<ElementB, ThreadMapB::kElementsPerAccess>;
   using IteratorB =
       cutlass::transform::threadblock::PredicatedTileAccessIterator<
           cutlass::MatrixShape<ThreadblockShape::kK, ThreadblockShape::kN>,
           ElementB, LayoutB, 0, ThreadMapB, AccessTypeB>;
 
-  /// Define iterators over tiles from scale/bias vectors
-  using IteratorVarMean =
-      cutlass::transform::threadblock::PredicatedScaleBiasVectorIterator<
-          cutlass::MatrixShape<1, WarpShape::kN>,
-          ElementScaleBias,
-          LayoutScaleBias>;
-
-  /// Define iterators over tiles from scale/bias vectors
-  using IteratorGammaBeta =
-      cutlass::transform::threadblock::PredicatedScaleBiasVectorAccessIterator<
-          cutlass::MatrixShape<1, ThreadblockShape::kK>, ElementScaleBias,
-          LayoutScaleBias>;
-
-  using SmemIteratorGammaBeta =
-      cutlass::transform::threadblock::RegularScaleBiasVectorAccessIterator<
-          cutlass::MatrixShape<1, ThreadblockShape::kK>, ElementScaleBias,
-          LayoutScaleBias>;
-
-  static int const kThreadCount = 32;
-
-  // Warp-level iterators to load scale and bias vectors
-  using WarpIteratorGammaBeta = cutlass::gemm::warp::ScaleBiasTileIterator<
-      MatrixShape<WarpShape::kM, WarpShape::kK>, ElementScaleBias,
-      LayoutScaleBias, MatrixShape<InstructionShape::kM, InstructionShape::kK>,
-      typename MmaCore::MmaTensorOp::IteratorA::Base::Policy, kThreadCount,
-      MmaCore::WarpCount::kK>;
-
   // Define the threadblock-scoped multistage matrix multiply
-  using ThreadblockMma = cutlass::gemm::threadblock::MmaLayernormMainloopFusionMultistage<
+  using ThreadblockMma = cutlass::gemm::threadblock::MmaMultistage<
       typename MmaCore::Shape, IteratorA, typename MmaCore::SmemIteratorA,
       MmaCore::kCacheOpA, IteratorB, typename MmaCore::SmemIteratorB,
-      MmaCore::kCacheOpB, IteratorVarMean, IteratorGammaBeta, SmemIteratorGammaBeta,
-      CacheOpGammaBeta,
-      ElementAccumulator, layout::RowMajor,
-      typename MmaCore::MmaPolicy, WarpIteratorGammaBeta, Stages, SharedMemoryClear>;
+      MmaCore::kCacheOpB, ElementAccumulator, layout::RowMajor,
+      typename MmaCore::MmaPolicy, Stages>;
 };
 
-////////////////////////////////////////////////////////////////////////////////
-
-} // namespace threadblock
-} // namespace gemm
-} // namespace cutlass 
+}  // namespace threadblock
+}  // namespace gemm
+}  // namespace cutlass
 
 ////////////////////////////////////////////////////////////////////////////////
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/default_mma_planar_complex_multistage.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/default_mma_planar_complex_multistage.h`

 * *Files 2% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/default_mma_planar_complex_pipelined.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/default_mma_planar_complex_pipelined.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/default_mma_softmax_mainloop_fusion.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/default_mma_with_reduction.h`

 * *Files 11% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -25,31 +25,27 @@
  * SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER
  * CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,
  * OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
  * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
  *
  **************************************************************************************************/
 /*! \file
-    \brief Template for a pipelined softmax-GEMM kernel.
+    \brief Template for a pipelined GEMM kernel. Does not compute batching or support split-K.
 */
 
 #pragma once
 
 #include "cutlass/cutlass.h"
 #include "cutlass/numeric_types.h"
 #include "cutlass/arch/arch.h"
 
 #include "cutlass/layout/matrix.h"
-#include "cutlass/gemm/threadblock/default_mma_core.h"
-#include "cutlass/gemm/threadblock/mma_softmax_mainloop_fusion_multistage.h"
-#include "cutlass/transform/threadblock/predicated_scale_bias_vector_iterator.h"
-#include "cutlass/transform/threadblock/predicated_scale_bias_vector_access_iterator.h"
-#include "cutlass/transform/threadblock/regular_scale_bias_vector_access_iterator.h"
-#include "cutlass/gemm/warp/scale_bias_tile_iterator.h"
 #include "cutlass/transform/threadblock/predicated_tile_iterator.h"
+#include "cutlass/transform/threadblock/predicated_tile_iterator_2dthreadtile.h"
+#include "cutlass/gemm/threadblock/default_mma_core_with_reduction.h"
 
 ////////////////////////////////////////////////////////////////////////////////
 
 namespace cutlass {
 namespace gemm {
 namespace threadblock {
 
@@ -64,64 +60,57 @@
     int kAlignmentA,
     /// Element type for B matrix operand
     typename ElementB,
     /// Layout type for B matrix operand
     typename LayoutB,
     /// Access granularity of B matrix in units of elements
     int kAlignmentB,
-    /// Element type for Scale/Bias vectors
-    typename ElementScaleBias,
-    /// Layout type for Scale/Bias vectors
-    typename LayoutScaleBias,
     /// Element type for internal accumulation
     typename ElementAccumulator,
     /// Layout type for C and D matrix operands
     typename LayoutC,
     /// Operator class tag
     typename OperatorClass,
+    ///                                                                                               
+    bool ReduceKForA_,
     /// Tag indicating architecture to tune for
     typename ArchTag,
     /// Threadblock-level tile size (concept: GemmShape)
     typename ThreadblockShape,
     /// Warp-level tile size (concept: GemmShape)
     typename WarpShape,
     /// Instruction-level tile size (concept: GemmShape)
     typename InstructionShape,
     /// Number of stages used in the pipelined mainloop
     int Stages,
-    /// Whether problem has been transformed. This determines to which operand
-    /// the softmax is applied.
-    bool InternalTranspose,
     /// Operation perfomed by GEMM
     typename Operator,
     /// Store the accumulators in row major or column major.  Row major is used
     /// when output layout is interleaved.
     bool AccumulatorsInRowMajor = false,
     /// Use zfill or predicate for SM80 out-of-bound cp.async 
     SharedMemoryClearOption SharedMemoryClear = SharedMemoryClearOption::kNone
     >
-struct DefaultMmaSoftmaxMainloopFusion {
+struct DefaultMmaWithReduction {
 
   static cutlass::arch::CacheOperation::Kind const CacheOpA =
       ((sizeof_bits<ElementA>::value * kAlignmentA) == 128)
           ? cutlass::arch::CacheOperation::Global
           : cutlass::arch::CacheOperation::Always;
 
   static cutlass::arch::CacheOperation::Kind const CacheOpB =
       ((sizeof_bits<ElementB>::value * kAlignmentB) == 128)
           ? cutlass::arch::CacheOperation::Global
           : cutlass::arch::CacheOperation::Always;
 
-  static cutlass::arch::CacheOperation::Kind const CacheOpGammaBeta = CacheOpA;
-
   // Define the MmaCore components
-  using MmaCore = typename cutlass::gemm::threadblock::DefaultMmaCore<
+  using MmaCore = typename cutlass::gemm::threadblock::DefaultMmaWithReductionCore<
       ThreadblockShape, WarpShape, InstructionShape, ElementA, LayoutA,
       ElementB, LayoutB, ElementAccumulator, layout::RowMajor, arch::OpClassTensorOp,
-      Stages, Operator, false, CacheOpA, CacheOpB>;
+      ReduceKForA_,  Stages, Operator, false, CacheOpA, CacheOpB>;
 
   // Define iterators over tiles from the A operand
   using ThreadMapA = typename MmaCore::IteratorThreadMapA;
   using AccessTypeA = cutlass::Array<ElementA, kAlignmentA>;
   using IteratorA =
       cutlass::transform::threadblock::PredicatedTileAccessIterator<
           cutlass::MatrixShape<ThreadblockShape::kM, ThreadblockShape::kK>,
@@ -131,28 +120,20 @@
   using ThreadMapB = typename MmaCore::IteratorThreadMapB;
   using AccessTypeB = cutlass::Array<ElementB, kAlignmentB>;
   using IteratorB =
       cutlass::transform::threadblock::PredicatedTileAccessIterator<
           cutlass::MatrixShape<ThreadblockShape::kK, ThreadblockShape::kN>,
           ElementB, LayoutB, 0, ThreadMapB, AccessTypeB>;
 
-  /// Define iterators over tiles from scale/bias vectors
-  using IteratorNormSum =
-      cutlass::transform::threadblock::PredicatedScaleBiasVectorIterator<
-          cutlass::MatrixShape<1, WarpShape::kN>,
-          ElementScaleBias,
-          LayoutScaleBias>;
-
   // Define the threadblock-scoped multistage matrix multiply
-  using ThreadblockMma = cutlass::gemm::threadblock::MmaSoftmaxMainloopFusionMultistage<
+  using ThreadblockMma = cutlass::gemm::threadblock::MmaWithReductionMultistage<
       typename MmaCore::Shape, IteratorA, typename MmaCore::SmemIteratorA,
       MmaCore::kCacheOpA, IteratorB, typename MmaCore::SmemIteratorB,
-      MmaCore::kCacheOpB, IteratorNormSum,
-      ElementAccumulator, layout::RowMajor,
-      typename MmaCore::MmaPolicy, Stages, InternalTranspose, SharedMemoryClear>;
+      MmaCore::kCacheOpB, ElementAccumulator, layout::RowMajor,
+      typename MmaCore::MmaPolicy, Stages, SharedMemoryClear>;
 };
 
 ////////////////////////////////////////////////////////////////////////////////
 
 } // namespace threadblock
 } // namespace gemm
 } // namespace cutlass
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/default_mma_with_reduction.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/examples/45_dual_gemm/threadblock/dual_mma_base.h`

 * *Files 18% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -25,117 +25,194 @@
  * SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER
  * CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,
  * OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
  * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
  *
  **************************************************************************************************/
 /*! \file
-    \brief Template for a pipelined GEMM kernel. Does not compute batching or support split-K.
+    \brief Template for a double-buffered threadblock-scoped GEMM kernel.
 */
 
 #pragma once
 
+#include "cutlass/aligned_buffer.h"
+#include "cutlass/arch/memory.h"
+#include "cutlass/array.h"
 #include "cutlass/cutlass.h"
+#include "cutlass/gemm/gemm.h"
+#include "cutlass/matrix_shape.h"
 #include "cutlass/numeric_types.h"
-#include "cutlass/arch/arch.h"
 
-#include "cutlass/layout/matrix.h"
-#include "cutlass/transform/threadblock/predicated_tile_iterator.h"
-#include "cutlass/transform/threadblock/predicated_tile_iterator_2dthreadtile.h"
-#include "cutlass/gemm/threadblock/default_mma_core_with_reduction.h"
+#include "cutlass/gemm/threadblock/mma_base.h"
 
 ////////////////////////////////////////////////////////////////////////////////
 
 namespace cutlass {
 namespace gemm {
 namespace threadblock {
 
 ////////////////////////////////////////////////////////////////////////////////
 
+/// Structure to compute the matrix product targeting CUDA cores and SIMT math
+/// instructions.
 template <
-    /// Element type for A matrix operand
-    typename ElementA,
-    /// Layout type for A matrix operand
-    typename LayoutA,
-    /// Access granularity of A matrix in units of elements
-    int kAlignmentA,
-    /// Element type for B matrix operand
-    typename ElementB,
-    /// Layout type for B matrix operand
-    typename LayoutB,
-    /// Access granularity of B matrix in units of elements
-    int kAlignmentB,
-    /// Element type for internal accumulation
-    typename ElementAccumulator,
-    /// Layout type for C and D matrix operands
-    typename LayoutC,
-    /// Operator class tag
-    typename OperatorClass,
-    ///                                                                                               
-    bool ReduceKForA_,
-    /// Tag indicating architecture to tune for
-    typename ArchTag,
-    /// Threadblock-level tile size (concept: GemmShape)
-    typename ThreadblockShape,
-    /// Warp-level tile size (concept: GemmShape)
-    typename WarpShape,
-    /// Instruction-level tile size (concept: GemmShape)
-    typename InstructionShape,
-    /// Number of stages used in the pipelined mainloop
+    /// Size of the Gemm problem - concept: gemm::GemmShape<>
+    typename Shape_,
+    /// Policy describing tuning details (concept: MmaPolicy)
+    typename Policy_,
+    /// Number of stages,
     int Stages,
-    /// Operation perfomed by GEMM
-    typename Operator,
-    /// Store the accumulators in row major or column major.  Row major is used
-    /// when output layout is interleaved.
-    bool AccumulatorsInRowMajor = false,
-    /// Use zfill or predicate for SM80 out-of-bound cp.async 
-    SharedMemoryClearOption SharedMemoryClear = SharedMemoryClearOption::kNone
-    >
-struct DefaultMmaWithReduction {
-
-  static cutlass::arch::CacheOperation::Kind const CacheOpA =
-      ((sizeof_bits<ElementA>::value * kAlignmentA) == 128)
-          ? cutlass::arch::CacheOperation::Global
-          : cutlass::arch::CacheOperation::Always;
-
-  static cutlass::arch::CacheOperation::Kind const CacheOpB =
-      ((sizeof_bits<ElementB>::value * kAlignmentB) == 128)
-          ? cutlass::arch::CacheOperation::Global
-          : cutlass::arch::CacheOperation::Always;
-
-  // Define the MmaCore components
-  using MmaCore = typename cutlass::gemm::threadblock::DefaultMmaWithReductionCore<
-      ThreadblockShape, WarpShape, InstructionShape, ElementA, LayoutA,
-      ElementB, LayoutB, ElementAccumulator, layout::RowMajor, arch::OpClassTensorOp,
-      ReduceKForA_,  Stages, Operator, false, CacheOpA, CacheOpB>;
-
-  // Define iterators over tiles from the A operand
-  using ThreadMapA = typename MmaCore::IteratorThreadMapA;
-  using AccessTypeA = cutlass::Array<ElementA, kAlignmentA>;
-  using IteratorA =
-      cutlass::transform::threadblock::PredicatedTileAccessIterator<
-          cutlass::MatrixShape<ThreadblockShape::kM, ThreadblockShape::kK>,
-          ElementA, LayoutA, 1, ThreadMapA, AccessTypeA>;
-
-  // Define iterators over tiles from the B operand
-  using ThreadMapB = typename MmaCore::IteratorThreadMapB;
-  using AccessTypeB = cutlass::Array<ElementB, kAlignmentB>;
-  using IteratorB =
-      cutlass::transform::threadblock::PredicatedTileAccessIterator<
-          cutlass::MatrixShape<ThreadblockShape::kK, ThreadblockShape::kN>,
-          ElementB, LayoutB, 0, ThreadMapB, AccessTypeB>;
-
-  // Define the threadblock-scoped multistage matrix multiply
-  using ThreadblockMma = cutlass::gemm::threadblock::MmaWithReductionMultistage<
-      typename MmaCore::Shape, IteratorA, typename MmaCore::SmemIteratorA,
-      MmaCore::kCacheOpA, IteratorB, typename MmaCore::SmemIteratorB,
-      MmaCore::kCacheOpB, ElementAccumulator, layout::RowMajor,
-      typename MmaCore::MmaPolicy, Stages, SharedMemoryClear>;
+    /// Used for partial specialization
+    typename Enable = bool>
+class DualMmaBase {
+ public:
+  ///< Size of the Gemm problem - concept: gemm::GemmShape<>
+  using Shape = Shape_;
+
+  ///< Policy describing tuning details
+  using Policy = Policy_;
+
+  //
+  // Dependent types
+  //
+
+  /// Warp-level Mma
+  using Operator = typename Policy::Operator;
+
+  /// Shape describing the overall GEMM computed from shared memory
+  /// by each warp.
+  using WarpGemm = typename Policy::Operator::Shape;
+
+  /// Shape describing the number of warps filling the CTA
+  using WarpCount = GemmShape<Shape::kM / WarpGemm::kM,
+                              Shape::kN / WarpGemm::kN,
+                              Shape::kK / WarpGemm::kK>;
+
+  /// Number of warp-level GEMM oeprations
+  static int const kWarpGemmIterations =
+      (WarpGemm::kK / Operator::Policy::MmaShape::kK);
+
+  /// Number of stages
+  static int const kStages = Stages;
+
+  /// Tensor reference to the A operand
+  using TensorRefA = TensorRef<typename Operator::ElementA, typename Operator::LayoutA>;
+
+  /// Tensor reference to the B operand
+  using TensorRefB = TensorRef<typename Operator::ElementB, typename Operator::LayoutB>;
+
+  static_assert(kWarpGemmIterations > 1,
+                "The pipelined structure requires at least two warp-level "
+                "GEMM operations.");
+
+  static_assert((kWarpGemmIterations % 2) == 0,
+                "Inner loop iteration must be an even number.");
+
+  //
+  // Nested structs
+  //
+
+  /// Shared storage object needed by threadblock-scoped GEMM
+  class SharedStorage {
+   public:
+    //
+    // Type definitions
+    //
+
+    /// Shape of the A matrix operand in shared memory
+    using ShapeA = MatrixShape<Shape::kM + Policy::SmemPaddingA::kRow,
+                               Shape::kK * kStages +
+                                   Policy::SmemPaddingA::kColumn>;
+
+    /// Shape of the B matrix operand in shared memory
+    using ShapeB =
+        MatrixShape<Shape::kK * kStages + Policy::SmemPaddingB::kRow,
+                    Shape::kN + Policy::SmemPaddingB::kColumn>;
+
+   public:
+    //
+    // Data members
+    //
+
+    /// Buffer for A operand
+    AlignedBuffer<typename Operator::ElementA, ShapeA::kCount> operand_A;
+
+    /// Buffer for B operand
+    AlignedBuffer<typename Operator::ElementB, ShapeB::kCount> operand_B0;
+    AlignedBuffer<typename Operator::ElementB, ShapeB::kCount> operand_B1;
+
+   public:
+
+    //
+    // Methods
+    //
+
+    /// Returns a layout object for the A matrix
+    CUTLASS_DEVICE
+    static typename Operator::LayoutA LayoutA() {
+      return Operator::LayoutA::packed({ShapeA::kRow, ShapeA::kColumn});
+    }
+
+    /// Returns a layout object for the B matrix
+    CUTLASS_HOST_DEVICE
+    static typename Operator::LayoutB LayoutB() {
+      return Operator::LayoutB::packed({ShapeB::kRow, ShapeB::kColumn});
+    }
+
+    /// Returns a TensorRef to the A operand
+    CUTLASS_HOST_DEVICE
+    TensorRefA operand_A_ref() {
+      return TensorRefA{operand_A.data(), LayoutA()};
+    }
+
+    /// Returns a TensorRef to the B operand
+    CUTLASS_HOST_DEVICE
+    TensorRefB operand_B0_ref() {
+      return TensorRefB{operand_B0.data(), LayoutB()};
+    }
+    CUTLASS_HOST_DEVICE
+    TensorRefB operand_B1_ref() {
+      return TensorRefB{operand_B1.data(), LayoutB()};
+    }
+  };
+
+ protected:
+
+  //
+  // Data members
+  //
+
+  /// Iterator to load a warp-scoped tile of A operand from shared memory
+  typename Operator::IteratorA warp_tile_iterator_A_;
+
+  /// Iterator to load a warp-scoped tile of B operand from shared memory
+  typename Operator::IteratorB warp_tile_iterator_B0_;
+  typename Operator::IteratorB warp_tile_iterator_B1_;
+
+public:
+
+  /// Construct from tensor references
+  CUTLASS_DEVICE
+  DualMmaBase(
+      ///< Shared storage needed for internal use by threadblock-scoped GEMM
+      SharedStorage &shared_storage,
+      ///< ID within the threadblock
+      int thread_idx,
+      ///< ID of warp
+      int warp_idx,
+      ///< ID of each thread within a warp
+      int lane_idx
+    ):
+      warp_tile_iterator_A_(shared_storage.operand_A_ref(), lane_idx),
+      warp_tile_iterator_B0_(shared_storage.operand_B0_ref(), lane_idx),
+      warp_tile_iterator_B1_(shared_storage.operand_B1_ref(), lane_idx) {
+
+  }
 };
 
-////////////////////////////////////////////////////////////////////////////////
+/////////////////////////////////////////////////////////////////////////////////////////////////
 
-} // namespace threadblock
-} // namespace gemm
-} // namespace cutlass 
+}  // namespace threadblock
+}  // namespace gemm
+}  // namespace cutlass
 
-////////////////////////////////////////////////////////////////////////////////
+/////////////////////////////////////////////////////////////////////////////////////////////////
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/default_multistage_mma_complex_core.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/default_multistage_mma_complex_core.h`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/default_multistage_mma_complex_core_sm80.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/default_multistage_mma_complex_core_sm80.h`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -596,15 +596,15 @@
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
 /// Partial specialization for complex floating-point
 ///
 ///   A: column-major
 ///   B: column-major
 ///   Operator: arch::OpMultiplyAddComplex
-///   Math Instruction: MMA.1688.F32.TF32
+///   Math Instruction: mma.sync.aligned.m16n8k8.f32.tf32.tf32.f32
 ///
 /// This uses the default warp-level operator given tile sizes
 template <
     /// Shape of threadblock-scoped matrix multiply operator (concept:
     /// GemmShape)
     typename Shape_,
     /// Shape of warp-level matrix multiply operator (concept: GemmShape)
@@ -726,15 +726,15 @@
 
 
 /// Partial specialization for complex floating-point
 ///
 ///   A: column-major
 ///   B: row-major
 ///   Operator: arch::OpMultiplyAddComplex
-///   Math Instruction: MMA.1688.F32.TF32
+///   Math Instruction: mma.sync.aligned.m16n8k8.f32.tf32.tf32.f32
 ///
 /// This uses the default warp-level operator given tile sizes
 template <
     /// Shape of threadblock-scoped matrix multiply operator (concept:
     /// GemmShape)
     typename Shape_,
     /// Shape of warp-level matrix multiply operator (concept: GemmShape)
@@ -857,15 +857,15 @@
 ////////////////////////////////////////////////////////////////////////////////
 
 /// Partial specialization for complex floating-point
 ///
 ///   A: row-major
 ///   B: column-major
 ///   Operator: arch::OpMultiplyAddComplex
-///   Math Instruction: MMA.1688.F32.TF32
+///   Math Instruction: mma.sync.aligned.m16n8k8.f32.tf32.tf32.f32
 ///
 /// This uses the default warp-level operator given tile sizes
 template <
     /// Shape of threadblock-scoped matrix multiply operator (concept:
     /// GemmShape)
     typename Shape_,
     /// Shape of warp-level matrix multiply operator (concept: GemmShape)
@@ -988,15 +988,15 @@
 ////////////////////////////////////////////////////////////////////////////////
 
 /// Partial specialization for complex floating-point
 ///
 ///   A: row-major
 ///   B: row-major
 ///   Operator: arch::OpMultiplyAddComplex
-///   Math Instruction: MMA.1688.F32.TF32
+///   Math Instruction: mma.sync.aligned.m16n8k8.f32.tf32.tf32.f32
 ///
 /// This uses the default warp-level operator given tile sizes
 template <
     /// Shape of threadblock-scoped matrix multiply operator (concept:
     /// GemmShape)
     typename Shape_,
     /// Shape of warp-level matrix multiply operator (concept: GemmShape)
@@ -1114,18 +1114,18 @@
   /// Policy used to define MmaPipelined
   using MmaPolicy = MmaPolicy<MmaTensorOp, MatrixShape<0, 0>,
                                         MatrixShape<0, 0>, WarpCount::kK>;
 };
 
 ////////////////////////////////////////////////////////////////////////////////
 
-/// Partial specialization for complex double-precision
+/// Partial specialization for complex SIMT operation
 ///
 ///   A: column-major
-///   B: row-major
+///   B: column-major
 ///   Operator: arch::OpMultiplyAddComplex or arch::OpMultiplyGaussianComplex
 ///
 /// This uses the default warp-level operator given tile sizes
 template <
     /// Shape of threadblock-scoped matrix multiply operator (concept:
     /// GemmShape)
     typename Shape_,
@@ -1263,33 +1263,36 @@
   using Policy = cutlass::gemm::warp::MmaSimtPolicy<
       cutlass::MatrixShape<WarpNumThreadsM, WarpNumThreadsN>,   // WarpShape
       cutlass::layout::RowMajorInterleaved<LaneLayout>,         // LaneLayout
       LaneMmaShape
   >;
 
   using MmaWarpSimt = cutlass::gemm::warp::MmaSimt<
-    WarpShape, /// Size of the Gemm problem - concept: gemm::GemmShape<> 128, 128, 8
-    ElementA,  /// Data type of A elements
-    SmemLayoutA,   /// Layout of A matrix (concept: MatrixLayout)
-    ElementB,  /// Data type of B elements
-    SmemLayoutB,   /// Layout of B matrix (concept: MatrixLayout)
-    ElementC,  /// Element type of C matrix
-    LayoutC,   /// Layout of C matrix (concept: MatrixLayout)
-    Policy     /// Policy describing warp-level MmaTensorOp (concept: MmaTensorOp policy)
-    >;         /// Used for partial specialization
+    WarpShape,    /// Size of the Gemm problem - concept: gemm::GemmShape<> 128, 128, 8
+    ElementA,     /// Data type of A elements
+    SmemLayoutA,  /// Layout of A matrix (concept: MatrixLayout)
+    ElementB,     /// Data type of B elements
+    SmemLayoutB,  /// Layout of B matrix (concept: MatrixLayout)
+    ElementC,     /// Element type of C matrix
+    LayoutC,      /// Layout of C matrix (concept: MatrixLayout)
+    Policy,       /// Policy describing warp-level MmaTensorOp (concept: MmaTensorOp policy)
+    1,            /// 1 partition along K dimension
+    kTransformA,  /// Transform for A
+    kTransformB   /// Transform for B
+    >;            /// Used for partial specialization
 
   /// Policy used to define MmaPipelined
   using MmaPolicy = MmaPolicy<
     MmaWarpSimt,
     MatrixShape<0, 0>,
     MatrixShape<0, Shape::kK / 32>,
     WarpCount::kK>;
 };
 
-/// Partial specialization for complex double-precision
+/// Partial specialization for complex SIMT operation
 ///
 ///   A: column-major
 ///   B: row-major
 ///   Operator: arch::OpMultiplyAddComplex or arch::OpMultiplyGaussianComplex
 ///
 /// This uses the default warp-level operator given tile sizes
 template <
@@ -1427,36 +1430,39 @@
   using Policy = cutlass::gemm::warp::MmaSimtPolicy<
       cutlass::MatrixShape<WarpNumThreadsM, WarpNumThreadsN>,   // WarpShape
       cutlass::layout::RowMajorInterleaved<LaneLayout>,         // LaneLayout
       LaneMmaShape
   >;
 
   using MmaWarpSimt = cutlass::gemm::warp::MmaSimt<
-    WarpShape, /// Size of the Gemm problem - concept: gemm::GemmShape<> 128, 128, 8
-    ElementA,  /// Data type of A elements
-    SmemLayoutA,   /// Layout of A matrix (concept: MatrixLayout)
-    ElementB,  /// Data type of B elements
-    SmemLayoutB,   /// Layout of B matrix (concept: MatrixLayout)
-    ElementC,  /// Element type of C matrix
-    LayoutC,   /// Layout of C matrix (concept: MatrixLayout)
-    Policy     /// Policy describing warp-level MmaTensorOp (concept: MmaTensorOp policy)
-    >;         /// Used for partial specialization
+    WarpShape,    /// Size of the Gemm problem - concept: gemm::GemmShape<> 128, 128, 8
+    ElementA,     /// Data type of A elements
+    SmemLayoutA,  /// Layout of A matrix (concept: MatrixLayout)
+    ElementB,     /// Data type of B elements
+    SmemLayoutB,  /// Layout of B matrix (concept: MatrixLayout)
+    ElementC,     /// Element type of C matrix
+    LayoutC,      /// Layout of C matrix (concept: MatrixLayout)
+    Policy,       /// Policy describing warp-level MmaTensorOp (concept: MmaTensorOp policy)
+    1,            /// 1 partition along K dimension
+    kTransformA,  /// Transform for A
+    kTransformB   /// Transform for B
+    >;            /// Used for partial specialization
 
   /// Policy used to define MmaPipelined
   using MmaPolicy = MmaPolicy<
     MmaWarpSimt,
     MatrixShape<0, 0>,
     MatrixShape<0, 0>,    // or Shape::kK / 32
     WarpCount::kK>;
 };
 
-/// Partial specialization for complex double-precision
+/// Partial specialization for complex SIMT operation
 ///
-///   A: column-major
-///   B: row-major
+///   A: row-major
+///   B: column-major
 ///   Operator: arch::OpMultiplyAddComplex or arch::OpMultiplyGaussianComplex
 ///
 /// This uses the default warp-level operator given tile sizes
 template <
     /// Shape of threadblock-scoped matrix multiply operator (concept:
     /// GemmShape)
     typename Shape_,
@@ -1597,35 +1603,38 @@
   using Policy = cutlass::gemm::warp::MmaSimtPolicy<
       cutlass::MatrixShape<WarpNumThreadsM, WarpNumThreadsN>,   // WarpShape
       cutlass::layout::RowMajorInterleaved<LaneLayout>,         // LaneLayout
       LaneMmaShape
   >;
 
   using MmaWarpSimt = cutlass::gemm::warp::MmaSimt<
-    WarpShape, /// Size of the Gemm problem - concept: gemm::GemmShape<> 128, 128, 8
-    ElementA,  /// Data type of A elements
-    SmemLayoutA,   /// Layout of A matrix (concept: MatrixLayout)
-    ElementB,  /// Data type of B elements
-    SmemLayoutB,   /// Layout of B matrix (concept: MatrixLayout)
-    ElementC,  /// Element type of C matrix
-    LayoutC,   /// Layout of C matrix (concept: MatrixLayout)
-    Policy     /// Policy describing warp-level MmaTensorOp (concept: MmaTensorOp policy)
-    >;         /// Used for partial specialization
+    WarpShape,    /// Size of the Gemm problem - concept: gemm::GemmShape<> 128, 128, 8
+    ElementA,     /// Data type of A elements
+    SmemLayoutA,  /// Layout of A matrix (concept: MatrixLayout)
+    ElementB,     /// Data type of B elements
+    SmemLayoutB,  /// Layout of B matrix (concept: MatrixLayout)
+    ElementC,     /// Element type of C matrix
+    LayoutC,      /// Layout of C matrix (concept: MatrixLayout)
+    Policy,       /// Policy describing warp-level MmaTensorOp (concept: MmaTensorOp policy)
+    1,            /// 1 partition along K dimension
+    kTransformA,  /// Transform for A
+    kTransformB   /// Transform for B
+    >;            /// Used for partial specialization
 
   /// Policy used to define MmaPipelined
   using MmaPolicy = MmaPolicy<
     MmaWarpSimt,
     MatrixShape<Shape::kK / 32, 0>,
     MatrixShape<0, Shape::kK / 32>,
     WarpCount::kK>;
 };
 
-/// Partial specialization for complex double-precision
+/// Partial specialization for complex SIMT operation
 ///
-///   A: column-major
+///   A: row-major
 ///   B: row-major
 ///   Operator: arch::OpMultiplyAddComplex or arch::OpMultiplyGaussianComplex
 ///
 /// This uses the default warp-level operator given tile sizes
 template <
     /// Shape of threadblock-scoped matrix multiply operator (concept:
     /// GemmShape)
@@ -1764,23 +1773,26 @@
   using Policy = cutlass::gemm::warp::MmaSimtPolicy<
       cutlass::MatrixShape<WarpNumThreadsM, WarpNumThreadsN>,   // WarpShape
       cutlass::layout::RowMajorInterleaved<LaneLayout>,         // LaneLayout
       LaneMmaShape
   >;
 
   using MmaWarpSimt = cutlass::gemm::warp::MmaSimt<
-    WarpShape, /// Size of the Gemm problem - concept: gemm::GemmShape<> 128, 128, 8
-    ElementA,  /// Data type of A elements
-    SmemLayoutA,   /// Layout of A matrix (concept: MatrixLayout)
-    ElementB,  /// Data type of B elements
-    SmemLayoutB,   /// Layout of B matrix (concept: MatrixLayout)
-    ElementC,  /// Element type of C matrix
-    LayoutC,   /// Layout of C matrix (concept: MatrixLayout)
-    Policy     /// Policy describing warp-level MmaTensorOp (concept: MmaTensorOp policy)
-    >;         /// Used for partial specialization
+    WarpShape,    /// Size of the Gemm problem - concept: gemm::GemmShape<> 128, 128, 8
+    ElementA,     /// Data type of A elements
+    SmemLayoutA,  /// Layout of A matrix (concept: MatrixLayout)
+    ElementB,     /// Data type of B elements
+    SmemLayoutB,  /// Layout of B matrix (concept: MatrixLayout)
+    ElementC,     /// Element type of C matrix
+    LayoutC,      /// Layout of C matrix (concept: MatrixLayout)
+    Policy,       /// Policy describing warp-level MmaTensorOp (concept: MmaTensorOp policy)
+    1,            /// 1 partition along K dimension
+    kTransformA,  /// Transform for A
+    kTransformB   /// Transform for B
+    >;            /// Used for partial specialization
 
   /// Policy used to define MmaPipelined
   using MmaPolicy = MmaPolicy<
     MmaWarpSimt,
     MatrixShape<Shape::kK / 32, 0>,
     MatrixShape<0, 0>,    // or Shape::kK / 32
     WarpCount::kK>;
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/default_multistage_trmm_complex.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/default_multistage_trmm_complex.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/default_sparse_mma.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/gemm/warp/mma_tensor_op_sm70.h`

 * *Files 24% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -25,172 +25,256 @@
  * SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER
  * CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,
  * OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
  * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
  *
  **************************************************************************************************/
 /*! \file
-    \brief Template for a pipelined GEMM kernel. Does not compute batching or support split-K.
+    \brief Templates implementing warp-level matrix multiply-accumulate operations targeting
+      Tensor Cores.
+
+    This is a work in progress.
 */
 
 #pragma once
 
 #include "cutlass/cutlass.h"
+#include "cutlass/array.h"
+
 #include "cutlass/numeric_types.h"
-#include "cutlass/arch/arch.h"
-#include "cutlass/arch/wmma.h"
+#include "cutlass/matrix_shape.h"
+
+#include "cutlass/arch/mma.h"
 
-#include "cutlass/layout/matrix.h"
-#include "cutlass/transform/threadblock/predicated_tile_iterator.h"
-#include "cutlass/transform/threadblock/predicated_tile_iterator_2dthreadtile.h"
-#include "cutlass/gemm/threadblock/default_mma_core_sm70.h"
-#include "cutlass/gemm/threadblock/default_mma_core_sm75.h"
-#include "cutlass/gemm/threadblock/default_mma_core_sm80.h"
-#include "cutlass/gemm/threadblock/default_mma_core_sparse_sm80.h"
-#if defined(CUTLASS_ARCH_WMMA_ENABLED)
-#include "cutlass/gemm/threadblock/default_mma_core_wmma.h"
-#endif //CUTLASS_ARCH_WMMA_ENABLED
+#include "cutlass/gemm/gemm.h"
+#include "cutlass/gemm/warp/mma.h"
 
-////////////////////////////////////////////////////////////////////////////////
+#include "cutlass/gemm/warp/mma_tensor_op_policy.h"
+#include "cutlass/gemm/warp/mma_tensor_op_tile_iterator_sm70.h"
+
+/////////////////////////////////////////////////////////////////////////////////////////////////
 
 namespace cutlass {
 namespace gemm {
-namespace threadblock {
+namespace warp {
 
-////////////////////////////////////////////////////////////////////////////////
+/////////////////////////////////////////////////////////////////////////////////////////////////
 
+/// Structure to compute the matrix product targeting CUDA cores and SIMT math instructions.
 template <
-    /// Element type for A matrix operand
-    typename ElementA_,
-    /// Layout type for A matrix operand
-    typename LayoutA_,
-    /// Access granularity of A matrix in units of elements
-    int kAlignmentA,
-    /// Element type for B matrix operand
-    typename ElementB_,
-    /// Layout type for B matrix operand
-    typename LayoutB_,
-    /// Access granularity of B matrix in units of elements
-    int kAlignmentB,
-    /// Element type for internal accumulation
-    typename ElementAccumulator_,
-    /// Layout type for C and D matrix operands
-    typename LayoutC_,
-    /// Operator class tag
-    typename OperatorClass_,
-    /// Tag indicating architecture to tune for
-    typename ArchTag_,
-    /// Threadblock-level tile size (concept: GemmShape)
-    typename ThreadblockShape_,
-    /// Warp-level tile size (concept: GemmShape)
-    typename WarpShape_,
-    /// Instruction-level tile size (concept: GemmShape)
-    typename InstructionShape_,
-    /// Number of stages used in the pipelined mainloop
-    int Stages,
-    /// Operation perfomed by GEMM
-    typename Operator,
-    /// Store the accumulators in row major or column major.  Row major is used
-    /// when output layout is interleaved.
-    bool AccumulatorsInRowMajor = false
-    >
-struct DefaultSparseMma;
+  /// Size of the Gemm problem - concept: gemm::GemmShape<>
+  typename Shape_,
+  /// Data type of A elements
+  typename ElementA_,
+  /// Layout of A matrix (concept: MatrixLayout)
+  typename LayoutA_,
+  /// Data type of B elements
+  typename ElementB_,
+  /// Layout of B matrix (concept: MatrixLayout)
+  typename LayoutB_,
+  /// Element type of C matrix
+  typename ElementC_,
+  /// Layout of C matrix (concept: MatrixLayout)
+  typename LayoutC_,
+  /// Policy describing warp-level MmaTensorOp (concept: MmaTensorOp policy)
+  typename Policy_,
+  /// Used for partial specialization
+  typename Enable = bool
+>
+class MmaVoltaTensorOp {
+public:
+  /// Shape of warp-level matrix operation (concept: GemmShape)
+  using Shape = Shape_;
+
+  /// Data type of multiplicand A
+  using ElementA = ElementA_;
+
+  /// Layout of multiplicand A
+  using LayoutA = LayoutA_;
+
+  /// Data type of multiplicand B
+  using ElementB = ElementB_;
+
+  /// Layout of multiplicand B
+  using LayoutB = LayoutB_;
+
+  /// Data type of accumulator matrix C
+  using ElementC = ElementC_;
+
+  /// Layout of accumulator matrix C
+  using LayoutC = LayoutC_;
+
+  /// Shape of the warp in units of thread (concept: MmaLanePolicySimt)
+  using Policy = Policy_;
+
+  /// Indicates class of matrix operator
+  using OperatorClass = arch::OpClassTensorOp;
 
-////////////////////////////////////////////////////////////////////////////////
+  /// Architecture tag
+  using ArchTag = arch::Sm70;
 
-/// Specialization for row-major output (OperatorClass TensorOp)
-template <
-    /// Element type for A matrix operand
-    typename ElementA,
-    /// Layout type for A matrix operand
-    typename LayoutA,
-    /// Access granularity of A matrix in units of elements
-    int kAlignmentA,
-    /// Element type for B matrix operand
-    typename ElementB,
-    /// Layout type for B matrix operand
-    typename LayoutB,
-    /// Access granularity of B matrix in units of elements
-    int kAlignmentB,
-    /// Element type for internal accumulation
-    typename ElementAccumulator,
-    /// Tag indicating architecture to tune for
-    typename ArchTag,
-    /// Threadblock-level tile size (concept: GemmShape)
-    typename ThreadblockShape,
-    /// Warp-level tile size (concept: GemmShape)
-    typename WarpShape,
-    /// Instruction-level tile size (concept: GemmShape)
-    typename InstructionShape,
-    /// Number of stages used in the multistage mainloop
-    int Stages,
-    /// Operation perfomed by GEMM
-    typename Operator
-    >
-struct DefaultSparseMma<ElementA, LayoutA, kAlignmentA, ElementB, LayoutB,
-                  kAlignmentB, ElementAccumulator, layout::RowMajor,
-                  arch::OpClassTensorOp, ArchTag, ThreadblockShape, WarpShape,
-                  InstructionShape, Stages, Operator, false> {
-  static cutlass::arch::CacheOperation::Kind const CacheOpA =
-      ((sizeof_bits<ElementA>::value * kAlignmentA) == 128)
-          ? cutlass::arch::CacheOperation::Global
-          : cutlass::arch::CacheOperation::Always;
-
-  static cutlass::arch::CacheOperation::Kind const CacheOpB =
-      ((sizeof_bits<ElementB>::value * kAlignmentB) == 128)
-          ? cutlass::arch::CacheOperation::Global
-          : cutlass::arch::CacheOperation::Always;
+  /// Underlying matrix multiply operator (concept: arch::Mma)
+  using ArchMmaOperator = typename Policy::Operator;
+
+  /// Indicates math operator 
+  using MathOperator = typename ArchMmaOperator::Operator;
   
+  /// Underlying instruction shape
+  using InstructionShape = typename ArchMmaOperator::Shape;
+
+  /// Complex transform on A operand
+  static ComplexTransform const kTransformA = ComplexTransform::kNone;
+
+  /// Complex transform on B operand
+  static ComplexTransform const kTransformB = ComplexTransform::kNone;
 
-  // Define the MmaCore components
-  using MmaCore = typename cutlass::gemm::threadblock::DefaultSparseMmaCore<
-      ThreadblockShape, WarpShape, InstructionShape, ElementA, LayoutA,
-      ElementB, LayoutB, ElementAccumulator, layout::RowMajor, arch::OpClassTensorOp,
-      Stages, Operator, false, CacheOpA, CacheOpB>;
-
-  static int const kSparse = MmaCore::kSparse;
-
-  // Define iterators over tiles from the A operand
-  using ThreadMapA = typename MmaCore::IteratorThreadMapA;
-  using AccessTypeA = cutlass::Array<ElementA, kAlignmentA>;
-  using IteratorA =
-      cutlass::transform::threadblock::PredicatedTileAccessIterator<
-          cutlass::MatrixShape<ThreadblockShape::kM, ThreadblockShape::kK / kSparse>,
-          ElementA, LayoutA, 1, ThreadMapA, AccessTypeA>;
-
-  // Define iterators over tiles from the B operand
-  using ThreadMapB = typename MmaCore::IteratorThreadMapB;
-  using AccessTypeB = cutlass::Array<ElementB, kAlignmentB>;
-  using IteratorB =
-      cutlass::transform::threadblock::PredicatedTileAccessIterator<
-          cutlass::MatrixShape<ThreadblockShape::kK, ThreadblockShape::kN>,
-          ElementB, LayoutB, 0, ThreadMapB, AccessTypeB>;
-
-  // Define iterators over tiles from the E operand
-  using ElementE = typename MmaCore::ElementE;
-  using LayoutE = typename MmaCore::GmemLayoutE;
-  using ThreadMapE = typename MmaCore::IteratorThreadMapE;
-  using AccessTypeE =
-      cutlass::Array<ElementE, 128 / sizeof_bits<ElementE>::value>;
-  using IteratorE =
-      cutlass::transform::threadblock::PredicatedTileAccessIterator<
-          cutlass::MatrixShape<ThreadblockShape::kM,
-                               ThreadblockShape::kK / kSparse /
-                                   MmaCore::kElementsPerElementE>,
-          ElementE, LayoutE, 1, ThreadMapE, AccessTypeE>;
-
-  // Define the threadblock-scoped multistage matrix multiply
-  using ThreadblockMma = cutlass::gemm::threadblock::SparseMmaMultistage<
-      typename MmaCore::Shape, IteratorA, typename MmaCore::SmemIteratorA,
-      MmaCore::kCacheOpA, IteratorB, typename MmaCore::SmemIteratorB,
-      MmaCore::kCacheOpB, ElementAccumulator, layout::RowMajor,
-      IteratorE, typename MmaCore::SmemIteratorE, MmaCore::kCacheOpE,
-      typename MmaCore::MmaPolicy, Stages>;
+  /// Number of threads participating in warp-level matrix product
+  static int const kThreadCount = 32;
+
+  /// interleaved 32x32 tiles
+  using InterleavedTileShape = GemmShape<32, 32, 4>;
+
+  static_assert(!(Shape::kM % InterleavedTileShape::kM) &&
+                !(Shape::kN % InterleavedTileShape::kN),
+                "Shape must be a multiple of InterleavedTileShape.");
+public:
+
+  /// Iterates over the A operand in memory
+  using IteratorA = MmaVoltaTensorOpMultiplicandTileIterator<
+    MatrixShape<Shape::kM, Shape::kK>,
+    Operand::kA,
+    ElementA,
+    LayoutA,
+    MatrixShape<
+      ArchMmaOperator::Shape::kM,
+      ArchMmaOperator::Shape::kK
+    >,
+    Policy::OpDelta::kRow,
+    kThreadCount
+  >;
+
+  /// Storage for A tile
+  using FragmentA = typename IteratorA::Fragment;
+
+  /// Iterates over the B operand in memory
+  using IteratorB = MmaVoltaTensorOpMultiplicandTileIterator<
+    MatrixShape<Shape::kK, Shape::kN>,
+    Operand::kB,
+    ElementB,
+    LayoutB,
+    MatrixShape<
+      ArchMmaOperator::Shape::kK,
+      ArchMmaOperator::Shape::kN
+    >,
+    Policy::OpDelta::kRow,
+    kThreadCount
+  >;
+
+  /// Storage for B tile
+  using FragmentB = typename IteratorB::Fragment;
+
+  /// Iterates over the C operand in memory
+  using IteratorC = MmaVoltaTensorOpAccumulatorTileIterator<
+    MatrixShape<Shape::kM, Shape::kN>,
+    ElementC,
+    LayoutC,
+    typename ArchMmaOperator::Shape,
+    typename Policy::OpDelta
+  >;
+
+  /// Storage for C tile
+  using FragmentC = typename IteratorC::Fragment;
+
+private:
+
+  static_assert(
+    !(Shape::kM % ArchMmaOperator::Shape::kM) && 
+    !(Shape::kN % ArchMmaOperator::Shape::kN),
+    "Shape of warp-level Mma must be divisible by operator shape.");
+
+  /// Number of mma operations performed
+  using MmaIterations = MatrixShape<
+    InterleavedTileShape::kM / ArchMmaOperator::Shape::kM,
+    InterleavedTileShape::kN / ArchMmaOperator::Shape::kN
+  >;
+  using TileIterations = MatrixShape<
+    Shape::kM / InterleavedTileShape::kM,
+    Shape::kN / InterleavedTileShape::kN
+  >;
+
+  // Whether matrix B is reordered
+  bool reorder_B_;
+
+public:
+
+  /// Underlying matrix multiply operator (concept: arch::Mma)
+  ArchMmaOperator mma;
+
+public:
+
+  //
+  // Methods
+  //
+  
+  /// Ctor
+  CUTLASS_DEVICE
+  MmaVoltaTensorOp() {}
+
+  /// Performs a warp-level matrix multiply-accumulate operation
+  CUTLASS_DEVICE
+  void operator()(
+    FragmentC &D, 
+    FragmentA const &A, 
+    FragmentB const &B, 
+    FragmentC const &C)  {
+
+    using MmaOperandA = typename ArchMmaOperator::FragmentA;
+    using MmaOperandB = typename ArchMmaOperator::FragmentB;
+    using MmaOperandC = typename ArchMmaOperator::FragmentC;
+
+    D = C;
+
+    MmaOperandA const *ptr_A = reinterpret_cast<MmaOperandA const *>(&A);
+    MmaOperandB const *ptr_B = reinterpret_cast<MmaOperandB const *>(&B);
+    MmaOperandC *ptr_D = reinterpret_cast<MmaOperandC *>(&D);
+
+    CUTLASS_PRAGMA_UNROLL
+    for (int outer_col = 0; outer_col < TileIterations::kColumn; ++outer_col) {
+      CUTLASS_PRAGMA_UNROLL
+      for (int inner_col = 0; inner_col < MmaIterations::kColumn; ++inner_col) {
+        CUTLASS_PRAGMA_UNROLL
+        for (int outer_row = 0; outer_row < TileIterations::kRow; ++outer_row) {
+          CUTLASS_PRAGMA_UNROLL
+
+          for (int inner_row = 0; inner_row < MmaIterations::kRow; ++inner_row) {
+      
+            int op_col = inner_col + MmaIterations::kColumn * outer_col;
+
+            // Column-major serpentine sequence to maximize reuse of A operand.
+            int inner_row_serp = inner_row;
+            int outer_row_serp = outer_row;
+            if (op_col & 1) {
+              inner_row_serp = MmaIterations::kRow - inner_row - 1;
+              outer_row_serp = TileIterations::kRow - outer_row - 1;
+            }
+            int op_row = inner_row_serp + MmaIterations::kRow * outer_row_serp;
+            int op_idx = inner_row_serp + MmaIterations::kRow * 
+                         (inner_col + MmaIterations::kColumn * 
+                          (outer_row_serp + TileIterations::kRow * outer_col));
+            mma(
+              ptr_D[op_idx],
+              ptr_A[op_row],
+              ptr_B[op_col],
+              ptr_D[op_idx]);
+
+          }
+        }
+      }
+    }
+  }
 };
 
-////////////////////////////////////////////////////////////////////////////////
+/////////////////////////////////////////////////////////////////////////////////////////////////
 
-} // namespace threadblock
+} // namespace warp
 } // namespace gemm
-} // namespace cutlass 
-
-////////////////////////////////////////////////////////////////////////////////
+} // namespace cutlass
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/default_trmm.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/default_trmm.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/ell_mma_multistage.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/ell_mma_multistage.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/ell_mma_pipelined.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/ell_mma_pipelined.h`

 * *Files 0% similar despite different names*

```diff
@@ -267,15 +267,15 @@
       iterator_A.ell_add_mask(ell_iterator.get_blocksize());
     }
     else {
       iterator_B.ell_add_mask(ell_iterator.get_blocksize());
     }
 
     // Issue loads during the first warp-level matrix multiply-add *AFTER* issuing 
-    // shared memory loads (which have the tightest latency requirement).
+    // shared memory loads (which have the tighest latency requirement).
 
     //
     // Mainloop
     //
 
     // Note: The main loop does not support Base::kWarpGemmIterations == 2.
     CUTLASS_GEMM_LOOP
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/gemv.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/gemv.h`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/index_remat.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/index_remat.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/mma_base.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/mma_base.h`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -30,14 +30,15 @@
  **************************************************************************************************/
 /*! \file
     \brief Template for a double-buffered threadblock-scoped GEMM kernel.
 */
 
 #pragma once
 
+#include "cutlass/tensor_ref.h"
 #include "cutlass/aligned_buffer.h"
 #include "cutlass/arch/memory.h"
 #include "cutlass/array.h"
 #include "cutlass/cutlass.h"
 #include "cutlass/gemm/gemm.h"
 #include "cutlass/matrix_shape.h"
 #include "cutlass/numeric_types.h"
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/mma_blas3_multistage.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/mma_blas3_multistage.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -680,15 +680,15 @@
                           arch::OpMultiplyAddFastF32>::value
       || platform::is_same<typename Operator::MathOperator,
                            arch::OpMultiplyAddComplexFastF32>::value) {
       accum = plus_accum(accum, tmp_accum); 
     }
  
     if (SharedMemoryClear == SharedMemoryClearOption::kZfill) {
-      // commit and drain all pending and predicated LDGSTS pnz from the GEMM mainloop
+      // commit and drain all pending and predicated cp.async pnz from the GEMM mainloop
       cutlass::arch::cp_async_fence();
       cutlass::arch::cp_async_wait<0>();
       __syncthreads();
     }
 
   }
 };
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/mma_layernorm_mainloop_fusion_multistage.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/mma_layernorm_mainloop_fusion_multistage.h`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -843,15 +843,15 @@
               warp_loaded_frag_A_gamma_beta[(warp_mma_k + 1) % 2]);
         }
       }
 
     }
     
     if (SharedMemoryClear == SharedMemoryClearOption::kZfill) {
-      // commit and drain all pending and predicated LDGSTS pnz from the GEMM mainloop
+      // commit and drain all pending and predicated cp.async pnz from the GEMM mainloop
       cutlass::arch::cp_async_fence();
       cutlass::arch::cp_async_wait<0>();
       __syncthreads();
     }
 
   }
 };
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/mma_multistage.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/mma_with_reduction_multistage.h`

 * *Files 5% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -83,15 +83,15 @@
     typename Policy_,
     /// Number of stages,
     int Stages,
     /// Use zfill or predicate for out-of-bound cp.async
     SharedMemoryClearOption SharedMemoryClear = SharedMemoryClearOption::kNone,
     /// Used for partial specialization
     typename Enable = bool>
-class MmaMultistage : 
+class MmaWithReductionMultistage : 
   public MmaBase<Shape_, Policy_, Stages> {
 public:
   ///< Base class
   using Base = MmaBase<Shape_, Policy_, Stages>;
   ///< Size of the Gemm problem - concept: gemm::GemmShape<>
   using Shape = Shape_;
   ///< Iterates over tiles of A operand in global memory
@@ -117,23 +117,27 @@
 
   /// Fragment of accumulator tile
   using FragmentC = typename Policy::Operator::FragmentC;
 
   /// Warp-level Mma
   using Operator = typename Policy::Operator;
 
+  using FragmentReduction = typename Operator::FragmentReduction;
+
   /// Minimum architecture is Sm80 to support cp.async
   using ArchTag = arch::Sm80;
   
   /// Complex transform on A operand
   static ComplexTransform const kTransformA = Operator::kTransformA;
 
   /// Complex transform on B operand
   static ComplexTransform const kTransformB = Operator::kTransformB;
 
+  static int const kReduceKForA = Operator::kReduceKForA;
+
   /// Internal structure exposed for introspection.
   struct Detail {
 
     /// Number of cp.async instructions to load one stage of operand A
     static int const AsyncCopyIterationsPerStageA =
         IteratorA::ThreadMap::Iterations::kCount;
 
@@ -172,15 +176,15 @@
   /// Iterator to write threadblock-scoped tile of B operand to shared memory
   SmemIteratorB smem_iterator_B_;
 
 public:
 
   /// Construct from tensor references
   CUTLASS_DEVICE
-  MmaMultistage(
+  MmaWithReductionMultistage(
       ///< Shared storage needed for internal use by threadblock-scoped GEMM
       typename Base::SharedStorage &shared_storage,
       ///< ID within the threadblock
       int thread_idx,
       ///< ID of warp
       int warp_idx,
       ///< ID of each thread within a warp
@@ -290,21 +294,22 @@
       ///< destination accumulator tile
       FragmentC &accum,
       ///< iterator over A operand in global memory
       IteratorA iterator_A,
       ///< iterator over B operand in global memory
       IteratorB iterator_B,
       ///< initial value of accumulator
-      FragmentC const &src_accum) {
+      FragmentC const &src_accum,
+      FragmentReduction &gemm_k_reduction_accum) {
 
     //
     // Prologue
     //
-
     // Issue several complete stages
+
     CUTLASS_PRAGMA_UNROLL
     for (int stage = 0; stage < Base::kStages - 1;
          ++stage, --gemm_k_iterations) {
 
       iterator_A.clear_mask(gemm_k_iterations == 0);
       iterator_B.clear_mask(gemm_k_iterations == 0);
 
@@ -372,64 +377,15 @@
       // Defines the boundary of a stage of cp.async.
       cutlass::arch::cp_async_fence();
     }
 
     // Perform accumulation in the 'd' output operand
     accum = src_accum;
 
-    //
-    // Clear the remaining tiles of SMEM. This is a functional requirement for some kernels
-    // so that all accumulator elements outside the GEMM footprint are zero.
-    //
-
-    if (SharedMemoryClear == SharedMemoryClearOption::kClearLastStage) {
-
-      /// Iterator to write threadblock-scoped tile of A operand to shared memory
-      SmemIteratorA last_smem_iterator_A(this->smem_iterator_A_);
-
-      typename IteratorA::AccessType zero_A;
-      zero_A.clear();
-
-      last_smem_iterator_A.set_iteration_index(0);
-
-      // Async Copy for operand A
-      CUTLASS_PRAGMA_UNROLL
-      for (int j = 0; j < Detail::AsyncCopyIterationsPerStageA; ++j) {
-
-        typename IteratorA::AccessType *dst_ptr =
-            reinterpret_cast<typename IteratorA::AccessType *>(
-                last_smem_iterator_A.get());
-
-        *dst_ptr = zero_A;
-
-        ++last_smem_iterator_A;
-      }
-
-      /// Iterator to write threadblock-scoped tile of B operand to shared memory
-      SmemIteratorB last_smem_iterator_B(this->smem_iterator_B_);
-      typename IteratorB::AccessType zero_B;
-
-      zero_B.clear();
-      last_smem_iterator_B.set_iteration_index(0);
-
-      // Async Copy for operand B
-      CUTLASS_PRAGMA_UNROLL
-      for (int j = 0; j < Detail::AsyncCopyIterationsPerStageB; ++j) {
-
-        typename IteratorB::AccessType *dst_ptr =
-            reinterpret_cast<typename IteratorB::AccessType *>(
-                last_smem_iterator_B.get());
-
-        *dst_ptr = zero_B;
-
-        ++last_smem_iterator_B;
-      }
-    }
-
-    // Waits until stages up to the previous (kStages-2)th stage have committed.
+    // Waits until kStages-2 stages have committed.
     cutlass::arch::cp_async_wait<Base::kStages - 2>();
     __syncthreads();
 
     // Pair of fragments used to overlap shared memory loads and math
     // instructions
     WarpLoadedFragmentA warp_loaded_frag_A[2];
     WarpLoadedFragmentB warp_loaded_frag_B[2];
@@ -452,29 +408,14 @@
 
     int smem_write_stage_idx = Base::kStages - 1;
     int smem_read_stage_idx = 0;
 
     warp_mma.transform(warp_transformed_frag_A[0], warp_transformed_frag_B[0],
                        warp_loaded_frag_A[0], warp_loaded_frag_B[0]);
 
-    // tf32x3 kernels use staging accumulation. warp_mma uses a temporary
-    // accumulator and this temporary accumulator is added to the final
-    // accumulator once in every mainloop iteration.
-    plus<FragmentC> plus_accum;
-
-    FragmentC tmp_accum;
-
-    if (platform::is_same<typename Operator::MathOperator,
-                          arch::OpMultiplyAddFastF32>::value
-      || platform::is_same<typename Operator::MathOperator,
-                           arch::OpMultiplyAddComplexFastF32>::value) {
-
-      tmp_accum.clear();
-    }
-
     //
     // Mainloop
     //
 
     CUTLASS_GEMM_LOOP
     for (; gemm_k_iterations > (-Base::kStages + 1);) {
       //
@@ -501,38 +442,21 @@
 
         if (warp_mma_k > 0)
           warp_mma.transform(warp_transformed_frag_A[warp_mma_k % 2],
                              warp_transformed_frag_B[warp_mma_k % 2],
                              warp_loaded_frag_A[warp_mma_k % 2],
                              warp_loaded_frag_B[warp_mma_k % 2]);
 
-        if (platform::is_same<typename Operator::MathOperator,
-                              arch::OpMultiplyAddFastF32>::value
-          || platform::is_same<typename Operator::MathOperator,
-                               arch::OpMultiplyAddComplexFastF32>::value) {
-
-          warp_mma(
-            tmp_accum, 
-            warp_transformed_frag_A[warp_mma_k % 2],
-            warp_transformed_frag_B[warp_mma_k % 2], 
-            tmp_accum
-          );
-
-          if (warp_mma_k == 0) {
-            accum = plus_accum(accum, tmp_accum);
-            tmp_accum.clear();
-          }
-        } else {
-          warp_mma(
-            accum, 
-            warp_transformed_frag_A[warp_mma_k % 2],
-            warp_transformed_frag_B[warp_mma_k % 2], 
-            accum
-          );
-        }
+        warp_mma(
+          accum, 
+          warp_transformed_frag_A[warp_mma_k % 2],
+          warp_transformed_frag_B[warp_mma_k % 2], 
+          accum,
+          gemm_k_reduction_accum
+        );
 
         // Issue global->shared copies for the this stage
         if (warp_mma_k < Base::kWarpGemmIterations - 1) {
           int group_start_iteration_A, group_start_iteration_B;
 
           group_start_iteration_A = warp_mma_k * Detail::kAccessesPerGroupA;
           group_start_iteration_B = warp_mma_k * Detail::kAccessesPerGroupB;
@@ -550,15 +474,15 @@
 
           copy_tiles_and_advance(iterator_A, iterator_B, group_start_iteration_A, 
                                group_start_iteration_B);
 
           // Inserts a memory fence between stages of cp.async instructions.
           cutlass::arch::cp_async_fence();
 
-          // Waits until stages up to the previous (kStages-2)th stage have committed.
+          // Waits until kStages-2 stages have committed.
           arch::cp_async_wait<Base::kStages - 2>();
           __syncthreads();
 
           // Move to the next stage
           iterator_A.add_tile_offset({0, 1});
           iterator_B.add_tile_offset({1, 0});
 
@@ -599,24 +523,17 @@
           warp_mma.transform(warp_transformed_frag_A[(warp_mma_k + 1) % 2],
                              warp_transformed_frag_B[(warp_mma_k + 1) % 2],
                              warp_loaded_frag_A[(warp_mma_k + 1) % 2],
                              warp_loaded_frag_B[(warp_mma_k + 1) % 2]);
       }
 
     }
-
-    if (platform::is_same<typename Operator::MathOperator,
-                          arch::OpMultiplyAddFastF32>::value
-      || platform::is_same<typename Operator::MathOperator,
-                           arch::OpMultiplyAddComplexFastF32>::value) {
-      accum = plus_accum(accum, tmp_accum); 
-    }
- 
+    
     if (SharedMemoryClear == SharedMemoryClearOption::kZfill) {
-      // commit and drain all pending and predicated LDGSTS pnz from the GEMM mainloop
+      // commit and drain all pending and predicated cp.async pnz from the GEMM mainloop
       cutlass::arch::cp_async_fence();
       cutlass::arch::cp_async_wait<0>();
       __syncthreads();
     }
 
   }
 };
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/mma_pipelined.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/mma_singlestage.h`

 * *Files 16% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -33,22 +33,23 @@
 */
 
 #pragma once
 
 #include "cutlass/cutlass.h"
 #include "cutlass/array.h"
 #include "cutlass/aligned_buffer.h"
-#include "cutlass/numeric_conversion.h"
 
 #include "cutlass/numeric_types.h"
 #include "cutlass/matrix_shape.h"
 
 #include "cutlass/gemm/gemm.h"
 #include "cutlass/gemm/threadblock/mma_base.h"
 
+
+
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
 namespace cutlass {
 namespace gemm {
 namespace threadblock {
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
@@ -71,47 +72,33 @@
   typename SmemIteratorB_,
   /// Data type of accumulator matrix
   typename ElementC_,
   /// Data type of accumulator matrix
   typename LayoutC_,
   /// Policy describing tuning details (concept: MmaPolicy)
   typename Policy_,
-  /// Transformation applied to A operand
-  typename TransformA_ = NumericArrayConverter<
-    typename SmemIteratorA_::Element, 
-    typename IteratorA_::Element, 
-    IteratorA_::Fragment::kElements>,
-  ///
-  /// Transformation applied to B operand
-  typename TransformB_ = NumericArrayConverter<
-    typename SmemIteratorB_::Element, 
-    typename IteratorB_::Element, 
-    IteratorB_::Fragment::kElements>,
   /// Used for partial specialization
   typename Enable = bool
 >
-class MmaPipelined : public MmaBase<Shape_, Policy_, 2> {
+class MmaSingleStage : public MmaBase<Shape_, Policy_, 1> {
 public:
 
   ///< Base class
-  using Base = MmaBase<Shape_, Policy_, 2>;
+  using Base = MmaBase<Shape_, Policy_, 1>;
 
   using Shape = Shape_;             ///< Size of the Gemm problem - concept: gemm::GemmShape<>
   using IteratorA = IteratorA_;     ///< Iterates over tiles of A operand in global memory
   using IteratorB = IteratorB_;     ///< Iterates over tiles of B operand in global memory
   using ElementC = ElementC_;       ///< Data type of accumulator matrix
   using LayoutC = LayoutC_;         ///< Layout of accumulator matrix
   using Policy = Policy_;           ///< Policy describing tuning details
 
   using SmemIteratorA = SmemIteratorA_;
   using SmemIteratorB = SmemIteratorB_;
 
-  using TransformA = TransformA_;
-  using TransformB = TransformB_;
-
   //
   // Dependent types
   //
 
   /// Fragment of operand A loaded from global memory
   using FragmentA = typename IteratorA::Fragment;
 
@@ -120,26 +107,24 @@
 
   /// Fragment of accumulator tile
   using FragmentC = typename Policy::Operator::FragmentC;
 
   /// Warp-level Mma
   using Operator = typename Policy::Operator;
 
-  /// Obtain the arch tag from the warp-level operator
-  using ArchTag = typename Policy::Operator::ArchTag;
+  using ArchTag = arch::Sm70;
 
   /// Complex transform on A operand
   static ComplexTransform const kTransformA = Operator::kTransformA;
 
   /// Complex transform on B operand
   static ComplexTransform const kTransformB = Operator::kTransformB;
 
-  // staticaly assert kStages for MmaPipelined is two (Double-buffered pipeline)
-  static_assert((Base::kStages==2), "MmaPipelined requires kStages set to value 2");
-
+  // staticaly assert kStages for MmaSingleStage is 1 (single stage mma pipeline)
+  static_assert((Base::kStages==1), "MmaSingleStage requires kStages set to value 1");
 private:
 
   using WarpFragmentA = typename Operator::FragmentA;
   using WarpFragmentB = typename Operator::FragmentB;
 
 protected:
 
@@ -149,15 +134,15 @@
   /// Iterator to write threadblock-scoped tile of B operand to shared memory
   SmemIteratorB smem_iterator_B_;
 
 public:
 
   /// Construct from tensor references
   CUTLASS_DEVICE
-  MmaPipelined(
+  MmaSingleStage(
     typename Base::SharedStorage &shared_storage,       ///< Shared storage needed for internal use by threadblock-scoped GEMM
     int thread_idx,                                     ///< ID within the threadblock
     int warp_idx,                                       ///< ID of warp
     int lane_idx                                        ///< ID of each thread within a warp
   ):
     Base(shared_storage, thread_idx, warp_idx, lane_idx),
     smem_iterator_A_(shared_storage.operand_A_ref(), thread_idx),
@@ -174,26 +159,25 @@
 
     int warp_idx_m = warp_idx_mn % Base::WarpCount::kM;
     int warp_idx_n = warp_idx_mn / Base::WarpCount::kM;
 
     // Add per-warp offsets in units of warp-level tiles
     this->warp_tile_iterator_A_.add_tile_offset({warp_idx_m, Base::kWarpGemmIterations * warp_idx_k});
     this->warp_tile_iterator_B_.add_tile_offset({Base::kWarpGemmIterations * warp_idx_k, warp_idx_n});
+
   }
 
   /// Perform a threadblock-scoped matrix multiply-accumulate
   CUTLASS_DEVICE
   void operator()(
-    int gemm_k_iterations,                            ///< number of iterations of the mainloop
-    FragmentC &accum,                                 ///< destination accumulator tile
-    IteratorA iterator_A,                             ///< iterator over A operand in global memory
-    IteratorB iterator_B,                             ///< iterator over B operand in global memory
-    FragmentC const &src_accum,                       ///< source accumulator tile
-    TransformA transform_A = TransformA(),            ///< transformation applied to A fragment
-    TransformB transform_B = TransformB()) {          ///< transformation applied to B fragment
+    int gemm_k_iterations,            ///< number of iterations of the mainloop
+    FragmentC &accum,                 ///< destination accumulator tile
+    IteratorA iterator_A,             ///< iterator over A operand in global memory
+    IteratorB iterator_B,             ///< iterator over B operand in global memory
+    FragmentC const &src_accum) {     ///< source accumualtor tile
 
     //
     // Prologue
     //
 
     // Perform accumulation in the 'd' output operand
     accum = src_accum;
@@ -207,121 +191,75 @@
     // The last kblock is loaded in the prolog
     iterator_A.load(tb_frag_A);
     iterator_B.load(tb_frag_B);
 
     ++iterator_A;
     ++iterator_B;
 
-    this->smem_iterator_A_.store(transform_A(tb_frag_A));
-    this->smem_iterator_B_.store(transform_B(tb_frag_B));
-
-    ++this->smem_iterator_A_;
-    ++this->smem_iterator_B_;
-
-    __syncthreads();
-
     // Pair of fragments used to overlap shared memory loads and math instructions
-    WarpFragmentA warp_frag_A[2];
-    WarpFragmentB warp_frag_B[2];
-
-    this->warp_tile_iterator_A_.set_kgroup_index(0);
-    this->warp_tile_iterator_B_.set_kgroup_index(0);
-
-    this->warp_tile_iterator_A_.load(warp_frag_A[0]);
-    this->warp_tile_iterator_B_.load(warp_frag_B[0]);
-
-    ++this->warp_tile_iterator_A_;
-    ++this->warp_tile_iterator_B_;
+    WarpFragmentA warp_frag_A;
+    WarpFragmentB warp_frag_B;
 
     Operator warp_mma;
 
-    int smem_write_stage_idx = 1;
-
     // Avoid reading out of bounds
     iterator_A.clear_mask(gemm_k_iterations <= 1);
     iterator_B.clear_mask(gemm_k_iterations <= 1);
 
-    // Issue loads during the first warp-level matrix multiply-add *AFTER* issuing 
-    // shared memory loads (which have the tighest latency requirement).
-
     //
     // Mainloop
     //
 
-    // Note: The main loop does not support Base::kWarpGemmIterations == 2.
     CUTLASS_GEMM_LOOP
     for (; gemm_k_iterations > 0; --gemm_k_iterations) {
+      this->smem_iterator_A_.store(tb_frag_A);
+      this->smem_iterator_B_.store(tb_frag_B);
+
+      __syncthreads();
+
       //
       // Loop over GEMM K dimension
       //
 
       CUTLASS_PRAGMA_UNROLL
       for (int warp_mma_k = 0; warp_mma_k < Base::kWarpGemmIterations; ++warp_mma_k) {
 
         // Load warp-level tiles from shared memory, wrapping to k offset if this is the last group
         // as the case may be.
-
-        if (warp_mma_k == Base::kWarpGemmIterations - 1) {
-
-          // Write fragments to shared memory
-          this->smem_iterator_A_.store(transform_A(tb_frag_A));
-
-          this->smem_iterator_B_.store(transform_B(tb_frag_B));
-
-          __syncthreads();
-          
-          ++this->smem_iterator_A_;
-          ++this->smem_iterator_B_;
-
-          // Add negative offsets to return iterators to the 'start' of the circular buffer in shared memory
-          if (smem_write_stage_idx == 1) {
-            this->smem_iterator_A_.add_tile_offset({0, -Base::kStages});
-            this->smem_iterator_B_.add_tile_offset({-Base::kStages, 0});
-          }
-          else {
-            this->warp_tile_iterator_A_.add_tile_offset(
-                {0, -Base::kStages * Policy::kPartitionsK * Base::kWarpGemmIterations});
-            this->warp_tile_iterator_B_.add_tile_offset(
-                {-Base::kStages * Policy::kPartitionsK * Base::kWarpGemmIterations,
-                 0});
-          }
-
-          smem_write_stage_idx ^= 1;
-        }
-
-        this->warp_tile_iterator_A_.set_kgroup_index((warp_mma_k + 1) % Base::kWarpGemmIterations);
-        this->warp_tile_iterator_B_.set_kgroup_index((warp_mma_k + 1) % Base::kWarpGemmIterations);
         
-        this->warp_tile_iterator_A_.load(warp_frag_A[(warp_mma_k + 1) % 2]);
-        this->warp_tile_iterator_B_.load(warp_frag_B[(warp_mma_k + 1) % 2]);
+        this->warp_tile_iterator_A_.set_kgroup_index(warp_mma_k % Base::kWarpGemmIterations);
+        this->warp_tile_iterator_B_.set_kgroup_index(warp_mma_k % Base::kWarpGemmIterations);
+
+        this->warp_tile_iterator_A_.load(warp_frag_A);
+        this->warp_tile_iterator_B_.load(warp_frag_B);
 
         ++this->warp_tile_iterator_A_;
         ++this->warp_tile_iterator_B_;
 
-        if (warp_mma_k == 0) {
+        warp_mma(accum, warp_frag_A, warp_frag_B, accum);
+      }
 
-          iterator_A.load(tb_frag_A);
-          iterator_B.load(tb_frag_B);
+      // Add negative offsets to return smem load iterators to the 'start' of the shared memory
+      this->warp_tile_iterator_A_.add_tile_offset({0, -Policy::kPartitionsK * Base::kWarpGemmIterations});
+      this->warp_tile_iterator_B_.add_tile_offset({-Policy::kPartitionsK * Base::kWarpGemmIterations, 0});
 
-          ++iterator_A;
-          ++iterator_B;
+      __syncthreads();
 
-          // Avoid reading out of bounds if this was the last loop iteration
-          iterator_A.clear_mask(gemm_k_iterations <= 2);
-          iterator_B.clear_mask(gemm_k_iterations <= 2);
-        }
+      iterator_A.load(tb_frag_A);
+      iterator_B.load(tb_frag_B);
 
-        warp_mma(accum, warp_frag_A[warp_mma_k % 2],
-                 warp_frag_B[warp_mma_k % 2], accum);
-      }
+      ++iterator_A;
+      ++iterator_B;
+
+      // Avoid reading out of bounds if this was the last loop iteration
+      iterator_A.clear_mask(gemm_k_iterations <= 2);
+      iterator_B.clear_mask(gemm_k_iterations <= 2);
     }
 
   }
 };
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
 } // namespace threadblock
 } // namespace gemm
 } // namespace cutlass
-
-/////////////////////////////////////////////////////////////////////////////////////////////////
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/mma_planar_complex_multistage.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/mma_planar_complex_multistage.h`

 * *Files 6% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -146,32 +146,30 @@
   /// Internal structure exposed for introspection.
   struct Detail {
 
     static_assert(Base::kWarpGemmIterations > 1,
                   "The pipelined structure requires at least two warp-level "
                   "GEMM operations.");
 
-    /// Number of LDGSTS instructions to load one stage of operand A
-    static int const TBLDGSTSIterationsA =
+    /// Number of cp.async instructions to load one stage of operand A
+    static int const TBLoadIterationsA =
         IteratorA::ThreadMap::Iterations::kCount;
 
-    /// Number of LDGSTS instructions to load one stage of operand B
-    static int const TBLDGSTSIterationsB =
+    /// Number of cp.async instructions to load one stage of operand B
+    static int const TBLoadIterationsB =
         IteratorB::ThreadMap::Iterations::kCount;
 
     /// Number of stages
     static int const kStages = Stages;
 
-    /// Number of LDGSTS instructions to load on group of operand A
     static int const kAccessesPerGroupA =
-        (TBLDGSTSIterationsA + Base::kWarpGemmIterations - 1) / Base::kWarpGemmIterations;
+        (TBLoadIterationsA + Base::kWarpGemmIterations - 1) / Base::kWarpGemmIterations;
 
-    /// Number of LDGSTS instructions to load on group of operand B
     static int const kAccessesPerGroupB =
-        (TBLDGSTSIterationsB + Base::kWarpGemmIterations - 1) / Base::kWarpGemmIterations;
+        (TBLoadIterationsB + Base::kWarpGemmIterations - 1) / Base::kWarpGemmIterations;
   };
 
  private:
 
   using WarpFragmentA = typename Operator::FragmentA;
   using WarpFragmentB = typename Operator::FragmentB;
 
@@ -235,15 +233,15 @@
     int group_start_A = 0, 
     int group_start_B = 0) {
 
     iterator_A_real.set_iteration_index(group_start_A * IteratorA::kAccessesPerVector);
     iterator_A_imag.set_iteration_index(group_start_A * IteratorA::kAccessesPerVector);
     this->smem_iterator_A_.set_iteration_index(group_start_A);
 
-    // LDGSTS for operand A
+    // Load for operand A
     CUTLASS_PRAGMA_UNROLL
     for (int j = 0; j < Detail::kAccessesPerGroupA; ++j) {
         
       typename IteratorA::AccessType *dst_ptr = 
         reinterpret_cast<typename IteratorA::AccessType *>(this->smem_iterator_A_.get());
           
       int const kSrcBytes = 
@@ -273,15 +271,15 @@
       ++this->smem_iterator_A_;
     }
 
     iterator_B_real.set_iteration_index(group_start_B * IteratorB::kAccessesPerVector);
     iterator_B_imag.set_iteration_index(group_start_B * IteratorB::kAccessesPerVector);
     this->smem_iterator_B_.set_iteration_index(group_start_B);
 
-    // LDGSTS for operand B
+    // Load for operand B
     CUTLASS_PRAGMA_UNROLL
     for (int j = 0; j < Detail::kAccessesPerGroupB; ++j) {
       typename IteratorB::AccessType *dst_ptr = 
         reinterpret_cast<typename IteratorB::AccessType *>(this->smem_iterator_B_.get());
       
       int const kSrcBytes = 
         sizeof_bits<typename IteratorB::Element>::value * 
@@ -382,17 +380,17 @@
       iterator_B_imag.clear_mask(gemm_k_iterations == 0);
 
       iterator_A_real.set_iteration_index(0);
       iterator_A_imag.set_iteration_index(0);
 
       this->smem_iterator_A_.set_iteration_index(0);
 
-      // LDGSTS for operand A
+      // Load for operand A
       CUTLASS_PRAGMA_UNROLL
-      for (int j = 0; j < Detail::TBLDGSTSIterationsA; ++j) {
+      for (int j = 0; j < Detail::TBLoadIterationsA; ++j) {
 
         typename IteratorA::AccessType *dst_ptr = 
           reinterpret_cast<typename IteratorA::AccessType *>(this->smem_iterator_A_.get());
 
         CUTLASS_PRAGMA_UNROLL
         for (int v = 0; v < IteratorA::kAccessesPerVector; ++v) {
 
@@ -423,17 +421,17 @@
       }
 
       iterator_B_real.set_iteration_index(0);
       iterator_B_imag.set_iteration_index(0);
 
       this->smem_iterator_B_.set_iteration_index(0);
 
-      // LDGSTS for operand B
+      // Load for operand B
       CUTLASS_PRAGMA_UNROLL
-      for (int j = 0; j < Detail::TBLDGSTSIterationsB; ++j) {
+      for (int j = 0; j < Detail::TBLoadIterationsB; ++j) {
 
         typename IteratorB::AccessType *dst_ptr = 
           reinterpret_cast<typename IteratorB::AccessType *>(this->smem_iterator_B_.get());
 
         CUTLASS_PRAGMA_UNROLL
         for (int v = 0; v < IteratorB::kAccessesPerVector; ++v) {
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/mma_planar_complex_pipelined.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/mma_planar_complex_pipelined.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/mma_softmax_mainloop_fusion_multistage.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/mma_softmax_mainloop_fusion_multistage.h`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -729,15 +729,15 @@
               }
         }
       }
 
     }
     
     if (SharedMemoryClear == SharedMemoryClearOption::kZfill) {
-      // commit and drain all pending and predicated LDGSTS pnz from the GEMM mainloop
+      // commit and drain all pending and predicated cp.async pnz from the GEMM mainloop
       cutlass::arch::cp_async_fence();
       cutlass::arch::cp_async_wait<0>();
       __syncthreads();
     }
 
   }
 };
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/mma_sparse_base.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/mma_sparse_base.h`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/mma_sparse_multistage.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/mma_sparse_multistage.h`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -153,39 +153,39 @@
   /// Complex transform on B operand
   static ComplexTransform const kTransformB = Operator::kTransformB;
 
   /// Internal structure exposed for introspection.
   struct Detail {
 
     /// Number of async copies to load one stage of operand A
-    static int const TBLDGSTSIterationsA =
+    static int const TBLoadIterationsA =
         IteratorA::ThreadMap::Iterations::kCount;
 
     /// Number of async copies to load one stage of operand B
-    static int const TBLDGSTSIterationsB =
+    static int const TBLoadIterationsB =
         IteratorB::ThreadMap::Iterations::kCount;
 
     /// Number of async copies to load one stage of operand E
-    static int const TBLDGSTSIterationsE =
+    static int const TBLoadIterationsE =
         IteratorE::ThreadMap::Iterations::kCount;
 
     /// Number of stages
     static int const kStages = Stages;
 
     /// Number of async copies to load one group of operand A
     static int const kAccessesPerGroupA =
-        (TBLDGSTSIterationsA + Base::kWarpGemmIterations - 1) / Base::kWarpGemmIterations;
+        (TBLoadIterationsA + Base::kWarpGemmIterations - 1) / Base::kWarpGemmIterations;
 
     /// Number of async copies to load one group of operand B
     static int const kAccessesPerGroupB =
-        (TBLDGSTSIterationsB + Base::kWarpGemmIterations - 1) / Base::kWarpGemmIterations;
+        (TBLoadIterationsB + Base::kWarpGemmIterations - 1) / Base::kWarpGemmIterations;
 
     /// Number of async copies to load one group of operand E
     static int const kAccessesPerGroupE =
-        (TBLDGSTSIterationsE + Base::kWarpGemmIterations - 1) / Base::kWarpGemmIterations;
+        (TBLoadIterationsE + Base::kWarpGemmIterations - 1) / Base::kWarpGemmIterations;
 
     /// E operand is tiny.  For the most of time, not all the warps are needed
     /// to load it from the global memory.
     static int const kValidWarps = IteratorE::ThreadMap::kThreads / 32;
 
     /// B operand is twice as big as A which brings very high register pressure.
     /// We have to sacrifice the double buffer when the warp tile size is big.
@@ -275,15 +275,15 @@
     iterator_A.set_iteration_index(group_start_A *
                                    IteratorA::kAccessesPerVector);
     this->smem_iterator_A_.set_iteration_index(group_start_A);
 
     // async copy for operand A
     CUTLASS_PRAGMA_UNROLL
     for (int j = 0; j < Detail::kAccessesPerGroupA; ++j) {
-      if (group_start_A + j < Detail::TBLDGSTSIterationsA) {
+      if (group_start_A + j < Detail::TBLoadIterationsA) {
         typename IteratorA::AccessType *dst_ptr =
             reinterpret_cast<typename IteratorA::AccessType *>(
                 this->smem_iterator_A_.get());
 
         int const kSrcBytes = sizeof_bits<typename IteratorA::Element>::value *
                               IteratorA::ThreadMap::kElementsPerAccess /
                               IteratorA::kAccessesPerVector / 8;
@@ -305,15 +305,15 @@
     iterator_B.set_iteration_index(group_start_B *
                                    IteratorB::kAccessesPerVector);
     this->smem_iterator_B_.set_iteration_index(group_start_B);
 
     // async copy for operand B
     CUTLASS_PRAGMA_UNROLL
     for (int j = 0; j < Detail::kAccessesPerGroupB; ++j) {
-      if (group_start_B + j < Detail::TBLDGSTSIterationsB) {
+      if (group_start_B + j < Detail::TBLoadIterationsB) {
         typename IteratorB::AccessType *dst_ptr =
             reinterpret_cast<typename IteratorB::AccessType *>(
                 this->smem_iterator_B_.get());
 
         int const kSrcBytes = sizeof_bits<typename IteratorB::Element>::value *
                               IteratorB::ThreadMap::kElementsPerAccess /
                               IteratorB::kAccessesPerVector / 8;
@@ -333,15 +333,15 @@
 
     iterator_E.set_iteration_index(group_start_E);
     this->smem_iterator_E_.set_iteration_index(group_start_E);
 
     // async copy for operand E
     CUTLASS_PRAGMA_UNROLL
     for (int j = 0; j < Detail::kAccessesPerGroupE; ++j) {
-      if (group_start_E + j < Detail::TBLDGSTSIterationsE) {
+      if (group_start_E + j < Detail::TBLoadIterationsE) {
         typename IteratorE::AccessType *dst_ptr =
             reinterpret_cast<typename IteratorE::AccessType *>(
                 this->smem_iterator_E_.get());
 
         int const kSrcBytes = sizeof_bits<typename IteratorE::Element>::value *
                               IteratorE::ThreadMap::kElementsPerAccess / 8;
 
@@ -386,15 +386,15 @@
       iterator_E.clear_mask(gemm_k_iterations == 0);
 
       iterator_A.set_iteration_index(0);
       this->smem_iterator_A_.set_iteration_index(0);
 
       // async copy for operand A
       CUTLASS_PRAGMA_UNROLL
-      for (int j = 0; j < Detail::TBLDGSTSIterationsA; ++j) {
+      for (int j = 0; j < Detail::TBLoadIterationsA; ++j) {
         typename IteratorA::AccessType *dst_ptr =
             reinterpret_cast<typename IteratorA::AccessType *>(
                 this->smem_iterator_A_.get());
 
         CUTLASS_PRAGMA_UNROLL
         for (int v = 0; v < IteratorA::kAccessesPerVector; ++v) {
           int const kSrcBytes =
@@ -412,15 +412,15 @@
       }
 
       iterator_B.set_iteration_index(0);
       this->smem_iterator_B_.set_iteration_index(0);
 
       // async copy for operand B
       CUTLASS_PRAGMA_UNROLL
-      for (int j = 0; j < Detail::TBLDGSTSIterationsB; ++j) {
+      for (int j = 0; j < Detail::TBLoadIterationsB; ++j) {
         typename IteratorB::AccessType *dst_ptr =
             reinterpret_cast<typename IteratorB::AccessType *>(
                 this->smem_iterator_B_.get());
 
         CUTLASS_PRAGMA_UNROLL
         for (int v = 0; v < IteratorB::kAccessesPerVector; ++v) {
           int const kSrcBytes =
@@ -438,15 +438,15 @@
       }
 
       iterator_E.set_iteration_index(0);
       this->smem_iterator_E_.set_iteration_index(0);
 
       // async copy for operand E
       CUTLASS_PRAGMA_UNROLL
-      for (int j = 0; j < Detail::TBLDGSTSIterationsE; ++j) {
+      for (int j = 0; j < Detail::TBLoadIterationsE; ++j) {
         typename IteratorE::AccessType *dst_ptr =
             reinterpret_cast<typename IteratorE::AccessType *>(
                 this->smem_iterator_E_.get());
 
         int const kSrcBytes = sizeof_bits<typename IteratorE::Element>::value *
                               IteratorE::ThreadMap::kElementsPerAccess / 8;
         if (is_warp_valid_)
@@ -463,22 +463,21 @@
       iterator_B.add_tile_offset({1, 0});
       iterator_E.add_tile_offset({0, 1});
 
       this->smem_iterator_A_.add_tile_offset({0, 1});
       this->smem_iterator_B_.add_tile_offset({1, 0});
       this->smem_iterator_E_.add_tile_offset({0, 1});
 
-      // LDGDEPBAR - completes a stage
+      // cp.async.commit_group - completes a stage
       cutlass::arch::cp_async_fence();
     }
 
     // Perform accumulation in the 'd' output operand
     accum = src_accum;
 
-    // DEPBAR+SYNC
     cutlass::arch::cp_async_wait<Base::kStages - 2>();
     __syncthreads();
 
     // Pair of fragments used to overlap shared memory loads and math
     // instructions
     WarpLoadedFragmentA warp_loaded_frag_A[2];
     WarpLoadedFragmentB warp_loaded_frag_B[Detail::kBBufferSize];
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/mma_with_reduction_multistage.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/mma_pipelined.h`

 * *Files 18% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -30,518 +30,410 @@
  **************************************************************************************************/
 /*! \file
     \brief Template for a double-buffered threadblock-scoped GEMM kernel.
 */
 
 #pragma once
 
-#include "cutlass/aligned_buffer.h"
-#include "cutlass/arch/memory.h"
-#include "cutlass/array.h"
 #include "cutlass/cutlass.h"
-#include "cutlass/gemm/gemm.h"
-#include "cutlass/matrix_shape.h"
+#include "cutlass/array.h"
+#include "cutlass/aligned_buffer.h"
+#include "cutlass/numeric_conversion.h"
+
 #include "cutlass/numeric_types.h"
+#include "cutlass/matrix_shape.h"
 
+#include "cutlass/gemm/gemm.h"
 #include "cutlass/gemm/threadblock/mma_base.h"
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
 namespace cutlass {
 namespace gemm {
 namespace threadblock {
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
-/// Structure to compute the matrix product targeting CUDA cores and SIMT math
-/// instructions.
+/// Structure to compute the matrix product targeting CUDA cores and SIMT math instructions.
 template <
-    /// Size of the Gemm problem - concept: gemm::GemmShape<>
-    typename Shape_,
-    /// Iterates over tiles of A operand in global memory
-    //  (concept: ReadableTileIterator | ForwardTileIterator |
-    //  MaskedTileIterator)
-    typename IteratorA_,
-    /// Iterates over tiles of A operand in shared memory
-    /// (concept: WriteableTileIterator | RandomAccessTileIterator)
-    typename SmemIteratorA_,
-    /// Cache operation for operand A
-    cutlass::arch::CacheOperation::Kind CacheOpA,
-    /// Iterates over tiles of B operand in global memory
-    //  (concept: ReadableTileIterator | ForwardTileIterator |
-    //  MaskedTileIterator)
-    typename IteratorB_,
-    /// Iterates over tiles of B operand in shared memory
-    /// (concept: WriteableTileIterator | RandomAccessTileIterator)
-    typename SmemIteratorB_,
-    /// Cache operation for operand B
-    cutlass::arch::CacheOperation::Kind CacheOpB,
-    /// Data type of accumulator matrix
-    typename ElementC_,
-    /// Data type of accumulator matrix
-    typename LayoutC_,
-    /// Policy describing tuning details (concept: MmaPolicy)
-    typename Policy_,
-    /// Number of stages,
-    int Stages,
-    /// Use zfill or predicate for out-of-bound cp.async
-    SharedMemoryClearOption SharedMemoryClear = SharedMemoryClearOption::kNone,
-    /// Used for partial specialization
-    typename Enable = bool>
-class MmaWithReductionMultistage : 
-  public MmaBase<Shape_, Policy_, Stages> {
+  /// Size of the Gemm problem - concept: gemm::GemmShape<>
+  typename Shape_,
+  /// Iterates over tiles of A operand in global memory 
+  //  (concept: ReadableTileIterator | ForwardTileIterator | MaskedTileIterator)
+  typename IteratorA_,
+  /// Iterates over tiles of A operand in shared memory
+  /// (concept: WriteableTileIterator | RandomAccessTileIterator)
+  typename SmemIteratorA_,
+  /// Iterates over tiles of B operand in global memory
+  //  (concept: ReadableTileIterator | ForwardTileIterator | MaskedTileIterator)
+  typename IteratorB_,
+  /// Iterates over tiles of B operand in shared memory
+  /// (concept: WriteableTileIterator | RandomAccessTileIterator)
+  typename SmemIteratorB_,
+  /// Data type of accumulator matrix
+  typename ElementC_,
+  /// Data type of accumulator matrix
+  typename LayoutC_,
+  /// Policy describing tuning details (concept: MmaPolicy)
+  typename Policy_,
+  /// Transformation applied to A operand
+  typename TransformA_ = NumericArrayConverter<
+    typename SmemIteratorA_::Element, 
+    typename IteratorA_::Element, 
+    IteratorA_::Fragment::kElements>,
+  ///
+  /// Transformation applied to B operand
+  typename TransformB_ = NumericArrayConverter<
+    typename SmemIteratorB_::Element, 
+    typename IteratorB_::Element, 
+    IteratorB_::Fragment::kElements>,
+  /// Used for partial specialization
+  typename Enable = bool
+>
+class MmaPipelined : public MmaBase<Shape_, Policy_, 2> {
 public:
+
   ///< Base class
-  using Base = MmaBase<Shape_, Policy_, Stages>;
-  ///< Size of the Gemm problem - concept: gemm::GemmShape<>
-  using Shape = Shape_;
-  ///< Iterates over tiles of A operand in global memory
-  using IteratorA = IteratorA_;
-  ///< Iterates over tiles of B operand in global memory
-  using IteratorB = IteratorB_;
-  ///< Data type of accumulator matrix
-  using ElementC = ElementC_;
-  ///< Layout of accumulator matrix
-  using LayoutC = LayoutC_;
-  ///< Policy describing tuning details
-  using Policy = Policy_;
+  using Base = MmaBase<Shape_, Policy_, 2>;
+
+  using Shape = Shape_;             ///< Size of the Gemm problem - concept: gemm::GemmShape<>
+  using IteratorA = IteratorA_;     ///< Iterates over tiles of A operand in global memory
+  using IteratorB = IteratorB_;     ///< Iterates over tiles of B operand in global memory
+  using ElementC = ElementC_;       ///< Data type of accumulator matrix
+  using LayoutC = LayoutC_;         ///< Layout of accumulator matrix
+  using Policy = Policy_;           ///< Policy describing tuning details
 
   using SmemIteratorA = SmemIteratorA_;
   using SmemIteratorB = SmemIteratorB_;
 
-  static cutlass::arch::CacheOperation::Kind const kCacheOpA = CacheOpA;
-  static cutlass::arch::CacheOperation::Kind const kCacheOpB = CacheOpB;
+  using TransformA = TransformA_;
+  using TransformB = TransformB_;
 
   //
   // Dependent types
   //
 
+  /// Fragment of operand A loaded from global memory
+  using FragmentA = typename IteratorA::Fragment;
+
+  /// Fragment of operand B loaded from global memory
+  using FragmentB = typename IteratorB::Fragment;
+
   /// Fragment of accumulator tile
   using FragmentC = typename Policy::Operator::FragmentC;
 
   /// Warp-level Mma
   using Operator = typename Policy::Operator;
 
-  using FragmentReduction = typename Operator::FragmentReduction;
+  /// Obtain the arch tag from the warp-level operator
+  using ArchTag = typename Policy::Operator::ArchTag;
 
-  /// Minimum architecture is Sm80 to support cp.async
-  using ArchTag = arch::Sm80;
-  
   /// Complex transform on A operand
   static ComplexTransform const kTransformA = Operator::kTransformA;
 
   /// Complex transform on B operand
   static ComplexTransform const kTransformB = Operator::kTransformB;
 
-  static int const kReduceKForA = Operator::kReduceKForA;
-
-  /// Internal structure exposed for introspection.
-  struct Detail {
-
-    /// Number of cp.async instructions to load one stage of operand A
-    static int const AsyncCopyIterationsPerStageA =
-        IteratorA::ThreadMap::Iterations::kCount;
-
-    /// Number of cp.async instructions to load one stage of operand B
-    static int const AsyncCopyIterationsPerStageB =
-        IteratorB::ThreadMap::Iterations::kCount;
+  // staticaly assert kStages for MmaPipelined is two (Double-buffered pipeline)
+  static_assert((Base::kStages==2), "MmaPipelined requires kStages set to value 2");
 
-    /// Number of stages
-    static int const kStages = Stages;
-
-    /// Number of cp.async instructions to load on group of operand A
-    static int const kAccessesPerGroupA =
-        (AsyncCopyIterationsPerStageA + Base::kWarpGemmIterations - 1) / Base::kWarpGemmIterations;
-
-    /// Number of cp.async instructions to load on group of operand B
-    static int const kAccessesPerGroupB =
-        (AsyncCopyIterationsPerStageB + Base::kWarpGemmIterations - 1) / Base::kWarpGemmIterations;
-  };
-
- private:
-
-  using WarpLoadedFragmentA = typename Operator::FragmentA;
-  using WarpLoadedFragmentB = typename Operator::FragmentB;
-  using WarpTransformedFragmentA = typename Operator::TransformedFragmentA;
-  using WarpTransformedFragmentB = typename Operator::TransformedFragmentB;
-
- private:
+protected:
 
   //
   // Data members
   //
 
+  /// Warp-level MMA operator
+  Operator warp_mma;
+
   /// Iterator to write threadblock-scoped tile of A operand to shared memory
   SmemIteratorA smem_iterator_A_;
 
   /// Iterator to write threadblock-scoped tile of B operand to shared memory
   SmemIteratorB smem_iterator_B_;
 
+  ///< transformation applied to A fragment
+  TransformA transform_A_;
+
+  ///< transformation applied to B fragment
+  TransformB transform_B_;
+
+  /// Shared memory write stage index
+  int smem_write_stage_idx;
+
 public:
 
   /// Construct from tensor references
   CUTLASS_DEVICE
-  MmaWithReductionMultistage(
-      ///< Shared storage needed for internal use by threadblock-scoped GEMM
-      typename Base::SharedStorage &shared_storage,
-      ///< ID within the threadblock
-      int thread_idx,
-      ///< ID of warp
-      int warp_idx,
-      ///< ID of each thread within a warp
-      int lane_idx
-    ):
-      Base(shared_storage, thread_idx, warp_idx, lane_idx),
-      smem_iterator_A_(shared_storage.operand_A_ref(), thread_idx),
-      smem_iterator_B_(shared_storage.operand_B_ref(), thread_idx)
+  MmaPipelined(
+    typename Base::SharedStorage &shared_storage,       ///< Shared storage needed for internal use by threadblock-scoped GEMM
+    int thread_idx,                                     ///< ID within the threadblock
+    int warp_idx,                                       ///< ID of warp
+    int lane_idx,                                       ///< ID of each thread within a warp
+    TransformA transform_A = TransformA(),              ///< transformation applied to A fragment
+    TransformB transform_B = TransformB()               ///< transformation applied to B fragment
+  ):
+    Base(shared_storage, thread_idx, warp_idx, lane_idx),
+    smem_iterator_A_(shared_storage.operand_A_ref(), thread_idx),
+    smem_iterator_B_(shared_storage.operand_B_ref(), thread_idx),
+    transform_A_(transform_A),
+    transform_B_(transform_B),
+    smem_write_stage_idx(0)
   {
+
     // Compute warp location within threadblock tile by mapping the warp_id to
     // three coordinates:
     //   _m: the warp's position within the threadblock along the M dimension
     //   _n: the warp's position within the threadblock along the N dimension
     //   _k: the warp's position within the threadblock along the K dimension
 
     int warp_idx_mn = warp_idx % (Base::WarpCount::kM * Base::WarpCount::kN);
     int warp_idx_k = warp_idx / (Base::WarpCount::kM * Base::WarpCount::kN);
 
     int warp_idx_m = warp_idx_mn % Base::WarpCount::kM;
     int warp_idx_n = warp_idx_mn / Base::WarpCount::kM;
 
     // Add per-warp offsets in units of warp-level tiles
-    this->warp_tile_iterator_A_.add_tile_offset(
-        {warp_idx_m, Base::kWarpGemmIterations * warp_idx_k});
-    this->warp_tile_iterator_B_.add_tile_offset(
-        {Base::kWarpGemmIterations * warp_idx_k, warp_idx_n});
+    this->warp_tile_iterator_A_.add_tile_offset({warp_idx_m, Base::kWarpGemmIterations * warp_idx_k});
+    this->warp_tile_iterator_B_.add_tile_offset({Base::kWarpGemmIterations * warp_idx_k, warp_idx_n});
   }
 
-  CUTLASS_DEVICE
-  void copy_tiles_and_advance(IteratorA &iterator_A, IteratorB &iterator_B,
-                              int group_start_A = 0, int group_start_B = 0) {
-    iterator_A.set_iteration_index(group_start_A *
-                                   IteratorA::kAccessesPerVector);
-    this->smem_iterator_A_.set_iteration_index(group_start_A);
-
-    // Async Copy for operand A
-    CUTLASS_PRAGMA_UNROLL
-    for (int j = 0; j < Detail::kAccessesPerGroupA; ++j) {
-      if (group_start_A + j < Detail::AsyncCopyIterationsPerStageA) {
-        typename IteratorA::AccessType *dst_ptr =
-            reinterpret_cast<typename IteratorA::AccessType *>(
-                this->smem_iterator_A_.get());
-
-        int const kSrcBytes = sizeof_bits<typename IteratorA::Element>::value *
-                              IteratorA::ThreadMap::kElementsPerAccess /
-                              IteratorA::kAccessesPerVector / 8;
-
-        CUTLASS_PRAGMA_UNROLL
-        for (int v = 0; v < IteratorA::kAccessesPerVector; ++v) {
-          auto gmem_ptr = iterator_A.get();
-
-          if (SharedMemoryClear == SharedMemoryClearOption::kZfill) {
-            cutlass::arch::cp_async_zfill<kSrcBytes, kCacheOpA>(
-                dst_ptr + v, gmem_ptr, iterator_A.valid());
-          } else {
-            cutlass::arch::cp_async<kSrcBytes, kCacheOpA>(
-                dst_ptr + v, gmem_ptr, iterator_A.valid());
-          }
 
-          ++iterator_A;
-        }
+  /// Advance shared memory write-iterators to the next stage
+  CUTLASS_DEVICE
+  void advance_smem_write_stage()
+  {
+    ++this->smem_iterator_A_;
+    ++this->smem_iterator_B_;
 
-        ++this->smem_iterator_A_;
-      }
+    // Add negative offsets to return iterators to the 'start' of the circular buffer in shared memory
+    if (smem_write_stage_idx == 1) {
+      this->smem_iterator_A_.add_tile_offset({0, -Base::kStages});
+      this->smem_iterator_B_.add_tile_offset({-Base::kStages, 0});
     }
 
-    iterator_B.set_iteration_index(group_start_B *
-                                   IteratorB::kAccessesPerVector);
-    this->smem_iterator_B_.set_iteration_index(group_start_B);
-
-    // Async Copy for operand B
-    CUTLASS_PRAGMA_UNROLL
-    for (int j = 0; j < Detail::kAccessesPerGroupB; ++j) {
-      if (group_start_B + j < Detail::AsyncCopyIterationsPerStageB) {
-        typename IteratorB::AccessType *dst_ptr =
-            reinterpret_cast<typename IteratorB::AccessType *>(
-                this->smem_iterator_B_.get());
-
-        int const kSrcBytes = sizeof_bits<typename IteratorB::Element>::value *
-                              IteratorB::ThreadMap::kElementsPerAccess /
-                              IteratorB::kAccessesPerVector / 8;
-
-        CUTLASS_PRAGMA_UNROLL
-        for (int v = 0; v < IteratorB::kAccessesPerVector; ++v) {
-          auto gmem_ptr = iterator_B.get();
-
-          if (SharedMemoryClear == SharedMemoryClearOption::kZfill) {
-            cutlass::arch::cp_async_zfill<kSrcBytes, kCacheOpB>(
-                dst_ptr + v, gmem_ptr, iterator_B.valid());
-          } else {
-            cutlass::arch::cp_async<kSrcBytes, kCacheOpB>(
-                dst_ptr + v, gmem_ptr, iterator_B.valid());
-          }
-
-          ++iterator_B;
-        }
-        ++this->smem_iterator_B_;
-      }
-    }
+    smem_write_stage_idx ^= 1;
   }
 
-  /// Perform a threadblock-scoped matrix multiply-accumulate
+  /// Advance shared memory read- and write-iterators to the next stage
   CUTLASS_DEVICE
-  void operator()(
-      ///< problem size of GEMM
-      int gemm_k_iterations,
-      ///< destination accumulator tile
-      FragmentC &accum,
-      ///< iterator over A operand in global memory
-      IteratorA iterator_A,
-      ///< iterator over B operand in global memory
-      IteratorB iterator_B,
-      ///< initial value of accumulator
-      FragmentC const &src_accum,
-      FragmentReduction &gemm_k_reduction_accum) {
-
-    //
-    // Prologue
-    //
-    // Issue several complete stages
-
-    CUTLASS_PRAGMA_UNROLL
-    for (int stage = 0; stage < Base::kStages - 1;
-         ++stage, --gemm_k_iterations) {
-
-      iterator_A.clear_mask(gemm_k_iterations == 0);
-      iterator_B.clear_mask(gemm_k_iterations == 0);
-
-      iterator_A.set_iteration_index(0);
-      this->smem_iterator_A_.set_iteration_index(0);
-
-      // Async Copy for operand A
-      CUTLASS_PRAGMA_UNROLL
-      for (int j = 0; j < Detail::AsyncCopyIterationsPerStageA; ++j) {
-        typename IteratorA::AccessType *dst_ptr =
-            reinterpret_cast<typename IteratorA::AccessType *>(
-                this->smem_iterator_A_.get());
-
-        CUTLASS_PRAGMA_UNROLL
-        for (int v = 0; v < IteratorA::kAccessesPerVector; ++v) {
-          int const kSrcBytes =
-              sizeof_bits<typename IteratorA::Element>::value *
-              IteratorA::ThreadMap::kElementsPerAccess /
-              IteratorA::kAccessesPerVector / 8;
-
-          int src_bytes = (iterator_A.valid() ? kSrcBytes : 0);
-
-          cutlass::arch::cp_async_zfill<kSrcBytes, kCacheOpA>(
-              dst_ptr + v, iterator_A.get(), iterator_A.valid());
-
-          ++iterator_A;
-        }
-
-        ++this->smem_iterator_A_;
-      }
-
-      iterator_B.set_iteration_index(0);
-      this->smem_iterator_B_.set_iteration_index(0);
-
-      // Async Copy for operand B
-      CUTLASS_PRAGMA_UNROLL
-      for (int j = 0; j < Detail::AsyncCopyIterationsPerStageB; ++j) {
-        typename IteratorB::AccessType *dst_ptr =
-            reinterpret_cast<typename IteratorB::AccessType *>(
-                this->smem_iterator_B_.get());
-
-        CUTLASS_PRAGMA_UNROLL
-        for (int v = 0; v < IteratorB::kAccessesPerVector; ++v) {
-          int const kSrcBytes =
-              sizeof_bits<typename IteratorB::Element>::value *
-              IteratorB::ThreadMap::kElementsPerAccess /
-              IteratorB::kAccessesPerVector / 8;
-
-          cutlass::arch::cp_async_zfill<kSrcBytes, kCacheOpB>(
-              dst_ptr + v, iterator_B.get(), iterator_B.valid());
+  void advance_smem_stages()
+  {
+    ++this->smem_iterator_A_;
+    ++this->smem_iterator_B_;
 
-          ++iterator_B;
-        }
+    // Add negative offsets to return iterators to the 'start' of the circular buffer in shared memory
+    if (smem_write_stage_idx == 1) {
+      // wrap write stage
+      this->smem_iterator_A_.add_tile_offset({0, -Base::kStages});
+      this->smem_iterator_B_.add_tile_offset({-Base::kStages, 0});
+    }
+    else
+    {
+      // wrap read stage
+      this->warp_tile_iterator_A_.add_tile_offset(
+        {0, -Base::kStages * Policy::kPartitionsK * Base::kWarpGemmIterations});
+      this->warp_tile_iterator_B_.add_tile_offset(
+        {-Base::kStages * Policy::kPartitionsK * Base::kWarpGemmIterations, 0});
+    }
 
-        ++this->smem_iterator_B_;
-      }
+    smem_write_stage_idx ^= 1;
+  }
 
-      // Move to the next stage
-      iterator_A.add_tile_offset({0, 1});
-      iterator_B.add_tile_offset({1, 0});
 
-      this->smem_iterator_A_.add_tile_offset({0, 1});
-      this->smem_iterator_B_.add_tile_offset({1, 0});
+  /// GEMM prologue.  Bootstrap the global->shared memory pipeline by fetching
+  /// the global fragments needed by the first kStages-1 threadblock mainloop iterations
+  CUTLASS_DEVICE
+  void prologue(
+    IteratorA &iterator_A,      ///< [in|out] iterator over A operand in global memory
+    IteratorB &iterator_B,      ///< [in|out] iterator over B operand in global memory
+    int &gemm_k_iterations)     ///< [in|out] number of threadblock mainloop iterations remaining
+  {
+    // The last kblock is loaded in the prolog
 
-      // Defines the boundary of a stage of cp.async.
-      cutlass::arch::cp_async_fence();
-    }
+    // Load A fragment from global A
+    FragmentA tb_frag_A;
+    tb_frag_A.clear();
+    iterator_A.load(tb_frag_A);
+    ++iterator_A;
+
+    // Load B fragment from global B
+    FragmentB tb_frag_B;
+    tb_frag_B.clear();
+    iterator_B.load(tb_frag_B);
+    ++iterator_B;
+
+    // Store A and B fragments to shared
+    this->smem_iterator_A_.store(transform_A_(tb_frag_A));
+    this->smem_iterator_B_.store(transform_B_(tb_frag_B));
 
-    // Perform accumulation in the 'd' output operand
-    accum = src_accum;
+    // Advance write stage
+    advance_smem_write_stage();
+  }
 
-    // Waits until kStages-2 stages have committed.
-    cutlass::arch::cp_async_wait<Base::kStages - 2>();
+  /// Wait until we have at least one completed global fetch stage
+  CUTLASS_DEVICE
+  void gmem_wait()
+  {
     __syncthreads();
+  }
 
-    // Pair of fragments used to overlap shared memory loads and math
-    // instructions
-    WarpLoadedFragmentA warp_loaded_frag_A[2];
-    WarpLoadedFragmentB warp_loaded_frag_B[2];
-    WarpTransformedFragmentA warp_transformed_frag_A[2];
-    WarpTransformedFragmentB warp_transformed_frag_B[2];
-
-    Operator warp_mma;
 
-    this->warp_tile_iterator_A_.set_kgroup_index(0);
-    this->warp_tile_iterator_B_.set_kgroup_index(0);
+  /// Perform the specified number of threadblock mainloop iterations of matrix
+  /// multiply-accumulate.  Assumes prologue has been initiated.
+  CUTLASS_DEVICE
+  void gemm_iters(
+    int gemm_k_iterations,        ///< number of threadblock mainloop iterations
+    FragmentC &accum,             ///< [in|out] accumulator tile
+    IteratorA &iterator_A,        ///< [in|out] iterator over A operand in global memory
+    IteratorB &iterator_B)        ///< [in|out] iterator over B operand in global memory
+  {
+    using WarpFragmentA = typename Operator::FragmentA;
+    using WarpFragmentB = typename Operator::FragmentB;
 
-    this->warp_tile_iterator_A_.load(warp_loaded_frag_A[0]);
-    this->warp_tile_iterator_B_.load(warp_loaded_frag_B[0]);
+    // Pair of fragments used to overlap shared memory loads and math instructions
+    WarpFragmentA warp_frag_A[2];
+    WarpFragmentB warp_frag_B[2];
 
+    // Load A fragment from shared A
+    this->warp_tile_iterator_A_.set_kgroup_index(0);
+    this->warp_tile_iterator_A_.load(warp_frag_A[0]);
     ++this->warp_tile_iterator_A_;
-    ++this->warp_tile_iterator_B_;
 
-    iterator_A.clear_mask(gemm_k_iterations == 0);
-    iterator_B.clear_mask(gemm_k_iterations == 0);
-
-    int smem_write_stage_idx = Base::kStages - 1;
-    int smem_read_stage_idx = 0;
+    // Load B fragment from shared B
+    this->warp_tile_iterator_B_.set_kgroup_index(0);
+    this->warp_tile_iterator_B_.load(warp_frag_B[0]);
+    ++this->warp_tile_iterator_B_;
 
-    warp_mma.transform(warp_transformed_frag_A[0], warp_transformed_frag_B[0],
-                       warp_loaded_frag_A[0], warp_loaded_frag_B[0]);
+    // Pair of fragments used to overlap global memory loads and math instructions;
+    FragmentA tb_frag_A;
+    FragmentB tb_frag_B;
+
+    // Avoid reading out of bounds
+    iterator_A.clear_mask(gemm_k_iterations <= 1);
+    iterator_B.clear_mask(gemm_k_iterations <= 1);
 
     //
     // Mainloop
     //
 
+    // Note: The main loop does not support Base::kWarpGemmIterations == 2.
     CUTLASS_GEMM_LOOP
-    for (; gemm_k_iterations > (-Base::kStages + 1);) {
+    for (; gemm_k_iterations > 0; --gemm_k_iterations) {
       //
       // Loop over GEMM K dimension
       //
 
-      // Computes a warp-level GEMM on data held in shared memory
-      // Each "warp_mma_k" refers to a warp-level matrix multiply-accumulate
       CUTLASS_PRAGMA_UNROLL
-      for (int warp_mma_k = 0; warp_mma_k < Base::kWarpGemmIterations;
-           ++warp_mma_k) {
+      for (int warp_mma_k = 0; warp_mma_k < Base::kWarpGemmIterations; ++warp_mma_k) {
+
+        // Load warp-level tiles from shared memory, wrapping to k offset if this is the last group
+        // as the case may be.
+
+        if (warp_mma_k == Base::kWarpGemmIterations - 1) {
+
+          // Write fragments to shared memory
+          this->smem_iterator_A_.store(transform_A_(tb_frag_A));
 
-        // Load warp-level tiles from shared memory, wrapping to k offset if
-        // this is the last group as the case may be.
+          this->smem_iterator_B_.store(transform_B_(tb_frag_B));
+
+          // Wait until we have at least one completed global fetch stage
+          gmem_wait();
+
+          // Advance smem read and write stages
+          advance_smem_stages();
+        }
 
         this->warp_tile_iterator_A_.set_kgroup_index((warp_mma_k + 1) % Base::kWarpGemmIterations);
         this->warp_tile_iterator_B_.set_kgroup_index((warp_mma_k + 1) % Base::kWarpGemmIterations);
-        
-        this->warp_tile_iterator_A_.load(warp_loaded_frag_A[(warp_mma_k + 1) % 2]);
-        this->warp_tile_iterator_B_.load(warp_loaded_frag_B[(warp_mma_k + 1) % 2]);
+
+        this->warp_tile_iterator_A_.load(warp_frag_A[(warp_mma_k + 1) % 2]);
+        this->warp_tile_iterator_B_.load(warp_frag_B[(warp_mma_k + 1) % 2]);
 
         ++this->warp_tile_iterator_A_;
         ++this->warp_tile_iterator_B_;
 
-        if (warp_mma_k > 0)
-          warp_mma.transform(warp_transformed_frag_A[warp_mma_k % 2],
-                             warp_transformed_frag_B[warp_mma_k % 2],
-                             warp_loaded_frag_A[warp_mma_k % 2],
-                             warp_loaded_frag_B[warp_mma_k % 2]);
-
-        warp_mma(
-          accum, 
-          warp_transformed_frag_A[warp_mma_k % 2],
-          warp_transformed_frag_B[warp_mma_k % 2], 
-          accum,
-          gemm_k_reduction_accum
-        );
-
-        // Issue global->shared copies for the this stage
-        if (warp_mma_k < Base::kWarpGemmIterations - 1) {
-          int group_start_iteration_A, group_start_iteration_B;
+        if (warp_mma_k == 0) {
 
-          group_start_iteration_A = warp_mma_k * Detail::kAccessesPerGroupA;
-          group_start_iteration_B = warp_mma_k * Detail::kAccessesPerGroupB;
+          // Load fragment from global A
+          tb_frag_A.clear();
+          iterator_A.load(tb_frag_A);
+          ++iterator_A;
 
-          copy_tiles_and_advance(iterator_A, iterator_B, group_start_iteration_A, 
-                               group_start_iteration_B);
-        }
+          // Load fragment from global B
+          tb_frag_B.clear();
+          iterator_B.load(tb_frag_B);
+          ++iterator_B;
 
-        if (warp_mma_k + 2 == Base::kWarpGemmIterations) {
-          int group_start_iteration_A, group_start_iteration_B;
-          group_start_iteration_A =
-              (warp_mma_k + 1) * Detail::kAccessesPerGroupA;
-          group_start_iteration_B =
-              (warp_mma_k + 1) * Detail::kAccessesPerGroupB;
-
-          copy_tiles_and_advance(iterator_A, iterator_B, group_start_iteration_A, 
-                               group_start_iteration_B);
-
-          // Inserts a memory fence between stages of cp.async instructions.
-          cutlass::arch::cp_async_fence();
-
-          // Waits until kStages-2 stages have committed.
-          arch::cp_async_wait<Base::kStages - 2>();
-          __syncthreads();
-
-          // Move to the next stage
-          iterator_A.add_tile_offset({0, 1});
-          iterator_B.add_tile_offset({1, 0});
-
-          this->smem_iterator_A_.add_tile_offset({0, 1});
-          this->smem_iterator_B_.add_tile_offset({1, 0});
-
-          // Add negative offsets to return iterators to the 'start' of the
-          // circular buffer in shared memory
-          if (smem_write_stage_idx == (Base::kStages - 1)) {
-            this->smem_iterator_A_.add_tile_offset({0, -Base::kStages});
-            this->smem_iterator_B_.add_tile_offset({-Base::kStages, 0});
-            smem_write_stage_idx = 0;
-          } else {
-            ++smem_write_stage_idx;
-          }
-
-          if (smem_read_stage_idx == (Base::kStages - 1)) {
-            this->warp_tile_iterator_A_.add_tile_offset(
-                {0, -Base::kStages * Policy::kPartitionsK *
-                        Base::kWarpGemmIterations});
-            this->warp_tile_iterator_B_.add_tile_offset(
-                {-Base::kStages * Policy::kPartitionsK *
-                     Base::kWarpGemmIterations,
-                 0});
-            smem_read_stage_idx = 0;
-          } else {
-            ++smem_read_stage_idx;
-          }
-
-          --gemm_k_iterations;
-          iterator_A.clear_mask(gemm_k_iterations == 0);
-          iterator_B.clear_mask(gemm_k_iterations == 0);
+          // Avoid reading out of bounds if this was the last loop iteration
+          iterator_A.clear_mask(gemm_k_iterations <= 2);
+          iterator_B.clear_mask(gemm_k_iterations <= 2);
         }
 
-        // Do any conversions feeding the first stage at the end of the loop so
-        // we can start right away on mma instructions
-        if (warp_mma_k + 1 == Base::kWarpGemmIterations)
-          warp_mma.transform(warp_transformed_frag_A[(warp_mma_k + 1) % 2],
-                             warp_transformed_frag_B[(warp_mma_k + 1) % 2],
-                             warp_loaded_frag_A[(warp_mma_k + 1) % 2],
-                             warp_loaded_frag_B[(warp_mma_k + 1) % 2]);
+        warp_mma(
+          accum,
+          warp_frag_A[warp_mma_k % 2],
+          warp_frag_B[warp_mma_k % 2],
+          accum);
       }
+    }
+
+  }
+
+
+  /// Prepares the class for another prologue.
+  CUTLASS_DEVICE
+  void wind_down()
+  {
+    // First, increment remaining warp tiles to catch it up with the write stage.
+    #pragma unroll
+    for (int warp_mma_k = 1; warp_mma_k < Base::kWarpGemmIterations; ++warp_mma_k)
+    {
+      this->warp_tile_iterator_A_.set_kgroup_index(warp_mma_k);
+      this->warp_tile_iterator_B_.set_kgroup_index(warp_mma_k);
 
+      ++this->warp_tile_iterator_A_;
+      ++this->warp_tile_iterator_B_;
     }
-    
-    if (SharedMemoryClear == SharedMemoryClearOption::kZfill) {
-      // commit and drain all pending and predicated LDGSTS pnz from the GEMM mainloop
-      cutlass::arch::cp_async_fence();
-      cutlass::arch::cp_async_wait<0>();
-      __syncthreads();
+
+    // If we bumped the read iterators to the end of the circular buffer, wrap them around to
+    // align them with the write iterators
+    if (smem_write_stage_idx == 0)
+    {
+      this->warp_tile_iterator_A_.add_tile_offset(
+        {0, -Base::kStages * Policy::kPartitionsK * Base::kWarpGemmIterations});
+      this->warp_tile_iterator_B_.add_tile_offset(
+        {-Base::kStages * Policy::kPartitionsK * Base::kWarpGemmIterations, 0});
     }
+  }
 
+  /// Perform a threadblock-scoped matrix multiply-accumulate
+  CUTLASS_DEVICE
+  void operator()(
+    int gemm_k_iterations,                            ///< number of iterations of the mainloop
+    FragmentC &accum,                                 ///< destination accumulator tile
+    IteratorA iterator_A,                             ///< iterator over A operand in global memory
+    IteratorB iterator_B,                             ///< iterator over B operand in global memory
+    FragmentC const &src_accum)                       ///< source accumulator tile
+  {
+    // Prologue
+    prologue(iterator_A, iterator_B, gemm_k_iterations);
+
+    // Wait until we have at least one completed global fetch stage
+    gmem_wait();
+
+    // Perform accumulation in the 'd' output operand
+    accum = src_accum;
+
+    // Perform the MAC-iterations
+    gemm_iters(gemm_k_iterations, accum, iterator_A, iterator_B);
   }
+
 };
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
-}  // namespace threadblock
-}  // namespace gemm
-}  // namespace cutlass
+} // namespace threadblock
+} // namespace gemm
+} // namespace cutlass
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/threadblock_swizzle.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/threadblock_swizzle.h`

 * *Files 17% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -37,79 +37,25 @@
 
 #include "cutlass/cutlass.h"
 #include "cutlass/layout/matrix.h"
 #include "cutlass/platform/platform.h"
 #include "cutlass/gemm/gemm.h"
 #include "cutlass/conv/conv2d_problem_size.h"
 #include "cutlass/conv/conv3d_problem_size.h"
+#include "cutlass/gemm/threadblock/index_remat.h"
+#include "cutlass/gemm/threadblock/threadblock_swizzle_streamk.h"
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
 namespace cutlass {
 namespace gemm {
 namespace threadblock {
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
-/// Helper to rematerialize block Idx. Reduces register liveness.
-CUTLASS_DEVICE
-int RematerializeThreadIdxX() {
-  return threadIdx.x;
-}
-
-/// Helper to rematerialize block Idx. Reduces register liveness.
-CUTLASS_DEVICE
-int RematerializeThreadIdxY() {
-  return threadIdx.y;
-}
-
-/// Helper to rematerialize block Idx. Reduces register liveness.
-CUTLASS_DEVICE
-int RematerializeThreadIdxZ() {
-  return threadIdx.z;
-}
-
-/// Helper to rematerialize block Idx. Reduces register liveness.
-CUTLASS_DEVICE
-int RematerializeBlockIdxX() {
-  return blockIdx.x;
-}
-
-/// Helper to rematerialize block Idx. Reduces register liveness.
-CUTLASS_DEVICE
-int RematerializeBlockIdxY() {
-  return blockIdx.y;
-}
-
-/// Helper to rematerialize block Idx. Reduces register liveness.
-CUTLASS_DEVICE
-int RematerializeBlockIdxZ() {
-  return blockIdx.z;
-}
-
-/// Helper to rematerialize block Dim. Reduces register liveness.
-CUTLASS_DEVICE
-int RematerializeBlockDimX() {
-  return blockDim.x;
-}
-
-/// Helper to rematerialize block Dim. Reduces register liveness.
-CUTLASS_DEVICE
-int RematerializeBlockDimY() {
-  return blockDim.y;
-}
-
-/// Helper to rematerialize block Dim. Reduces register liveness.
-CUTLASS_DEVICE
-int RematerializeBlockDimZ() {
-  return blockDim.z;
-}
-
-/////////////////////////////////////////////////////////////////////////////////////////////////
-
 /// Threadblock swizzling function for GEMMs
 template <int N = 1>
 struct GemmIdentityThreadblockSwizzle {
 
   CUTLASS_HOST_DEVICE
   GemmIdentityThreadblockSwizzle() { }
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/threadblock_swizzle_streamk.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/threadblock_swizzle_streamk.h`

 * *Files 0% similar despite different names*

```diff
@@ -39,19 +39,18 @@
 #include "cutlass/layout/matrix.h"
 #include "cutlass/platform/platform.h"
 #include "cutlass/gemm/gemm.h"
 #include "cutlass/conv/conv2d_problem_size.h"
 #include "cutlass/conv/conv3d_problem_size.h"
 #include "cutlass/gemm/threadblock/index_remat.h"
 
-#if !defined(__CUDACC_RTC__)
 #include <iostream>
 #include "cutlass/core_io.h"
 #include "cutlass/trace.h"
-#endif
+
 
 
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
 namespace cutlass {
 namespace gemm {
@@ -634,27 +633,34 @@
 // Guards needed for PyCUTLASS library generation
 #if defined(__NVCC__) || (defined(__clang__) && defined(__CUDA__)) || defined(__CUDACC_RTC__)
 
   //
   // Device-side interface
   //
 
+  /// Proves to the compiler that val is warp-uniform
+  CUTLASS_DEVICE
+  int uniform(int val) const
+  {
+    return __shfl_sync(0xffffffff, val, 0);
+  }
+
   /// Obtains number of threadblocks per GEMM
   CUTLASS_DEVICE
   int device_num_blocks() const
   {
     return gridDim.x;
   }
 
   /// Obtains tile index for the given sk iteration
   CUTLASS_DEVICE
   int get_sk_tile_idx(int iter) const
   {
     int tile_idx = div_mod_iters_per_tile.div(iter);
-    return tile_idx;
+    return uniform(tile_idx);
   }
 
   /// Obtains the batch index
   CUTLASS_DEVICE
   int get_batch_idx() const
   {
     return RematerializeBlockIdxZ();
@@ -724,15 +730,15 @@
     {
       int block_in_region;
       int region;
       div_mod_sk_regions(block_in_region, region, block_idx);
       block_idx = (region * sk_blocks_per_region()) + block_in_region;
     }
 
-    return block_idx;
+    return uniform(block_idx);
   }
 
 
   /// Obtains calling linear threadblock index of the first block to work on the given tile
   CUTLASS_DEVICE
   int get_sk_block_idx(int iter) const
   {
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/warp/default_mma_complex_tensor_op.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/gemm/warp/default_mma_complex_tensor_op.h`

 * *Files 11% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -210,15 +210,15 @@
     TransformB>;
 };
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 /// Partial specialization - input and output types are complex<float>*complex<float> 
 //  Use TF32 tensor operation internally
-//  4 real-valued MMA.1688.F32.TF32 operations on TF32 
+//  4 real-valued mma.sync.aligned.m16n8k8.f32.tf32.tf32.f32 operations on TF32 
 //  A = (ar + j ai), B (br +j bi), D = AB
 //  D = dr + j di = (ar*br - ai*bi) + j (ar*bi + ai*br) 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 template <
     /// Size of the Gemm problem - concept: gemm::GemmShape<>
     typename WarpShape_,
     /// Shape of one matrix production operation (concept: GemmShape)
@@ -242,15 +242,15 @@
     LayoutB,
     complex<float>,
     LayoutC,
     TransformA,
     TransformB,
     arch::OpMultiplyAddComplex> {
 
-  // Complex floating point tensor operation use MMA.1688.F32.TF32 mma instruction
+  // Complex floating point tensor operation use mma.sync.aligned.m16n8k8.f32.tf32.tf32.f32 mma instruction
   using Policy = cutlass::gemm::warp::MmaTensorOpPolicy<
       cutlass::arch::Mma<
         InstructionShape_, 
         32, 
         tfloat32_t,
         cutlass::layout::RowMajor,
         tfloat32_t,
@@ -274,15 +274,15 @@
     TransformA,
     TransformB>;
 };
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 /// Partial specialization - input and output types are complex<float>*complex<float> 
 //  Use BF16 tensor operation internally
-//  4 real-valued MMA.1688.F32.BF16 operations on BF16
+//  4 real-valued mma.sync.aligned.m16n8k8.f32.bf16.bf16.f32 operations on BF16
 //  A = (ar + j ai), B (br +j bi), D = AB
 //  D = dr + j di = (ar*br - ai*bi) + j (ar*bi + ai*br) 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 template <
     /// Size of the Gemm problem - concept: gemm::GemmShape<>
     typename WarpShape_,
     /// Shape of one matrix production operation (concept: GemmShape)
@@ -306,15 +306,15 @@
     LayoutB,
     complex<float>,
     LayoutC,
     TransformA,
     TransformB,
     arch::OpMultiplyAddFastBF16> {
 
-  // Complex floating point tensor operation use MMA.1688.F32.BF16 mma instruction
+  // Complex floating point tensor operation use mma.sync.aligned.m16n8k8.f32.bf16.bf16.f32 mma instruction
   using Policy = cutlass::gemm::warp::MmaTensorOpPolicy<
       cutlass::arch::Mma<
         InstructionShape_, 
         32, 
         bfloat16_t,
         cutlass::layout::RowMajor,
         bfloat16_t,
@@ -338,15 +338,15 @@
     TransformA,
     TransformB>;
 };
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 /// Partial specialization - input and output types are complex<float>*complex<float> 
 //  Use F16 tensor operation internally
-//  4 real-valued MMA.1688.F32.F16 operations on F16
+//  4 real-valued mma.sync.aligned.m16n8k8.f32.f16.f16.f32 operations on F16
 //  A = (ar + j ai), B (br +j bi), D = AB
 //  D = dr + j di = (ar*br - ai*bi) + j (ar*bi + ai*br) 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 template <
     /// Size of the Gemm problem - concept: gemm::GemmShape<>
     typename WarpShape_,
     /// Shape of one matrix production operation (concept: GemmShape)
@@ -370,15 +370,15 @@
     LayoutB,
     complex<float>,
     LayoutC,
     TransformA,
     TransformB,
     arch::OpMultiplyAddFastF16> {
 
-  // Complex floating point tensor operation use MMA.1688.F32.F16 mma instruction
+  // Complex floating point tensor operation use mma.sync.aligned.m16n8k8.f32.f16.f16.f32 mma instruction
   using Policy = cutlass::gemm::warp::MmaTensorOpPolicy<
       cutlass::arch::Mma<
         InstructionShape_, 
         32, 
         half_t,
         cutlass::layout::RowMajor,
         half_t,
@@ -403,15 +403,15 @@
     TransformB>;
 };
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 /// 3xTF32 or 4xTF32 (fast and accurate complex<float> operation)
 /// Partial specialization - input and output types are complex<float> * complex<float> 
 //  Use 3xTF32 or 4xTF32 tensor operation internally
-//  4 real-valued MMA.1688.F32.TF32 operations on TF32 
+//  4 real-valued mma.sync.aligned.m16n8k8.f32.tf32.tf32.f32 operations on TF32 
 //  A = (ar + j ai), B (br +j bi), D = AB
 //  D = dr + j di = 3x[(ar*br - ai*bi) + j (ar*bi + ai*br)]
 /////////////////////////////////////////////////////////////////////////////////////////////////
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
 template <
     /// Size of the Gemm problem - concept: gemm::GemmShape<>
@@ -437,15 +437,15 @@
     LayoutB,
     complex<float>,
     LayoutC,
     TransformA,
     TransformB,
     arch::OpMultiplyAddComplexFastF32> {
 
-  // Complex floating point tensor operation use MMA.1688.F32.TF32 mma instruction
+  // Complex floating point tensor operation use mma.sync.aligned.m16n8k8.f32.tf32.tf32.f32 mma instruction
   using Policy = cutlass::gemm::warp::MmaTensorOpPolicy<
       cutlass::arch::Mma<
         InstructionShape_, 
         32, 
         tfloat32_t,
         cutlass::layout::RowMajor,
         tfloat32_t,
@@ -466,10 +466,147 @@
     complex<float>,
     LayoutC, 
     Policy,
     TransformA,
     TransformB>;
 };
 
+/////////////////////////////////////////////////////////////////////////////////////////////////
+
+/// Partial specialization for complex<double>*complex<double> case
+//  4 real-valued mma.sync.aligned.m16n8k4.f64.f64.f64.f64 operations
+//  A = (ar + j ai), B (br +j bi), D = AB
+//  D = dr + j di = (ar*br - ai*bi) + j (ar*bi + ai*br) 
+/////////////////////////////////////////////////////////////////////////////////////////////////
+template <
+    /// Size of the Gemm problem - concept: gemm::GemmShape<>
+    typename WarpShape_,
+    /// Real-valued underlying type of complex-valued A operand
+    typename RealElementA,
+    /// Layout of A matrix (concept: MatrixLayout)
+    typename LayoutA,
+    /// Real-valued underlying type of complex-valued B operand
+    typename RealElementB,
+    /// Layout of B matrix (concept: MatrixLayout)
+    typename LayoutB,
+    /// Real-valued underlying type of complex-valued C operand
+    typename RealElementC,
+    /// Layout of C matrix (concept: MatrixLayout)
+    typename LayoutC,
+    /// Complex transform on A operand
+    ComplexTransform TransformA,
+    /// Complex transform on B operand
+    ComplexTransform TransformB>
+struct DefaultMmaComplexTensorOp<
+    WarpShape_,
+    GemmShape<16, 8, 4>,
+    complex<RealElementA>,
+    LayoutA,
+    complex<RealElementB>,
+    LayoutB,
+    complex<RealElementC>,
+    LayoutC,
+    TransformA,
+    TransformB,
+    arch::OpMultiplyAddComplex> {
+
+  using Policy = cutlass::gemm::warp::MmaTensorOpPolicy<
+      cutlass::arch::Mma<
+        GemmShape<16, 8, 4>,
+        32, 
+        RealElementA,
+        cutlass::layout::RowMajor,
+        RealElementB,
+        cutlass::layout::ColumnMajor,
+        RealElementC,
+        cutlass::layout::RowMajor, 
+        arch::OpMultiplyAdd>,
+      cutlass::MatrixShape<1, 1>
+    >;
+
+  // Define the warp-level tensor op
+  using Type = cutlass::gemm::warp::MmaComplexTensorOp<
+    WarpShape_,
+    complex<RealElementA>,
+    LayoutA,
+    complex<RealElementB>,
+    LayoutB,
+    complex<RealElementC>,
+    LayoutC, 
+    Policy,
+    TransformA,
+    TransformB,
+    true>;
+};
+
+/////////////////////////////////////////////////////////////////////////////////////////////////
+/// Partial specialization for complex<T>*complex<T> case using GaussianComplex operation
+//  3 real-valued mma.sync.aligned.m16n8k4.f64.f64.f64.f64 operations 
+//  A  = (ar + j ai), B = (br +j bi), D = AB
+//  P1 = (ar + ai) * br, P2 = - ar * (br - bi), P3 = ai * (br + bi) 
+//  D  = dr + j di = (P1 - P3) + j (P1 + P2)
+/////////////////////////////////////////////////////////////////////////////////////////////////
+template <
+    /// Size of the Gemm problem - concept: gemm::GemmShape<>
+    typename WarpShape_,
+    /// Real-valued underlying type of complex-valued A operand
+    typename RealElementA,
+    /// Layout of A matrix (concept: MatrixLayout)
+    typename LayoutA,
+    /// Real-valued underlying type of complex-valued B operand
+    typename RealElementB,
+    /// Layout of B matrix (concept: MatrixLayout)
+    typename LayoutB,
+    /// Real-valued underlying type of complex-valued C operand
+    typename RealElementC,
+    /// Layout of C matrix (concept: MatrixLayout)
+    typename LayoutC,
+    /// Complex transform on A operand
+    ComplexTransform TransformA,
+    /// Complex transform on B operand
+    ComplexTransform TransformB>
+struct DefaultMmaComplexTensorOp<
+    WarpShape_,
+    GemmShape<16, 8, 4>,
+    complex<RealElementA>,
+    LayoutA,
+    complex<RealElementB>,
+    LayoutB,
+    complex<RealElementC>,
+    LayoutC,
+    TransformA,
+    TransformB,
+    arch::OpMultiplyAddGaussianComplex> {
+
+  using Policy = cutlass::gemm::warp::MmaTensorOpPolicy<
+      cutlass::arch::Mma<
+        GemmShape<16, 8, 4>,
+        32, 
+        RealElementA,
+        cutlass::layout::RowMajor,
+        RealElementB,
+        cutlass::layout::ColumnMajor,
+        RealElementC,
+        cutlass::layout::RowMajor, 
+        arch::OpMultiplyAdd>,
+      cutlass::MatrixShape<1, 1>
+    >;
+
+  // Define the warp-level tensor op
+  using Type = cutlass::gemm::warp::MmaGaussianComplexTensorOp<
+    WarpShape_,
+    complex<RealElementA>,
+    LayoutA,
+    complex<RealElementB>,
+    LayoutB,
+    complex<RealElementC>,
+    LayoutC, 
+    Policy,
+    TransformA,
+    TransformB,
+    true>;
+};
+/////////////////////////////////////////////////////////////////////////////////////////////////
+
 } // namespace warp
 } // namespace gemm
 } // namespace cutlass
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/warp/default_mma_sparse_tensor_op.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/gemm/warp/default_mma_sparse_tensor_op.h`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/warp/default_mma_tensor_op.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/gemm/warp/default_mma_tensor_op.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/warp/default_mma_tensor_op_sm80.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/gemm/warp/default_mma_tensor_op_sm80.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/warp/default_mma_with_reduction_tensor_op.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/gemm/warp/default_mma_with_reduction_tensor_op.h`

 * *Files 3% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -57,15 +57,17 @@
     /// Layout of B matrix (concept: MatrixLayout)
     typename LayoutB,
     /// Element type of C matrix
     typename ElementC,
     /// Layout of C matrix (concept: MatrixLayout)
     typename LayoutC,
     /// Operator describing the tensor operation
-    typename Operator_ = arch::OpMultiplyAdd,
+    typename Operator_,
+    /// Reduce operand A or B along K dimension
+    bool ReduceKForA_,
     /// Number of partitions along K dimension
     int PartitionsK = 1,
     /// Store the accumulators in row major or column major.  Row major is used
     /// when output layout is interleaved.
     bool AccumulatorsInRowMajor = false>
 struct DefaultMmaWithReductionTensorOp {
   using Policy = cutlass::gemm::warp::MmaTensorOpPolicy<
@@ -74,15 +76,15 @@
                          cutlass::layout::ColumnMajor, ElementC,
                          cutlass::layout::RowMajor, Operator_>,
       cutlass::MatrixShape<1, 1> >;
 
   // Define the warp-level tensor op
   using Type = cutlass::gemm::warp::MmaWithReductionTensorOp<
       WarpShape_, ElementA, LayoutA, ElementB, LayoutB, ElementC, LayoutC,
-      Policy, PartitionsK, AccumulatorsInRowMajor>;
+      Policy, ReduceKForA_, PartitionsK, AccumulatorsInRowMajor>;
 };
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
 } // namespace warp
 } // namespace gemm
 } // namespace cutlass
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/warp/default_mma_wmma_tensor_op.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/gemm/warp/default_mma_wmma_tensor_op.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/warp/layernorm_scale_bias_transform.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/gemm/warp/layernorm_scale_bias_transform.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/warp/mma.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/gemm/warp/mma.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/warp/mma_complex_tensor_op.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/gemm/warp/mma_gaussian_complex_tensor_op.h`

 * *Files 18% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -37,202 +37,36 @@
 
 #include "cutlass/cutlass.h"
 
 #include "cutlass/array.h"
 #include "cutlass/complex.h"
 #include "cutlass/numeric_types.h"
 #include "cutlass/matrix_shape.h"
-#include "cutlass/functional.h"
 
 #include "cutlass/arch/memory_sm75.h"
 #include "cutlass/arch/mma_sm75.h"
 #include "cutlass/arch/mma_sm80.h"
+
 #include "cutlass/gemm/gemm.h"
 #include "cutlass/gemm/warp/mma.h"
 
 #include "cutlass/gemm/warp/mma_tensor_op_policy.h"
 #include "cutlass/gemm/warp/mma_tensor_op.h"
 
 #include "cutlass/gemm/warp/mma_tensor_op_tile_iterator.h"
-#include "cutlass/gemm/warp/mma_tensor_op_tile_iterator_sm80.h"
-#include "cutlass/gemm/warp/mma_complex_tensor_op_tile_iterator_sm80.h"
+#include "cutlass/gemm/warp/mma_gaussian_complex_tensor_op_tile_iterator_sm80.h"
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
 namespace cutlass {
 namespace gemm {
 namespace warp {
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
-namespace detail {
-
-template <
-  /// Data type of real & imag members of complex numbers in the SourceFragment
-  typename RealElement,
-  /// Destination fragment required by the mma operation 
-  typename DestinationFragment,
-  /// Source fragment holding complex<RealElement> elements
-  typename SourceFragment,
-  /// Number of mma operations performed
-  typename MmaIterations,
-  /// Shape of operand elements
-  typename MmaOperandShape,
-  /// Complex transform on A operand
-  ComplexTransform Transform_,
-  /// Operand A or Operand B
-  Operand Operand_,
-  /// Floating-point rounding style
-  FloatRoundStyle Round_>
-struct UnpackComplexConvertAndPackForMma;
-
-// Partial specialization for OperandA and Congruous smem layout
-template <
-  typename RealElement,
-  typename DestinationFragment, 
-  typename SourceFragment,
-  typename MmaIterations,
-  typename MmaOperandShape,
-  ComplexTransform Transform_,
-  FloatRoundStyle Round_>
-struct UnpackComplexConvertAndPackForMma <
-  RealElement,
-  DestinationFragment,
-  SourceFragment,
-  MmaIterations,
-  MmaOperandShape,
-  Transform_,
-  Operand::kA,
-  Round_> {
-  
-  //
-  // Type definitions
-  //
-  static Operand const kOperand = Operand::kA;
-  static ComplexTransform const kTransform = Transform_;
-  static FloatRoundStyle const kRound = Round_;
-
-  // Data type of elements in the destination fragment
-  using MmaElement = typename DestinationFragment::Element;
-
-  // Numeric convertor MmaElement <= RealElement
-  using Converter = NumericConverter<MmaElement, RealElement, kRound>;
-
-  // Operand layout parameters
-  using SourceFragmentLayout = layout::ColumnMajor;
-  static int const kLdm = MmaIterations::kRow * MmaOperandShape::kRow;
-
-  /// Ctor
-  CUTLASS_DEVICE
-  UnpackComplexConvertAndPackForMma() {}
-
-  CUTLASS_DEVICE
-  void operator()(DestinationFragment *dest, SourceFragment const &source) {
-    
-    Converter convert_op;
-    SourceFragmentLayout layout(kLdm);
-
-    CUTLASS_PRAGMA_UNROLL
-    for(int i=0; i<MmaIterations::kRow; i++) {
-      int pos = 0;
-      CUTLASS_PRAGMA_UNROLL
-      for(int c=0; c<MmaOperandShape::kColumn; c++) {
-        CUTLASS_PRAGMA_UNROLL
-        for(int r=0; r<MmaOperandShape::kRow; r++) {
-          // Logical position of element in source fragment
-          int row = r + i * MmaOperandShape::kRow;
-          int col = c;
-
-          // Access complex<RealElement> and apply rounding on real and imag parts
-          MmaElement a = convert_op(source[layout(MatrixCoord{row,col})].real());
-          MmaElement b = convert_op(source[layout(MatrixCoord{row,col})].imag());
-
-          // Unpack rounded complex<MmaElement> and pack into DestinationFragment for mma operation
-          dest[i][pos] = a;
-          dest[i+MmaIterations::kRow][pos++] = (kTransform == ComplexTransform::kConjugate ? -b : b);
-
-        }
-      }
-    }
-  }
-};
-
-// Partial specialization for OperandB and Congruous smem layout
-template <
-  typename RealElement,
-  typename DestinationFragment, 
-  typename SourceFragment,
-  typename MmaIterations,
-  typename MmaOperandShape,
-  ComplexTransform Transform_,
-  FloatRoundStyle Round_>
-struct UnpackComplexConvertAndPackForMma <
-  RealElement,
-  DestinationFragment,
-  SourceFragment,
-  MmaIterations,
-  MmaOperandShape,
-  Transform_,
-  Operand::kB,
-  Round_> {
-  
-  //
-  // Type definitions
-  //
-  static Operand const kOperand = Operand::kB;
-  static ComplexTransform const kTransform = Transform_;
-  static FloatRoundStyle const kRound = Round_;
-
-  // Data type of elements in the destination fragment
-  using MmaElement = typename DestinationFragment::Element;
-
-  // Numeric convertor MmaElement <= RealElement
-  using Converter = NumericConverter<MmaElement, RealElement, kRound>;
-
-  // Operand layout parameters
-  using SourceFragmentLayout = layout::RowMajor;
-  static int const kLdm = MmaIterations::kColumn * MmaOperandShape::kColumn;
-
-  /// Ctor
-  CUTLASS_DEVICE
-  UnpackComplexConvertAndPackForMma() {}
-
-  CUTLASS_HOST_DEVICE
-  void operator()(DestinationFragment *dest, SourceFragment const &source) {
-    
-    Converter convert_op;
-    SourceFragmentLayout layout(kLdm);
-
-    CUTLASS_PRAGMA_UNROLL
-    for(int i=0; i<MmaIterations::kColumn; i++) {
-      int pos = 0;
-      CUTLASS_PRAGMA_UNROLL
-      for(int c=0; c<MmaOperandShape::kColumn; c++) {
-        CUTLASS_PRAGMA_UNROLL
-        for(int r=0; r<MmaOperandShape::kRow; r++) {
-          // Logical position of element in source fragment
-          int row = r;
-          int col = c + i * MmaOperandShape::kColumn;
-
-          // Access complex<RealElement> apply rounding on real and imag parts
-          MmaElement a = convert_op(source[layout(MatrixCoord{row,col})].real());
-          MmaElement b = convert_op(source[layout(MatrixCoord{row,col})].imag());
-
-          // Unpack rounded complex<MmaElement> and pack into DestinationFragment for mma operation
-          dest[i][pos] = a;
-          dest[i+MmaIterations::kColumn][pos++] = (kTransform == ComplexTransform::kConjugate ? -b : b);
-        }
-      }
-    }
-  }
-};
-} // namespace detail 
-
-/////////////////////////////////////////////////////////////////////////////////////////////////
-
 template <
   /// Size of the Gemm problem - concept: gemm::GemmShape<>
   typename Shape_,
   /// Data type of A elements
   typename RealElementA,
   /// Layout of A matrix (concept: MatrixLayout)
   typename LayoutA_,
@@ -251,15 +85,15 @@
   /// Complex transform on B operand
   ComplexTransform TransformB = ComplexTransform::kNone,
   /// Do source operands need more than one elements
   bool GeneralizedOperatorElements = false,
   /// Used for partial specialization
   typename Enable = bool
 >
-class MmaComplexTensorOp;
+class MmaGaussianComplexTensorOp;
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
 /// Partial specialization for complex*complex+complex => complex using real-valued TensorOps
 template <
   /// Size of the Gemm problem - concept: gemm::GemmShape<>
   typename Shape_,
@@ -278,15 +112,15 @@
   /// Policy describing warp-level MmaTensorOp (concept: MmaTensorOp policy)
   typename Policy_,
   /// Complex transform on A operand
   ComplexTransform TransformA,
   /// Complex transform on B operand
   ComplexTransform TransformB
 >
-class MmaComplexTensorOp<
+class MmaGaussianComplexTensorOp<
   Shape_, 
   complex<RealElementA>, 
   LayoutA_, 
   complex<RealElementB>,
   LayoutB_,
   complex<RealElementC>,
   LayoutC_,
@@ -311,38 +145,39 @@
 
   /// Data type of accumulator matrix C
   using ElementC = complex<RealElementC>;
 
   /// Layout of accumulator matrix C
   using LayoutC = LayoutC_;
 
-  /// Shape of the warp in units of thread (concept: MmaLanePolicyTensorOp)
+  /// Shape of the warp in units of thread (concept: MmaLanePolicySimt)
   using Policy = Policy_;
 
   /// Underlying matrix multiply operator (concept: arch::Mma)
   using ArchMmaOperator = typename Policy::Operator;
 
-  /// Architecture tag from underlying instruction
+  /// Shape of underlying instruction
+  using InstructionShape = typename ArchMmaOperator::Shape;
+
+  /// Underlying arch tag
   using ArchTag = typename ArchMmaOperator::ArchTag;
 
   /// Indicates class of matrix operator
   using OperatorClass = arch::OpClassTensorOp;
 
-  /// Shape of underlying instruction
-  using InstructionShape = typename ArchMmaOperator::Shape;
-
   /// Indicates math operator 
-  using MathOperator = arch::OpMultiplyAddComplex;
-
+  using MathOperator = arch::OpMultiplyAddGaussianComplex;
+  
   /// Complex transform on A operand
   static ComplexTransform const kTransformA = TransformA;
 
   /// Complex transform on B operand
   static ComplexTransform const kTransformB = TransformB;
 
+
   /// Number of threads participating in warp-level matrix product
   static int const kThreadCount = 32;
 
 public:
 
   /// Iterates over the A operand in memory
   using IteratorA = MmaTensorOpMultiplicandTileIterator<
@@ -388,30 +223,30 @@
   /// Number of mma operations performed
   using MmaIterations = MatrixShape<
     Shape::kM / ArchMmaOperator::Shape::kM,
     Shape::kN / ArchMmaOperator::Shape::kN
   >;
 
   /// Iterates over the C operand in memory
-  using IteratorC = MmaTensorOpAccumulatorTileIterator<
+  using IteratorC = MmaTensorOpGaussianComplexAccumulatorTileIterator<
      MatrixShape<Shape::kM, Shape::kN>, 
      ElementC, 
      LayoutC,
      typename ArchMmaOperator::Shape, 
      typename Policy::OpDelta>;
 
   /// Storage for C tile, the accumulator. Note, regardless of multiplicand type, this
-  /// storage arrangement is to be considered 'planar complex' in the sense that all real-valued
-  /// parts are stored consecutively followed by all imaginary parts. This matches the structure
-  /// of Tensor Cores which are always real-valued matrix multiplies.
+  /// storage arrangement is to be considered 'gaussian complex' in the sense that the accumulation is
+  /// done in three parts namely part1, part2, and part3. The parts 1, 2, and 3 are stored consecutively 
+  /// in InteratorC::Frament. This matches the structure of Tensor Cores which are always real-valued matrix multiplies.
   using FragmentC = typename IteratorC::Fragment;
 
   static_assert(
-    FragmentC::kElements == 2 * MmaIterations::kCount * ArchMmaOperator::FragmentC::kElements,
-    "Unexpected planar complex fragment length.");
+    FragmentC::kElements == 3 * MmaIterations::kCount * ArchMmaOperator::FragmentC::kElements,
+    "Unexpected gaussian complex fragment length.");
 
 private:
 
   //
   // Data members
   //
 
@@ -422,15 +257,15 @@
 
   //
   // Methods
   //
 
   /// Ctor
   CUTLASS_DEVICE
-  MmaComplexTensorOp() {}
+  MmaGaussianComplexTensorOp() {}
 
   /// Performs a warp-level matrix multiply-accumulate operation
   CUTLASS_DEVICE
   void operator()(
     FragmentC &D, 
     FragmentA const &A, 
     FragmentB const &B, 
@@ -451,85 +286,66 @@
       "We can geneneralize later.");
 
     D = C;
 
     CUTLASS_PRAGMA_UNROLL
     for (int m = 0; m < MmaIterations::kRow; ++m) {
 
-      // mma(accum.real(), a.real(), b.real(), accum.real());
+      // mma(accum.part1(), (a.real() + a.imag()), b.real(), accum.part1());
       CUTLASS_PRAGMA_UNROLL
       for (int n = 0; n < MmaIterations::kColumn; ++n) {
 
         // Pack operands together. This may result in actual MOVs 
-        MmaOperandA operand_A;
-        MmaOperandB operand_B;
+        MmaOperandA operand_Asum;
+        MmaOperandB operand_Br;
 
-        operand_A[0] = A[m].real();
-        operand_B[0] = B[n].real();
+        operand_Asum[0] = A[m].real() + ((kTransformA == ComplexTransform::kConjugate) ? -A[m].imag() : +A[m].imag());
+        operand_Br[0] = B[n].real();
 
-        // Real-valued accumulator part
+        // accumulator part1
         MmaOperandC *accum = reinterpret_cast<MmaOperandC *>(&D) + 
           (m + n * MmaIterations::kRow);
 
-          mma(*accum, operand_A, operand_B, *accum);
+        mma(*accum, operand_Asum, operand_Br, *accum);
       }
 
-      // mma(accum.imag(), a.real(), b.imag(), accum.imag()); 
+      // mma(accum.part2(), -a.real(), (b.real() - b.imag()), accum.part2()); 
       CUTLASS_PRAGMA_UNROLL
       for (int n = MmaIterations::kColumn - 1; n >= 0; --n) {
 
         // Pack operands together. This may result in actual MOVs 
-        MmaOperandA operand_A;
-        MmaOperandB operand_B;
+        MmaOperandA operand_Ar;
+        MmaOperandB operand_Bdiff;
 
-        operand_A[0] = A[m].real();
-        operand_B[0] = (kTransformB == ComplexTransform::kConjugate ? -B[n].imag() : B[n].imag());
+        operand_Ar[0] = -A[m].real();
+        operand_Bdiff[0] = B[n].real() - ((kTransformB == ComplexTransform::kConjugate) ? -B[n].imag() : +B[n].imag());
 
-        // Complex-valued accumulator part
+        // accumulator part2
         MmaOperandC *accum = reinterpret_cast<MmaOperandC *>(&D) + 
           (m + n * MmaIterations::kRow) + MmaIterations::kCount;
 
-        mma(*accum, operand_A, operand_B, *accum);
+        mma(*accum, operand_Ar, operand_Bdiff, *accum);
       }
 
-      // mma(accum.real(), -a.imag(), b.imag(), accum.real())
+      // mma(accum.part3(), a.imag(), (b.real() + b.imag()), accum.part3())
       CUTLASS_PRAGMA_UNROLL
       for (int n = 0; n < MmaIterations::kColumn; ++n) {
 
         // Pack operands together. This may result in actual MOVs 
-        MmaOperandA operand_A;
-        MmaOperandB operand_B;
-
-        // A imaginary part is intentionally negated
-        operand_A[0] = (kTransformA == ComplexTransform::kConjugate ? A[m].imag() : -A[m].imag());
-        operand_B[0] = (kTransformB == ComplexTransform::kConjugate ? -B[n].imag() : B[n].imag());
-
-        // Real-valued accumulator part
-        MmaOperandC *accum = reinterpret_cast<MmaOperandC *>(&D) + 
-          (m + n * MmaIterations::kRow);
-
-        mma(*accum, operand_A, operand_B, *accum);
-      }
-
-      // mma(accum.imag(), a.imag(), b.real(), accum.imag())
-      CUTLASS_PRAGMA_UNROLL
-      for (int n = MmaIterations::kColumn - 1; n >= 0; --n) {
-
-        // Pack operands together. This may result in actual MOVs 
-        MmaOperandA operand_A;
-        MmaOperandB operand_B;
+        MmaOperandA operand_Ai;
+        MmaOperandB operand_Bsum;
 
-        operand_A[0] = (kTransformA == ComplexTransform::kConjugate ? -A[m].imag() : A[m].imag());
-        operand_B[0] = B[n].real();
+        operand_Ai[0] = (kTransformA == ComplexTransform::kConjugate) ? -A[m].imag() : +A[m].imag();
+        operand_Bsum[0] = B[n].real() + ((kTransformB == ComplexTransform::kConjugate) ? -B[n].imag() : +B[n].imag());
 
-        // Complex-valued accumulator part
+        // accumulator part3
         MmaOperandC *accum = reinterpret_cast<MmaOperandC *>(&D) + 
-          (m + n * MmaIterations::kRow) + MmaIterations::kCount;
+          (m + n * MmaIterations::kRow) + 2 * MmaIterations::kCount;
 
-        mma(*accum, operand_A, operand_B, *accum);
+        mma(*accum, operand_Ai, operand_Bsum, *accum);
       }
     }
   }
 
   /// Transform the mma operands to the required types
   CUTLASS_DEVICE
   void transform(TransformedFragmentA &dst_A, TransformedFragmentB &dst_B,
@@ -538,73 +354,65 @@
     dst_A = A;
     dst_B = B;
   }
 };
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
-/// Partial specialization for complex*complex+complex => complex:
-//  Operands data type: complex<float>
-//  Rounding: float -> tfloat32_t (round half_ulp_truncate nearest)
-//  Math instruction: MMA.1688.F32.TF32
-//  Output data type: complex<float>
-// 
-/////////////////////////////////////////////////////////////////////////////////////////////////
+/// Partial specialization for complex*complex+complex => complex using real-valued TensorOps
 template <
   /// Size of the Gemm problem - concept: gemm::GemmShape<>
   typename Shape_,
+  /// Data type of A elements
+  typename RealElementA,
   /// Layout of A matrix (concept: MatrixLayout)
   typename LayoutA_,
+  /// Data type of B elements
+  typename RealElementB,
   /// Layout of B matrix (concept: MatrixLayout)
   typename LayoutB_,
+  /// Element type of C matrix
+  typename RealElementC,
   /// Layout of C matrix (concept: MatrixLayout)
   typename LayoutC_,
   /// Policy describing warp-level MmaTensorOp (concept: MmaTensorOp policy)
   typename Policy_,
   /// Complex transform on A operand
   ComplexTransform TransformA,
   /// Complex transform on B operand
   ComplexTransform TransformB
 >
-class MmaComplexTensorOp<
+class MmaGaussianComplexTensorOp<
   Shape_, 
-  complex<float>, 
+  complex<RealElementA>, 
   LayoutA_, 
-  complex<float>,
+  complex<RealElementB>,
   LayoutB_,
-  complex<float>,
+  complex<RealElementC>,
   LayoutC_,
   Policy_,
   TransformA,
-  TransformB>  {
+  TransformB,
+  true>  {
 public:
   /// Shape of warp-level matrix operation (concept: GemmShape)
   using Shape = Shape_;
 
-  /// Data type of members of complex multiplicand A
-  using RealElementA = float;
-
   /// Data type of multiplicand A
   using ElementA = complex<RealElementA>;
 
   /// Layout of multiplicand A
   using LayoutA = LayoutA_;
 
-  /// Data type of members of complex multiplicand B
-  using RealElementB = float;
-
   /// Data type of multiplicand B
   using ElementB = complex<RealElementB>;
 
   /// Layout of multiplicand B
   using LayoutB = LayoutB_;
 
-  /// Data type of members of complex accumulator matrix C
-  using RealElementC = float;
-
   /// Data type of accumulator matrix C
   using ElementC = complex<RealElementC>;
 
   /// Layout of accumulator matrix C
   using LayoutC = LayoutC_;
 
   /// Shape of the warp in units of thread (concept: MmaLanePolicySimt)
@@ -619,22 +427,23 @@
   /// Underlying arch tag
   using ArchTag = typename ArchMmaOperator::ArchTag;
 
   /// Indicates class of matrix operator
   using OperatorClass = arch::OpClassTensorOp;
 
   /// Indicates math operator 
-  using MathOperator = typename arch::OpMultiplyAddComplex;
+  using MathOperator = arch::OpMultiplyAddGaussianComplex;
   
   /// Complex transform on A operand
   static ComplexTransform const kTransformA = TransformA;
 
   /// Complex transform on B operand
   static ComplexTransform const kTransformB = TransformB;
 
+
   /// Number of threads participating in warp-level matrix product
   static int const kThreadCount = 32;
 
 public:
 
   /// Iterates over the A operand in memory
   using IteratorA = MmaTensorOpMultiplicandTileIterator<
@@ -648,16 +457,15 @@
     1
   >;
 
   /// Storage for A tile
   using FragmentA = typename IteratorA::Fragment;
 
   /// Storage for transformed A tile
-  using TransformedFragmentA =
-      Array<typename ArchMmaOperator::ElementA, FragmentA::kElements * 2>;
+  using TransformedFragmentA = FragmentA;
 
   /// Iterates over the B operand in memory
   using IteratorB = MmaTensorOpMultiplicandTileIterator<
     MatrixShape<Shape::kK, Shape::kN>,
     Operand::kB,
     ElementB,
     LayoutB,
@@ -667,42 +475,45 @@
     1
   >;
 
   /// Storage for B tile
   using FragmentB = typename IteratorB::Fragment;
 
   /// Storage for transformed B tile
-  using TransformedFragmentB =
-      Array<typename ArchMmaOperator::ElementB, FragmentB::kElements * 2>;
+  using TransformedFragmentB = FragmentB;
 
   static_assert(
     !(Shape::kM % ArchMmaOperator::Shape::kM) && 
     !(Shape::kN % ArchMmaOperator::Shape::kN),
     "Shape of warp-level Mma must be divisible by operator shape.");
 
-  /// Number of complex products operations performed (one complex product needs four mma instructions)
+  /// Number of mma operations performed
   using MmaIterations = MatrixShape<
     Shape::kM / ArchMmaOperator::Shape::kM,
     Shape::kN / ArchMmaOperator::Shape::kN
   >;
 
   /// Iterates over the C operand in memory
-  using IteratorC = MmaTensorOpAccumulatorTileIterator<
+  using IteratorC = MmaTensorOpGaussianComplexAccumulatorTileIterator<
      MatrixShape<Shape::kM, Shape::kN>, 
      ElementC, 
      LayoutC,
      typename ArchMmaOperator::Shape, 
      typename Policy::OpDelta>;
 
   /// Storage for C tile, the accumulator. Note, regardless of multiplicand type, this
-  /// storage arrangement is to be considered 'planar complex' in the sense that all real-valued
-  /// parts are stored consecutively followed by all imaginary parts. This matches the structure
-  /// of Tensor Cores which are always real-valued matrix multiplies.
+  /// storage arrangement is to be considered 'gaussian complex' in the sense that the accumulation is
+  /// done in three parts namely part1, part2, and part3. The parts 1, 2, and 3 are stored consecutively 
+  /// in InteratorC::Frament. This matches the structure of Tensor Cores which are always real-valued matrix multiplies.
   using FragmentC = typename IteratorC::Fragment;
 
+  static_assert(
+    FragmentC::kElements == 3 * MmaIterations::kCount * ArchMmaOperator::FragmentC::kElements,
+    "Unexpected gaussian complex fragment length.");
+
 private:
 
   //
   // Data members
   //
 
   /// Underlying real-valued matrix multiply operator (concept: arch::Mma)
@@ -712,148 +523,121 @@
 
   //
   // Methods
   //
 
   /// Ctor
   CUTLASS_DEVICE
-  MmaComplexTensorOp() {}
+  MmaGaussianComplexTensorOp() {}
 
   /// Performs a warp-level matrix multiply-accumulate operation
   CUTLASS_DEVICE
   void operator()(
     FragmentC &D, 
-    TransformedFragmentA const &A, 
-    TransformedFragmentB const &B, 
+    FragmentA const &A, 
+    FragmentB const &B, 
     FragmentC const &C
   ) const {
 
     // Alias types for underlying real-valued matrix multiply operator
-    using InstMmaOperandA = typename ArchMmaOperator::FragmentA;
-    using InstMmaOperandB = typename ArchMmaOperator::FragmentB;
+    using MmaOperandA = typename ArchMmaOperator::FragmentA;
+    using MmaOperandB = typename ArchMmaOperator::FragmentB;
     using MmaOperandC = typename ArchMmaOperator::FragmentC;
 
-    static_assert(platform::is_same<cutlass::gemm::GemmShape<16, 8, 8>, typename ArchMmaOperator::Shape>::value, 
-      "This implementation only supports MMA.1688 math instructions.");
-
-    static_assert(InstMmaOperandA::kElements == 4, 
-      "This implementation only supports math instructions in which exactly four element is needed for the A operand."
-      "We can geneneralize later.");
-
-    static_assert(InstMmaOperandB::kElements == 2, 
-      "This implementation only supports math instructions in which exactly two element is needed for the B operand."
-      "We can geneneralize later.");
-
-    // Instruction Operands A & B holding real part followed by imaginary part for mma operations
-    InstMmaOperandA const *operand_A = reinterpret_cast<InstMmaOperandA const *>(&A);
-    InstMmaOperandB const *operand_B = reinterpret_cast<InstMmaOperandB const *>(&B);
-
-    //
-    // Accumulate in place
-    //
     D = C;
 
     CUTLASS_PRAGMA_UNROLL
     for (int m = 0; m < MmaIterations::kRow; ++m) {
 
-      // mma(accum.real(), a.real(), b.real(), accum.real());
+      // mma(accum.part1(), (a.real() + a.imag()), b.real(), accum.part1());
       CUTLASS_PRAGMA_UNROLL
       for (int n = 0; n < MmaIterations::kColumn; ++n) {
 
-        // Real-valued accumulator part
+        // Pack operands together. This may result in actual MOVs 
+        MmaOperandA operand_Asum;
+        MmaOperandB operand_Br;
+
+        CUTLASS_PRAGMA_UNROLL
+        for (int mk = 0; mk < MmaOperandA::kElements; ++mk)
+          operand_Asum[mk] = A[m*MmaOperandA::kElements + mk].real() + ((kTransformA == ComplexTransform::kConjugate) ?
+                            -A[m*MmaOperandA::kElements + mk].imag() : +A[m*MmaOperandA::kElements + mk].imag());
+
+        CUTLASS_PRAGMA_UNROLL
+        for (int nk = 0; nk < MmaOperandB::kElements; ++nk)
+          operand_Br[nk] = B[n*MmaOperandB::kElements + nk].real();
+
+        // accumulator part1
         MmaOperandC *accum = reinterpret_cast<MmaOperandC *>(&D) + 
           (m + n * MmaIterations::kRow);
 
-          mma(*accum, operand_A[m], operand_B[n], *accum);
+        mma(*accum, operand_Asum, operand_Br, *accum);
       }
 
-      // mma(accum.imag(), a.real(), b.imag(), accum.imag()); 
+      // mma(accum.part2(), -a.real(), (b.real() - b.imag()), accum.part2()); 
       CUTLASS_PRAGMA_UNROLL
       for (int n = MmaIterations::kColumn - 1; n >= 0; --n) {
 
-        // Complex-valued accumulator part
+        // Pack operands together. This may result in actual MOVs 
+        MmaOperandA operand_Ar;
+        MmaOperandB operand_Bdiff;
+
+        CUTLASS_PRAGMA_UNROLL
+        for (int mk = 0; mk < MmaOperandA::kElements; ++mk)
+          operand_Ar[mk] = -A[m*MmaOperandA::kElements + mk].real();
+
+        CUTLASS_PRAGMA_UNROLL
+        for (int nk = 0; nk < MmaOperandB::kElements; ++nk)
+          operand_Bdiff[nk] = B[n*MmaOperandB::kElements + nk].real() - ((kTransformB == ComplexTransform::kConjugate) ?
+                              -B[n*MmaOperandB::kElements + nk].imag() : +B[n*MmaOperandB::kElements + nk].imag());
+
+        // accumulator part2
         MmaOperandC *accum = reinterpret_cast<MmaOperandC *>(&D) + 
           (m + n * MmaIterations::kRow) + MmaIterations::kCount;
 
-        mma(*accum, operand_A[m], operand_B[n+MmaIterations::kColumn], *accum);
+        mma(*accum, operand_Ar, operand_Bdiff, *accum);
       }
 
-      // mma(accum.real(), a.imag(), -b.imag(), accum.real())
+      // mma(accum.part3(), a.imag(), (b.real() + b.imag()), accum.part3())
       CUTLASS_PRAGMA_UNROLL
       for (int n = 0; n < MmaIterations::kColumn; ++n) {
 
-        // negate OperandB to accumulate  -(a.imag()*b.imag())
-        // negating OperandB emits less instrucitons than negating OperandA as OperandB has less elements
-        negate<InstMmaOperandB> negate_op;
-
-        // Real-valued accumulator part
-        MmaOperandC *accum = reinterpret_cast<MmaOperandC *>(&D) + 
-          (m + n * MmaIterations::kRow);
+        // Pack operands together. This may result in actual MOVs 
+        MmaOperandA operand_Ai;
+        MmaOperandB operand_Bsum;
 
-        mma(*accum, operand_A[m+MmaIterations::kRow], negate_op(operand_B[n+MmaIterations::kColumn]), *accum);
-      }
+        CUTLASS_PRAGMA_UNROLL
+        for (int mk = 0; mk < MmaOperandA::kElements; ++mk)
+          operand_Ai[mk] = (kTransformA == ComplexTransform::kConjugate) ?
+                           -A[m*MmaOperandA::kElements + mk].imag() : +A[m*MmaOperandA::kElements + mk].imag();
 
-      // mma(accum.imag(), a.imag(), b.real(), accum.imag())
-      CUTLASS_PRAGMA_UNROLL
-      for (int n = MmaIterations::kColumn - 1; n >= 0; --n) {
+        CUTLASS_PRAGMA_UNROLL
+        for (int nk = 0; nk < MmaOperandB::kElements; ++nk)
+          operand_Bsum[nk] = B[n*MmaOperandB::kElements + nk].real() + ((kTransformB == ComplexTransform::kConjugate) ?
+                             -B[n*MmaOperandB::kElements + nk].imag() : +B[n*MmaOperandB::kElements + nk].imag());
 
-        // Complex-valued accumulator part
+        // accumulator part3
         MmaOperandC *accum = reinterpret_cast<MmaOperandC *>(&D) + 
-          (m + n * MmaIterations::kRow) + MmaIterations::kCount;
+          (m + n * MmaIterations::kRow) + 2 * MmaIterations::kCount;
 
-        mma(*accum, operand_A[m+MmaIterations::kRow], operand_B[n], *accum);
+        mma(*accum, operand_Ai, operand_Bsum, *accum);
       }
     }
   }
 
   /// Transform the mma operands to the required types
   CUTLASS_DEVICE
   void transform(TransformedFragmentA &dst_A, TransformedFragmentB &dst_B,
                  FragmentA const &A, FragmentB const &B) const {
-    // Alias types for underlying real-valued matrix multiply operator
-    using InstMmaOperandA = typename ArchMmaOperator::FragmentA;
-    using InstMmaOperandB = typename ArchMmaOperator::FragmentB;
-
-    //
-    // Define conversions from source type to instruction operands' type
-    //
-
-    FloatRoundStyle const kRoundA = FloatRoundStyle::round_half_ulp_trunc_dntz; 
-    FloatRoundStyle const kRoundB = FloatRoundStyle::round_half_ulp_trunc_dntz;
-
-    detail::UnpackComplexConvertAndPackForMma <
-      RealElementA,
-      InstMmaOperandA,
-      FragmentA,
-      MmaIterations,
-      MatrixShape<2, 2>,
-      kTransformA,
-      Operand::kA,
-      kRoundA> convert_A;
-
-    detail::UnpackComplexConvertAndPackForMma <
-      RealElementB,
-      InstMmaOperandB,
-      FragmentB,
-      MmaIterations,
-      MatrixShape<2, 1>,
-      kTransformB,
-      Operand::kB,
-      kRoundB> convert_B;
-
-    // Convert Fragment[A|B] holding complex<RealElement[A|B]> to InstMmaOperand[A|B] holding InstMmaOperand[A|B]::Element
-    convert_A(reinterpret_cast<InstMmaOperandA *>(&dst_A), A); 
-    convert_B(reinterpret_cast<InstMmaOperandB *>(&dst_B), B); 
+    dst_A = A;
+    dst_B = B;
   }
 };
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
-// TODO - partial specializations of real*complex and complex*real
-
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
 } // namespace warp
 } // namespace gemm
 } // namespace cutlass
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/warp/mma_complex_tensor_op_fast_f32.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/gemm/warp/mma_complex_tensor_op_fast_f32.h`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -297,15 +297,15 @@
 class MmaComplexTensorOpFastF32;
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
 /// Partial specialization for complex*complex+complex => complex:
 //  Operands data type: complex<float>
 //  Rounding: float -> tfloat32_t (round half_ulp_truncate nearest)
-//  Math instruction: MMA.1688.F32.TF32
+//  Math instruction: mma.sync.aligned.m16n8k8.f32.tf32.tf32.f32
 //  Output data type: complex<float>
 // 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 template <
   /// Size of the Gemm problem - concept: gemm::GemmShape<>
   typename Shape_,
   /// Layout of A matrix (concept: MatrixLayout)
@@ -493,15 +493,15 @@
   // Alias types for underlying real-valued matrix multiply operator
   //
   using InstMmaOperandA = typename ArchMmaOperator::FragmentA;
   using InstMmaOperandB = typename ArchMmaOperator::FragmentB;
   using MmaOperandC = typename ArchMmaOperator::FragmentC;
 
   static_assert(platform::is_same<cutlass::gemm::GemmShape<16, 8, 8>, typename ArchMmaOperator::Shape>::value, 
-    "This implementation only supports MMA.1688 math instructions.");
+    "This implementation only supports mma.m16n8k8 math instructions.");
 
   static_assert(InstMmaOperandA::kElements == 4, 
     "This implementation only supports math instructions in which exactly four element is needed for the A operand."
     "We can geneneralize later.");
 
   static_assert(InstMmaOperandB::kElements == 2, 
     "This implementation only supports math instructions in which exactly two element is needed for the B operand."
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/warp/mma_complex_tensor_op_tile_iterator_sm80.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/gemm/warp/mma_complex_tensor_op_tile_iterator_sm80.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/warp/mma_gaussian_complex_tensor_op.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/gemm/warp/mma_tensor_op.h`

 * *Files 18% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -32,337 +32,400 @@
     \brief Templates implementing warp-level matrix multiply-accumulate operations targeting
       Tensor Cores.
 */
 
 #pragma once
 
 #include "cutlass/cutlass.h"
-
 #include "cutlass/array.h"
-#include "cutlass/complex.h"
+#include "cutlass/platform/platform.h"
+
+#include "cutlass/numeric_conversion.h"
 #include "cutlass/numeric_types.h"
 #include "cutlass/matrix_shape.h"
 
 #include "cutlass/arch/memory_sm75.h"
-#include "cutlass/arch/mma_sm75.h"
+#include "cutlass/arch/mma_sm75.h" 
 #include "cutlass/arch/mma_sm80.h"
 
 #include "cutlass/gemm/gemm.h"
 #include "cutlass/gemm/warp/mma.h"
 
 #include "cutlass/gemm/warp/mma_tensor_op_policy.h"
-#include "cutlass/gemm/warp/mma_tensor_op.h"
 
 #include "cutlass/gemm/warp/mma_tensor_op_tile_iterator.h"
-#include "cutlass/gemm/warp/mma_gaussian_complex_tensor_op_tile_iterator_sm80.h"
+#include "cutlass/gemm/warp/mma_tensor_op_tile_iterator_sm80.h"
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
 namespace cutlass {
 namespace gemm {
 namespace warp {
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
-template <
-  /// Size of the Gemm problem - concept: gemm::GemmShape<>
-  typename Shape_,
-  /// Data type of A elements
-  typename RealElementA,
-  /// Layout of A matrix (concept: MatrixLayout)
-  typename LayoutA_,
-  /// Data type of B elements
-  typename RealElementB,
-  /// Layout of B matrix (concept: MatrixLayout)
-  typename LayoutB_,
-  /// Element type of C matrix
-  typename RealElementC,
-  /// Layout of C matrix (concept: MatrixLayout)
-  typename LayoutC_,
-  /// Policy describing warp-level MmaTensorOp (concept: MmaTensorOp policy)
-  typename Policy_,
-  /// Complex transform on A operand
-  ComplexTransform TransformA = ComplexTransform::kNone,
-  /// Complex transform on B operand
-  ComplexTransform TransformB = ComplexTransform::kNone,
-  /// Used for partial specialization
-  typename Enable = bool
->
-class MmaGaussianComplexTensorOp;
+namespace detail {
+
+template <typename T, typename S, int N, FloatRoundStyle Round>
+struct ConvertAndPack {
+
+  using Converter = NumericArrayConverter<T, S, N, Round>;
+
+  CUTLASS_HOST_DEVICE
+  Array<T, N> operator()(Array<S, N> const &source) {
+    Converter converter;
+
+    return converter(source);
+  }
+};
+
+template <typename T, int N, FloatRoundStyle Round>
+struct ConvertAndPack<T, T, N, Round> {
+
+  CUTLASS_HOST_DEVICE
+  Array<T, N> operator()(Array<T, N> const &source) {
+		return source;
+  }
+};
+
+template <int N, FloatRoundStyle Round>
+struct ConvertAndPack<bfloat16_t, float, N, Round> {
+
+  using Converter = NumericArrayConverter<bfloat16_t, float, N, Round>;
+
+  CUTLASS_HOST_DEVICE
+  Array<bfloat16_t, N> operator()(Array<float, N> const &source) {
+    Converter converter;
+
+    Array<float, N> tmp;
+
+    CUTLASS_PRAGMA_UNROLL
+    for (int i = 0; i < N; ++i) {
+      int idx = (((i << 1) & 2) | ((i >> 1) & 1) | (i & 0xfffffffc));
+      tmp[i] = source[idx];
+    }
+
+    return converter(tmp);
+  }
+};
+
+template <int N, FloatRoundStyle Round>
+struct ConvertAndPack<half_t, float, N, Round> {
+
+  using Converter = NumericArrayConverter<half_t, float, N, Round>;
+
+  CUTLASS_HOST_DEVICE
+  Array<half_t, N> operator()(Array<float, N> const &source) {
+    Converter converter;
+
+    Array<float, N> tmp;
+
+    CUTLASS_PRAGMA_UNROLL
+    for (int i = 0; i < N; ++i) {
+      int idx = (((i << 1) & 2) | ((i >> 1) & 1) | (i & 0xfffffffc));
+      tmp[i] = source[idx];
+    }
+
+    return converter(tmp);
+  }
+};
+
+/////////////////////////////////////////////////////////////////////////////////////////////////
+
+
+/////////////////////////////////////////////////////////////////////////////////////////////////
+
+} // namespace detail
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
-/// Partial specialization for complex*complex+complex => complex using real-valued TensorOps
+/// Structure to compute the matrix product targeting CUDA cores and SIMT math instructions.
 template <
   /// Size of the Gemm problem - concept: gemm::GemmShape<>
   typename Shape_,
   /// Data type of A elements
-  typename RealElementA,
+  typename ElementA_,
   /// Layout of A matrix (concept: MatrixLayout)
   typename LayoutA_,
   /// Data type of B elements
-  typename RealElementB,
+  typename ElementB_,
   /// Layout of B matrix (concept: MatrixLayout)
   typename LayoutB_,
   /// Element type of C matrix
-  typename RealElementC,
+  typename ElementC_,
   /// Layout of C matrix (concept: MatrixLayout)
   typename LayoutC_,
   /// Policy describing warp-level MmaTensorOp (concept: MmaTensorOp policy)
   typename Policy_,
-  /// Complex transform on A operand
-  ComplexTransform TransformA,
-  /// Complex transform on B operand
-  ComplexTransform TransformB,
+  /// Number of partitions along K dimension
+  int PartitionsK_ = 1,
+  /// Store the accumulators in row major or column major.  Row major is used
+  /// when output layout is interleaved.
+  bool AccumulatorsInRowMajor = false,
   /// Used for partial specialization
-  typename Enable
+  typename Enable = bool
 >
-class MmaGaussianComplexTensorOp<
-  Shape_, 
-  complex<RealElementA>, 
-  LayoutA_, 
-  complex<RealElementB>,
-  LayoutB_,
-  complex<RealElementC>,
-  LayoutC_,
-  Policy_,
-  TransformA,
-  TransformB,
-  Enable>  {
+class MmaTensorOp {
 public:
   /// Shape of warp-level matrix operation (concept: GemmShape)
   using Shape = Shape_;
 
   /// Data type of multiplicand A
-  using ElementA = complex<RealElementA>;
+  using ElementA = ElementA_;
 
   /// Layout of multiplicand A
   using LayoutA = LayoutA_;
 
   /// Data type of multiplicand B
-  using ElementB = complex<RealElementB>;
+  using ElementB = ElementB_;
 
   /// Layout of multiplicand B
   using LayoutB = LayoutB_;
 
   /// Data type of accumulator matrix C
-  using ElementC = complex<RealElementC>;
+  using ElementC = ElementC_;
 
   /// Layout of accumulator matrix C
   using LayoutC = LayoutC_;
 
   /// Shape of the warp in units of thread (concept: MmaLanePolicySimt)
   using Policy = Policy_;
 
   /// Underlying matrix multiply operator (concept: arch::Mma)
   using ArchMmaOperator = typename Policy::Operator;
 
-  /// Shape of underlying instruction
-  using InstructionShape = typename ArchMmaOperator::Shape;
+  /// Indicates math operator 
+  using MathOperator = typename ArchMmaOperator::Operator;
 
-  /// Underlying arch tag
+  /// Architecture tag from underlying instruction
   using ArchTag = typename ArchMmaOperator::ArchTag;
 
   /// Indicates class of matrix operator
   using OperatorClass = arch::OpClassTensorOp;
 
-  /// Indicates math operator 
-  using MathOperator = arch::OpMultiplyAddGaussianComplex;
-  
+  /// Shape of underlying instruction
+  using InstructionShape = typename ArchMmaOperator::Shape;
+
   /// Complex transform on A operand
-  static ComplexTransform const kTransformA = TransformA;
+  static ComplexTransform const kTransformA = ComplexTransform::kNone;
 
   /// Complex transform on B operand
-  static ComplexTransform const kTransformB = TransformB;
-
+  static ComplexTransform const kTransformB = ComplexTransform::kNone;
 
   /// Number of threads participating in warp-level matrix product
   static int const kThreadCount = 32;
 
+  /// Number of partitions along K dimension
+  static int const kPartitionsK = PartitionsK_;
+
 public:
 
   /// Iterates over the A operand in memory
   using IteratorA = MmaTensorOpMultiplicandTileIterator<
-    MatrixShape<Shape::kM, Shape::kK>,
-    Operand::kA,
-    ElementA,
-    LayoutA,
-    MatrixShape<ArchMmaOperator::Shape::kM, ArchMmaOperator::Shape::kK>,
-    Policy::OpDelta::kRow,
-    32,
-    1
-  >;
+     MatrixShape<Shape::kM, Shape::kK>, Operand::kA, ElementA, LayoutA,
+     MatrixShape<ArchMmaOperator::Shape::kM, ArchMmaOperator::Shape::kK>,
+     Policy::OpDelta::kRow, kThreadCount, kPartitionsK>;
 
   /// Storage for A tile
   using FragmentA = typename IteratorA::Fragment;
 
   /// Storage for transformed A tile
-  using TransformedFragmentA = FragmentA;
+  using TransformedFragmentA =
+      Array<typename ArchMmaOperator::ElementA, FragmentA::kElements>;
 
   /// Iterates over the B operand in memory
   using IteratorB = MmaTensorOpMultiplicandTileIterator<
-    MatrixShape<Shape::kK, Shape::kN>,
-    Operand::kB,
-    ElementB,
-    LayoutB,
-    MatrixShape<ArchMmaOperator::Shape::kK, ArchMmaOperator::Shape::kN>,
-    Policy::OpDelta::kColumn,
-    32,
-    1
-  >;
+      MatrixShape<Shape::kK, Shape::kN>, Operand::kB, ElementB, LayoutB,
+      MatrixShape<ArchMmaOperator::Shape::kK, ArchMmaOperator::Shape::kN>,
+      Policy::OpDelta::kRow, kThreadCount, kPartitionsK>;
 
   /// Storage for B tile
   using FragmentB = typename IteratorB::Fragment;
 
   /// Storage for transformed B tile
-  using TransformedFragmentB = FragmentB;
+  using TransformedFragmentB =
+      Array<typename ArchMmaOperator::ElementB, FragmentB::kElements>;
 
-  static_assert(
-    !(Shape::kM % ArchMmaOperator::Shape::kM) && 
-    !(Shape::kN % ArchMmaOperator::Shape::kN),
-    "Shape of warp-level Mma must be divisible by operator shape.");
+  /// Iterates over the C operand in memory
+  using IteratorC = MmaTensorOpAccumulatorTileIterator<
+     MatrixShape<Shape::kM, Shape::kN>, ElementC, LayoutC,
+     typename ArchMmaOperator::Shape, typename Policy::OpDelta>;
+
+  /// Storage for C tile
+  using FragmentC = typename IteratorC::Fragment;
 
   /// Number of mma operations performed
   using MmaIterations = MatrixShape<
-    Shape::kM / ArchMmaOperator::Shape::kM,
-    Shape::kN / ArchMmaOperator::Shape::kN
+    (Shape::kM + ArchMmaOperator::Shape::kM - 1) / ArchMmaOperator::Shape::kM,
+    (Shape::kN + ArchMmaOperator::Shape::kN - 1) / ArchMmaOperator::Shape::kN
   >;
 
-  /// Iterates over the C operand in memory
-  using IteratorC = MmaTensorOpGaussianComplexAccumulatorTileIterator<
-     MatrixShape<Shape::kM, Shape::kN>, 
-     ElementC, 
-     LayoutC,
-     typename ArchMmaOperator::Shape, 
-     typename Policy::OpDelta>;
-
-  /// Storage for C tile, the accumulator. Note, regardless of multiplicand type, this
-  /// storage arrangement is to be considered 'gaussian complex' in the sense that the accumulation is
-  /// done in three parts namely part1, part2, and part3. The parts 1, 2, and 3 are stored consecutively 
-  /// in InteratorC::Frament. This matches the structure of Tensor Cores which are always real-valued matrix multiplies.
-  using FragmentC = typename IteratorC::Fragment;
-
-  static_assert(
-    FragmentC::kElements == 3 * MmaIterations::kCount * ArchMmaOperator::FragmentC::kElements,
-    "Unexpected gaussian complex fragment length.");
-
-private:
-
-  //
-  // Data members
-  //
+public:
 
-  /// Underlying real-valued matrix multiply operator (concept: arch::Mma)
+  /// Underlying matrix multiply operator (concept: arch::Mma)
   ArchMmaOperator mma;
 
 public:
 
   //
   // Methods
   //
 
   /// Ctor
   CUTLASS_DEVICE
-  MmaGaussianComplexTensorOp() {}
+  MmaTensorOp() {}
 
   /// Performs a warp-level matrix multiply-accumulate operation
   CUTLASS_DEVICE
   void operator()(
     FragmentC &D, 
-    FragmentA const &A, 
-    FragmentB const &B, 
+    TransformedFragmentA const &A, 
+    TransformedFragmentB const &B, 
     FragmentC const &C
   ) const {
 
-    // Alias types for underlying real-valued matrix multiply operator
     using MmaOperandA = typename ArchMmaOperator::FragmentA;
     using MmaOperandB = typename ArchMmaOperator::FragmentB;
     using MmaOperandC = typename ArchMmaOperator::FragmentC;
 
-    static_assert(MmaOperandA::kElements == 1, 
-      "This implementation only supports math instructions in which exactly one element is needed for the A operand."
-      "We can geneneralize later.");
-
-    static_assert(MmaOperandB::kElements == 1, 
-      "This implementation only supports math instructions in which exactly one element is needed for the B operand."
-      "We can geneneralize later.");
-
     D = C;
 
-    CUTLASS_PRAGMA_UNROLL
-    for (int m = 0; m < MmaIterations::kRow; ++m) {
+    MmaOperandA const *ptr_A = reinterpret_cast<MmaOperandA const *>(&A);
+    MmaOperandB const *ptr_B = reinterpret_cast<MmaOperandB const *>(&B);
+    MmaOperandC *ptr_D = reinterpret_cast<MmaOperandC *>(&D);
+
+    #if defined(__CUDA_ARCH__) && (__CUDA_ARCH__ < 800)
+      // Serpentine visitation order maximizing reuse of Rb
+      // The visitation order is like
+      //      _   
+      //   | | | |
+      //   | | | |
+      //   |_| |_|
+      //
+      // Down Up Down Up
 
-      // mma(accum.part1(), (a.real() + a.imag()), b.real(), accum.part1());
       CUTLASS_PRAGMA_UNROLL
       for (int n = 0; n < MmaIterations::kColumn; ++n) {
 
-        // Pack operands together. This may result in actual MOVs 
-        MmaOperandA operand_Asum;
-        MmaOperandB operand_Br;
-
-        operand_Asum[0] = A[m].real() + ((kTransformA == ComplexTransform::kConjugate) ? -A[m].imag() : +A[m].imag());
-        operand_Br[0] = B[n].real();
-
-        // accumulator part1
-        MmaOperandC *accum = reinterpret_cast<MmaOperandC *>(&D) + 
-          (m + n * MmaIterations::kRow);
+        CUTLASS_PRAGMA_UNROLL
+        for (int m = 0; m < MmaIterations::kRow; ++m) {
 
-        mma(*accum, operand_Asum, operand_Br, *accum);
-      }
+          int m_serpentine = ((n % 2) ? (MmaIterations::kRow - 1 - m) : m);
 
-      // mma(accum.part2(), -a.real(), (b.real() - b.imag()), accum.part2()); 
-      CUTLASS_PRAGMA_UNROLL
-      for (int n = MmaIterations::kColumn - 1; n >= 0; --n) {
-
-        // Pack operands together. This may result in actual MOVs 
-        MmaOperandA operand_Ar;
-        MmaOperandB operand_Bdiff;
-
-        operand_Ar[0] = -A[m].real();
-        operand_Bdiff[0] = B[n].real() - ((kTransformB == ComplexTransform::kConjugate) ? -B[n].imag() : +B[n].imag());
-
-        // accumulator part2
-        MmaOperandC *accum = reinterpret_cast<MmaOperandC *>(&D) + 
-          (m + n * MmaIterations::kRow) + MmaIterations::kCount;
-
-        mma(*accum, operand_Ar, operand_Bdiff, *accum);
+          if (AccumulatorsInRowMajor) {  // matrix B is reordered
+            mma(
+              ptr_D[n + m_serpentine * MmaIterations::kColumn],
+              ptr_A[m_serpentine],
+              ptr_B[n],
+              ptr_D[n + m_serpentine * MmaIterations::kColumn]);
+          } else {
+            mma(
+              ptr_D[m_serpentine + n * MmaIterations::kRow],
+              ptr_A[m_serpentine],
+              ptr_B[n],
+              ptr_D[m_serpentine + n * MmaIterations::kRow]);
+          }
+        }
       }
+    #elif defined(__CUDA_ARCH__) && (__CUDA_ARCH__ >= 800)
+      // Serpentine visitation order maximizing reuse of Ra
+      // The visitation order is like
+      //   _________
+      //   _________|
+      //  |_________
+      //  __________|
+      //
+      // Right Left Right Left 
 
-      // mma(accum.part3(), a.imag(), (b.real() + b.imag()), accum.part3())
       CUTLASS_PRAGMA_UNROLL
-      for (int n = 0; n < MmaIterations::kColumn; ++n) {
+      for (int m = 0; m < MmaIterations::kRow; ++m) {
 
-        // Pack operands together. This may result in actual MOVs 
-        MmaOperandA operand_Ai;
-        MmaOperandB operand_Bsum;
-
-        operand_Ai[0] = (kTransformA == ComplexTransform::kConjugate) ? -A[m].imag() : +A[m].imag();
-        operand_Bsum[0] = B[n].real() + ((kTransformB == ComplexTransform::kConjugate) ? -B[n].imag() : +B[n].imag());
-
-        // accumulator part3
-        MmaOperandC *accum = reinterpret_cast<MmaOperandC *>(&D) + 
-          (m + n * MmaIterations::kRow) + 2 * MmaIterations::kCount;
+        CUTLASS_PRAGMA_UNROLL
+        for (int n = 0; n < MmaIterations::kColumn; ++n) {
 
-        mma(*accum, operand_Ai, operand_Bsum, *accum);
+          int n_serpentine = ((m % 2) ? (MmaIterations::kColumn - 1 - n) : n);
+
+          if (AccumulatorsInRowMajor) {  // matrix B is reordered
+            mma(
+              ptr_D[n_serpentine + m * MmaIterations::kColumn],
+              ptr_A[m],
+              ptr_B[n_serpentine],
+              ptr_D[n_serpentine + m * MmaIterations::kColumn]);
+          } else {
+            mma(ptr_D[m + n_serpentine * MmaIterations::kRow],
+                ptr_A[m],
+                ptr_B[n_serpentine],
+                ptr_D[m + n_serpentine * MmaIterations::kRow]);
+          }
+        }
       }
-    }
+    #else
+      assert(0);
+    #endif
   }
 
   /// Transform the mma operands to the required types
   CUTLASS_DEVICE
   void transform(TransformedFragmentA &dst_A, TransformedFragmentB &dst_B,
                  FragmentA const &A, FragmentB const &B) const {
-    //TODO: Implement this
-    dst_A = A;
-    dst_B = B;
+
+    //
+    // Define conversions from source type to instruction type
+    //
+    FloatRoundStyle const kRoundA =
+        PreferredRoundingMode<typename ArchMmaOperator::ElementA,
+                              ElementA>::kRound;
+    FloatRoundStyle const kRoundB =
+        PreferredRoundingMode<typename ArchMmaOperator::ElementB,
+                              ElementB>::kRound;
+    #if defined(__CUDA_ARCH__) && (__CUDA_ARCH__ < 800)
+      detail::ConvertAndPack<typename ArchMmaOperator::ElementA, ElementA,
+                            FragmentA::kElements, kRoundA>
+          convert_A;
+      NumericArrayConverter<typename ArchMmaOperator::ElementB, ElementB,
+                            FragmentB::kElements / 2, kRoundB>
+          convert_B;
+      Array<ElementB, FragmentB::kElements / 2> const *ptr_B =
+          reinterpret_cast<Array<ElementB, FragmentB::kElements / 2> const *>(&B);
+      Array<typename ArchMmaOperator::ElementB, FragmentB::kElements / 2> *
+          ptr_dst_B = reinterpret_cast<Array<typename ArchMmaOperator::ElementB,
+                                             FragmentB::kElements / 2> *>(&dst_B);
+  
+      dst_A = convert_A(A);
+  
+      ptr_dst_B[0] = convert_B(ptr_B[0]);
+      ptr_dst_B[1] = convert_B(ptr_B[1]);
+
+    #elif defined(__CUDA_ARCH__) && (__CUDA_ARCH__ >= 800)
+      detail::ConvertAndPack<typename ArchMmaOperator::ElementA, ElementA,
+                            FragmentA::kElements / 2, kRoundA>
+          convert_A;
+      NumericArrayConverter<typename ArchMmaOperator::ElementB, ElementB,
+                            FragmentB::kElements, kRoundB>
+          convert_B;
+      Array<ElementA, FragmentA::kElements / 2> const *ptr_A =
+          reinterpret_cast<Array<ElementA, FragmentA::kElements / 2> const *>(&A);
+      Array<typename ArchMmaOperator::ElementA, FragmentA::kElements / 2> *
+          ptr_dst_A = reinterpret_cast<Array<typename ArchMmaOperator::ElementA,
+                                             FragmentA::kElements / 2> *>(&dst_A);
+  
+      dst_B = convert_B(B);
+  
+      ptr_dst_A[0] = convert_A(ptr_A[0]);
+      ptr_dst_A[1] = convert_A(ptr_A[1]);
+    #else
+      assert(0);
+    #endif
   }
 };
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
-/////////////////////////////////////////////////////////////////////////////////////////////////
-
 } // namespace warp
 } // namespace gemm
 } // namespace cutlass
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
+
+#include "cutlass/gemm/warp/mma_tensor_op_fast_f32.h"
+
+/////////////////////////////////////////////////////////////////////////////////////////////////
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/warp/mma_gaussian_complex_tensor_op_tile_iterator_sm80.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/gemm/warp/mma_gaussian_complex_tensor_op_tile_iterator_sm80.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/warp/mma_planar_complex.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/gemm/warp/mma_planar_complex.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/warp/mma_simt_policy.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/gemm/warp/mma_simt_policy.h`

 * *Files 2% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/warp/mma_simt_tile_iterator.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/gemm/warp/mma_simt_tile_iterator.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/warp/mma_sparse_tensor_op.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/gemm/warp/mma_sparse_tensor_op.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/warp/mma_tensor_op.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/gemm/warp/mma_tensor_op_fast_f32.h`

 * *Files 15% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -24,14 +24,15 @@
  * DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR
  * SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER
  * CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,
  * OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
  * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
  *
  **************************************************************************************************/
+
 /*! \file
     \brief Templates implementing warp-level matrix multiply-accumulate operations targeting
       Tensor Cores.
 */
 
 #pragma once
 
@@ -39,105 +40,110 @@
 #include "cutlass/array.h"
 #include "cutlass/platform/platform.h"
 
 #include "cutlass/numeric_conversion.h"
 #include "cutlass/numeric_types.h"
 #include "cutlass/matrix_shape.h"
 
-#include "cutlass/arch/memory_sm75.h"
-#include "cutlass/arch/mma_sm75.h" 
 #include "cutlass/arch/mma_sm80.h"
 
 #include "cutlass/gemm/gemm.h"
 #include "cutlass/gemm/warp/mma.h"
 
 #include "cutlass/gemm/warp/mma_tensor_op_policy.h"
+#include "cutlass/gemm/warp/mma_tensor_op.h"
 
 #include "cutlass/gemm/warp/mma_tensor_op_tile_iterator.h"
 #include "cutlass/gemm/warp/mma_tensor_op_tile_iterator_sm80.h"
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
 namespace cutlass {
 namespace gemm {
 namespace warp {
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
-namespace detail {
-
-template <typename T, typename S, int N, FloatRoundStyle Round>
-struct ConvertAndPack {
-
-  using Converter = NumericArrayConverter<T, S, N, Round>;
-
-  CUTLASS_HOST_DEVICE
-  Array<T, N> operator()(Array<S, N> const &source) {
-    Converter converter;
-
-    return converter(source);
-  }
-};
-
-template <typename T, int N, FloatRoundStyle Round>
-struct ConvertAndPack<T, T, N, Round> {
-
-  CUTLASS_HOST_DEVICE
-  Array<T, N> operator()(Array<T, N> const &source) {
-		return source;
-  }
-};
+enum class TensorFloat32Op {
+  k3xTF32, 
+  k4xTF32 
+}; 
 
-template <int N, FloatRoundStyle Round>
-struct ConvertAndPack<bfloat16_t, float, N, Round> {
-
-  using Converter = NumericArrayConverter<bfloat16_t, float, N, Round>;
-
-  CUTLASS_HOST_DEVICE
-  Array<bfloat16_t, N> operator()(Array<float, N> const &source) {
-    Converter converter;
-
-    Array<float, N> tmp;
-
-    CUTLASS_PRAGMA_UNROLL
-    for (int i = 0; i < N; ++i) {
-      int idx = (((i << 1) & 2) | ((i >> 1) & 1) | (i & 0xfffffffc));
-      tmp[i] = source[idx];
-    }
-
-    return converter(tmp);
-  }
+template <
+  /// Floating-point rounding style
+  FloatRoundStyle RoundBigA_,
+  /// Floating-point rounding style
+  FloatRoundStyle RoundSmallA_,
+  /// Floating-point rounding style
+  FloatRoundStyle RoundBigB_ = RoundBigA_,
+  /// Floating-point rounding style
+  FloatRoundStyle RoundSmallB_ = RoundSmallA_,
+  /// Precision for TensorFloat32Op 
+  // (k3xTF32: BigxBig, BigxSmall, SmallxBig)
+  // (k4xTF32: BigxBig, BigxSmall, SmallxBig, SmallxSmall)
+  TensorFloat32Op Precision_ = TensorFloat32Op::k3xTF32
+  >
+struct FastF32 {
+
+  static FloatRoundStyle const kRoundBigA = RoundBigA_;
+  static FloatRoundStyle const kRoundSmallA = RoundSmallA_;
+  static FloatRoundStyle const kRoundBigB = RoundBigB_;
+  static FloatRoundStyle const kRoundSmallB = RoundSmallB_;
+  static TensorFloat32Op const kPrecision = Precision_;
 };
 
-template <int N, FloatRoundStyle Round>
-struct ConvertAndPack<half_t, float, N, Round> {
-
-  using Converter = NumericArrayConverter<half_t, float, N, Round>;
 
-  CUTLASS_HOST_DEVICE
-  Array<half_t, N> operator()(Array<float, N> const &source) {
-    Converter converter;
+namespace detail {
 
-    Array<float, N> tmp;
+  template<
+    int N,
+    FloatRoundStyle RoundBig = FloatRoundStyle::round_toward_zero,
+    FloatRoundStyle RoundSmall = FloatRoundStyle::round_half_ulp_truncate
+  >
+  struct ConvertAndPackAccurateF32 {
+  
+    /// Rounding styles for big and small part
+    static FloatRoundStyle const kRoundBig = RoundBig;
+    static FloatRoundStyle const kRoundSmall = RoundSmall;
+
+    /// Converter type
+    using Converter = NumericConverterFastF32<kRoundBig, kRoundSmall>;
+
+    /// Source fragement
+    using SourceFragment = Array<float, N>;
+
+    /// Destination fragment
+    using DestinationFragment = Array<tfloat32_t, N>;
+
+    /// Converter Fragment holding two tfloat32_t elements for every float
+    using ConverterFragment = Array<tfloat32_t, 2>;
+
+    /// Index in fargments for the big and small part
+    static int const kBigIndex = 0;
+    static int const kSmallIndex = 1;
+
+    CUTLASS_HOST_DEVICE
+    void operator()(SourceFragment const &source,
+                    DestinationFragment &dst_big,
+                    DestinationFragment &dst_small) {
+      
+      Converter convert_;
+      ConverterFragment result_;
 
-    CUTLASS_PRAGMA_UNROLL
-    for (int i = 0; i < N; ++i) {
-      int idx = (((i << 1) & 2) | ((i >> 1) & 1) | (i & 0xfffffffc));
-      tmp[i] = source[idx];
+      CUTLASS_PRAGMA_UNROLL
+      for (int i = 0; i < N; ++i) {
+        // convert source to result fragment
+        result_ = convert_(source[i]);
+
+        // store converted result fragments to destination fragment
+        dst_big[i] = result_[kBigIndex];
+        dst_small[i] = result_[kSmallIndex];
+      }
     }
-
-    return converter(tmp);
-  }
-};
-
-/////////////////////////////////////////////////////////////////////////////////////////////////
-
-
-/////////////////////////////////////////////////////////////////////////////////////////////////
-
+  };
 } // namespace detail
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
 /// Structure to compute the matrix product targeting CUDA cores and SIMT math instructions.
 template <
   /// Size of the Gemm problem - concept: gemm::GemmShape<>
@@ -160,45 +166,75 @@
   int PartitionsK_ = 1,
   /// Store the accumulators in row major or column major.  Row major is used
   /// when output layout is interleaved.
   bool AccumulatorsInRowMajor = false,
   /// Used for partial specialization
   typename Enable = bool
 >
-class MmaTensorOp {
+class MmaTensorOpFastF32;
+
+/////////////////////////////////////////////////////////////////////////////////////////////////
+
+/// Partial specialization for float*float+float => float using TF32 TensorOps
+template <
+  /// Size of the Gemm problem - concept: gemm::GemmShape<>
+  typename Shape_,
+  /// Layout of A matrix (concept: MatrixLayout)
+  typename LayoutA_,
+  /// Layout of B matrix (concept: MatrixLayout)
+  typename LayoutB_,
+  /// Layout of C matrix (concept: MatrixLayout)
+  typename LayoutC_,
+  /// Policy describing warp-level MmaTensorOp (concept: MmaTensorOp policy)
+  typename Policy_,
+  /// Number of partitions along K dimension
+  int PartitionsK_,
+  /// Store the accumulators in row major or column major.  Row major is used
+  /// when output layout is interleaved.
+  bool AccumulatorsInRowMajor,
+  /// Used for partial specialization
+  typename Enable
+>
+class MmaTensorOpFastF32<
+  Shape_,
+  float, LayoutA_,
+  float, LayoutB_,
+  float, LayoutC_,
+  Policy_, PartitionsK_,
+  AccumulatorsInRowMajor, Enable> {
 public:
   /// Shape of warp-level matrix operation (concept: GemmShape)
   using Shape = Shape_;
 
   /// Data type of multiplicand A
-  using ElementA = ElementA_;
+  using ElementA = float;
 
   /// Layout of multiplicand A
   using LayoutA = LayoutA_;
 
   /// Data type of multiplicand B
-  using ElementB = ElementB_;
+  using ElementB = float;
 
   /// Layout of multiplicand B
   using LayoutB = LayoutB_;
 
   /// Data type of accumulator matrix C
-  using ElementC = ElementC_;
+  using ElementC = float;
 
   /// Layout of accumulator matrix C
   using LayoutC = LayoutC_;
 
   /// Shape of the warp in units of thread (concept: MmaLanePolicySimt)
   using Policy = Policy_;
 
   /// Underlying matrix multiply operator (concept: arch::Mma)
   using ArchMmaOperator = typename Policy::Operator;
 
   /// Indicates math operator 
-  using MathOperator = typename ArchMmaOperator::Operator;
+  using MathOperator = arch::OpMultiplyAddFastF32;
 
   /// Architecture tag from underlying instruction
   using ArchTag = typename ArchMmaOperator::ArchTag;
 
   /// Indicates class of matrix operator
   using OperatorClass = arch::OpClassTensorOp;
 
@@ -213,42 +249,78 @@
 
   /// Number of threads participating in warp-level matrix product
   static int const kThreadCount = 32;
 
   /// Number of partitions along K dimension
   static int const kPartitionsK = PartitionsK_;
 
+  /// Tune F32 to TF32 big small conversion for float operation
+  /// Different combination of big small conversin can cause different tradeoff
+  /// between speed and accuracy.  Generally, use round_half_ulp_truncate can
+  /// improve the performance but hur the accuracy.
+  using MmaFastF32 = FastF32 <
+    FloatRoundStyle::round_toward_zero,        // kRoundBigA
+    FloatRoundStyle::round_half_ulp_truncate,  // kRoundSmallA
+    FloatRoundStyle::round_toward_zero,        // kRoundBigB
+    FloatRoundStyle::round_half_ulp_truncate,  // kRoundSmallB
+    TensorFloat32Op::k3xTF32                   // Number of TF32 operations 
+  >;
+
 public:
 
   /// Iterates over the A operand in memory
   using IteratorA = MmaTensorOpMultiplicandTileIterator<
-     MatrixShape<Shape::kM, Shape::kK>, Operand::kA, ElementA, LayoutA,
-     MatrixShape<ArchMmaOperator::Shape::kM, ArchMmaOperator::Shape::kK>,
-     Policy::OpDelta::kRow, kThreadCount, kPartitionsK>;
+      MatrixShape<Shape::kM, Shape::kK>, 
+      Operand::kA, 
+      ElementA, 
+      LayoutA,
+      MatrixShape<ArchMmaOperator::Shape::kM, ArchMmaOperator::Shape::kK>,
+      Policy::OpDelta::kRow, 
+      kThreadCount, 
+      kPartitionsK
+  >;
 
   /// Storage for A tile
   using FragmentA = typename IteratorA::Fragment;
 
   /// Storage for transformed A tile
   using TransformedFragmentA =
+      Array<typename ArchMmaOperator::ElementA, FragmentA::kElements * 2>;
+
+  /// Fragment bisecting big and small sections
+  using AccessTypeFragmentA = 
       Array<typename ArchMmaOperator::ElementA, FragmentA::kElements>;
 
   /// Iterates over the B operand in memory
   using IteratorB = MmaTensorOpMultiplicandTileIterator<
-      MatrixShape<Shape::kK, Shape::kN>, Operand::kB, ElementB, LayoutB,
+      MatrixShape<Shape::kK, Shape::kN>, 
+      Operand::kB, 
+      ElementB, 
+      LayoutB,
       MatrixShape<ArchMmaOperator::Shape::kK, ArchMmaOperator::Shape::kN>,
-      Policy::OpDelta::kRow, kThreadCount, kPartitionsK>;
+      Policy::OpDelta::kRow, 
+      kThreadCount, 
+      kPartitionsK
+  >;
 
   /// Storage for B tile
   using FragmentB = typename IteratorB::Fragment;
 
   /// Storage for transformed B tile
   using TransformedFragmentB =
+      Array<typename ArchMmaOperator::ElementB, FragmentB::kElements * 2>;
+
+  /// Fragment bisecting big and small sections
+  using AccessTypeFragmentB = 
       Array<typename ArchMmaOperator::ElementB, FragmentB::kElements>;
 
+  /// Index in fargments for the big and small part
+  static int const kBigIndex = 0;
+  static int const kSmallIndex = 1;
+
   /// Iterates over the C operand in memory
   using IteratorC = MmaTensorOpAccumulatorTileIterator<
      MatrixShape<Shape::kM, Shape::kN>, ElementC, LayoutC,
      typename ArchMmaOperator::Shape, typename Policy::OpDelta>;
 
   /// Storage for C tile
   using FragmentC = typename IteratorC::Fragment;
@@ -268,164 +340,132 @@
 
   //
   // Methods
   //
 
   /// Ctor
   CUTLASS_DEVICE
-  MmaTensorOp() {}
+  MmaTensorOpFastF32() {}
 
   /// Performs a warp-level matrix multiply-accumulate operation
   CUTLASS_DEVICE
   void operator()(
     FragmentC &D, 
     TransformedFragmentA const &A, 
     TransformedFragmentB const &B, 
     FragmentC const &C
   ) const {
 
-    using MmaOperandA = typename ArchMmaOperator::FragmentA;
-    using MmaOperandB = typename ArchMmaOperator::FragmentB;
-    using MmaOperandC = typename ArchMmaOperator::FragmentC;
+    AccessTypeFragmentA const *ptr_A = reinterpret_cast<AccessTypeFragmentA const*>(&A);
+    AccessTypeFragmentB const *ptr_B = reinterpret_cast<AccessTypeFragmentB const*>(&B);
 
+    //
+    // Accumulate in place
+    //
     D = C;
+    
+    mma_operator(D, ptr_A[kSmallIndex], ptr_B[kBigIndex], D);
 
-    MmaOperandA const *ptr_A = reinterpret_cast<MmaOperandA const *>(&A);
-    MmaOperandB const *ptr_B = reinterpret_cast<MmaOperandB const *>(&B);
-    MmaOperandC *ptr_D = reinterpret_cast<MmaOperandC *>(&D);
-
-    #if defined(__CUDA_ARCH__) && (__CUDA_ARCH__ < 800)
-      // Serpentine visitation order maximizing reuse of Rb
-      // The visitation order is like
-      //      _   
-      //   | | | |
-      //   | | | |
-      //   |_| |_|
-      //
-      // Down Up Down Up
+    mma_operator(D, ptr_A[kBigIndex], ptr_B[kSmallIndex], D);
 
-      CUTLASS_PRAGMA_UNROLL
-      for (int n = 0; n < MmaIterations::kColumn; ++n) {
+    mma_operator(D, ptr_A[kBigIndex], ptr_B[kBigIndex], D);
 
-        CUTLASS_PRAGMA_UNROLL
-        for (int m = 0; m < MmaIterations::kRow; ++m) {
+    if (MmaFastF32::kPrecision == TensorFloat32Op::k4xTF32)
+      mma_operator(D, ptr_A[kSmallIndex], ptr_B[kSmallIndex], D);
+  }
 
-          int m_serpentine = ((n % 2) ? (MmaIterations::kRow - 1 - m) : m);
+  /// Performs a warp-level matrix multiply-accumulate operation
+  CUTLASS_DEVICE
+  void mma_operator(
+    FragmentC &D, 
+    AccessTypeFragmentA const &A, 
+    AccessTypeFragmentB const &B, 
+    FragmentC const &C
+  ) const {
 
-          if (AccumulatorsInRowMajor) {  // matrix B is reordered
-            mma(
-              ptr_D[n + m_serpentine * MmaIterations::kColumn],
-              ptr_A[m_serpentine],
-              ptr_B[n],
-              ptr_D[n + m_serpentine * MmaIterations::kColumn]);
-          } else {
-            mma(
-              ptr_D[m_serpentine + n * MmaIterations::kRow],
-              ptr_A[m_serpentine],
-              ptr_B[n],
-              ptr_D[m_serpentine + n * MmaIterations::kRow]);
-          }
-        }
-      }
-    #elif defined(__CUDA_ARCH__) && (__CUDA_ARCH__ >= 800)
-      // Serpentine visitation order maximizing reuse of Ra
-      // The visitation order is like
-      //   _________
-      //   _________|
-      //  |_________
-      //  __________|
-      //
-      // Right Left Right Left 
+    #if defined(__CUDA_ARCH__) && (__CUDA_ARCH__ >= 800)
+
+      using MmaOperandA = typename ArchMmaOperator::FragmentA;
+      using MmaOperandB = typename ArchMmaOperator::FragmentB;
+      using MmaOperandC = typename ArchMmaOperator::FragmentC;
+
+      MmaOperandA const *ptr_A = reinterpret_cast<MmaOperandA const *>(&A);
+      MmaOperandB const *ptr_B = reinterpret_cast<MmaOperandB const *>(&B);
+      MmaOperandC *ptr_D = reinterpret_cast<MmaOperandC *>(&D);
 
+      // Serpentine visitation order maximizing reuse of Ra
       CUTLASS_PRAGMA_UNROLL
       for (int m = 0; m < MmaIterations::kRow; ++m) {
 
         CUTLASS_PRAGMA_UNROLL
         for (int n = 0; n < MmaIterations::kColumn; ++n) {
 
+          // This allows to reuse of Rb when at serpentine turns
           int n_serpentine = ((m % 2) ? (MmaIterations::kColumn - 1 - n) : n);
 
           if (AccumulatorsInRowMajor) {  // matrix B is reordered
             mma(
               ptr_D[n_serpentine + m * MmaIterations::kColumn],
               ptr_A[m],
               ptr_B[n_serpentine],
               ptr_D[n_serpentine + m * MmaIterations::kColumn]);
           } else {
-            mma(ptr_D[m + n_serpentine * MmaIterations::kRow],
-                ptr_A[m],
-                ptr_B[n_serpentine],
-                ptr_D[m + n_serpentine * MmaIterations::kRow]);
+            mma(
+              ptr_D[m + n_serpentine * MmaIterations::kRow],
+              ptr_A[m],
+              ptr_B[n_serpentine],
+              ptr_D[m + n_serpentine * MmaIterations::kRow]);
           }
-        }
-      }
+        } // end n loop
+      } // end m loop
     #else
       assert(0);
     #endif
   }
 
   /// Transform the mma operands to the required types
   CUTLASS_DEVICE
   void transform(TransformedFragmentA &dst_A, TransformedFragmentB &dst_B,
                  FragmentA const &A, FragmentB const &B) const {
 
     //
     // Define conversions from source type to instruction type
     //
-    FloatRoundStyle const kRoundA =
-        PreferredRoundingMode<typename ArchMmaOperator::ElementA,
-                              ElementA>::kRound;
-    FloatRoundStyle const kRoundB =
-        PreferredRoundingMode<typename ArchMmaOperator::ElementB,
-                              ElementB>::kRound;
-    #if defined(__CUDA_ARCH__) && (__CUDA_ARCH__ < 800)
-      detail::ConvertAndPack<typename ArchMmaOperator::ElementA, ElementA,
-                            FragmentA::kElements, kRoundA>
-          convert_A;
-      NumericArrayConverter<typename ArchMmaOperator::ElementB, ElementB,
-                            FragmentB::kElements / 2, kRoundB>
-          convert_B;
-      Array<ElementB, FragmentB::kElements / 2> const *ptr_B =
-          reinterpret_cast<Array<ElementB, FragmentB::kElements / 2> const *>(&B);
-      Array<typename ArchMmaOperator::ElementB, FragmentB::kElements / 2> *
-          ptr_dst_B = reinterpret_cast<Array<typename ArchMmaOperator::ElementB,
-                                             FragmentB::kElements / 2> *>(&dst_B);
-  
-      dst_A = convert_A(A);
-  
-      ptr_dst_B[0] = convert_B(ptr_B[0]);
-      ptr_dst_B[1] = convert_B(ptr_B[1]);
-
-    #elif defined(__CUDA_ARCH__) && (__CUDA_ARCH__ >= 800)
-      detail::ConvertAndPack<typename ArchMmaOperator::ElementA, ElementA,
-                            FragmentA::kElements / 2, kRoundA>
-          convert_A;
-      NumericArrayConverter<typename ArchMmaOperator::ElementB, ElementB,
-                            FragmentB::kElements, kRoundB>
-          convert_B;
-      Array<ElementA, FragmentA::kElements / 2> const *ptr_A =
-          reinterpret_cast<Array<ElementA, FragmentA::kElements / 2> const *>(&A);
-      Array<typename ArchMmaOperator::ElementA, FragmentA::kElements / 2> *
-          ptr_dst_A = reinterpret_cast<Array<typename ArchMmaOperator::ElementA,
-                                             FragmentA::kElements / 2> *>(&dst_A);
-  
-      dst_B = convert_B(B);
-  
-      ptr_dst_A[0] = convert_A(ptr_A[0]);
-      ptr_dst_A[1] = convert_A(ptr_A[1]);
+    #if defined(__CUDA_ARCH__) && (__CUDA_ARCH__ >= 800)
+      
+      detail::ConvertAndPackAccurateF32<
+        FragmentA::kElements / 2,
+        MmaFastF32::kRoundBigA,
+        MmaFastF32::kRoundSmallA> convert_A;
+      
+      detail::ConvertAndPackAccurateF32<
+        FragmentB::kElements,
+        MmaFastF32::kRoundBigB,
+        MmaFastF32::kRoundSmallB> convert_B;
+      
+      Array<typename ArchMmaOperator::ElementB, FragmentB::kElements> *ptr_dst_B = 
+        reinterpret_cast<Array<typename ArchMmaOperator::ElementB, FragmentB::kElements> *>(&dst_B);
+      
+      convert_B(B, ptr_dst_B[0], ptr_dst_B[1]);
+
+      Array<typename ArchMmaOperator::ElementA, FragmentA::kElements / 2> *ptr_dst_A =
+        reinterpret_cast<Array<typename ArchMmaOperator::ElementA, FragmentA::kElements / 2> *>(&dst_A);
+      
+      Array<ElementA, FragmentA::kElements / 2> const *ptr_A = 
+        reinterpret_cast<Array<ElementA, FragmentA::kElements / 2> const *>(&A);
+      
+      convert_A(ptr_A[0], ptr_dst_A[0], ptr_dst_A[2]);
+      
+      convert_A(ptr_A[1], ptr_dst_A[1], ptr_dst_A[3]);
     #else
       assert(0);
     #endif
   }
 };
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
 } // namespace warp
 } // namespace gemm
 } // namespace cutlass
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
-
-#include "cutlass/gemm/warp/mma_tensor_op_fast_f32.h"
-
-/////////////////////////////////////////////////////////////////////////////////////////////////
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/warp/mma_tensor_op_fragment_iterator.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/gemm/warp/mma_tensor_op_fragment_iterator.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/warp/mma_tensor_op_policy.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/gemm/warp/mma_tensor_op_policy.h`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/warp/mma_tensor_op_sm70.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/gemm/warp/mma_tensor_op_wmma.h`

 * *Files 20% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -27,254 +27,197 @@
  * OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
  * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
  *
  **************************************************************************************************/
 /*! \file
     \brief Templates implementing warp-level matrix multiply-accumulate operations targeting
       Tensor Cores.
-
-    This is a work in progress.
 */
 
 #pragma once
 
 #include "cutlass/cutlass.h"
-#include "cutlass/array.h"
+#include "cutlass/arch/wmma.h"
+
+#if defined(CUTLASS_ARCH_WMMA_ENABLED)
 
+#include "cutlass/wmma_array.h"
 #include "cutlass/numeric_types.h"
 #include "cutlass/matrix_shape.h"
 
-#include "cutlass/arch/mma.h"
+#include "cutlass/arch/memory_sm75.h"
+#include "cutlass/arch/mma_sm75.h"
+#include "cutlass/arch/mma_sm80.h"
 
 #include "cutlass/gemm/gemm.h"
 #include "cutlass/gemm/warp/mma.h"
 
 #include "cutlass/gemm/warp/mma_tensor_op_policy.h"
-#include "cutlass/gemm/warp/mma_tensor_op_tile_iterator_sm70.h"
+
+#include "cutlass/gemm/warp/mma_tensor_op_tile_iterator_wmma.h"
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
 namespace cutlass {
 namespace gemm {
 namespace warp {
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
-/// Structure to compute the matrix product targeting CUDA cores and SIMT math instructions.
-template <
-  /// Size of the Gemm problem - concept: gemm::GemmShape<>
+///< Structure to compute the matrix product targeting CUDA cores via WMMA.
+template < 
+  ///< Size of the Gemm problem - concept: gemm::GemmShape<>
   typename Shape_,
-  /// Data type of A elements
+  ///< Data type of A elements
   typename ElementA_,
-  /// Layout of A matrix (concept: MatrixLayout)
+  ///< Layout of A matrix (concept: MatrixLayout)
   typename LayoutA_,
-  /// Data type of B elements
+  ///< Data type of B elements
   typename ElementB_,
   /// Layout of B matrix (concept: MatrixLayout)
   typename LayoutB_,
-  /// Element type of C matrix
+  ///< Element type of C matrix
   typename ElementC_,
-  /// Layout of C matrix (concept: MatrixLayout)
+  ///< Layout of C matrix (concept: MatrixLayout)
   typename LayoutC_,
-  /// Policy describing warp-level MmaTensorOp (concept: MmaTensorOp policy)
+  ///< Policy describing warp-level Wmma operation (concept: MmaTensorOpPolicy)
   typename Policy_,
-  /// Used for partial specialization
+  ///< Number of partitions along K dimension
+  int PartitionsK_ = 1,
+  ///< Used for partial specialization
   typename Enable = bool
 >
-class MmaVoltaTensorOp {
+class MmaTensorOpWmma {
 public:
-  /// Shape of warp-level matrix operation (concept: GemmShape)
+  ///< Shape of warp-level matrix operation (concept: GemmShape)
   using Shape = Shape_;
 
-  /// Data type of multiplicand A
+  ///< Data type of multiplicand A
   using ElementA = ElementA_;
 
-  /// Layout of multiplicand A
+  ///< Layout of multiplicand A
   using LayoutA = LayoutA_;
 
-  /// Data type of multiplicand B
+  ///< Data type of multiplicand B
   using ElementB = ElementB_;
 
-  /// Layout of multiplicand B
+  ///< Layout of multiplicand B
   using LayoutB = LayoutB_;
 
-  /// Data type of accumulator matrix C
+  ///< Data type of accumulator matrix C
   using ElementC = ElementC_;
 
-  /// Layout of accumulator matrix C
+  ///< Layout of accumulator matrix C
   using LayoutC = LayoutC_;
 
-  /// Shape of the warp in units of thread (concept: MmaLanePolicySimt)
+  /// Shape of the warp in units of thread (concept: MmaTensorOpPolicy)
   using Policy = Policy_;
 
-  /// Indicates class of matrix operator
-  using OperatorClass = arch::OpClassTensorOp;
-
-  /// Architecture tag
-  using ArchTag = arch::Sm70;
+  /// Underlying instruction shape
+  using InstructionShape = typename Policy::Operator::Shape;
 
   /// Underlying matrix multiply operator (concept: arch::Mma)
   using ArchMmaOperator = typename Policy::Operator;
 
   /// Indicates math operator 
   using MathOperator = typename ArchMmaOperator::Operator;
   
-  /// Underlying instruction shape
-  using InstructionShape = typename ArchMmaOperator::Shape;
+  /// Underlying architecture tag
+  using ArchTag = typename Policy::Operator::ArchTag;
 
   /// Complex transform on A operand
   static ComplexTransform const kTransformA = ComplexTransform::kNone;
 
   /// Complex transform on B operand
   static ComplexTransform const kTransformB = ComplexTransform::kNone;
 
+  /// Indicates class of matrix operator
+  using OperatorClass = arch::OpClassWmmaTensorOp;
+
   /// Number of threads participating in warp-level matrix product
   static int const kThreadCount = 32;
 
-  /// interleaved 32x32 tiles
-  using InterleavedTileShape = GemmShape<32, 32, 4>;
+  /// Number of partitions along K dimension
+  static int const kPartitionsK = PartitionsK_;
 
-  static_assert(!(Shape::kM % InterleavedTileShape::kM) &&
-                !(Shape::kN % InterleavedTileShape::kN),
-                "Shape must be a multiple of InterleavedTileShape.");
 public:
 
   /// Iterates over the A operand in memory
-  using IteratorA = MmaVoltaTensorOpMultiplicandTileIterator<
-    MatrixShape<Shape::kM, Shape::kK>,
-    Operand::kA,
-    ElementA,
-    LayoutA,
-    MatrixShape<
-      ArchMmaOperator::Shape::kM,
-      ArchMmaOperator::Shape::kK
-    >,
-    Policy::OpDelta::kRow,
-    kThreadCount
-  >;
+  using IteratorA = MmaTensorOpWmmaMultiplicandTileIterator<
+     MatrixShape<Shape::kM, Shape::kK>, Operand::kA, ElementA, LayoutA,
+     Policy::OpDelta::kRow, kThreadCount, Policy>;
 
   /// Storage for A tile
   using FragmentA = typename IteratorA::Fragment;
 
   /// Iterates over the B operand in memory
-  using IteratorB = MmaVoltaTensorOpMultiplicandTileIterator<
-    MatrixShape<Shape::kK, Shape::kN>,
-    Operand::kB,
-    ElementB,
-    LayoutB,
-    MatrixShape<
-      ArchMmaOperator::Shape::kK,
-      ArchMmaOperator::Shape::kN
-    >,
-    Policy::OpDelta::kRow,
-    kThreadCount
-  >;
+  using IteratorB = MmaTensorOpWmmaMultiplicandTileIterator<
+     MatrixShape<Shape::kK, Shape::kN>, Operand::kB, ElementB, LayoutB,
+     Policy::OpDelta::kRow, kThreadCount, Policy>;
 
   /// Storage for B tile
   using FragmentB = typename IteratorB::Fragment;
 
   /// Iterates over the C operand in memory
-  using IteratorC = MmaVoltaTensorOpAccumulatorTileIterator<
-    MatrixShape<Shape::kM, Shape::kN>,
-    ElementC,
-    LayoutC,
-    typename ArchMmaOperator::Shape,
-    typename Policy::OpDelta
-  >;
+  using IteratorC = MmaTensorOpWmmaAccumulatorTileIterator<
+     MatrixShape<Shape::kM, Shape::kN>, ElementC, LayoutC,
+    typename Policy::OpDelta, Policy>;
 
   /// Storage for C tile
   using FragmentC = typename IteratorC::Fragment;
 
 private:
 
   static_assert(
-    !(Shape::kM % ArchMmaOperator::Shape::kM) && 
-    !(Shape::kN % ArchMmaOperator::Shape::kN),
-    "Shape of warp-level Mma must be divisible by operator shape.");
-
-  /// Number of mma operations performed
-  using MmaIterations = MatrixShape<
-    InterleavedTileShape::kM / ArchMmaOperator::Shape::kM,
-    InterleavedTileShape::kN / ArchMmaOperator::Shape::kN
-  >;
-  using TileIterations = MatrixShape<
-    Shape::kM / InterleavedTileShape::kM,
-    Shape::kN / InterleavedTileShape::kN
+    !(Shape::kM % Policy::Operator::Shape::kM) && 
+    !(Shape::kN % Policy::Operator::Shape::kN),
+    "Shape of warp-level Wmma must be divisible by operator shape (wmma native size)");
+
+  /// Number of wmma operations performed
+  using WmmaIterations = MatrixShape<
+    Shape::kM / Policy::Operator::Shape::kM,
+    Shape::kN / Policy::Operator::Shape::kN 
   >;
 
-  // Whether matrix B is reordered
-  bool reorder_B_;
-
 public:
 
-  /// Underlying matrix multiply operator (concept: arch::Mma)
-  ArchMmaOperator mma;
+  /// Underlying matrix multiply operator (concept: cutlass::arch::Wmma)
+  typename Policy::Operator wmma;
 
 public:
 
   //
   // Methods
   //
-  
+
   /// Ctor
   CUTLASS_DEVICE
-  MmaVoltaTensorOp() {}
+  MmaTensorOpWmma() {}
 
   /// Performs a warp-level matrix multiply-accumulate operation
   CUTLASS_DEVICE
   void operator()(
     FragmentC &D, 
     FragmentA const &A, 
     FragmentB const &B, 
-    FragmentC const &C)  {
-
-    using MmaOperandA = typename ArchMmaOperator::FragmentA;
-    using MmaOperandB = typename ArchMmaOperator::FragmentB;
-    using MmaOperandC = typename ArchMmaOperator::FragmentC;
-
-    D = C;
-
-    MmaOperandA const *ptr_A = reinterpret_cast<MmaOperandA const *>(&A);
-    MmaOperandB const *ptr_B = reinterpret_cast<MmaOperandB const *>(&B);
-    MmaOperandC *ptr_D = reinterpret_cast<MmaOperandC *>(&D);
+    FragmentC const &C) const {
 
     CUTLASS_PRAGMA_UNROLL
-    for (int outer_col = 0; outer_col < TileIterations::kColumn; ++outer_col) {
+    for (int n = 0; n < WmmaIterations::kColumn; ++n) {
       CUTLASS_PRAGMA_UNROLL
-      for (int inner_col = 0; inner_col < MmaIterations::kColumn; ++inner_col) {
-        CUTLASS_PRAGMA_UNROLL
-        for (int outer_row = 0; outer_row < TileIterations::kRow; ++outer_row) {
-          CUTLASS_PRAGMA_UNROLL
-
-          for (int inner_row = 0; inner_row < MmaIterations::kRow; ++inner_row) {
-      
-            int op_col = inner_col + MmaIterations::kColumn * outer_col;
-
-            // Column-major serpentine sequence to maximize reuse of A operand.
-            int inner_row_serp = inner_row;
-            int outer_row_serp = outer_row;
-            if (op_col & 1) {
-              inner_row_serp = MmaIterations::kRow - inner_row - 1;
-              outer_row_serp = TileIterations::kRow - outer_row - 1;
-            }
-            int op_row = inner_row_serp + MmaIterations::kRow * outer_row_serp;
-            int op_idx = inner_row_serp + MmaIterations::kRow * 
-                         (inner_col + MmaIterations::kColumn * 
-                          (outer_row_serp + TileIterations::kRow * outer_col));
-            mma(
-              ptr_D[op_idx],
-              ptr_A[op_row],
-              ptr_B[op_col],
-              ptr_D[op_idx]);
+      for (int m = 0; m < WmmaIterations::kRow; ++m) {
 
-          }
-        }
+        // accumulate wmma mma
+        wmma(D[m * WmmaIterations::kColumn + n], A[m], B[n], C[m * WmmaIterations::kColumn + n]);
       }
-    }
+    }  
   }
 };
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
 } // namespace warp
 } // namespace gemm
 } // namespace cutlass
+
+#endif // if defined(CUTLASS_ARCH_WMMA_ENABLED)
+
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/warp/mma_tensor_op_tile_access_iterator.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/gemm/warp/mma_tensor_op_tile_access_iterator.h`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/warp/mma_tensor_op_tile_iterator.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/gemm/warp/mma_tensor_op_tile_iterator.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/warp/mma_tensor_op_tile_iterator_sm70.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/gemm/warp/mma_tensor_op_tile_iterator_sm70.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/warp/mma_tensor_op_tile_iterator_sm80.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/gemm/warp/mma_tensor_op_tile_iterator_sm80.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/warp/mma_tensor_op_tile_iterator_sparse.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/gemm/warp/mma_tensor_op_tile_iterator_sparse.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/warp/mma_tensor_op_tile_iterator_wmma.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/gemm/warp/mma_tensor_op_tile_iterator_wmma.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -489,15 +489,15 @@
 
   /// Advances the iterator along the opposite of the advance dimension
   CUTLASS_HOST_DEVICE
   MmaTensorOpWmmaMultiplicandTileIterator & operator--() {
 
     Index elements_offset = layout_({WmmaShape::kRow, 0});
 
-    byte_offset_ -= (elements_offset + sizeof_bits<Element>::value) / 8;
+    byte_offset_ -= (elements_offset * sizeof_bits<Element>::value) / 8;
     return *this;
   }
 
   ///< advances in units of whole tiles along the logical coordinate space of the tensor
   CUTLASS_DEVICE
   MmaTensorOpWmmaMultiplicandTileIterator & operator+=(TensorCoord const &tile_offset) {
     add_tile_offset(tile_offset);
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/warp/mma_with_reduction_tensor_op.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/gemm/warp/mma_with_reduction_tensor_op.h`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -77,15 +77,15 @@
   typename LayoutB_,
   /// Element type of C matrix
   typename ElementC_,
   /// Layout of C matrix (concept: MatrixLayout)
   typename LayoutC_,
   /// Policy describing warp-level MmaTensorOp (concept: MmaTensorOp policy)
   typename Policy_,
-  ///
+  /// Reduce operand A or B along K dimension
   bool ReduceKForA_,
   /// Number of partitions along K dimension
   int PartitionsK_ = 1,
   /// Store the accumulators in row major or column major.  Row major is used
   /// when output layout is interleaved.
   bool AccumulatorsInRowMajor = false,
   /// Used for partial specialization
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/warp/scale_bias_tile_iterator.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/gemm/warp/scale_bias_tile_iterator.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/warp/softmax_scale_bias_transform.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/gemm/warp/softmax_scale_bias_transform.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/warp/tile_iterator_planar_complex.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/gemm/warp/tile_iterator_planar_complex.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/half.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/half.h`

 * *Files 3% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -36,33 +36,16 @@
 #pragma once
 
 #ifndef CUTLASS_ENABLE_F16C
 #define CUTLASS_ENABLE_F16C 0
 #endif
 
 #if defined(__CUDACC_RTC__)
-/* All floating-point numbers can be put in one of these categories.  */
-enum
-  {
-    FP_NAN =
-# define FP_NAN 0
-      FP_NAN,
-    FP_INFINITE =
-# define FP_INFINITE 1
-      FP_INFINITE,
-    FP_ZERO =
-# define FP_ZERO 2
-      FP_ZERO,
-    FP_SUBNORMAL =
-# define FP_SUBNORMAL 3
-      FP_SUBNORMAL,
-    FP_NORMAL =
-# define FP_NORMAL 4
-      FP_NORMAL
-  };
+
+#include "cutlass/floating_point_nvrtc.h"
 
 // F16C extensions are not meaningful when compiling for NVRTC which only accommodates device code.
 #undef CUTLASS_ENABLE_F16C
 #define CUTLASS_ENABLE_F16C 0
 
 #else
 //
@@ -75,14 +58,15 @@
 #endif
 
 ///////////////////////////////////////////////////////////////////////////////////////////////////
 
 #include <cuda_fp16.h>
 
 #include "cutlass/cutlass.h"
+#include "cutlass/float8.h"
 #include "cutlass/platform/platform.h"
 
 ///////////////////////////////////////////////////////////////////////////////////////////////////
 
 // Optionally target F16C extentions to accelerate half-precision conversion.
 #if !defined(__CUDA_ARCH__) && (CUTLASS_ENABLE_F16C)
 #if defined(_MSC_VER)
@@ -368,16 +352,15 @@
   }
 
   //
   // Methods
   //
 
   /// Default constructor
-  CUTLASS_HOST_DEVICE
-  half_t() : storage(0) { }
+  half_t() = default;
 
   /// Reinterpret cast from CUDA's half type
   CUTLASS_HOST_DEVICE
   explicit half_t(half const & x) {
     #if defined(__CUDA_ARCH__)
     storage = reinterpret_cast<uint16_t const &>(x);
     #else
@@ -394,14 +377,26 @@
 
   /// Floating point conversion
   CUTLASS_HOST_DEVICE
   explicit half_t(double x): half_t(float(x)) {
 
   }
 
+  /// float_e4m3_t conversion
+  CUTLASS_HOST_DEVICE
+  explicit half_t(float_e4m3_t x): half_t(float(x)) {
+
+  }
+
+  /// float_e5m2_t conversion
+  CUTLASS_HOST_DEVICE
+  explicit half_t(float_e5m2_t x): half_t(float(x)) {
+
+  }
+
   /// Integer conversion - round to nearest even
   CUTLASS_HOST_DEVICE
   explicit half_t(int x) {
     storage = convert(x).storage;
   }
 
   /// Integer conversion - round toward zero
@@ -614,27 +609,27 @@
 
   /// Maximum finite value
   static cutlass::half_t max() { return cutlass::half_t::bitcast(0x7bff); }
 
   /// Returns smallest finite value
   static cutlass::half_t epsilon() { return cutlass::half_t::bitcast(0x1800); }
 
-  /// Returns smallest finite value
+  /// Returns maximum rounding error
   static cutlass::half_t round_error() { return cutlass::half_t(0.5f); }
 
-  /// Returns smallest finite value
+  /// Returns positive infinity value
   static cutlass::half_t infinity() { return cutlass::half_t::bitcast(0x7c00); }
 
-  /// Returns smallest finite value
+  /// Returns quiet NaN value
   static cutlass::half_t quiet_NaN() { return cutlass::half_t::bitcast(0x7fff); }
 
-  /// Returns smallest finite value
+  /// Returns signaling NaN value
   static cutlass::half_t signaling_NaN() { return cutlass::half_t::bitcast(0x7fff); }
 
-  /// Returns smallest finite value
+  /// Returns smallest positive subnormal value
   static cutlass::half_t denorm_min() { return cutlass::half_t::bitcast(0x0001); }
 };
 }  // namespace std
 #endif
 
 namespace platform {
 
@@ -676,31 +671,31 @@
   CUTLASS_HOST_DEVICE
   static cutlass::half_t max() { return cutlass::half_t::bitcast(0x7bff); }
 
   /// Returns smallest finite value
   CUTLASS_HOST_DEVICE
   static cutlass::half_t epsilon() { return cutlass::half_t::bitcast(0x1800); }
 
-  /// Returns smallest finite value
+  /// Returns maximum rounding error
   CUTLASS_HOST_DEVICE
   static cutlass::half_t round_error() { return cutlass::half_t(0.5f); }
 
-  /// Returns smallest finite value
+  /// Returns positive infinity value
   CUTLASS_HOST_DEVICE
   static cutlass::half_t infinity() { return cutlass::half_t::bitcast(0x7c00); }
 
-  /// Returns smallest finite value
+  /// Returns quiet NaN value
   CUTLASS_HOST_DEVICE
   static cutlass::half_t quiet_NaN() { return cutlass::half_t::bitcast(0x7fff); }
 
-  /// Returns smallest finite value
+  /// Returns signaling NaN value
   CUTLASS_HOST_DEVICE
   static cutlass::half_t signaling_NaN() { return cutlass::half_t::bitcast(0x7fff); }
 
-  /// Returns smallest finite value
+  /// Returns smallest positive subnormal value
   CUTLASS_HOST_DEVICE
   static cutlass::half_t denorm_min() { return cutlass::half_t::bitcast(0x0001); }
 };
 }  // namespace platform 
 
 ///////////////////////////////////////////////////////////////////////////////////////////////////
 //
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/integer_subbyte.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/integer_subbyte.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -73,16 +73,15 @@
   Storage storage;
 
   //
   // Methods
   //
 
   /// No operation
-  CUTLASS_HOST_DEVICE
-  integer_subbyte() { }
+  integer_subbyte() = default;
 
   /// Conversion from integer type
   CUTLASS_HOST_DEVICE
   integer_subbyte(int value)
       : storage(reinterpret_cast<Storage const &>(value) & kMask) {}
 
   CUTLASS_HOST_DEVICE
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/kernel_launch.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/kernel_launch.h`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/layout/layout.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/layout/layout.h`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/layout/matrix.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/layout/matrix.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/layout/permute.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/layout/permute.h`

 * *Files 3% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -68,17 +68,14 @@
 
   MatrixCoord extent_;
 
   Index stride_unit_; //  sizeof(AccessType) / kElementsPerAccess in epilogue's predicated_tile_iterator
 
   Index stride_permute_;
 
-  Index col_permute_;
-  Index row_permute_;
-
 public:
   //
   // Methods
   //
 
   /// Constructor
   CUTLASS_HOST_DEVICE
@@ -115,17 +112,14 @@
   // Data members
   //
 
   MatrixCoord extent_;
 
   Index stride_permute_;
   
-  Index col_permute_;
-  Index row_permute_;
-
 public:
   //
   // Methods
   //
 
   /// Constructor
   CUTLASS_HOST_DEVICE
@@ -155,18 +149,18 @@
 
     int l = col_init % D3;
     int k = col_init / D3;
     int j = row_init % D1;
     int i = row_init / D1;
 
     // After the Permute Op
-    col_permute_ = l + j * D3;
-    row_permute_ = k + i * D2;
+    Index col_permute = l + j * D3;
+    Index row_permute = k + i * D2;
 
-    return LongIndex(row_permute_) * LongIndex(stride_permute_) + LongIndex(col_permute_);
+    return LongIndex(row_permute) * LongIndex(stride_permute_) + LongIndex(col_permute);
   }
 
   /// Return D1
   CUTLASS_HOST_DEVICE
   Index d1() const {
     return D1;
   }
@@ -194,17 +188,14 @@
   // Data members
   //
 
   MatrixCoord extent_;
 
   Index stride_permute_;
   
-  Index col_permute_;
-  Index row_permute_;
-
 public:
   //
   // Methods
   //
 
   /// Constructor
   CUTLASS_HOST_DEVICE
@@ -236,29 +227,29 @@
 
     int l = col_init;
     int k = row_init;
     int j = BMM_batch_idx % D1;
     int i = BMM_batch_idx / D1;
 
     // After the Permute Op
-    col_permute_ = l + j * D3;
-    row_permute_ = k + i * D2;
+    Index col_permute = l + j * D3;
+    Index row_permute = k + i * D2;
 
-    return LongIndex(row_permute_) * LongIndex(stride_permute_) + LongIndex(col_permute_);
+    return LongIndex(row_permute) * LongIndex(stride_permute_) + LongIndex(col_permute);
   }
 
   /// Return D1
   CUTLASS_HOST_DEVICE
   Index d1() const {
     return D1;
   }
 };
 
 /// Permute layout function for 5-D permuted tensors with output matrix (dimension as [M, N]) reshaped
-/// as [M/T1, T1, T2, T3, N/T3]. Then perform permute([2, 0, 3, 1, 4]) on the corresponding output tensor.
+/// as [M/T1, T1, T2, T3, N/T2/T3]. Then perform permute([2, 0, 3, 1, 4]) on the corresponding output tensor.
 template <int T1, int T2, int T3>
 class Tensor5DPermute20314 {
 public:
   /// Index type used for coordinates
   using Index = int32_t;
 
   /// Long index type used for offsets
@@ -269,17 +260,14 @@
   // Data members
   //
 
   MatrixCoord extent_;
 
   Index stride_permute_;
   
-  Index col_permute_;
-  Index row_permute_;
-
 public:
   //
   // Methods
   //
 
   /// Constructor
   CUTLASS_HOST_DEVICE
@@ -309,18 +297,18 @@
     int m = col_init % T4;
     int l = int(col_init / T4) % T3;
     int k = int(col_init / T4) / T3;
     int j = row_init % T1;
     int i = row_init / T1;
 
     // After the Permute Op
-    col_permute_ = m + j * T4 + l * T1 * T4;
-    row_permute_ = i + k * T0;
+    Index col_permute = m + j * T4 + l * T1 * T4;
+    Index row_permute = i + k * T0;
 
-    return LongIndex(row_permute_) * LongIndex(stride_permute_) + LongIndex(col_permute_);
+    return LongIndex(row_permute) * LongIndex(stride_permute_) + LongIndex(col_permute);
   }
 };
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
 } // namespace layout
 } // namespace cutlass
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/layout/pitch_linear.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/layout/pitch_linear.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/layout/tensor.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/layout/tensor.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/layout/tensor_op_multiplicand_sm70.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/layout/tensor_op_multiplicand_sm70.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/layout/tensor_op_multiplicand_sm75.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/layout/tensor_op_multiplicand_sm75.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/layout/tensor_op_multiplicand_sm80.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/layout/tensor_op_multiplicand_sm80.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/layout/vector.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/layout/vector.h`

 * *Files 2% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/matrix.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/matrix.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/matrix_coord.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/matrix_coord.h`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/matrix_shape.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/matrix_shape.h`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/numeric_conversion.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/numeric_conversion.h`

 * *Files 22% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -970,15 +970,15 @@
   result_type operator()(source_type const &s) {
     return convert(s);
   }
 };
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
-/// Partial specialization for Array<half> <= Array<float>
+/// Partial specialization for Array<bfloat16_t> <= Array<float>
 template <
   int N,
   FloatRoundStyle Round
 >
 struct NumericArrayConverter<bfloat16_t, float, N, Round> {
 
   using result_type = Array<bfloat16_t, N>;
@@ -1268,14 +1268,857 @@
     return convert(s);
   }
 };
 
 #endif
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
+//
+// Partial specializations for Array<float, N> <=> Array<float_e4m3_t, N>
+//
+/////////////////////////////////////////////////////////////////////////////////////////////////
+
+/// Partial specialization for Array<float, 4> <= Array<float_e4m3_t, 4>
+template <
+  FloatRoundStyle Round
+>
+struct NumericArrayConverter<float, float_e4m3_t, 4, Round> {
+  using result_element = float;
+  using source_element = float_e4m3_t;
+
+  using result_type = Array<result_element, 4>;
+  using source_type = Array<source_element, 4>;
+  static FloatRoundStyle const round_style = Round;
+
+  CUTLASS_DEVICE
+  static result_type convert(source_type const & source) {
+
+  #if defined(CUDA_PTX_FP8_CVT_ENABLED)
+    uint32_t out_fp16[2];
+    uint32_t const& src_packed = reinterpret_cast<uint32_t const&>(source);
+
+    asm volatile( \
+        "{\n" \
+        ".reg .b16 lo, hi;\n" \
+        "mov.b32 {lo, hi}, %2;\n" \
+        "cvt.rn.f16x2.e4m3x2 %0, lo;\n" \
+        "cvt.rn.f16x2.e4m3x2 %1, hi;\n" \
+        "}\n" : "=r"(out_fp16[0]), "=r"(out_fp16[1]) : "r"(src_packed));
+
+    float2 res0 = __half22float2(reinterpret_cast<__half2 &>(out_fp16[0]));
+    float2 res1 = __half22float2(reinterpret_cast<__half2 &>(out_fp16[1]));
+
+    result_type out;
+    out[0] = res0.x;
+    out[1] = res0.y;
+    out[2] = res1.x;
+    out[3] = res1.y;
+    return out;
+  #else
+    result_type result;
+    NumericConverter<result_element, source_element, Round> converter;
+
+    CUTLASS_PRAGMA_UNROLL
+    for (int i = 0; i < 4; ++i) {
+      result[i] = converter(source[i]);
+    }
+
+    return result;
+  #endif
+  }
+
+  CUTLASS_HOST_DEVICE
+  result_type operator()(source_type const &s) {
+    return convert(s);
+  }
+};
+
+/// Partial specialization for Array<float_e4m3_t, 4> <= Array<float, 4>
+template <
+  FloatRoundStyle Round
+>
+struct NumericArrayConverter<float_e4m3_t, float, 4, Round> {
+  using result_element = float_e4m3_t;
+  using source_element = float;
+
+  using result_type = Array<result_element, 4>;
+  using source_type = Array<source_element, 4>;
+  static FloatRoundStyle const round_style = Round;
+
+  CUTLASS_DEVICE
+  static result_type convert(source_type const & source) {
+
+  #if defined(CUDA_PTX_FP8_CVT_ENABLED)
+    uint32_t out;
+
+    asm volatile( \
+        "{\n" \
+        ".reg .b16 lo;\n" \
+        ".reg .b16 hi;\n" \
+        "cvt.rn.satfinite.e4m3x2.f32   lo, %2, %1;\n" \
+        "cvt.rn.satfinite.e4m3x2.f32   hi, %4, %3;\n" \
+        "mov.b32 %0, {lo, hi};\n" \
+        "}" \
+        : "=r"(out) : "f"(source[0]), "f"(source[1]), "f"(source[2]), "f"(source[3]));
+
+    return reinterpret_cast<result_type const &>(out);
+  #else
+    result_type result;
+    NumericConverter<result_element, source_element, Round> converter;
+
+    CUTLASS_PRAGMA_UNROLL
+    for (int i = 0; i < 4; ++i) {
+      result[i] = converter(source[i]);
+    }
+
+    return result;
+  #endif
+  }
+
+  CUTLASS_HOST_DEVICE
+  result_type operator()(source_type const &s) {
+    return convert(s);
+  }
+};
+
+/// Partial specialization for Array<float, 4> <= Array<float_e5m2_t, 4>
+template <
+  FloatRoundStyle Round
+>
+struct NumericArrayConverter<float, float_e5m2_t, 4, Round> {
+  using result_element = float;
+  using source_element = float_e5m2_t;
+
+  using result_type = Array<result_element, 4>;
+  using source_type = Array<source_element, 4>;
+  static FloatRoundStyle const round_style = Round;
+
+  CUTLASS_DEVICE
+  static result_type convert(source_type const & source) {
+
+  #if defined(CUDA_PTX_FP8_CVT_ENABLED)
+    uint32_t out_fp16[2];
+    uint32_t const& src_packed = reinterpret_cast<uint32_t const&>(source);
+
+    asm volatile( \
+        "{\n" \
+        ".reg .b16 lo, hi;\n" \
+        "mov.b32 {lo, hi}, %2;\n" \
+        "cvt.rn.f16x2.e5m2x2 %0, lo;\n" \
+        "cvt.rn.f16x2.e5m2x2 %1, hi;\n" \
+        "}\n" : "=r"(out_fp16[0]), "=r"(out_fp16[1]) : "r"(src_packed));
+
+    float2 res0 = __half22float2(reinterpret_cast<__half2 &>(out_fp16[0]));
+    float2 res1 = __half22float2(reinterpret_cast<__half2 &>(out_fp16[1]));
+
+    result_type out;
+    out[0] = res0.x;
+    out[1] = res0.y;
+    out[2] = res1.x;
+    out[3] = res1.y;
+    return out;
+  #else
+    result_type result;
+    NumericConverter<result_element, source_element, Round> converter;
+
+    CUTLASS_PRAGMA_UNROLL
+    for (int i = 0; i < 4; ++i) {
+      result[i] = converter(source[i]);
+    }
+
+    return result;
+  #endif
+  }
+
+  CUTLASS_HOST_DEVICE
+  result_type operator()(source_type const &s) {
+    return convert(s);
+  }
+};
+
+/// Partial specialization for Array<float_e5m2_t, 4> <= Array<float, 4>
+template <
+  FloatRoundStyle Round
+>
+struct NumericArrayConverter<float_e5m2_t, float, 4, Round> {
+  using result_element = float_e5m2_t;
+  using source_element = float;
+
+  using result_type = Array<result_element, 4>;
+  using source_type = Array<source_element, 4>;
+  static FloatRoundStyle const round_style = Round;
+
+  CUTLASS_DEVICE
+  static result_type convert(source_type const & source) {
+
+  #if defined(CUDA_PTX_FP8_CVT_ENABLED)
+    uint32_t out;
+
+    asm volatile( \
+        "{\n" \
+        ".reg .b16 lo;\n" \
+        ".reg .b16 hi;\n" \
+        "cvt.rn.satfinite.e5m2x2.f32   lo, %2, %1;\n" \
+        "cvt.rn.satfinite.e5m2x2.f32   hi, %4, %3;\n" \
+        "mov.b32 %0, {lo, hi};\n" \
+        "}" \
+        : "=r"(out) : "f"(source[0]), "f"(source[1]), "f"(source[2]), "f"(source[3]));
+
+    return reinterpret_cast<result_type const &>(out);
+  #else
+    result_type result;
+    NumericConverter<result_element, source_element, Round> converter;
+
+    CUTLASS_PRAGMA_UNROLL
+    for (int i = 0; i < 4; ++i) {
+      result[i] = converter(source[i]);
+    }
+
+    return result;
+  #endif
+  }
+
+  CUTLASS_HOST_DEVICE
+  result_type operator()(source_type const &s) {
+    return convert(s);
+  }
+};
+
+/////////////////////////////////////////////////////////////////////////////////////////////////
+//
+// Partial specializations for Array<half_t, 4> <=> Array<float_e4m3_t, 4>
+//
+/////////////////////////////////////////////////////////////////////////////////////////////////
+
+/// Partial specialization for Array<half_t, 4> <= Array<float_e4m3_t, 4>
+template <
+  FloatRoundStyle Round
+>
+struct NumericArrayConverter<half_t, float_e4m3_t, 4, Round> {
+  using result_element = half_t;
+  using source_element = float_e4m3_t;
+
+  using result_type = Array<result_element, 4>;
+  using source_type = Array<source_element, 4>;
+  static FloatRoundStyle const round_style = Round;
+
+  CUTLASS_DEVICE
+  static result_type convert(source_type const & source) {
+
+  #if defined(CUDA_PTX_FP8_CVT_ENABLED)
+    uint32_t out[2];
+    uint32_t const& src_packed = reinterpret_cast<uint32_t const&>(source);
+    asm volatile( \
+        "{\n" \
+        ".reg .b16 lo, hi;\n" \
+        "mov.b32 {lo, hi}, %2;\n" \
+        "cvt.rn.f16x2.e4m3x2 %0, lo;\n" \
+        "cvt.rn.f16x2.e4m3x2 %1, hi;\n" \
+        "}\n" : "=r"(out[0]), "=r"(out[1]) : "r"(src_packed));
+    return reinterpret_cast<result_type const &>(out);
+  #else
+    result_type result;
+    NumericConverter<result_element, source_element, Round> converter;
+
+    CUTLASS_PRAGMA_UNROLL
+    for (int i = 0; i < 4; ++i) {
+      result[i] = converter(source[i]);
+    }
+
+    return result;
+  #endif
+  }
+
+  CUTLASS_HOST_DEVICE
+  result_type operator()(source_type const &s) {
+    return convert(s);
+  }
+};
+
+/// Partial specialization for Array<float_e4m3_t, 4> <= Array<half_t, 4>
+template <
+  FloatRoundStyle Round
+>
+struct NumericArrayConverter<float_e4m3_t, half_t, 4, Round> {
+  using result_element = float_e4m3_t;
+  using source_element = half_t;
+
+  using result_type = Array<result_element, 4>;
+  using source_type = Array<source_element, 4>;
+  static FloatRoundStyle const round_style = Round;
+
+  CUTLASS_DEVICE
+  static result_type convert(source_type const & source) {
+
+  #if defined(CUDA_PTX_FP8_CVT_ENABLED)
+    uint32_t out;
+    uint32_t const* src_packed = reinterpret_cast<uint32_t const*>(&source);
+
+    asm volatile( \
+        "{\n" \
+        ".reg .b16 lo;\n" \
+        ".reg .b16 hi;\n" \
+        "cvt.rn.satfinite.e4m3x2.f16x2   lo, %1;\n" \
+        "cvt.rn.satfinite.e4m3x2.f16x2   hi, %2;\n" \
+        "mov.b32 %0, {lo, hi};\n" \
+        "}" \
+        : "=r"(out) : "r"(src_packed[0]), "r"(src_packed[1]));
+
+    return reinterpret_cast<result_type const &>(out);
+  #else
+    result_type result;
+    NumericConverter<result_element, source_element, Round> converter;
+
+    CUTLASS_PRAGMA_UNROLL
+    for (int i = 0; i < 4; ++i) {
+      result[i] = converter(source[i]);
+    }
+
+    return result;
+  #endif
+  }
+
+  CUTLASS_HOST_DEVICE
+  result_type operator()(source_type const &s) {
+    return convert(s);
+  }
+};
+
+/// Partial specialization for Array<half_t, 4> <= Array<float_e5m2_t, 4>
+template <
+  FloatRoundStyle Round
+>
+struct NumericArrayConverter<half_t, float_e5m2_t, 4, Round> {
+  using result_element = half_t;
+  using source_element = float_e5m2_t;
+
+  using result_type = Array<result_element, 4>;
+  using source_type = Array<source_element, 4>;
+  static FloatRoundStyle const round_style = Round;
+
+  CUTLASS_DEVICE
+  static result_type convert(source_type const & source) {
+
+  #if defined(CUDA_PTX_FP8_CVT_ENABLED)
+    uint32_t out[2];
+    uint32_t const& src_packed = reinterpret_cast<uint32_t const&>(source);
+    asm volatile( \
+        "{\n" \
+        ".reg .b16 lo, hi;\n" \
+        "mov.b32 {lo, hi}, %2;\n" \
+        "cvt.rn.f16x2.e5m2x2 %0, lo;\n" \
+        "cvt.rn.f16x2.e5m2x2 %1, hi;\n" \
+        "}\n" : "=r"(out[0]), "=r"(out[1]) : "r"(src_packed));
+    return reinterpret_cast<result_type const &>(out);
+  #else
+    result_type result;
+    NumericConverter<result_element, source_element, Round> converter;
+
+    CUTLASS_PRAGMA_UNROLL
+    for (int i = 0; i < 4; ++i) {
+      result[i] = converter(source[i]);
+    }
+
+    return result;
+  #endif
+  }
+
+  CUTLASS_HOST_DEVICE
+  result_type operator()(source_type const &s) {
+    return convert(s);
+  }
+};
+
+/// Partial specialization for Array<float_e5m2_t, 4> <= Array<half_t, 4>
+template <
+  FloatRoundStyle Round
+>
+struct NumericArrayConverter<float_e5m2_t, half_t, 4, Round> {
+  using result_element = float_e5m2_t;
+  using source_element = half_t;
+
+  using result_type = Array<result_element, 4>;
+  using source_type = Array<source_element, 4>;
+  static FloatRoundStyle const round_style = Round;
+
+  CUTLASS_DEVICE
+  static result_type convert(source_type const & source) {
+
+  #if defined(CUDA_PTX_FP8_CVT_ENABLED)
+    uint32_t out;
+    uint32_t const* src_packed = reinterpret_cast<uint32_t const*>(&source);
+
+    asm volatile( \
+        "{\n" \
+        ".reg .b16 lo;\n" \
+        ".reg .b16 hi;\n" \
+        "cvt.rn.satfinite.e5m2x2.f16x2   lo, %1;\n" \
+        "cvt.rn.satfinite.e5m2x2.f16x2   hi, %2;\n" \
+        "mov.b32 %0, {lo, hi};\n" \
+        "}" \
+        : "=r"(out) : "r"(src_packed[0]), "r"(src_packed[1]));
+
+    return reinterpret_cast<result_type const &>(out);
+  #else
+    result_type result;
+    NumericConverter<result_element, source_element, Round> converter;
+
+    CUTLASS_PRAGMA_UNROLL
+    for (int i = 0; i < 4; ++i) {
+      result[i] = converter(source[i]);
+    }
+
+    return result;
+  #endif
+  }
+
+  CUTLASS_HOST_DEVICE
+  result_type operator()(source_type const &s) {
+    return convert(s);
+  }
+};
+
+/////////////////////////////////////////////////////////////////////////////////////////////////
+//
+// Partial specializations for Array<bfloat16_t, 4> <=> Array<float_e4m3_t, 4>
+//
+/////////////////////////////////////////////////////////////////////////////////////////////////
+
+/// Partial specialization for Array<bfloat16_t, 4> <= Array<float_e4m3_t, 4>
+template <
+  FloatRoundStyle Round
+>
+struct NumericArrayConverter<bfloat16_t, float_e4m3_t, 4, Round> {
+  using result_element = bfloat16_t;
+  using source_element = float_e4m3_t;
+
+  using result_type = Array<result_element, 4>;
+  using source_type = Array<source_element, 4>;
+  static FloatRoundStyle const round_style = Round;
+
+  CUTLASS_DEVICE
+  static result_type convert(source_type const & source) {
+
+  #if defined(CUDA_PTX_FP8_CVT_ENABLED)
+    // Convert f8 to float
+    NumericArrayConverter<float, source_element, 4, Round> src2float;
+    Array<float, 4> tmp_floats = src2float(source);
+
+    // Convert float to bf16
+    result_type out;
+    Array<float, 2>* packed_tmp = reinterpret_cast<Array<float, 2>*>(&tmp_floats);
+    Array<result_element, 2>* packed_out = reinterpret_cast<Array<result_element, 2>*>(&out);
+    NumericArrayConverter<result_element, float, 2, Round> float2result;
+    packed_out[0] = float2result(packed_tmp[0]);
+    packed_out[1] = float2result(packed_tmp[1]);
+
+    return out;
+  #else
+    result_type result;
+    NumericConverter<result_element, source_element, Round> converter;
+
+    CUTLASS_PRAGMA_UNROLL
+    for (int i = 0; i < 4; ++i) {
+      result[i] = converter(source[i]);
+    }
+
+    return result;
+  #endif
+  }
+
+  CUTLASS_HOST_DEVICE
+  result_type operator()(source_type const &s) {
+    return convert(s);
+  }
+};
+
+/// Partial specialization for Array<float_e4m3_t, 4> <= Array<bfloat16_t, 4>
+template <
+  FloatRoundStyle Round
+>
+struct NumericArrayConverter<float_e4m3_t, bfloat16_t, 4, Round> {
+  using result_element = float_e4m3_t;
+  using source_element = bfloat16_t;
+
+  using result_type = Array<result_element, 4>;
+  using source_type = Array<source_element, 4>;
+  static FloatRoundStyle const round_style = Round;
+
+  CUTLASS_DEVICE
+  static result_type convert(source_type const & source) {
+
+  #if defined(CUDA_PTX_FP8_CVT_ENABLED)
+    // Convert bf16 to float
+    Array<float, 4> tmp;
+    Array<float, 2>* packed_tmp = reinterpret_cast<Array<float, 2>*>(&tmp);
+    Array<source_element, 2> const* packed_source = reinterpret_cast<Array<source_element, 2> const*>(&source);
+    NumericArrayConverter<float, source_element, 2, Round> src2float;
+    packed_tmp[0] = src2float(packed_source[0]);
+    packed_tmp[1] = src2float(packed_source[1]);
+
+    // Convert float to f8
+    NumericArrayConverter<result_element, float, 4, Round> float2result;
+    return float2result(tmp);
+  #else
+    result_type result;
+    NumericConverter<result_element, source_element, Round> converter;
+
+    CUTLASS_PRAGMA_UNROLL
+    for (int i = 0; i < 4; ++i) {
+      result[i] = converter(source[i]);
+    }
+
+    return result;
+  #endif
+  }
+
+  CUTLASS_HOST_DEVICE
+  result_type operator()(source_type const &s) {
+    return convert(s);
+  }
+};
+
+/// Partial specialization for Array<bfloat16_t, 4> <= Array<float_e5m2_t, 4>
+template <
+  FloatRoundStyle Round
+>
+struct NumericArrayConverter<bfloat16_t, float_e5m2_t, 4, Round> {
+  using result_element = bfloat16_t;
+  using source_element = float_e5m2_t;
+
+  using result_type = Array<result_element, 4>;
+  using source_type = Array<source_element, 4>;
+  static FloatRoundStyle const round_style = Round;
+
+  CUTLASS_DEVICE
+  static result_type convert(source_type const & source) {
+
+  #if defined(CUDA_PTX_FP8_CVT_ENABLED)
+    // Convert f8 to float
+    NumericArrayConverter<float, source_element, 4, Round> src2float;
+    Array<float, 4> tmp_floats = src2float(source);
+
+    // Convert float to bf16
+    result_type out;
+    Array<float, 2>* packed_tmp = reinterpret_cast<Array<float, 2>*>(&tmp_floats);
+    Array<result_element, 2>* packed_out = reinterpret_cast<Array<result_element, 2>*>(&out);
+    NumericArrayConverter<result_element, float, 2, Round> float2result;
+    packed_out[0] = float2result(packed_tmp[0]);
+    packed_out[1] = float2result(packed_tmp[1]);
+
+    return out;
+  #else
+    result_type result;
+    NumericConverter<result_element, source_element, Round> converter;
+
+    CUTLASS_PRAGMA_UNROLL
+    for (int i = 0; i < 4; ++i) {
+      result[i] = converter(source[i]);
+    }
+
+    return result;
+  #endif
+  }
+
+  CUTLASS_HOST_DEVICE
+  result_type operator()(source_type const &s) {
+    return convert(s);
+  }
+};
+
+/// Partial specialization for Array<float_e5m2_t, 4> <= Array<bfloat16_t, 4>
+template <
+  FloatRoundStyle Round
+>
+struct NumericArrayConverter<float_e5m2_t, bfloat16_t, 4, Round> {
+  using result_element = float_e5m2_t;
+  using source_element = bfloat16_t;
+
+  using result_type = Array<result_element, 4>;
+  using source_type = Array<source_element, 4>;
+  static FloatRoundStyle const round_style = Round;
+
+  CUTLASS_DEVICE
+  static result_type convert(source_type const & source) {
+
+  #if defined(CUDA_PTX_FP8_CVT_ENABLED)
+    // Convert bf16 to float
+    Array<float, 4> tmp;
+    Array<float, 2>* packed_tmp = reinterpret_cast<Array<float, 2>*>(&tmp);
+    Array<source_element, 2> const* packed_source = reinterpret_cast<Array<source_element, 2> const*>(&source);
+    NumericArrayConverter<float, source_element, 2, Round> src2float;
+    packed_tmp[0] = src2float(packed_source[0]);
+    packed_tmp[1] = src2float(packed_source[1]);
+
+    // Convert float to f8
+    NumericArrayConverter<result_element, float, 4, Round> float2result;
+    return float2result(tmp);
+  #else
+    result_type result;
+    NumericConverter<result_element, source_element, Round> converter;
+
+    CUTLASS_PRAGMA_UNROLL
+    for (int i = 0; i < 4; ++i) {
+      result[i] = converter(source[i]);
+    }
+
+    return result;
+  #endif
+  }
+
+  CUTLASS_HOST_DEVICE
+  result_type operator()(source_type const &s) {
+    return convert(s);
+  }
+};
+
+/////////////////////////////////////////////////////////////////////////////////////////////////
+//
+// Partial specializations for Array<float_e4m3_t, 4> <=> Array<float_e5m2_t, 4>
+//
+/////////////////////////////////////////////////////////////////////////////////////////////////
+
+/// Partial specialization for Array<float_e4m3_t, 4> <= Array<float_e5m2_t, 4>
+template <
+  FloatRoundStyle Round
+>
+struct NumericArrayConverter<float_e4m3_t, float_e5m2_t, 4, Round> {
+  using result_element = float_e4m3_t;
+  using source_element = float_e5m2_t;
+
+  using result_type = Array<result_element, 4>;
+  using source_type = Array<source_element, 4>;
+  static FloatRoundStyle const round_style = Round;
+
+  CUTLASS_DEVICE
+  static result_type convert(source_type const & source) {
+    result_type result;
+    NumericConverter<result_element, source_element, Round> converter;
+
+    CUTLASS_PRAGMA_UNROLL
+    for (int i = 0; i < 4; ++i) {
+      result[i] = converter(source[i]);
+    }
+
+    return result;
+  }
+
+  CUTLASS_HOST_DEVICE
+  result_type operator()(source_type const &s) {
+    return convert(s);
+  }
+};
+
+/// Partial specialization for Array<float_e5m2_t, 4> <= Array<float_e4m3_t, 4>
+template <
+  FloatRoundStyle Round
+>
+struct NumericArrayConverter<float_e5m2_t, float_e4m3_t, 4, Round> {
+  using result_element = float_e5m2_t;
+  using source_element = float_e4m3_t;
+
+  using result_type = Array<result_element, 4>;
+  using source_type = Array<source_element, 4>;
+  static FloatRoundStyle const round_style = Round;
+
+  CUTLASS_DEVICE
+  static result_type convert(source_type const & source) {
+    result_type result;
+    NumericConverter<result_element, source_element, Round> converter;
+
+    CUTLASS_PRAGMA_UNROLL
+    for (int i = 0; i < 4; ++i) {
+      result[i] = converter(source[i]);
+    }
+
+    return result;
+  }
+
+  CUTLASS_HOST_DEVICE
+  result_type operator()(source_type const &s) {
+    return convert(s);
+  }
+};
+
+/////////////////////////////////////////////////////////////////////////////////////////////////
+//
+// Partial specializations for:
+//      Array<float_e4m3_t, 4> <=> Array<float_e4m3_t, 4>
+//      Array<float_e5m2_t, 4> <=> Array<float_e5m2_t, 4>
+//
+// These are needed to avoid multiple-matching-template compilation errors (e.g., when
+// compiling float_e4m3_t <=> float_e4m3_t, which among T <= float_e4m3_t and float_e4m3_t <= T
+// should be used?)
+//
+/////////////////////////////////////////////////////////////////////////////////////////////////
+
+/// Partial specialization for Array<float_e4m3_t, 4> <= Array<float_e4m3_t, 4>
+template <
+  FloatRoundStyle Round
+>
+struct NumericArrayConverter<float_e4m3_t, float_e4m3_t, 4, Round> {
+  using result_element = float_e4m3_t;
+  using source_element = float_e4m3_t;
+
+  using result_type = Array<result_element, 4>;
+  using source_type = Array<source_element, 4>;
+  static FloatRoundStyle const round_style = Round;
+
+  CUTLASS_HOST_DEVICE
+  result_type operator()(source_type const &s) {
+    return s;
+  }
+};
+
+/// Partial specialization for Array<float_e5m2_t, 4> <= Array<float_e5m2_t, 4>
+template <
+  FloatRoundStyle Round
+>
+struct NumericArrayConverter<float_e5m2_t, float_e5m2_t, 4, Round> {
+  using result_element = float_e5m2_t;
+  using source_element = float_e5m2_t;
+
+  using result_type = Array<result_element, 4>;
+  using source_type = Array<source_element, 4>;
+  static FloatRoundStyle const round_style = Round;
+
+  CUTLASS_HOST_DEVICE
+  result_type operator()(source_type const &s) {
+    return s;
+  }
+};
+
+/////////////////////////////////////////////////////////////////////////////////////////////////
+//
+// Partial specialziations for:
+//       Array<T, N> <=> Array<float_e4m3_t, N>
+//       Array<T, N> <=> Array<float_e5m2_t, N>
+// using packed converter under the hood
+//
+/////////////////////////////////////////////////////////////////////////////////////////////////
+
+template <
+  typename T,
+  typename S,
+  int N,
+  FloatRoundStyle Round
+>
+struct PackedNumericArrayConverter {
+  using result_element = T;
+  using source_element = S;
+
+  using result_type = Array<result_element, N>;
+  using source_type = Array<source_element, N>;
+
+  static FloatRoundStyle const round_style = Round;
+
+private:
+  using packed_result_type = Array<result_element, 4>;
+  using packed_source_type = Array<source_element, 4>;
+
+public:
+  CUTLASS_DEVICE
+  static result_type convert(source_type const & source) {
+    result_type result;
+    packed_result_type* packed_result = reinterpret_cast<packed_result_type*>(&result);
+    const packed_source_type* packed_source = reinterpret_cast<const packed_source_type*>(&source);
+
+    NumericArrayConverter<result_element, source_element, 4, Round> packed_converter;
+
+    CUTLASS_PRAGMA_UNROLL
+    for (int i = 0; i < N / 4; ++i) {
+      packed_result[i] = packed_converter(packed_source[i]);
+    }
+
+    // Handle leftovers
+    NumericConverter<result_element, source_element, Round> converter;
+    CUTLASS_PRAGMA_UNROLL
+    for (int i = 0; i < N % 4; ++i) {
+      int idx = ((N / 4) * 4) + i;
+      result[idx] = converter(source[idx]);
+    }
+
+    return result;
+  }
+
+  CUTLASS_HOST_DEVICE
+  result_type operator()(source_type const &s) {
+    return convert(s);
+  }
+};
+
+/// Partial specialization for Array<T, N> <= Array<float_e4m3_t, N>
+template <
+  typename T,
+  int N,
+  FloatRoundStyle Round
+>
+struct NumericArrayConverter<T, float_e4m3_t, N, Round> :
+  public PackedNumericArrayConverter<T, float_e4m3_t, N, Round> {};
+
+/// Partial specialization for Array<T, N> <= Array<float_e5m2_t, N>
+template <
+  typename T,
+  int N,
+  FloatRoundStyle Round
+>
+struct NumericArrayConverter<T, float_e5m2_t, N, Round> :
+  public PackedNumericArrayConverter<T, float_e5m2_t, N, Round> {};
+
+/// Partial specialization for Array<float_e4m3_t, N> <= Array<S, N>
+template <
+  typename S,
+  int N,
+  FloatRoundStyle Round
+>
+struct NumericArrayConverter<float_e4m3_t, S, N, Round> :
+  public PackedNumericArrayConverter<float_e4m3_t, S, N, Round> {};
+
+/// Partial specialization for Array<float_e5m2_t, N> <= Array<S, N>
+template <
+  typename S,
+  int N,
+  FloatRoundStyle Round
+>
+struct NumericArrayConverter<float_e5m2_t, S, N, Round> :
+  public PackedNumericArrayConverter<float_e5m2_t, S, N, Round> {};
+
+/// Partial specialization for Array<float_e4m3_t, N> <= Array<float_e5m2_t, N>
+template <
+  int N,
+  FloatRoundStyle Round
+>
+struct NumericArrayConverter<float_e4m3_t, float_e5m2_t, N, Round> :
+  public PackedNumericArrayConverter<float_e4m3_t, float_e5m2_t, N, Round> {};
+
+/// Partial specialization for Array<float_e5m2_t, N> <= Array<float_e4m3_t, N>
+template <
+  int N,
+  FloatRoundStyle Round
+>
+struct NumericArrayConverter<float_e5m2_t, float_e4m3_t, N, Round> :
+  public PackedNumericArrayConverter<float_e5m2_t, float_e4m3_t, N, Round> {};
+
+/// Partial specialization for Array<float_e4m3_t, N> <= Array<float_e4m3_t, N>
+template <
+  int N,
+  FloatRoundStyle Round
+>
+struct NumericArrayConverter<float_e4m3_t, float_e4m3_t, N, Round> :
+  public PackedNumericArrayConverter<float_e4m3_t, float_e4m3_t, N, Round> {};
+
+/// Partial specialization for Array<float_e5m2_t, N> <= Array<float_e5m2_t, N>
+template <
+  int N,
+  FloatRoundStyle Round
+>
+struct NumericArrayConverter<float_e5m2_t, float_e5m2_t, N, Round> :
+  public PackedNumericArrayConverter<float_e5m2_t, float_e5m2_t, N, Round> {};
+
+/////////////////////////////////////////////////////////////////////////////////////////////////
 
 /// Partial specialization for Array<int8_t> <= Array<float>
 /// Conversion is performed with saturation regardless of setting of
 /// the `Round` template parameter.
 template <
   int N,
   FloatRoundStyle Round
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/numeric_types.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/numeric_types.h`

 * *Files 5% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -84,10 +84,11 @@
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
 #include "cutlass/integer_subbyte.h"
 
 #include "cutlass/half.h"
 #include "cutlass/bfloat16.h"
 #include "cutlass/tfloat32.h"
+#include "cutlass/float8.h"
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/pitch_linear_coord.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/pitch_linear_coord.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/platform/platform.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/platform/platform.h`

 * *Files 2% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -571,14 +571,28 @@
 #else
 
 using std::is_trivially_copyable;
 
 #endif
 
 //-----------------------------------------------------------------------------
+// bit_cast <bit>
+//-----------------------------------------------------------------------------
+
+template< class To, class From >
+constexpr To CUTLASS_HOST_DEVICE bit_cast(const From& from ) noexcept;
+
+template <class To, class From>
+constexpr To CUTLASS_HOST_DEVICE bit_cast(const From& src) noexcept
+{
+  static_assert(sizeof(To) == sizeof(From), "sizes must match");
+  return reinterpret_cast<To const &>(src);
+}
+
+//-----------------------------------------------------------------------------
 // Alignment and layout utilities
 //-----------------------------------------------------------------------------
 
 #if defined(__CUDACC_RTC__) || (!defined(_MSC_VER) && (__cplusplus < 201103L)) || (defined(_MSC_VER) && (_MSC_VER < 1500))
 
 /// std::alignment_of
 template <typename value_t>
@@ -861,9 +875,17 @@
   CUTLASS_HOST_DEVICE
   static constexpr uint8_t lowest() noexcept { return 0;}
   CUTLASS_HOST_DEVICE
   static constexpr uint8_t max() noexcept { return 255U;}
   static constexpr bool is_integer = true;
 };
 
+template <>
+struct numeric_limits<float> {
+  CUTLASS_HOST_DEVICE
+  static constexpr float infinity() noexcept { return bit_cast<float, int32_t>(0x7f800000);}
+  static constexpr bool is_integer = false;
+  static constexpr bool has_infinity = true;
+};
+
 }  // namespace platform
 }  // namespace cutlass
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/predicate_vector.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/predicate_vector.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/quaternion.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/quaternion.h`

 * *Files 4% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -31,14 +31,15 @@
 /*! \file
     \brief Defines a densely packed quaternion object intended for storing data in registers and
     executing quaternion operations within a CUDA or host thread.
 */
 #pragma once
 
 #include "cutlass/cutlass.h"
+#include "cutlass/functional.h"
 #include "cutlass/array.h"
 #include "cutlass/real.h"
 #include "cutlass/coord.h"
 #include "cutlass/matrix.h"
 #include "cutlass/fast_math.h"
 #include "cutlass/layout/vector.h"
 
@@ -647,15 +648,18 @@
 
 CUTLASS_HOST_DEVICE
   static Quaternion<T> from_real(double x) {
     return Quaternion<T>(static_cast<T>(x));
   }
 };
 
-//////////////////////////////////////////////////////////////////////////////////////////////////
+
+////////////////////////////////////////////////////////////////////////////////////////////////////
+// Factories
+////////////////////////////////////////////////////////////////////////////////////////////////////
 
 template <>
 CUTLASS_HOST_DEVICE
 cutlass::Quaternion<half_t> from_real<cutlass::Quaternion<half_t> >(double r) {
   return cutlass::Quaternion<half_t>(half_t(r));
 }
 
@@ -669,11 +673,82 @@
 CUTLASS_HOST_DEVICE
 cutlass::Quaternion<double> from_real<cutlass::Quaternion<double> >(double r) {
   return cutlass::Quaternion<double>(r);
 }
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
+
+/////////////////////////////////////////////////////////////////////////////////////////////////
+// functional.h numeric specializations
+/////////////////////////////////////////////////////////////////////////////////////////////////
+
+template <typename T>
+struct multiplies<Quaternion<T>> {
+  CUTLASS_HOST_DEVICE
+  Quaternion<T> operator()(Quaternion<T> lhs, Quaternion<T> const &rhs) const {
+    lhs = lhs * rhs;
+    return lhs;
+  }
+};
+
+/// Squares with optional conversion
+template <typename T, typename Output>
+struct magnitude_squared<Quaternion<T>, Output> {
+  CUTLASS_HOST_DEVICE
+  Output operator()(Quaternion<T> lhs) const {
+    multiplies<Output> mul_op;
+
+    Output y_w = Output(lhs.w());
+    Output y_x = Output(lhs.x());
+    Output y_y = Output(lhs.y());
+    Output y_z = Output(lhs.z());
+
+    return mul_op(y_w, y_w) + mul_op(y_x, y_x) + mul_op(y_y, y_y) + \
+           mul_op(y_z, y_z);
+  }
+};
+
+template <typename T>
+struct multiply_add<Quaternion<T>, Quaternion<T>, Quaternion<T>> {
+  CUTLASS_HOST_DEVICE
+  Quaternion<T> operator()(
+    Quaternion<T> const &a,
+    Quaternion<T> const &b,
+    Quaternion<T> const &c) const {
+
+    T x = c.x();
+    T y = c.y();
+    T z = c.z();
+    T w = c.w();
+
+    x += a.w() * b.x();
+    x += b.w() * a.x();
+    x += a.y() * b.z();
+    x += -a.z() * b.y(),
+
+    y += a.w() * b.y();
+    y += b.w() * a.y();
+    y += a.z() * b.x();
+    y += -a.x() * b.z();
+
+    z += a.w() * b.z();
+    z += b.w() * a.z();
+    z += a.x() * b.y();
+    z += -a.y() * b.x();
+
+    w += a.w() * b.w();
+    w += -a.x() * b.x();
+    w += -a.y() * b.y();
+    w += -a.z() * b.z();
+
+    return cutlass::make_Quaternion(x, y, z, w);
+  }
+};
+
+
+/////////////////////////////////////////////////////////////////////////////////////////////////
+
 } // namespace cutlass
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/real.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/real.h`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/reduction/device/reduce_split_k.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/reduction/device/reduce_split_k.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/reduction/device/tensor_reduce.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/reduction/device/tensor_reduce.h`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/reduction/device/tensor_reduce_affine_contiguous.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/reduction/device/tensor_reduce_affine_contiguous.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/reduction/device/tensor_reduce_affine_strided.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/reduction/device/tensor_reduce_affine_strided.h`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/reduction/kernel/reduce_softmax_final.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/reduction/kernel/reduce_softmax_final.h`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/reduction/kernel/reduce_split_k.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/transform/warp/vector_fragment_iterator.h`

 * *Files 24% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -24,225 +24,260 @@
  * DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR
  * SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER
  * CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,
  * OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
  * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
  *
  **************************************************************************************************/
+
+
 /*! \file
-  \brief Kernel performing a reduction over densely packed tensors in global memory
+    \brief This defines a "fragment" iterator for visiting the fragments of a warp vector
+      that participate in one warp-level mma operation.
+
+      Typically, this is used to access the scale/bias fragement of a warp-level mma operation.
+      The scale/bias vector is then partitioned into smaller fragments that can be fed into 
+      next warp-level mma operation. 
+
+      This iterator is necessary to accomplish warp-level mma fusion where the scale/bias vector is 
+      applied to the multiplicand for the next mma.
+
 */
 
 #pragma once
 
 #include "cutlass/cutlass.h"
-#include "cutlass/tensor_ref.h"
-#include "cutlass/numeric_types.h"
+
 #include "cutlass/array.h"
-#include "cutlass/functional.h"
 #include "cutlass/matrix_shape.h"
+#include "cutlass/layout/matrix.h"
+#include "cutlass/layout/tensor.h"
 #include "cutlass/numeric_conversion.h"
 
-#include "cutlass/layout/matrix.h"
+namespace cutlass {
+namespace transform {
+namespace warp {
 
-/////////////////////////////////////////////////////////////////////////////////////////////////
 
-namespace cutlass {
-namespace reduction {
-namespace kernel {
+////////////////////////////////////////////////////////////////////////////////
+
+template <
+    /// Size of the input fragment tile shape (concept: MatrixShape)
+    typename Shape_,
+    /// Element type
+    typename Element_,
+    /// Layout of operand in memory
+    typename Layout_,
+    /// Shape of one matrix product operation (concept: MatrixShape)
+    typename InstructionShape_,
+    //// Number of elements per access when loading fragment
+    int ElementsPerAccess>
+class VectorFragmentIterator;
 
-/////////////////////////////////////////////////////////////////////////////////////////////////
+
+// Partial specialization for PitchLinear layout tile
 
 template <
-  typename Shape_,              ///< shape of CTA        (concept: MatrixShape)
-  typename OutputOp_ ,          ///< output operator     (concept: epilogue::thread operator)
-  typename ReductionOp_,        ///< reduction operator  (concept: ReductionOperator)
-  int PartitionsPerStage = 4    ///< number of partitions to issue 
->
-class ReduceSplitK {
+    /// Size of the input fragment vector shape (concept: MatrixShape)
+    typename Shape_,
+    /// Element type
+    typename Element_,
+    /// Shape of one matrix product operation (concept: MatrixShape)
+    typename InstructionShape_,
+    //// Number of elements per access when loading fragment
+    int ElementsPerAccess>
+class VectorFragmentIterator<Shape_, Element_,
+                                         cutlass::layout::PitchLinear,
+                                         InstructionShape_, ElementsPerAccess> {
+ public:
+    
+  /// Size of the input threadblock tile shape (concept: MatrixShape)
+  using Shape = Shape_;
+
+  /// Element type
+  using Element = Element_;
+
+  /// Layout of source tile
+  using Layout = cutlass::layout::PitchLinear;
+
+  /// Shape of one matrix product operation (concept: MatrixShape)
+  using InstructionShape = InstructionShape_;
+
+  /// Number of participating threads
+  static int const kThreads = 32;
+
+  static int const kElementsPerAccess = ElementsPerAccess;
+  static int const kRowsPerIteration = 8;
+  static int const kColumnsPerAccess = 8;
+  static int const kElementsPerIteration = kRowsPerIteration * InstructionShape::kK / kThreads;
+  static int const kAccessPerIteration = kElementsPerIteration / kElementsPerAccess;
+  
+  /// Number of iterations
+  using Iterations = MatrixShape<InstructionShape::kM / kRowsPerIteration, Shape::kContiguous / kElementsPerIteration>;
+
 public:
 
-  using Shape = Shape_;
-  using ReductionOp = ReductionOp_;
-  using OutputOp = OutputOp_;
-  static int const kElementsPerAccess = OutputOp::kCount;
-  static int const kPartitionsPerStage = PartitionsPerStage;
-
-  using ElementWorkspace = typename ReductionOp::Element;
-  using ElementAccumulator = typename ReductionOp::ElementAccumulator;
-  using ElementOutput = typename OutputOp::ElementOutput;
-
-  using WorkspaceTensorRef = TensorRef<ElementWorkspace, layout::RowMajor>;
-  using OutputTensorRef = TensorRef<ElementOutput, layout::RowMajor>;
-  using StrideIndex = typename WorkspaceTensorRef::Layout::Stride::Index;
-
-  using FragmentWorkspace = AlignedArray<ElementWorkspace, kElementsPerAccess>;
-  using FragmentAccumulator = Array<ElementAccumulator, kElementsPerAccess>;
-  using FragmentOutput = AlignedArray<ElementOutput, kElementsPerAccess>;
-
-  //
-  // Types
-  //
-
-  /// Params structure
-  struct Params {
-
-    MatrixCoord problem_size;
-    int partitions;
-    size_t partition_stride;
-    WorkspaceTensorRef workspace;
-    OutputTensorRef destination;
-    OutputTensorRef source;
-    typename OutputOp::Params output;
-    typename ReductionOp::Params reduction;
-
-    //
-    // Methods
-    //
-
-    CUTLASS_HOST_DEVICE
-    Params() { }
-
-    CUTLASS_HOST_DEVICE
-    Params(
-      MatrixCoord problem_size_,
-      int partitions_,
-      size_t partition_stride_,
-      WorkspaceTensorRef workspace_,
-      OutputTensorRef destination_,
-      OutputTensorRef source_,
-      typename OutputOp::Params output_ = typename OutputOp::Params(),
-      typename ReductionOp::Params reduction_ = typename ReductionOp::Params()
-    ):
-      problem_size(problem_size_),
-      partitions(partitions_),
-      partition_stride(sizeof(FragmentWorkspace) * partition_stride_ / kElementsPerAccess),
-      workspace(workspace_),
-      destination(destination_),
-      source(source_),
-      output(output_),
-      reduction(reduction_) {
+  //
+  // Derived quantities
+  //
+  // All fragments have kElementsPerAccess scale followed by bias
 
-    }
-  };
+  /// Fragment object holding a thread's part of a tile
+  /// This is the fragment size produced by one iteration of the iterator.
+  using Fragment = Array<Element, kElementsPerIteration * Iterations::kRow>;
 
-  struct SharedStorage { };
+  /// Input threadblock fragment tile
+  using ThreadblockFragment = Array<Element, Shape::kContiguous >;
 
+private:
+
+  /// Internal access type
+  using AccessType = Array<Element, kElementsPerAccess>;
+
+private:
+  //
+  // Data members
+  //
+
+  /// Input threadblock fragment tile
+  AccessType const *iterator_;
+
+  /// Internal index
+  int index_;
 
 public:
+  /// Constructs an iterator
+  CUTLASS_HOST_DEVICE
+  VectorFragmentIterator(ThreadblockFragment const &threadblock_frag)
+      : iterator_(reinterpret_cast<AccessType const *>(&threadblock_frag)),
+        index_(0) {}
 
-  /// Computes the grid size given a chosen threadblock shape
+  /// Add offset
   CUTLASS_HOST_DEVICE
-  static dim3 grid_shape(
-    cutlass::MatrixCoord problem_size) {
+  void add_offset(int index_offset) {
+    index_ += index_offset; 
 
-    return dim3(
-      (problem_size.row() + Shape::kRow - 1) / Shape::kRow,
-      (problem_size.column() + Shape::kColumn - 1) / Shape::kColumn);
+    if(index_ >= Iterations::kColumn)
+        index_ = 0;
   }
 
-  /// Determines the threadblock shape
+  /// Increments
   CUTLASS_HOST_DEVICE
-  static dim3 block_shape() {
-    return dim3(Shape::kColumn / kElementsPerAccess, Shape::kRow);
+  VectorFragmentIterator &operator++() {
+    add_offset(1);
+    return *this;
   }
 
-  /// Perform a reduction
-  CUTLASS_DEVICE
-  void operator()(Params const &params, SharedStorage &storage) {
+  CUTLASS_HOST_DEVICE
+  void set_index(int idx) {
+    index_ = idx;
+  }
 
-    // Determine CTA position
-    MatrixCoord thread_offset(
-      MatrixCoord::Index(int(blockIdx.x) * Shape::kRow + threadIdx.y),
-      MatrixCoord::Index(int(blockIdx.y) * Shape::kColumn + threadIdx.x * kElementsPerAccess)
-    );
+  /// Loads a fragment from the referenced part of the accumulator tile
+  CUTLASS_HOST_DEVICE
+  void load(Fragment &frag) const {
 
-    // One guard conditional
-    if (!(thread_offset.row() < params.problem_size.row() && 
-          thread_offset.column() < params.problem_size.column())) {
+    AccessType *frag_ptr = reinterpret_cast<AccessType *>(&frag);
 
-      return;
+    CUTLASS_PRAGMA_UNROLL
+    for (int r = 0; r < Iterations::kRow; r++) {
+        CUTLASS_PRAGMA_UNROLL
+        for (int i = 0; i < kAccessPerIteration; i++) {
+    
+          frag_ptr[i * Iterations::kRow + r].clear();
+          frag_ptr[i * Iterations::kRow + r] = iterator_[index_ * kAccessPerIteration + i];
+        }
     }
+  }
 
+};
 
-    ReductionOp reduction_op(params.reduction);
-
-    FragmentAccumulator accumulator;
+// Partial specialization for Row-Major layout tile
 
-    accumulator.clear();  
+template <
+    /// Size of the input fragment tile shape (concept: MatrixShape)
+    typename Shape_,
+    /// Element type
+    typename Element_,
+    /// Shape of one matrix product operation (concept: MatrixShape)
+    typename InstructionShape_,
+    //// Number of elements per access when loading fragment
+    int ElementsPerAccess>
+class VectorFragmentIterator<Shape_, Element_,
+                                         cutlass::layout::RowMajor,
+                                         InstructionShape_, ElementsPerAccess> {
+ public:
     
-    //
-    // Load the first slice
-    //
-
-    char const *workspace_ptr = 
-      reinterpret_cast<char const *>(
-        params.workspace.data() + params.workspace.offset(thread_offset));
+  /// Size of the input threadblock tile shape (concept: MatrixShape)
+  using Shape = Shape_;
 
-    FragmentWorkspace workspace_frag[kPartitionsPerStage];
-    
-    //
-    // Construct the output operator
-    //
-    
-    OutputOp output_op(params.output);
+  /// Element type
+  using Element = Element_;
 
-    //
-    // Load and accumulate with a simple batched loading sequence.
-    //
-
-    CUTLASS_PRAGMA_NO_UNROLL
-    for (int k = 0; k < params.partitions; k += kPartitionsPerStage) {
-
-      CUTLASS_PRAGMA_UNROLL
-      for (int i = 0; i < kPartitionsPerStage; ++i) {
-        if (k + i < params.partitions) {
-          workspace_frag[i] = *reinterpret_cast<FragmentWorkspace const *>(workspace_ptr);
-          workspace_ptr += params.partition_stride;
-        }
-      }   
+  /// Layout of source tile
+  using Layout = cutlass::layout::RowMajor;
 
-      CUTLASS_PRAGMA_UNROLL
-      for (int i = 0; i < kPartitionsPerStage; ++i) {
-        if (k + i < params.partitions) {
-          accumulator = reduction_op(accumulator, workspace_frag[i]);
-        }
-      }
-    }
+  /// Shape of one matrix product operation (concept: MatrixShape)
+  using InstructionShape = InstructionShape_;
 
-    //
-    // Conditionally load the source
-    //
+  /// Underlying iterator
+  using Base = VectorFragmentIterator<
+    layout::PitchLinearShape<Shape::kColumn, Shape::kRow>, Element,
+    layout::PitchLinear, InstructionShape, ElementsPerAccess>;
 
-    FragmentOutput source_frag;
 
-    source_frag.clear();
+ public:
 
-    FragmentOutput const *source_ptr = reinterpret_cast<FragmentOutput const *>(
-      params.source.data() + params.source.offset(thread_offset));
+  //
+  // Derived quantities
+  //
+  /// Fragment object holding a thread's part of a tile
+  /// This is the fragment size produced by one iteration of the iterator.
+  using Fragment = typename Base::Fragment;
+
+  /// Input threadblock fragment tile
+  using ThreadblockFragment = typename Base::ThreadblockFragment;
+
+ private:
+  /// Underlying iterator
+  Base iterator_;
 
-    if (output_op.is_source_needed()) {
-      reinterpret_cast<FragmentOutput &>(source_frag) = *source_ptr;
-    }
-    
-    //
-    // Compute the output
-    //
+public:
+  /// Constructs an iterator
+  CUTLASS_HOST_DEVICE
+  VectorFragmentIterator(ThreadblockFragment const &threadblock_frag)
+      : iterator_(threadblock_frag) {}
 
-    typename OutputOp::FragmentOutput output_frag = output_op(accumulator, source_frag);
+  /// Add offset
+  CUTLASS_HOST_DEVICE
+  void add_offset(int index_offset) {
+    iterator_.add_offset(index_offset);
+  }
 
-    //
-    // Store
-    //
+  /// Increments
+  CUTLASS_HOST_DEVICE
+  VectorFragmentIterator &operator++() {
+    add_offset(1);
+    return *this;
+  }
 
-    FragmentOutput *dest_ptr = reinterpret_cast<FragmentOutput *>(
-      params.destination.data() + params.destination.offset(thread_offset));
+  CUTLASS_HOST_DEVICE
+  void set_index(int idx) {
+    iterator_.set_index(idx);
+  }
 
-    *dest_ptr = reinterpret_cast<FragmentOutput const &>(output_frag);
+  /// Loads a fragment from the referenced part of the accumulator tile
+  CUTLASS_HOST_DEVICE
+  void load(Fragment &frag) const {
+    iterator_.load(frag);
   }
+
 };
 
-/////////////////////////////////////////////////////////////////////////////////////////////////
 
-} // namespace kernel
-} // namespace reduction
+////////////////////////////////////////////////////////////////////////////////
+
+} // namespace warp
+} // namespace conv
 } // namespace cutlass
+
+////////////////////////////////////////////////////////////////////////////////
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/reduction/kernel/tensor_reduce_affine_contiguous.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/reduction/kernel/tensor_reduce_affine_contiguous.h`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/reduction/kernel/tensor_reduce_affine_strided.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/reduction/kernel/tensor_reduce_affine_strided.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/reduction/thread/reduce.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/reduction/thread/reduce.h`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/reduction/thread/reduction_operators.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/reduction/thread/reduction_operators.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/reduction/threadblock_swizzle.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/reduction/threadblock_swizzle.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/relatively_equal.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/relatively_equal.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/semaphore.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/semaphore.h`

 * *Files 4% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -32,15 +32,14 @@
     \brief Implementation of a CTA-wide semaphore for inter-CTA synchronization.
 */
 
 #pragma once
 
 #include "cutlass/cutlass.h"
 
-#include "cutlass/aligned_buffer.h"
 #include "cutlass/array.h"
 
 #include "cutlass/numeric_types.h"
 #include "cutlass/matrix_shape.h"
 
 #include "cutlass/gemm/gemm.h"
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/subbyte_reference.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/subbyte_reference.h`

 * *Files 10% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -290,16 +290,26 @@
   explicit operator double() const {
     return double(get());
   }
 };
 
 template <
   typename Element_,              /// CUTLASS numeric element type.
-  typename Storage_ = uint8_t     /// Underlying storage type. Must be able to hold an integer 
+  typename Storage_ =             /// Underlying storage type. Must be able to hold an integer
                                   ///   number of objects of type Element.
+
+#if defined(__CUDA_ARCH__)        /// Default size depends on width of atomicCas() overloads.
+  #if (__CUDA_ARCH__ >= 700)      ///
+  uint16_t
+  #else
+  uint32_t
+  #endif
+#else
+  uint8_t
+#endif
 >
 class SubbyteReference {
 public:
 
   using Element = Element_;
   using Storage = Storage_;
   using StoragePointer = Storage *;
@@ -383,22 +393,49 @@
     return reinterpret_cast<Element const &>(item);
   }
 
   /// Stores an element to memory
   CUTLASS_HOST_DEVICE
   SubbyteReference & set(Element const &x) {
 
-    Storage item = (reinterpret_cast<Storage const &>(x) & kMask);
+    Storage item        = (reinterpret_cast<Storage const &>(x) & kMask);
+    Storage kUpdateMask = Storage(~(kMask << (offset_ * cutlass::sizeof_bits<Element>::value)));
+    Storage new_bits    = Storage(item << (offset_ * cutlass::sizeof_bits<Element>::value));
+
+#if defined(__CUDA_ARCH__)
+
+    //
+    // Homebrew read-modify-write
+    //
+    Storage original;
+    Storage updated;
+
+    do {
+
+      original = (*ptr_);
 
-    Storage kUpdateMask = Storage(~(kMask << (offset_ * sizeof_bits<Element>::value)));
-    *ptr_ = Storage((*ptr_ & kUpdateMask) | Storage(item << (offset_ * sizeof_bits<Element>::value)));
+      updated  = Storage((original & kUpdateMask) | new_bits);
+
+      original = atomicCAS(ptr_, original, updated);
+
+    } while (updated != original);
+
+#else
+
+    Storage original = (*ptr_);
+    Storage updated  = Storage((original & kUpdateMask) | new_bits);
+    *ptr_ = updated;
+
+#endif
 
     return *this;
   }
 
+  ////
+
   /// Unpacks an element from memory
   CUTLASS_HOST_DEVICE
   operator Element() const {
     return get();
   }
 
   /// Stores an element to memory
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/tensor_coord.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/tensor_coord.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/tensor_ref.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/tensor_ref.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/tensor_ref_planar_complex.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/tensor_ref_planar_complex.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/tensor_view.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/tensor_view.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/tensor_view_planar_complex.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/tensor_view_planar_complex.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/tfloat32.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/tfloat32.h`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -30,15 +30,17 @@
  **************************************************************************************************/
 /*!
     \file
     \brief Defines a proxy class for storing Tensor Float 32 data type.
 */
 #pragma once
 
-#if !defined(__CUDACC_RTC__)
+#if defined(__CUDACC_RTC__)
+#include "cutlass/floating_point_nvrtc.h"
+#else
 #include <cmath>
 #include <limits>
 #include <cstdint>
 #endif
 
 #include "cutlass/cutlass.h"
 
@@ -83,30 +85,31 @@
     }
     #endif
 
     return tfloat32_t::bitcast(x);
   }
 
   /// Default constructor
-  CUTLASS_HOST_DEVICE
-  tfloat32_t() : storage(0) { }
+  tfloat32_t() = default;
 
   /// Floating-point conversion - round toward nearest even
   CUTLASS_HOST_DEVICE
-  explicit tfloat32_t(float x): storage(round_half_ulp_truncate(x).storage) { }
+//  explicit tfloat32_t(float x): storage(round_half_ulp_truncate(x).storage) { }
+  tfloat32_t(float x): storage(round_half_ulp_truncate(x).storage) { }
 
   /// Floating-point conversion - round toward nearest even
   CUTLASS_HOST_DEVICE
-  explicit tfloat32_t(double x): tfloat32_t(float(x)) {
-
+//  explicit tfloat32_t(double x): tfloat32_t(float(x)) {
+  tfloat32_t(double x): tfloat32_t(float(x)) {
   }
 
   /// Integer conversion - round toward zero
   CUTLASS_HOST_DEVICE
-  explicit tfloat32_t(int x) {
+//  explicit tfloat32_t(int x) {
+  tfloat32_t(int x) {
     float flt = static_cast<float>(x);
     #if defined(__CUDA_ARCH__)
     storage = reinterpret_cast<uint32_t const &>(flt);
     #else
     std::memcpy(&storage, &flt, sizeof(storage));
     #endif
   }
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/thread/matrix.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/thread/matrix.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/trace.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/trace.h`

 * *Files 2% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/transform/pitch_linear_thread_map.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/transform/pitch_linear_thread_map.h`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -84,43 +84,39 @@
   using ThreadAccessShape = layout::PitchLinearShape<kElementsPerAccess, 1>;
 
   /// Internal implementation details
   struct Detail {
 
     static_assert(!(Shape::kContiguous % kElementsPerAccess), "");
 
-    static_assert(!((Shape::kContiguous * Shape::kStrided) % (kThreads * kElementsPerAccess)), 
-      "Shape must be divisible thread count.");
-
     /// Shape of the tile in units of vectors
     using ShapeVec = layout::PitchLinearShape<
       Shape::kContiguous / kElementsPerAccess,
       Shape::kStrided
     >;
 
-    static_assert(
-      (Threads < ShapeVec::kContiguous && !(ShapeVec::kContiguous % kThreads)) ||
-      (!(kThreads % ShapeVec::kContiguous) && !(ShapeVec::kStrided % (kThreads / ShapeVec::kContiguous))),
-      "Shape must be divisible by number of iterations of each thread."
-    );
+    static_assert((Threads < ShapeVec::kContiguous && !(ShapeVec::kContiguous % kThreads)) ||
+                      (!(kThreads % ShapeVec::kContiguous)),
+                  "Shape must be divisible by number of iterations of each thread.");
   };
 
   /// Number of iterations by each thread
   using Iterations = typename platform::conditional<
       Threads >= Detail::ShapeVec::kContiguous,
       layout::PitchLinearShape<
           1,
           // Redo the comparison here to work around divide by zero compiler
           // error.  The compiler evaluates both path of platform::conditional.
           (Threads >= Detail::ShapeVec::kContiguous
-               ? Detail::ShapeVec::kStrided /
+               ? (Detail::ShapeVec::kStrided + (kThreads / Detail::ShapeVec::kContiguous - 1)) /
                      (kThreads / Detail::ShapeVec::kContiguous)
                : 0)>,
       layout::PitchLinearShape<Detail::ShapeVec::kContiguous / kThreads,
                                Detail::ShapeVec::kStrided>>::type;
+  
 
   /// Interval between accesses along each dimension of the tensor's logical coordinate space
   /// (in units of Elements)
   using Delta = typename platform::conditional<
     Threads >= Detail::ShapeVec::kContiguous,
     layout::PitchLinearShape<
       1,
@@ -128,14 +124,21 @@
     >,
     layout::PitchLinearShape<
       kThreads * kElementsPerAccess,
       1
     >
   >::type;
 
+  /// Shape of the tile in units of vectors
+  using StorageShape = typename platform::conditional<
+      Threads >= Detail::ShapeVec::kContiguous,
+      layout::PitchLinearShape<Shape::kContiguous,
+                               Iterations::kStrided*(kThreads / Detail::ShapeVec::kContiguous)>,
+      layout::PitchLinearShape<Shape::kContiguous, Shape::kStrided>>::type;
+
   /// Maps thread ID to a coordinate offset within the tensor's logical coordinate space
   /// (in units of Elements)
   CUTLASS_HOST_DEVICE
   static TensorCoord initial_offset(int thread_id) {
     return TensorCoord(
       (thread_id % Detail::ShapeVec::kContiguous) * kElementsPerAccess, 
       thread_id / Detail::ShapeVec::kContiguous);
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/transform/thread/transpose.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/transform/thread/transpose.h`

 * *Files 2% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/transform/thread/unary_op.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/transform/thread/unary_op.h`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/transform/threadblock/ell_iterator.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/transform/threadblock/ell_iterator.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/transform/threadblock/ell_predicated_tile_access_iterator.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/transform/threadblock/ell_predicated_tile_access_iterator.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/transform/threadblock/ell_predicated_tile_iterator.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/transform/threadblock/ell_predicated_tile_iterator.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/transform/threadblock/predicated_scale_bias_vector_access_iterator.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/transform/threadblock/predicated_scale_bias_vector_access_iterator.h`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/transform/threadblock/predicated_scale_bias_vector_iterator.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/transform/threadblock/predicated_scale_bias_vector_iterator.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/transform/threadblock/predicated_tile_access_iterator.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/transform/threadblock/predicated_tile_access_iterator.h`

 * *Files 2% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -217,15 +217,18 @@
     thread_offset_ = threadblock_offset + ThreadMap::initial_offset(thread_id);
 
     compute_predicates_(residue_extent, false);
 
     set_iteration_index(0);
   }
 
-    /// Constructs a TileIterator from its precomputed state, threadblock offset,
+  /// Default constructor
+  PredicatedTileAccessIteratorPredicates() = default;
+
+  /// Constructs a TileIterator from its precomputed state, threadblock offset,
   /// and thread ID
   CUTLASS_HOST_DEVICE
   PredicatedTileAccessIteratorPredicates(
       /// Extent of tensor
       TensorCoord extent)
       : extent_(extent) {
 	}
@@ -356,17 +359,16 @@
   using Mask = typename UnderlyingPredicates::Mask;
 
   /// Uses a non-template class
   struct Params : PredicatedTileAccessIteratorParams {
     
     using Base = PredicatedTileAccessIteratorParams;
 
-    // Default ctor
-    CUTLASS_HOST_DEVICE
-    Params() { }
+    /// Default constructor
+    Params() = default;
 
     /// Construct the Params object given a pitch-linear tensor's layout
     CUTLASS_HOST_DEVICE
     Params(Layout const &layout) : 
       Base(layout.stride(0),
             MakePredicatedTileAccessIteratorDesc<Shape, Element, Layout, kAdvanceRank, ThreadMap>()()
         ) { }
@@ -384,15 +386,15 @@
   //
   // Data members
   //
 
   UnderlyingPredicates the_predicates;
 
   /// Parameters object with precomputed internal state
-  Params const &params_;
+  Params params_;
 
   /// Internal pointer to first access of tile
   BytePointer pointer_;
 
   /// Used for out-of-order visitation
   bool is_residue_tile_;
 
@@ -415,15 +417,18 @@
       TensorCoord extent,
       /// optionally, simplify predicate calculation during 'steady state' phase
       bool is_steady_state = false) {
 	  the_predicates.compute_predicates_(extent, is_steady_state);
   }
 
  public:
-          
+
+  /// Default constructor
+  PredicatedTileAccessIterator() = default;
+
   /// Constructs a TileIterator from its precomputed state, threadblock offset,
   /// and thread ID
   CUTLASS_HOST_DEVICE
   PredicatedTileAccessIterator(
       /// Precomputed parameters object
       Params const &params,
       /// Pointer to start of tensor
@@ -695,17 +700,16 @@
     friend PredicatedTileAccessIterator;
 
     /// Parameters object
     typename UnderlyingIterator::Params params_;
 
    public:
 
-    /// Default ctor
-    CUTLASS_HOST_DEVICE
-    Params() { }
+    /// Default constructor
+    Params() = default;
 
     /// Construct the Params object given a pitch-linear tensor's layout
     CUTLASS_HOST_DEVICE
     Params(Layout const &layout)
         : params_(layout::PitchLinear(layout.stride(0))){};
 
     /// Construct the Params object given a pitch-linear tensor's layout
@@ -719,14 +723,18 @@
   // Data members
   //
 
   /// Underlying pitch-linear tile iterator
   UnderlyingIterator iterator_;
 
  public:
+
+  /// Default constructor
+  PredicatedTileAccessIterator() = default;
+
   /// Constructs a TileIterator from its precomputed state, threadblock offset,
   /// and thread ID
   CUTLASS_HOST_DEVICE
   PredicatedTileAccessIterator(
       ///< Precomputed parameters object
       Params const &params,
       ///< Pointer to start of tensor
@@ -879,17 +887,16 @@
     friend PredicatedTileAccessIterator;
 
     /// Parameters object
     typename UnderlyingIterator::Params params_;
 
    public:
 
-    /// Default ctor
-    CUTLASS_HOST_DEVICE
-    Params() { }
+    /// Default constructor
+    Params() = default;
 
     /// Construct the Params object given a pitch-linear tensor's layout
     CUTLASS_HOST_DEVICE
     Params(Layout const &layout)
         : params_(layout::PitchLinear(layout.stride(0))){};
 
     /// Construct the Params object given a pitch-linear tensor's layout
@@ -903,14 +910,18 @@
   // Data members
   //
 
   /// Underlying pitch-linear tile iterator
   UnderlyingIterator iterator_;
 
  public:
+
+  /// Default constructor
+  PredicatedTileAccessIterator() = default;
+
   /// Constructs a TileIterator from its precomputed state, threadblock offset,
   /// and thread ID
   CUTLASS_HOST_DEVICE
   PredicatedTileAccessIterator(
       ///< Precomputed parameters object
       Params const &params,
       ///< Pointer to start of tensor
@@ -1118,15 +1129,15 @@
   using BytePointer = char *;
 
   //
   // Data members
   //
 
   /// Parameters object with precomputed internal state
-  Params const &params_;
+  Params params_;
 
   /// Internal pointer to first access of tile
   BytePointer pointer_;
 
   UnderlyingPredicates the_predicates;
 
   /// Used for out-of-order visitation
@@ -1140,14 +1151,18 @@
       TensorCoord extent,
       /// optionally, simplify predicate calculation during 'steady state' phase
       bool is_steady_state = false) {
           the_predicates.compute_predicates_(extent, is_steady_state);
   }
 
  public:
+
+  /// Default constructor
+  PredicatedTileAccessIterator() = default;
+
   /// Constructs a TileIterator from its precomputed state, threadblock offset,
   /// and thread ID
   CUTLASS_HOST_DEVICE
   PredicatedTileAccessIterator(
       ///< Precomputed parameters object
       Params const &params,
       ///< Pointer to start of tensor
@@ -1367,17 +1382,16 @@
     friend PredicatedTileAccessIterator;
 
     /// Parameters object
     typename UnderlyingIterator::Params params_;
 
    public:
 
-    /// Default ctor
-    CUTLASS_HOST_DEVICE
-    Params() { }
+    /// Default constructor
+    Params() = default;
 
     /// Construct the Params object given an AffineRankN<2> tensor's layout
     CUTLASS_HOST_DEVICE
     Params(Layout const &layout)
         : params_(layout::AffineRankN<2>(layout.stride(0), layout.stride(1))){};
   };
 
@@ -1386,14 +1400,18 @@
   // Data members
   //
 
   /// Underlying AffineRankN<2> tile iterator
   UnderlyingIterator iterator_;
 
  public:
+
+  /// Default constructor
+  PredicatedTileAccessIterator() = default;
+
   /// Constructs a TileIterator from its precomputed state, threadblock offset,
   /// and thread ID
   CUTLASS_HOST_DEVICE
   PredicatedTileAccessIterator(
       ///< Precomputed parameters object
       Params const &params,
       ///< Pointer to start of tensor
@@ -1546,17 +1564,16 @@
     friend PredicatedTileAccessIterator;
 
     /// Parameters object
     typename UnderlyingIterator::Params params_;
 
    public:
 
-    /// Default ctor
-    CUTLASS_HOST_DEVICE
-    Params() { }
+    /// Default constructor
+    Params() = default;
 
     /// Construct the Params object given an AffineRankN<2> tensor's layout
     CUTLASS_HOST_DEVICE
     Params(Layout const &layout)
         : params_(layout::AffineRankN<2>(layout.stride(1), layout.stride(0))){};
   };
 
@@ -1565,14 +1582,18 @@
   // Data members
   //
 
   /// Underlying AffineRankN<2> tile iterator
   UnderlyingIterator iterator_;
 
  public:
+
+  /// Default constructor
+  PredicatedTileAccessIterator() = default;
+
   /// Constructs a TileIterator from its precomputed state, threadblock offset,
   /// and thread ID
   CUTLASS_HOST_DEVICE
   PredicatedTileAccessIterator(
       ///< Precomputed parameters object
       Params const &params,
       ///< Pointer to start of tensor
@@ -1729,16 +1750,17 @@
    private:
     friend PredicatedTileAccessIterator;
 
     /// Parameters object
     typename UnderlyingIterator::Params params_;
 
    public:
-    CUTLASS_HOST_DEVICE
-    Params() {}
+
+    /// Default constructor
+    Params() = default;
 
     /// Construct the Params object given a pitch-linear tensor's layout
     CUTLASS_HOST_DEVICE
     Params(Layout const &layout)
         : params_(layout::PitchLinear(layout.stride(0))) {}
 
     CUTLASS_HOST_DEVICE
@@ -1751,14 +1773,18 @@
   // Data members
   //
 
   /// Underlying pitch-linear tile iterator
   UnderlyingIterator iterator_;
 
  public:
+
+  /// Default constructor
+  PredicatedTileAccessIterator() = default;
+
   /// Constructs a TileIterator from its precomputed state, threadblock offset,
   /// and thread ID
   CUTLASS_HOST_DEVICE
   PredicatedTileAccessIterator(
       /// Precomputed parameters object
       Params const &params,
       /// Pointer to start of tensor
@@ -1915,16 +1941,17 @@
    private:
     friend PredicatedTileAccessIterator;
 
     /// Parameters object
     typename UnderlyingIterator::Params params_;
 
    public:
-    CUTLASS_HOST_DEVICE
-    Params() {}
+
+    /// Default constructor
+    Params() = default;
 
     /// Construct the Params object given a pitch-linear tensor's layout
     CUTLASS_HOST_DEVICE
     Params(Layout const &layout)
         : params_(layout::PitchLinear(layout.stride(0))) {}
 
     CUTLASS_HOST_DEVICE
@@ -1937,14 +1964,18 @@
   // Data members
   //
 
   /// Underlying pitch-linear tile iterator
   UnderlyingIterator iterator_;
 
  public:
+
+  /// Default constructor
+  PredicatedTileAccessIterator() = default;
+
   /// Constructs a TileIterator from its precomputed state, threadblock offset,
   /// and thread ID
   CUTLASS_HOST_DEVICE
   PredicatedTileAccessIterator(
       /// Precomputed parameters object
       Params const &params,
       /// Pointer to start of tensor
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/transform/threadblock/predicated_tile_access_iterator_2dthreadtile.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/transform/threadblock/predicated_tile_access_iterator_2dthreadtile.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/transform/threadblock/predicated_tile_access_iterator_params.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/transform/threadblock/predicated_tile_access_iterator_params.h`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/transform/threadblock/predicated_tile_access_iterator_triangular_matrix.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/transform/threadblock/predicated_tile_access_iterator_triangular_matrix.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/transform/threadblock/predicated_tile_iterator.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/transform/threadblock/predicated_tile_iterator.h`

 * *Files 2% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -203,21 +203,21 @@
    private:
     /// Parameters object
     typename TileAccessIterator::Params params_;
 
    public:
     /// Construct the Params object given a pitch-linear tensor's layout
     CUTLASS_HOST_DEVICE
-    Params(Layout const &layout) : params_(layout) { }
-    
-    CUTLASS_HOST_DEVICE
-    Params() { }
+    Params(Layout const &layout) : params_(layout) {}
+
+    /// Default constructor
+    Params() = default;
 
     CUTLASS_HOST_DEVICE
-    Params(Base const &base) 
+    Params(Base const &base)
         : params_(base) {}
   };
 
  private:
   /// Internal pointer type permits fast address arithmetic
   using BytePointer = char *;
 
@@ -226,14 +226,18 @@
   // Data members
   //
 
   /// Data member to the tile access iterator
   TileAccessIterator address_iterator_;
 
  public:
+
+  /// Default constructor
+  PredicatedTileIterator() = default;
+
   /// Constructs a TileIterator from its precomputed state, threadblock offset,
   /// and thread ID
   CUTLASS_HOST_DEVICE
   PredicatedTileIterator(
       /// Precomputed parameters object
       Params const &params,
       /// Pointer to start of tensor
@@ -453,26 +457,25 @@
 
     friend PredicatedTileIterator;
 
     /// Parameters object
     typename UnderlyingIterator::Params params_;
 
   public:
-    
-    CUTLASS_HOST_DEVICE
-    Params() { }
+
+    /// Default constructor
+    Params() = default;
 
     /// Construct the Params object given a pitch-linear tensor's layout
     CUTLASS_HOST_DEVICE
-    Params(Layout const &layout): params_(layout::PitchLinear(layout.stride(0))) {
-
-    }
+    Params(Layout const &layout): params_(layout::PitchLinear(layout.stride(0)))
+    {}
 
     CUTLASS_HOST_DEVICE
-    Params(typename UnderlyingIterator::Params::Base const &base) 
+    Params(typename UnderlyingIterator::Params::Base const &base)
         : params_(base) {}
   };
 
 
 private:
 
   //
@@ -480,14 +483,17 @@
   //
 
   /// Underlying pitch-linear tile iterator
   UnderlyingIterator iterator_;
 
 public:
 
+  /// Default constructor
+  PredicatedTileIterator() = default;
+
   /// Constructs a TileIterator from its precomputed state, threadblock offset, and thread ID
   CUTLASS_HOST_DEVICE
   PredicatedTileIterator(
     Params const &params,                         ///< Precomputed parameters object 
     Pointer pointer,                              ///< Pointer to start of tensor
     TensorCoord extent,                           ///< Extent of tensor
     int thread_id,                                ///< ID of each participating thread
@@ -666,24 +672,24 @@
 
     friend PredicatedTileIterator;
 
     /// Parameters object
     typename UnderlyingIterator::Params params_;
 
   public:
-    
-    CUTLASS_HOST_DEVICE
-    Params() { } 
+
+    /// Default constructor
+    Params() = default;
 
     /// Construct the Params object given a pitch-linear tensor's layout
     CUTLASS_HOST_DEVICE
     Params(Layout const &layout): params_(layout::PitchLinear(layout.stride(0))) {}
 
     CUTLASS_HOST_DEVICE
-    Params(typename UnderlyingIterator::Params::Base const &base) 
+    Params(typename UnderlyingIterator::Params::Base const &base)
         : params_(base) {}
 
   };
 
 private:
 
   //
@@ -691,14 +697,17 @@
   //
 
   /// Underlying pitch-linear tile iterator
   UnderlyingIterator iterator_;
 
 public:
 
+  /// Default constructor
+  PredicatedTileIterator() = default;
+
   /// Constructs a TileIterator from its precomputed state, threadblock offset, and thread ID
   CUTLASS_HOST_DEVICE
   PredicatedTileIterator(
     Params const &params,                         ///< Precomputed parameters object 
     Pointer pointer,                              ///< Pointer to start of tensor
     TensorCoord extent,                           ///< Extent of tensor
     int thread_id,                                ///< ID of each participating thread
@@ -874,18 +883,18 @@
    private:
     /// Parameters object
     typename TileAccessIterator::Params params_;
 
    public:
     /// Construct the Params object given a pitch-linear tensor's layout
     CUTLASS_HOST_DEVICE
-    Params(Layout const &layout) : params_(layout) { }
-    
-    CUTLASS_HOST_DEVICE
-    Params() { }
+    Params(Layout const &layout) : params_(layout) {}
+
+    /// Default constructor
+    Params() = default;
   };
 
  private:
   /// Internal pointer type permits fast address arithmetic
   using BytePointer = char *;
 
  private:
@@ -893,14 +902,18 @@
   // Data members
   //
 
   /// Data member to the tile access iterator
   TileAccessIterator address_iterator_;
 
  public:
+
+  /// Default constructor
+  PredicatedTileIterator() = default;
+
   /// Constructs a TileIterator from its precomputed state, threadblock offset,
   /// and thread ID
   CUTLASS_HOST_DEVICE
   PredicatedTileIterator(
       /// Precomputed parameters object
       Params const &params,
       /// Pointer to start of tensor
@@ -1119,36 +1132,38 @@
 
     friend PredicatedTileIterator;
 
     /// Parameters object
     typename UnderlyingIterator::Params params_;
 
   public:
-    
-    CUTLASS_HOST_DEVICE
-    Params() { }
+
+    /// Default constructor
+    Params() = default;
 
     /// Construct the Params object given an AffineRankN<2> tensor's layout
     CUTLASS_HOST_DEVICE
-    Params(Layout const &layout): params_(layout::AffineRankN<2>(layout.stride(0), layout.stride(1))) {
-
-    }
+    Params(Layout const &layout): params_(layout::AffineRankN<2>(layout.stride(0), layout.stride(1)))
+    {}
   };
 
 private:
 
   //
   // Data members
   //
 
   /// Underlying AffineRankN<2> tile iterator
   UnderlyingIterator iterator_;
 
 public:
 
+  /// Default constructor
+  PredicatedTileIterator() = default;
+
   /// Constructs a TileIterator from its precomputed state, threadblock offset, and thread ID
   CUTLASS_HOST_DEVICE
   PredicatedTileIterator(
     Params const &params,                         ///< Precomputed parameters object 
     Pointer pointer,                              ///< Pointer to start of tensor
     TensorCoord extent,                           ///< Extent of tensor
     int thread_id,                                ///< ID of each participating thread
@@ -1325,17 +1340,17 @@
 
     friend PredicatedTileIterator;
 
     /// Parameters object
     typename UnderlyingIterator::Params params_;
 
   public:
-    
-    CUTLASS_HOST_DEVICE
-    Params() { } 
+
+    /// Default constructor
+    Params() = default;
 
     /// Construct the Params object given an AffineRankN<2> tensor's layout
     CUTLASS_HOST_DEVICE
     Params(Layout const &layout): params_(layout::AffineRankN<2>(layout.stride(1), layout.stride(0))) {}
   };
 
 
@@ -1346,14 +1361,17 @@
   //
 
   /// Underlying AffineRankN<2> tile iterator
   UnderlyingIterator iterator_;
 
 public:
 
+  /// Default constructor
+  PredicatedTileIterator() = default;
+
   /// Constructs a TileIterator from its precomputed state, threadblock offset, and thread ID
   CUTLASS_HOST_DEVICE
   PredicatedTileIterator(
     Params const &params,                         ///< Precomputed parameters object 
     Pointer pointer,                              ///< Pointer to start of tensor
     TensorCoord extent,                           ///< Extent of tensor
     int thread_id,                                ///< ID of each participating thread
@@ -1526,37 +1544,42 @@
    private:
     friend PredicatedTileIterator;
 
     /// Parameters object
     typename UnderlyingIterator::Params params_;
 
    public:
-    CUTLASS_HOST_DEVICE
-    Params() {}
+
+    /// Default constructor
+    Params() = default;
 
     /// Construct the Params object given a pitch-linear tensor's layout
     CUTLASS_HOST_DEVICE
     Params(Layout const &layout)
         : params_(layout::PitchLinear(layout.stride(0))) {}
 
     CUTLASS_HOST_DEVICE
-    Params(typename UnderlyingIterator::Params::Base const &base) 
+    Params(typename UnderlyingIterator::Params::Base const &base)
         : params_(base) {}
 
   };
 
  private:
   //
   // Data members
   //
 
   /// Underlying pitch-linear tile iterator
   UnderlyingIterator iterator_;
 
  public:
+
+  /// Default constructor
+  PredicatedTileIterator() = default;
+
   /// Constructs a TileIterator from its precomputed state, threadblock offset,
   /// and thread ID
   CUTLASS_HOST_DEVICE
   PredicatedTileIterator(
       /// Precomputed parameters object
       Params const &params,
       /// Pointer to start of tensor
@@ -1697,15 +1720,15 @@
   using UnderlyingIterator = PredicatedTileIterator<
       layout::PitchLinearShape<Shape::kColumn * kInterleavedK,
                                Shape::kRow / kInterleavedK>,
       Element, layout::PitchLinear, (kAdvanceRank == 0 ? 1 : 0), ThreadMap, AccessSize>;
 
 
   using AccessType = typename UnderlyingIterator::AccessType;
-  
+
   /// Fragment object to be loaded or stored
   using Fragment = cutlass::Array<Element, ThreadMap::Iterations::kCount *
                                                ThreadMap::kElementsPerAccess>;
 
   /// Predicate vector stores mask to guard accesses
   using Mask = typename UnderlyingIterator::Mask;
 
@@ -1714,36 +1737,41 @@
    private:
     friend PredicatedTileIterator;
 
     /// Parameters object
     typename UnderlyingIterator::Params params_;
 
    public:
-    CUTLASS_HOST_DEVICE
-    Params() {}
+
+    /// Default constructor
+    Params() = default;
 
     /// Construct the Params object given a pitch-linear tensor's layout
     CUTLASS_HOST_DEVICE
     Params(Layout const &layout)
         : params_(layout::PitchLinear(layout.stride(0))) {}
 
     CUTLASS_HOST_DEVICE
-    Params(typename UnderlyingIterator::Params::Base const &base) 
+    Params(typename UnderlyingIterator::Params::Base const &base)
         : params_(base) {}
   };
 
  private:
   //
   // Data members
   //
 
   /// Underlying pitch-linear tile iterator
   UnderlyingIterator iterator_;
 
  public:
+
+  /// Default constructor
+  PredicatedTileIterator() = default;
+
   /// Constructs a TileIterator from its precomputed state, threadblock offset,
   /// and thread ID
   CUTLASS_HOST_DEVICE
   PredicatedTileIterator(
       /// Precomputed parameters object
       Params const &params,
       /// Pointer to start of tensor
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/transform/threadblock/predicated_tile_iterator_2dthreadtile.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/transform/threadblock/predicated_tile_iterator_2dthreadtile.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/transform/threadblock/predicated_tile_iterator_triangular_matrix.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/transform/threadblock/regular_tile_access_iterator_tensor_op.h`

 * *Files 14% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -25,794 +25,796 @@
  * SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER
  * CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,
  * OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
  * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
  *
  **************************************************************************************************/
 /*! \file
-    \brief Templates implementing loading of tiles from pitch-linear rank=2 tensors. 
-
-    This iterator uses masks to guard out-of-bounds accesses and visits the last "residue" tile
-    first, with the objective of minimizing predicate mask updates during steady-state operation.
-
-    A precomputed "Params" object minimizes the amount of state that must be stored in registers,
-    and integer addition is used to advance the pointer through memory.
+    \brief Templates implementing computing the addresses of storing of tiles
+   from pitch-linear rank=2 tensors.
 */
 
 #pragma once
 
-#include "cutlass/arch/memory.h"
-#include "cutlass/transform/threadblock/predicated_tile_access_iterator_triangular_matrix.h"
+#include "cutlass/array.h"
+#include "cutlass/cutlass.h"
+#include "cutlass/layout/pitch_linear.h"
+#include "cutlass/layout/tensor_op_multiplicand_sm75.h"
+#include "cutlass/matrix_coord.h"
+#include "cutlass/matrix_shape.h"
+#include "cutlass/tensor_ref.h"
+#include "cutlass/transform/threadblock/regular_tile_access_iterator.h"
 
 ////////////////////////////////////////////////////////////////////////////////
 
 namespace cutlass {
 namespace transform {
 namespace threadblock {
 
 ////////////////////////////////////////////////////////////////////////////////
 
-/// PredicatedTileIteratorTriangularMatrix
+/// Tile iterator specialized for congruous arrangements for TensorOps
 ///
-/// Satisfies: ForwardTileIteratorConcept | 
-///            ReadableContiguousTileIteratorConcept | 
-///            WriteableContiguousTileIteratorConcept |
-///            MaskedTileIteratorConcept
-///
-/// Regular tile iterator using a precomputed control structure to minimize register liveness
-/// and integer arithmetic.
-///
-/// Layout is assumed to be invariant at the time the precomputed "Params" object is constructed.
-///
-/// Base pointer and tensor extents may be specified at the time the iterator is constructed.
-/// Subsequently, they are assumed to be immutable.
-///
-/// Adding a logical coordinate offset may be performed at the time the iterator is constructed.
-/// Subsequent additions to logical coordinate offset may be performed but are relatively expensive.
-///
-/// Vistitation order is intended to first visit a "residual" tile that may be partially full in
-/// both the advance dimension and the steady-state dimension. This is assumed to be the last
-/// tile in the iteration sequence. Advancing an iterator that has just been constructed moves to
-/// the first tile that is full in the advance dimension and recomputes predicates. Subsequent
-/// accesses may be performed without updating internal predicates and are efficient in terms of
-/// live register state and pointer arithmetic instructions.
-///
-/// To be efficient, this assumes the iteraor will be dereferenced and advanced at least once
-/// outside any looping structure to minimize integer arithmetic. 
-///
-/// Acceses out of bounds are safe so long as `clear_mask()` is called prior to dereferencing
-/// the iterator.
-///
-///
-/// Example:
-///
-/// An efficient pipeline structure may be constructed as follows:
-///
-// template <typename Iterator>
-// __global__ void kernel(
-//   typename Iterator::Params params, 
-//   typename Iterator::Element *ptr,
-//   TensorCoord extent) {
-//
-//   typename Iterator::Fragment fragment;
-//
-//   TensorCoord threadblock_offset(0, 0);
-//
-//   Iterator iter(params, ptr, extent, threadIdx.x, threadblock_offsets);
-//
-//
-//   fragment = *iter;        // load "residue" tile first
-//   ++iter;                  // advance to first "steady state" tile and update internal masks
-//
-//
-//   #pragma unroll
-//   for (int i = Remaining - 1; i >= 0; --i) {
-//
-//     f(fragment);
-//
-//     if (!i) {
-//       iter.clear_mask();   // light-weight operation to clear masks - subsequent loads become NO-OPs.
-//     }
-//  
-//     fragment = *iter;      // load tile during "steady state" phase
-//     ++iter;                // advance to next tile - lightweight due to steady-state masks
-//   }
-// }
-//
-// void host(TensorView<Element, 2, layout::PitchLinear> view) {
-//
-//   using Iterator = transform::threadblock::PredicatedTileIteratorTriangularMatrix;
-//
-//   typename Iterator::Params params(view.layout());
-//
-//   kernel<Iterator>(params, view.data());
-// }
-///
-///
-template <
-  typename Shape,
-  typename Element,
-  typename Layout,
-  int AdvanceRank,
-  typename ThreadMap,
-  SideMode kSideMode, 
-  FillMode kFillMode, 
-  DiagType kDiagType,
-  int AccessSize = ThreadMap::kElementsPerAccess
->
-class PredicatedTileIteratorTriangularMatrix;
-
-////////////////////////////////////////////////////////////////////////////////
-
-/// Specialization of PredicatedTileIteratorTriangularMatrix for pitch-linear data.
 ///
-/// Satisfies: ForwardTileIteratorConcept | 
-///            ReadableContiguousTileIteratorConcept | 
-///            WriteableContiguousTileIteratorConcept |
-///            MaskedTileIteratorConcept
-///
-template <typename Shape_, typename Element_, int AdvanceRank, typename ThreadMap_, 
-          SideMode kSideMode, FillMode kFillMode, DiagType kDiagType, 
-          int AccessSize>
-class PredicatedTileIteratorTriangularMatrix<Shape_, Element_, layout::PitchLinear, AdvanceRank, ThreadMap_, 
-                                             kSideMode, kFillMode, kDiagType,
-                                             AccessSize> {
+/// Satisfies: ForwardTileIteratorConcept |
+///            ReadableContiguousTileIteratorConcept |
+///            WriteableContiguousTileIteratorConcept
+///
+template <typename Shape_, typename Element_, int AdvanceRank,
+          typename ThreadMap_, int Alignment>
+class RegularTileAccessIterator<
+    Shape_, Element_,
+    layout::TensorOpMultiplicandCongruous<sizeof_bits<Element_>::value,
+                                          int(128 / sizeof(Element_))>,
+    AdvanceRank, ThreadMap_, Alignment> {
  public:
   static_assert(
       AdvanceRank == 0 || AdvanceRank == 1,
       "Specialization for pitch-linear iterator may along advance along the "
       "contiguous(rank=0) or strided(rank=1) dimension.");
 
   using Shape = Shape_;
   using Element = Element_;
-  using Layout = layout::PitchLinear;
+  using Layout =
+      layout::TensorOpMultiplicandCongruous<sizeof_bits<Element_>::value,
+                                            int(128 / sizeof(Element_))>;
   static int const kAdvanceRank = AdvanceRank;
-  using ThreadMap = ThreadMap_;
+  static int const kAlignment = Alignment;
 
   using Index = typename Layout::Index;
   using LongIndex = typename Layout::LongIndex;
+  using StrideIndex = typename Layout::Stride::Index;
 
   using TensorRef = TensorRef<Element, Layout>;
-  using TensorView = TensorView<Element, Layout>;
   using TensorCoord = typename Layout::TensorCoord;
 
-  using Pointer = Element *;
-  using NonConstPointer = typename platform::remove_const<Element>::type *;
-
-  /// Type used for internal memory accesses
-  using AccessType = AlignedArray<Element, AccessSize, (AccessSize * sizeof_bits<Element>::value / 8)>;
+  using ThreadMap = ThreadMap_;
 
-  /// Underlying iterator to compute the addresses
-  using TileAccessIterator =
-      PredicatedTileAccessIteratorTriangularMatrix<Shape, Element, Layout, kAdvanceRank,
-                                   ThreadMap, kSideMode, kFillMode, kDiagType, AccessType>;
-
-  static int const kAccessesPerVector = TileAccessIterator::kAccessesPerVector;
-
-  /// Fragment object to be loaded or stored
-  using Fragment = cutlass::Array<Element, ThreadMap::Iterations::kCount *
-                                               ThreadMap::kElementsPerAccess>;
-
-  /// Predicate vector stores mask to guard accesses
-  using Mask = typename TileAccessIterator::Mask;
-
-  /// Parameters object is precomputed state and is host-constructible
-  class Params {
-   public:
-    friend PredicatedTileIteratorTriangularMatrix;
-
-   private:
-    /// Parameters object
-    typename TileAccessIterator::Params params_;
-
-   public:
-    /// Construct the Params object given a pitch-linear tensor's layout
-    CUTLASS_HOST_DEVICE
-    Params(Layout const &layout) : params_(layout) { }
-    
-    CUTLASS_HOST_DEVICE
-    Params() { }
+  /// Internal details made public to facilitate introspection
+  struct Detail {
+    /// This iterator is specialized for an access size that is 128 bits in
+    /// length.
+    static int const kAccessSizeInBits = 128;
+
+    static_assert(sizeof_bits<Element_>::value *
+                          ThreadMap::kElementsPerAccess ==
+                      kAccessSizeInBits,
+                  "This iterator requires a policy whose access size is 128bs");
+
+    ///< Number of pointers
+    static int const kPointerCount =
+        (ThreadMap::Iterations::kStrided > 1 ? 2 : 1);
   };
 
- private:
-  /// Internal pointer type permits fast address arithmetic
-  using BytePointer = char *;
+  /// Element type per access
+  using AccessType = Array<Element, Layout::kElementsPerAccess>;
 
  private:
   //
   // Data members
   //
 
-  /// Data member to the tile access iterator
-  TileAccessIterator address_iterator_;
+  /// Stride value
+  StrideIndex stride_;
+
+  /// Internal pointer to first access of tile
+  AccessType *pointer_[Detail::kPointerCount];
+
+  /// Internal byte offset
+  Index byte_offset_;
+
+  /// Iteration in the contiguous dimension
+  int iteration_contiguous_;
+
+  /// Iteration in the strided dimension
+  int iteration_strided_;
 
  public:
-  /// Constructs a TileIterator from its precomputed state, threadblock offset,
-  /// and thread ID
+  /// Construct a TileIterator with zero threadblock offset
   CUTLASS_HOST_DEVICE
-  PredicatedTileIteratorTriangularMatrix(
-      /// Precomputed parameters object
-      Params const &params,
-      /// Pointer to start of tensor
-      Pointer pointer,
-      /// Extent of tensor
-      TensorCoord extent,
-      /// ID of each participating thread
-      int thread_id,
-      /// Initial offset of threadblock
-      TensorCoord const &threadblock_offset)
-      : address_iterator_(params.params_, pointer, extent, thread_id,
-                          threadblock_offset) {}
-
-  /// Construct a PredicatedTileIteratorTriangularMatrix with zero threadblock offset
-  CUTLASS_HOST_DEVICE
-  PredicatedTileIteratorTriangularMatrix(
-      Params const &params,  ///< Precomputed parameters object
-      Pointer pointer,       ///< Pointer to start of tensor
-      TensorCoord extent,    ///< Extent of tensor
-      int thread_id          ///< ID of each participating thread
-      )
-      : PredicatedTileIteratorTriangularMatrix(params, pointer, extent, thread_id,
-                               make_Coord(0, 0)) {}
+  RegularTileAccessIterator(TensorRef ref,  ///< Pointer to start of tensor
+                            int thread_id   ///< ID of each participating thread
+                            )
+      : stride_(ref.stride(0) / Layout::kElementsPerAccess),
+        byte_offset_(0) {
+    layout::PitchLinearCoord thread_offset_base =
+        ThreadMap::initial_offset(thread_id);
 
-  /// Adds a pointer offset in units of Element
-  CUTLASS_HOST_DEVICE
-  void add_pointer_offset(LongIndex pointer_offset) {
-    address_iterator_.add_pointer_offset(pointer_offset);
+    CUTLASS_PRAGMA_UNROLL
+    for (int i = 0; i < Detail::kPointerCount; ++i) {
+      // This is the offset of a thread within a threadblock tile for a specific
+      // pointer (units of elements)
+      layout::PitchLinearCoord thread_offset_in_threadblock_tile =
+          thread_offset_base +
+          layout::PitchLinearCoord{
+              0, ThreadMap::Detail::WarpThreadArrangement::kStrided * i};
+
+      // initialize pointer
+      pointer_[i] = reinterpret_cast<AccessType *>(
+          ref.data() + ref.offset(thread_offset_in_threadblock_tile));
+    }
+
+    set_iteration_index(0);
   }
 
-  /// Advances to the next tile in memory.
-  ///
-  /// The first time this method is called, predicates are updated, and the
-  /// iterator's internal pointer is reverted to the first "steady state" tile.
-  /// Subsequent calls are lightweight and must only update the internal
-  /// pointer.
+  /// Overrides the internal iteration index
   CUTLASS_HOST_DEVICE
-  PredicatedTileIteratorTriangularMatrix &operator++() {
-    if (kAdvanceRank)
-      address_iterator_.add_tile_offset({0, 1});
-    else
-      address_iterator_.add_tile_offset({1, 0});
-
-    return *this;
+  void set_iteration_index(int index) {
+    iteration_contiguous_ = index % ThreadMap::Iterations::kContiguous;
+    iteration_strided_ = index / ThreadMap::Iterations::kContiguous;
   }
 
-  /// Advances to the next tile in memory.
-  ///
-  /// The first time this method is called, predicates are updated, and the
-  /// iterator's internal pointer is reverted to the first "steady state" tile.
-  /// Subsequent calls are lightweight and must only update the internal
-  /// pointer.
+  /// Adds a pointer offset in units of Element
   CUTLASS_HOST_DEVICE
-  PredicatedTileIteratorTriangularMatrix operator++(int) {
-    PredicatedTileIteratorTriangularMatrix self(*this);
-    operator++();
-    return self;
+  void add_pointer_offset(LongIndex pointer_offset) {
+    byte_offset_ += pointer_offset * sizeof(Element);
   }
 
-  /// Clears the predicate set efficiently
+  /// Returns a pointer
   CUTLASS_HOST_DEVICE
-  void clear_mask(bool enable = true) { address_iterator_.clear_mask(enable); }
+  AccessType *get() const {
+    AccessType *access_ptr = pointer_[iteration_strided_ & 1];
+    int stride_idx = (iteration_strided_ & ~1);
 
-  /// Clears the predicate set efficiently
-  CUTLASS_HOST_DEVICE
-  void enable_mask() { address_iterator_.enable_mask(); }
+    int access_offset = stride_idx * ThreadMap::Delta::kStrided * stride_ +
+                        iteration_contiguous_ * ThreadMap::Delta::kContiguous /
+                            ThreadMap::kElementsPerAccess;
 
-  /// Sets the predicate mask, overriding value stored in predicate iterator
-  CUTLASS_HOST_DEVICE
-  void set_mask(Mask const &mask) { address_iterator_.set_mask(mask); }
+    char *access_byte_ptr =
+        reinterpret_cast<char *>(access_ptr + access_offset);
+    return reinterpret_cast<AccessType *>(access_byte_ptr + byte_offset_);
+  }
 
-  /// Gets the mask
+  /// Advances to the next tile in memory.
   CUTLASS_HOST_DEVICE
-  void get_mask(Mask &mask) { address_iterator_.get_mask(mask); }
-
-  CUTLASS_DEVICE
-  void load_with_pointer_offset(Fragment &frag, Index pointer_offset) {
-    load_with_byte_offset(frag, pointer_offset * sizeof_bits<Element>::value / 8);
-  }
+  RegularTileAccessIterator &operator++() {
+    ++iteration_contiguous_;
 
-  CUTLASS_DEVICE
-  void load_with_byte_offset(Fragment &frag, LongIndex byte_offset) {
+    if (iteration_contiguous_ < ThreadMap::Iterations::kContiguous)
+      return *this;
 
-    AccessType *frag_ptr = reinterpret_cast<AccessType *>(&frag);
+    // Enter here only if (iteration_contiguous_ ==
+    // ThreadMap::Iteration::kContiguous)
+    iteration_contiguous_ = 0;
+    ++iteration_strided_;
 
-    CUTLASS_PRAGMA_UNROLL
-    for (int s = 0; s < ThreadMap::Iterations::kStrided; ++s) {
-      CUTLASS_PRAGMA_UNROLL
-      for (int c = 0; c < ThreadMap::Iterations::kContiguous; ++c) {
-
-        CUTLASS_PRAGMA_UNROLL
-        for (int v = 0; v < kAccessesPerVector; ++v) {
-
-          int idx = v + kAccessesPerVector * (c + s * ThreadMap::Iterations::kContiguous);
-          
-          address_iterator_.set_iteration_index(idx);
-          char const *byte_ptr = reinterpret_cast<char const *>(address_iterator_.get()) + byte_offset;
-
-          AccessType const *access_ptr = reinterpret_cast<AccessType const *>(byte_ptr);
-
-          cutlass::arch::global_load<AccessType,
-                                     sizeof(AccessType)
-                                    >(
-              frag_ptr[idx], access_ptr, address_iterator_.valid());
-
-          ++address_iterator_;
-        }
-      }
+    if (iteration_strided_ < ThreadMap::Iterations::kStrided) {
+      return *this;
     }
-  }
 
-  /// Loads a fragment from memory
-  CUTLASS_DEVICE
-  void load(Fragment &frag) { load_with_byte_offset(frag, 0); }
+    // Enter here only if (iteration_strided_ == ThreadMap::Iteration::kStrided)
+    // which means we enter the next tile.
+    iteration_strided_ = 0;
 
-  /// Store a fragment to memory
-  CUTLASS_DEVICE
-  void store_with_pointer_offset(Fragment const &frag, Index pointer_offset) {
-    store_with_byte_offset(frag, pointer_offset * sizeof_bits<Element>::value / 8);
+    return *this;
   }
 
-  /// Store a fragment to memory
-  CUTLASS_DEVICE
-  void store_with_byte_offset(Fragment const &frag, LongIndex byte_offset) {
-    address_iterator_.set_iteration_index(0);
-    AccessType const *frag_ptr = reinterpret_cast<AccessType const *>(&frag);
+  /// Advances to the next tile in memory.
+  CUTLASS_HOST_DEVICE
+  RegularTileAccessIterator operator++(int) {
+    RegularTileAccessIterator prev(*this);
+    this->operator++();
 
-    CUTLASS_PRAGMA_UNROLL
-    for (int s = 0; s < ThreadMap::Iterations::kStrided; ++s) {
-      CUTLASS_PRAGMA_UNROLL
-      for (int c = 0; c < ThreadMap::Iterations::kContiguous; ++c) {
-        CUTLASS_PRAGMA_UNROLL
-        for (int v = 0; v < kAccessesPerVector; ++v) {
-
-          int idx = v + kAccessesPerVector * (c + s * ThreadMap::Iterations::kContiguous);
-
-          char *byte_ptr = reinterpret_cast<char *>(address_iterator_.get()) + byte_offset;
-          AccessType *access_ptr = reinterpret_cast<AccessType *>(byte_ptr);
-
-          if (address_iterator_.valid()) {
-            *access_ptr = frag_ptr[idx];
-          }
-          ++address_iterator_;
-        }
-      }
-    }
+    return prev;
   }
 
-  /// Store a fragment to memory
+  /// Adds a tile offset
   CUTLASS_DEVICE
-  void store(Fragment const &frag) { store_with_byte_offset(frag, 0); }
+  void add_tile_offset(TensorCoord const &coord) {
+    add_pointer_offset(coord.contiguous() * Shape::kContiguous +
+                       coord.strided() * Shape::kStrided * stride_ *
+                           Layout::kElementsPerAccess);
+  }
 };
 
 ////////////////////////////////////////////////////////////////////////////////
 
-/// Specialization of PredicatedTileIteratorTriangularMatrix for column-major data.
+/// Tile Iterator specialized for column-major congruous TensorOp formats.
 ///
-/// Satisfies: ForwardTileIteratorConcept | 
-///            ReadableContiguousTileIteratorConcept | 
-///            WriteableContiguousTileIteratorConcept |
-///            MaskedTileIteratorConcept
-///
-template <
-  typename Shape_,
-  typename Element_,
-  int AdvanceRank,
-  typename ThreadMap_,
-  SideMode kSideMode, 
-  FillMode kFillMode, 
-  DiagType kDiagType,
-  int AccessSize
->
-class PredicatedTileIteratorTriangularMatrix<Shape_, Element_, layout::ColumnMajor, AdvanceRank, ThreadMap_, 
-                                              kSideMode, kFillMode, kDiagType,
-                                              AccessSize> {
-public:
-
-  static_assert(AdvanceRank == 0 || AdvanceRank == 1, 
-    "Specialization for pitch-linear iterator may along advance along the "
-    "contiguous(rank=0) or strided(rank=1) dimension.");
+///
+/// Satisfies: ForwardTileIteratorConcept |
+///            ReadableContiguousTileIteratorConcept |
+///            WriteableContiguousTileIteratorConcept
+///
+template <typename Shape_, typename Element_, int AdvanceRank,
+          typename ThreadMap_, int Alignment>
+class RegularTileAccessIterator<
+    Shape_, Element_,
+    layout::ColumnMajorTensorOpMultiplicandCongruous<
+        sizeof_bits<Element_>::value, int(128 / sizeof(Element_))>,
+    AdvanceRank, ThreadMap_, Alignment> {
+ public:
+  static_assert(
+      AdvanceRank == 0 || AdvanceRank == 1,
+      "Specialization for column-major iterator may along advance along the "
+      "columns(rank=0) or rows(rank=1) dimension.");
 
   using Shape = Shape_;
   using Element = Element_;
-  using Layout = layout::ColumnMajor;
+  using Layout = layout::ColumnMajorTensorOpMultiplicandCongruous<
+      sizeof_bits<Element_>::value, int(128 / sizeof(Element_))>;
   static int const kAdvanceRank = AdvanceRank;
-  using ThreadMap = ThreadMap_;
+  static int const kAlignment = Alignment;
 
   using Index = typename Layout::Index;
   using LongIndex = typename Layout::LongIndex;
 
   using TensorRef = TensorRef<Element, Layout>;
-  using TensorView = TensorView<Element, Layout>;
   using TensorCoord = typename Layout::TensorCoord;
 
-  using Pointer = Element *;
-  using NonConstPointer = typename platform::remove_const<Element>::type *;
+  using ThreadMap = ThreadMap_;
 
-  using UnderlyingIterator = PredicatedTileIteratorTriangularMatrix<
-    layout::PitchLinearShape<Shape::kRow, Shape::kColumn>,
-    Element,
-    layout::PitchLinear,
-    (kAdvanceRank == 0 ? 0 : 1),
-    ThreadMap,
-    kSideMode, 
-    kFillMode, 
-    kDiagType,
-    AccessSize
-  >;
+  /// Underlying iterator type
+  using UnderlyingIterator = RegularTileAccessIterator<
+      layout::PitchLinearShape<Shape::kRow, Shape::kColumn>, Element,
+      layout::TensorOpMultiplicandCongruous<sizeof_bits<Element_>::value,
+                                            int(128 / sizeof(Element_))>,
+      (kAdvanceRank == 0 ? 0 : 1), ThreadMap_>;
 
   using AccessType = typename UnderlyingIterator::AccessType;
 
-  /// Fragment object to be loaded or stored
-  using Fragment = cutlass::Array<Element, ThreadMap::Iterations::kCount * ThreadMap::kElementsPerAccess>;
+ private:
+  /// Underlying iterator
+  UnderlyingIterator iterator_;
 
-  /// Predicate vector stores mask to guard accesses
-  using Mask = typename UnderlyingIterator::Mask;
+ public:
+  /// Construct a TileIterator with zero threadblock offset
+  CUTLASS_HOST_DEVICE
+  RegularTileAccessIterator(TensorRef ref,  ///< Pointer to start of tensor
+                            int thread_id   ///< ID of each participating thread
+                            )
+      : iterator_({ref.data(), ref.stride()}, thread_id) {}
 
-  /// Parameters object is precomputed state and is host-constructible
-  class Params {
-  private:
+  /// Overrides the internal iteration index
+  CUTLASS_HOST_DEVICE
+  void set_iteration_index(int index) { iterator_.set_iteration_index(index); }
 
-    friend PredicatedTileIteratorTriangularMatrix;
+  /// Adds a pointer offset in units of Element
+  CUTLASS_HOST_DEVICE
+  void add_pointer_offset(LongIndex pointer_offset) {
+    iterator_.add_pointer_offset(pointer_offset);
+  }
 
-    /// Parameters object
-    typename UnderlyingIterator::Params params_;
+  /// Returns a pointer
+  CUTLASS_HOST_DEVICE
+  AccessType *get() const {
+    return reinterpret_cast<AccessType *>(iterator_.get());
+  }
 
-  public:
-    
-    CUTLASS_HOST_DEVICE
-    Params() { }
+  /// Adds a tile offset
+  CUTLASS_DEVICE
+  void add_tile_offset(TensorCoord const &coord) {
+    iterator_.add_tile_offset({coord.row(), coord.column()});
+  }
 
-    /// Construct the Params object given a pitch-linear tensor's layout
-    CUTLASS_HOST_DEVICE
-    Params(Layout const &layout): params_(layout::PitchLinear(layout.stride(0))) {
+  /// Advances to the next tile in memory.
+  CUTLASS_HOST_DEVICE
+  RegularTileAccessIterator &operator++() {
+    ++iterator_;
+    return *this;
+  }
 
-    }
-  };
+  /// Advances to the next tile in memory.
+  CUTLASS_HOST_DEVICE
+  RegularTileAccessIterator operator++(int) {
+    RegularTileAccessIterator prev(*this);
+    ++iterator_;
 
+    return prev;
+  }
+};
 
-private:
+////////////////////////////////////////////////////////////////////////////////
 
-  //
-  // Data members
-  //
+/// Tile Iterator specialized for row-major congruous TensorOp formats.
+///
+///
+/// Satisfies: ForwardTileIteratorConcept |
+///            ReadableContiguousTileIteratorConcept |
+///            WriteableContiguousTileIteratorConcept
+///
+template <typename Shape_, typename Element_, int AdvanceRank,
+          typename ThreadMap_, int Alignment>
+class RegularTileAccessIterator<
+    Shape_, Element_,
+    layout::RowMajorTensorOpMultiplicandCongruous<sizeof_bits<Element_>::value,
+                                                  int(128 / sizeof(Element_))>,
+    AdvanceRank, ThreadMap_, Alignment> {
+ public:
+  static_assert(
+      AdvanceRank == 0 || AdvanceRank == 1,
+      "Specialization for row-major iterator may along advance along the "
+      "columns(rank=0) or rows(rank=1) dimension.");
+
+  using Shape = Shape_;
+  using Element = Element_;
+  using Layout = layout::RowMajorTensorOpMultiplicandCongruous<
+      sizeof_bits<Element_>::value, int(128 / sizeof(Element_))>;
+  static int const kAdvanceRank = AdvanceRank;
+  static int const kAlignment = Alignment;
+
+  using Index = typename Layout::Index;
+  using LongIndex = typename Layout::LongIndex;
 
-  /// Underlying pitch-linear tile iterator
+  using TensorRef = TensorRef<Element, Layout>;
+  using TensorCoord = typename Layout::TensorCoord;
+
+  using ThreadMap = ThreadMap_;
+
+  /// Underlying iterator type
+  using UnderlyingIterator = RegularTileAccessIterator<
+      layout::PitchLinearShape<Shape::kColumn, Shape::kRow>, Element,
+      layout::TensorOpMultiplicandCongruous<sizeof_bits<Element_>::value,
+                                            int(128 / sizeof(Element_))>,
+      (kAdvanceRank == 0 ? 1 : 0), ThreadMap_>;
+
+  using AccessType = typename UnderlyingIterator::AccessType;
+
+ private:
+  /// Underlying iterator
   UnderlyingIterator iterator_;
 
-public:
+ public:
+  /// Construct a TileIterator with zero threadblock offset
+  CUTLASS_HOST_DEVICE
+  RegularTileAccessIterator(TensorRef ref,  ///< Pointer to start of tensor
+                            int thread_id   ///< ID of each participating thread
+                            )
+      : iterator_({ref.data(), ref.stride()}, thread_id) {}
 
-  /// Constructs a TileIterator from its precomputed state, threadblock offset, and thread ID
+  /// Overrides the internal iteration index
   CUTLASS_HOST_DEVICE
-  PredicatedTileIteratorTriangularMatrix(
-    Params const &params,                         ///< Precomputed parameters object 
-    Pointer pointer,                              ///< Pointer to start of tensor
-    TensorCoord extent,                           ///< Extent of tensor
-    int thread_id,                                ///< ID of each participating thread
-    TensorCoord const &threadblock_offset         ///< Initial offset of threadblock
-  ):
-    iterator_(
-      params.params_,
-      pointer,
-      layout::PitchLinearCoord(extent.row(), extent.column()),
-      thread_id,
-      layout::PitchLinearCoord(threadblock_offset.row(), threadblock_offset.column())
-    ) { }
-
-  /// Construct a PredicatedTileIteratorTriangularMatrix with zero threadblock offset
-  CUTLASS_HOST_DEVICE
-  PredicatedTileIteratorTriangularMatrix(
-    Params const &params,                         ///< Precomputed parameters object
-    Pointer pointer,                              ///< Pointer to start of tensor
-    TensorCoord extent,                           ///< Extent of tensor
-    int thread_id                                 ///< ID of each participating thread
-  ): PredicatedTileIteratorTriangularMatrix(params, pointer, extent, thread_id, make_Coord(0, 0)) { }
+  void set_iteration_index(int index) { iterator_.set_iteration_index(index); }
 
   /// Adds a pointer offset in units of Element
   CUTLASS_HOST_DEVICE
   void add_pointer_offset(LongIndex pointer_offset) {
     iterator_.add_pointer_offset(pointer_offset);
   }
 
+  /// Returns a pointer
+  CUTLASS_HOST_DEVICE
+  AccessType *get() const {
+    return reinterpret_cast<AccessType *>(iterator_.get());
+  }
+
+  /// Adds a tile offset
+  CUTLASS_DEVICE
+  void add_tile_offset(TensorCoord const &coord) {
+    iterator_.add_tile_offset({coord.column(), coord.row()});
+  }
+
   /// Advances to the next tile in memory.
-  ///
-  /// The first time this method is called, predicates are updated, and the iterator's
-  /// internal pointer is reverted to the first "steady state" tile. Subsequent calls
-  /// are lightweight and must only update the internal pointer.
   CUTLASS_HOST_DEVICE
-  PredicatedTileIteratorTriangularMatrix &operator++() {
+  RegularTileAccessIterator &operator++() {
     ++iterator_;
     return *this;
   }
 
   /// Advances to the next tile in memory.
-  ///
-  /// The first time this method is called, predicates are updated, and the iterator's
-  /// internal pointer is reverted to the first "steady state" tile. Subsequent calls
-  /// are lightweight and must only update the internal pointer.
   CUTLASS_HOST_DEVICE
-  PredicatedTileIteratorTriangularMatrix operator++(int) {
-    PredicatedTileIteratorTriangularMatrix self(*this);
-    operator++();
-    return self;
+  RegularTileAccessIterator operator++(int) {
+    RegularTileAccessIterator prev(*this);
+    ++iterator_;
+
+    return prev;
   }
+};
+
+////////////////////////////////////////////////////////////////////////////////
+
+/// Tile iterator specialized for crosswise arrangements for TensorOps
+///
+///
+/// Satisfies: ForwardTileIteratorConcept |
+///            ReadableContiguousTileIteratorConcept |
+///            WriteableContiguousTileIteratorConcept
+///
+template <typename Shape_, typename Element_, int AdvanceRank,
+          typename ThreadMap_, int Alignment, int Crosswise>
+class RegularTileAccessIterator<Shape_, Element_,
+                                layout::TensorOpMultiplicandCrosswise<
+                                    sizeof_bits<Element_>::value, Crosswise>,
+                                AdvanceRank, ThreadMap_, Alignment> {
+ public:
+  static_assert(
+      AdvanceRank == 0 || AdvanceRank == 1,
+      "Specialization for pitch-linear iterator may along advance along the "
+      "contiguous(rank=0) or strided(rank=1) dimension.");
+
+  using Shape = Shape_;
+  using Element = Element_;
+  using Layout =
+      layout::TensorOpMultiplicandCrosswise<sizeof_bits<Element_>::value,
+                                            Crosswise>;
+  static int const kAdvanceRank = AdvanceRank;
+  static int const kAlignment = Alignment;
+  static int const kCrosswise = Crosswise;
+
+  using Index = typename Layout::Index;
+  using LongIndex = typename Layout::LongIndex;
+  using StrideIndex = typename Layout::Stride::Index;
+
+  using TensorRef = TensorRef<Element, Layout>;
+  using TensorCoord = typename Layout::TensorCoord;
+
+  using ThreadMap = ThreadMap_;
+
+  static_assert(!(ThreadMap::Delta::kContiguous % kCrosswise),
+                "kCrosswise is the smallest unit in the contiguous dimension "
+                "for shared memory swizzling.");
+
+  /// Internal details made public to facilitate introspection
+  struct Detail {
+    /// This iterator is specialized for an access size that is 128 bits in
+    /// length.
+    static int const kAccessSizeInBits = 128;
+
+    static_assert(sizeof_bits<Element_>::value *
+                          ThreadMap::kElementsPerAccess ==
+                      kAccessSizeInBits,
+                  "This iterator requires a policy whose access size is 128bs");
+
+    /// Number of pointers
+    ///
+    /// Note:TN kblock32 layouts only needs 1 pointer, but strangely
+    /// reducing pointer count hurts perfomrnace
+    static int const kPointerCount =
+        (ThreadMap::Iterations::kStrided > 1 ? 2 : 1);
+  };
+
+  /// Element type per access
+  using AccessType = Array<Element, Layout::kElementsPerAccess>;
+
+ private:
+  //
+  // Data members
+  //
+
+  /// Total number of sections.  The memory is divided into stages.  One stage
+  /// can store one tile.  Stage is divided into sections.  Interleaved layout
+  /// can have multiple sections in a stage.  The rest layout only has one section
+  /// in a stage.
+  int sections_;
 
-  /// Clears the predicate set efficiently
+  /// Sections that a stage has
+  int sections_per_stage_;
+
+  /// Stride value
+  StrideIndex stride_;
+
+  /// Internal pointer to first access of tile
+  AccessType *pointer_[Detail::kPointerCount];
+
+  /// Internal byte offset
+  Index byte_offset_;
+
+  /// Iteration in the contiguous dimension
+  int iteration_contiguous_;
+
+  /// Iteration in the strided dimension
+  int iteration_strided_;
+
+ public:
+  /// Construct a TileIterator with zero threadblock offset
   CUTLASS_HOST_DEVICE
-  void clear_mask(bool enable = true) {
-    iterator_.clear_mask(enable);
+  RegularTileAccessIterator(TensorRef ref,  ///< Pointer to start of tensor
+                            int thread_id   ///< ID of each participating thread
+                            )
+      : sections_(ref.stride(0) / kCrosswise),
+        sections_per_stage_(Shape::kContiguous / kCrosswise),
+        // stride_ = kCrosswise x sections_ x kFactor
+        stride_(ref.stride(0) * Layout::kFactor / Layout::kElementsPerAccess),
+        byte_offset_(0) {
+    layout::PitchLinearCoord thread_offset_base =
+        ThreadMap::initial_offset(thread_id);
+
+    CUTLASS_PRAGMA_UNROLL
+    for (int i = 0; i < Detail::kPointerCount; ++i) {
+      // This is the offset of a thread within a threadblock tile for a specific
+      // pointer (units of elements)
+      layout::PitchLinearCoord thread_offset_in_threadblock_tile =
+          thread_offset_base +
+          layout::PitchLinearCoord{
+              0, ThreadMap::Detail::WarpThreadArrangement::kStrided * i};
+      // initialize pointer
+      pointer_[i] = reinterpret_cast<AccessType *>(ref.data()) +
+                    ref.offset(thread_offset_in_threadblock_tile) /
+                        Layout::kElementsPerAccess;
+    }
+
+    set_iteration_index(0);
   }
 
-  /// Clears the predicate set efficiently
+  /// Overrides the internal iteration index
   CUTLASS_HOST_DEVICE
-  void enable_mask() {
-    iterator_.enable_mask();
+  void set_iteration_index(int index) {
+    iteration_contiguous_ = index % ThreadMap::Iterations::kContiguous;
+    iteration_strided_ = index / ThreadMap::Iterations::kContiguous;
   }
 
-  /// Sets the predicate mask, overriding value stored in predicate iterator
+  /// Adds a pointer offset in units of Element
   CUTLASS_HOST_DEVICE
-  void set_mask(Mask const &mask) {
-    iterator_.set_mask(mask);
+  void add_pointer_offset(LongIndex pointer_offset) {
+    byte_offset_ += pointer_offset * sizeof_bits<Element>::value / 8;
   }
 
-  /// Gets the mask
+  /// Returns a pointer
   CUTLASS_HOST_DEVICE
-  void get_mask(Mask &mask) {
-    iterator_.get_mask(mask);
+  AccessType *get() const {
+    AccessType *access_ptr = pointer_[iteration_strided_ & 1];
+    int stride_idx = (iteration_strided_ & ~1);
+
+    int access_offset =
+        stride_idx * ThreadMap::Delta::kStrided * stride_ / Layout::kFactor +
+        // kCrosswise elements in the contiguous dimension would span to a
+        // shared memory cache line.
+        iteration_contiguous_ * (ThreadMap::Delta::kContiguous / kCrosswise) *
+            Layout::TileShape::kContiguous;
+    char *access_byte_ptr =
+        reinterpret_cast<char *>(access_ptr + access_offset);
+    return reinterpret_cast<AccessType *>(access_byte_ptr + byte_offset_);
   }
 
-  /// Loads a fragment from memory
-  CUTLASS_DEVICE
-  void load_with_pointer_offset(Fragment &frag, Index pointer_offset) {
-    iterator_.load_with_pointer_offset(frag, pointer_offset);
-  }
+  /// Advances to the next tile in memory.
+  CUTLASS_HOST_DEVICE
+  RegularTileAccessIterator &operator++() {
+    ++iteration_contiguous_;
 
-  /// Loads a fragment from memory
-  CUTLASS_DEVICE
-  void load_with_byte_offset(Fragment &frag, LongIndex byte_offset) {
-    iterator_.load_with_byte_offset(frag, byte_offset);
-  }
+    if (iteration_contiguous_ < ThreadMap::Iterations::kContiguous)
+      return *this;
 
-  /// Loads a fragment from memory
-  CUTLASS_DEVICE
-  void load(Fragment &frag) {
-    load_with_pointer_offset(frag, 0);
-  }
+    // Enter here only if (iteration_contiguous_ ==
+    // ThreadMap::Iteration::kContiguous)
+    iteration_contiguous_ = 0;
+    ++iteration_strided_;
 
-  /// Store a fragment to memory
-  CUTLASS_DEVICE
-  void store_with_pointer_offset(Fragment const &frag, Index pointer_offset) {
-    iterator_.store_with_pointer_offset(frag, pointer_offset);
+    if (iteration_strided_ < ThreadMap::Iterations::kStrided) {
+      return *this;
+    }
+
+    // Enter here only if (iteration_strided_ == ThreadMap::Iteration::kStrided)
+    // which means we enter the next section.
+    iteration_strided_ = 0;
+
+    return *this;
   }
 
-  /// Store a fragment to memory
-  CUTLASS_DEVICE
-  void store_with_byte_offset(Fragment const &frag, LongIndex byte_offset) {
-    iterator_.store_with_byte_offset(frag, byte_offset);
+  /// Advances to the next tile in memory.
+  CUTLASS_HOST_DEVICE
+  RegularTileAccessIterator operator++(int) {
+    RegularTileAccessIterator prev(*this);
+    this->operator++();
+
+    return prev;
   }
 
-  /// Store a fragment to memory
+  /// Adds a tile offset
   CUTLASS_DEVICE
-  void store(Fragment const &frag) {
-    store_with_pointer_offset(frag, 0);
+  void add_tile_offset(TensorCoord const &coord) {
+    add_pointer_offset(coord.contiguous() * sections_per_stage_ * stride_ *
+                           ThreadMap::kElementsPerAccess / sections_ +
+                       coord.strided() * Shape::kStrided * stride_ *
+                           Layout::kElementsPerAccess / Layout::kFactor);
   }
 };
 
 ////////////////////////////////////////////////////////////////////////////////
 
-/// Specialization of PredicatedTileIteratorTriangularMatrix for row-major data.
+/// Tile Iterator specialized for column-major crosswise TensorOp formats.
 ///
-/// Satisfies: ForwardTileIteratorConcept | 
-///            ReadableContiguousTileIteratorConcept | 
-///            WriteableContiguousTileIteratorConcept |
-///            MaskedTileIteratorConcept
-///
-template <
-  typename Shape_,
-  typename Element_,
-  int AdvanceRank,
-  typename ThreadMap_,
-  SideMode kSideMode, 
-  FillMode kFillMode, 
-  DiagType kDiagType,
-  int AccessSize
->
-class PredicatedTileIteratorTriangularMatrix<Shape_, Element_, layout::RowMajor, AdvanceRank, ThreadMap_, 
-                                            kSideMode, kFillMode, kDiagType,
-                                            AccessSize> {
-public:
-
-  static_assert(AdvanceRank == 0 || AdvanceRank == 1, 
-    "Specialization for pitch-linear iterator may along advance along the "
-    "contiguous(rank=0) or strided(rank=1) dimension.");
+///
+/// Satisfies: ForwardTileIteratorConcept |
+///            ReadableContiguousTileIteratorConcept |
+///            WriteableContiguousTileIteratorConcept
+///
+template <typename Shape_, typename Element_, int AdvanceRank,
+          typename ThreadMap_, int Alignment, int Crosswise>
+class RegularTileAccessIterator<
+    Shape_, Element_,
+    layout::ColumnMajorTensorOpMultiplicandCrosswise<
+        sizeof_bits<Element_>::value, Crosswise>,
+    AdvanceRank, ThreadMap_, Alignment> {
+ public:
+  static_assert(
+      AdvanceRank == 0 || AdvanceRank == 1,
+      "Specialization for column-major iterator may along advance along the "
+      "columns(rank=0) or rows(rank=1) dimension.");
 
   using Shape = Shape_;
   using Element = Element_;
-  using Layout = layout::RowMajor;
+  using Layout = layout::ColumnMajorTensorOpMultiplicandCrosswise<
+      sizeof_bits<Element_>::value, Crosswise>;
   static int const kAdvanceRank = AdvanceRank;
-  using ThreadMap = ThreadMap_;
+  static int const kAlignment = Alignment;
 
   using Index = typename Layout::Index;
   using LongIndex = typename Layout::LongIndex;
 
   using TensorRef = TensorRef<Element, Layout>;
-  using TensorView = TensorView<Element, Layout>;
   using TensorCoord = typename Layout::TensorCoord;
 
-  using Pointer = Element *;
-  using NonConstPointer = typename platform::remove_const<Element>::type *;
+  using ThreadMap = ThreadMap_;
 
-  using UnderlyingIterator = PredicatedTileIteratorTriangularMatrix<
-    layout::PitchLinearShape<Shape::kColumn, Shape::kRow>,
-    Element,
-    layout::PitchLinear,
-    (kAdvanceRank == 0 ? 1 : 0),
-    ThreadMap,
-    kSideMode, 
-    kFillMode, 
-    kDiagType,
-    AccessSize
-  >;
+  /// Underlying iterator type
+  using UnderlyingIterator = RegularTileAccessIterator<
+      layout::PitchLinearShape<Shape::kRow, Shape::kColumn>, Element,
+      layout::TensorOpMultiplicandCrosswise<sizeof_bits<Element_>::value,
+                                            Crosswise>,
+      (kAdvanceRank == 0 ? 0 : 1), ThreadMap_>;
 
   using AccessType = typename UnderlyingIterator::AccessType;
 
-  /// Fragment object to be loaded or stored
-  using Fragment = cutlass::Array<Element, ThreadMap::Iterations::kCount * ThreadMap::kElementsPerAccess>;
-
-  /// Predicate vector stores mask to guard accesses
-  using Mask = typename UnderlyingIterator::Mask;
-
-  /// Parameters object is precomputed state and is host-constructible
-  class Params {
-  private:
-
-    friend PredicatedTileIteratorTriangularMatrix;
-
-    /// Parameters object
-    typename UnderlyingIterator::Params params_;
-
-  public:
-    
-    CUTLASS_HOST_DEVICE
-    Params() { } 
-
-    /// Construct the Params object given a pitch-linear tensor's layout
-    CUTLASS_HOST_DEVICE
-    Params(Layout const &layout): params_(layout::PitchLinear(layout.stride(0))) {
-
-    };
-  };
-
-
-private:
-
-  //
-  // Data members
-  //
-
-  /// Underlying pitch-linear tile iterator
+ private:
+  /// Underlying iterator
   UnderlyingIterator iterator_;
 
-public:
+ public:
+  /// Construct a TileIterator with zero threadblock offset
+  CUTLASS_HOST_DEVICE
+  RegularTileAccessIterator(TensorRef ref,  ///< Pointer to start of tensor
+                            int thread_id   ///< ID of each participating thread
+                            )
+      : iterator_({ref.data(), ref.stride()}, thread_id) {}
 
-  /// Constructs a TileIterator from its precomputed state, threadblock offset, and thread ID
+  /// Overrides the internal iteration index
   CUTLASS_HOST_DEVICE
-  PredicatedTileIteratorTriangularMatrix(
-    Params const &params,                         ///< Precomputed parameters object 
-    Pointer pointer,                              ///< Pointer to start of tensor
-    TensorCoord extent,                           ///< Extent of tensor
-    int thread_id,                                ///< ID of each participating thread
-    TensorCoord const &threadblock_offset         ///< Initial offset of threadblock
-  ):
-    iterator_(
-      params.params_,
-      pointer,
-      layout::PitchLinearCoord(extent.column(), extent.row()),
-      thread_id,
-      layout::PitchLinearCoord(threadblock_offset.column(), threadblock_offset.row())
-    ) { }
-
-  /// Construct a PredicatedTileIteratorTriangularMatrix with zero threadblock offset
-  CUTLASS_HOST_DEVICE
-  PredicatedTileIteratorTriangularMatrix(
-    Params const &params,                         ///< Precomputed parameters object
-    Pointer pointer,                              ///< Pointer to start of tensor
-    TensorCoord extent,                           ///< Extent of tensor
-    int thread_id                                 ///< ID of each participating thread
-  ): PredicatedTileIteratorTriangularMatrix(params, pointer, extent, thread_id, make_Coord(0, 0)) { }
+  void set_iteration_index(int index) { iterator_.set_iteration_index(index); }
 
   /// Adds a pointer offset in units of Element
   CUTLASS_HOST_DEVICE
   void add_pointer_offset(LongIndex pointer_offset) {
     iterator_.add_pointer_offset(pointer_offset);
   }
 
+  /// Returns a pointer
+  CUTLASS_HOST_DEVICE
+  AccessType *get() const {
+    return reinterpret_cast<AccessType *>(iterator_.get());
+  }
+
+  /// Adds a tile offset
+  CUTLASS_DEVICE
+  void add_tile_offset(TensorCoord const &coord) {
+    iterator_.add_tile_offset({coord.row(), coord.column()});
+  }
+
   /// Advances to the next tile in memory.
-  ///
-  /// The first time this method is called, predicates are updated, and the iterator's
-  /// internal pointer is reverted to the first "steady state" tile. Subsequent calls
-  /// are lightweight and must only update the internal pointer.
   CUTLASS_HOST_DEVICE
-  PredicatedTileIteratorTriangularMatrix &operator++() {
+  RegularTileAccessIterator &operator++() {
     ++iterator_;
     return *this;
   }
 
   /// Advances to the next tile in memory.
-  ///
-  /// The first time this method is called, predicates are updated, and the iterator's
-  /// internal pointer is reverted to the first "steady state" tile. Subsequent calls
-  /// are lightweight and must only update the internal pointer.
   CUTLASS_HOST_DEVICE
-  PredicatedTileIteratorTriangularMatrix operator++(int) {
-    PredicatedTileIteratorTriangularMatrix self(*this);
-    operator++();
-    return self;
-  }
+  RegularTileAccessIterator operator++(int) {
+    RegularTileAccessIterator prev(*this);
+    ++iterator_;
 
-  /// Clears the predicate set efficiently
-  CUTLASS_HOST_DEVICE
-  void clear_mask(bool enable = true) {
-    iterator_.clear_mask(enable);
+    return prev;
   }
+};
 
-  /// Clears the predicate set efficiently
+////////////////////////////////////////////////////////////////////////////////
+
+/// Tile Iterator specialized for row-major crosswise TensorOp formats.
+///
+///
+/// Satisfies: ForwardTileIteratorConcept |
+///            ReadableContiguousTileIteratorConcept |
+///            WriteableContiguousTileIteratorConcept
+///
+template <typename Shape_, typename Element_, int AdvanceRank,
+          typename ThreadMap_, int Alignment, int Crosswise>
+class RegularTileAccessIterator<Shape_, Element_,
+                                layout::RowMajorTensorOpMultiplicandCrosswise<
+                                    sizeof_bits<Element_>::value, Crosswise>,
+                                AdvanceRank, ThreadMap_, Alignment> {
+ public:
+  static_assert(
+      AdvanceRank == 0 || AdvanceRank == 1,
+      "Specialization for row-major iterator may along advance along the "
+      "columns(rank=0) or rows(rank=1) dimension.");
+
+  using Shape = Shape_;
+  using Element = Element_;
+  using Layout = layout::RowMajorTensorOpMultiplicandCrosswise<
+      sizeof_bits<Element_>::value, Crosswise>;
+  static int const kAdvanceRank = AdvanceRank;
+  static int const kAlignment = Alignment;
+
+  using Index = typename Layout::Index;
+  using LongIndex = typename Layout::LongIndex;
+
+  using TensorRef = TensorRef<Element, Layout>;
+  using TensorCoord = typename Layout::TensorCoord;
+
+  using ThreadMap = ThreadMap_;
+
+  /// Underlying iterator type
+  using UnderlyingIterator = RegularTileAccessIterator<
+      layout::PitchLinearShape<Shape::kColumn, Shape::kRow>, Element,
+      layout::TensorOpMultiplicandCrosswise<sizeof_bits<Element_>::value,
+                                            Crosswise>,
+      (kAdvanceRank == 0 ? 1 : 0), ThreadMap_>;
+
+  using AccessType = typename UnderlyingIterator::AccessType;
+
+ private:
+  /// Underlying iterator
+  UnderlyingIterator iterator_;
+
+ public:
+  /// Construct a TileIterator with zero threadblock offset
   CUTLASS_HOST_DEVICE
-  void enable_mask() {
-    iterator_.enable_mask();
-  }
+  RegularTileAccessIterator(TensorRef ref,  ///< Pointer to start of tensor
+                            int thread_id   ///< ID of each participating thread
+                            )
+      : iterator_({ref.data(), ref.stride()}, thread_id) {}
 
-  /// Sets the predicate mask, overriding value stored in predicate iterator
+  /// Overrides the internal iteration index
   CUTLASS_HOST_DEVICE
-  void set_mask(Mask const &mask) {
-    iterator_.set_mask(mask);
-  }
+  void set_iteration_index(int index) { iterator_.set_iteration_index(index); }
 
-  /// Gets the mask
+  /// Adds a pointer offset in units of Element
   CUTLASS_HOST_DEVICE
-  void get_mask(Mask &mask) {
-    iterator_.get_mask(mask);
+  void add_pointer_offset(LongIndex pointer_offset) {
+    iterator_.add_pointer_offset(pointer_offset);
   }
 
-  /// Loads a fragment from memory
-  CUTLASS_DEVICE
-  void load_with_pointer_offset(Fragment &frag, Index pointer_offset) {
-    iterator_.load_with_pointer_offset(frag, pointer_offset);
+  /// Returns a pointer
+  CUTLASS_HOST_DEVICE
+  AccessType *get() const {
+    return reinterpret_cast<AccessType *>(iterator_.get());
   }
 
-  /// Loads a fragment from memory
+  /// Adds a tile offset
   CUTLASS_DEVICE
-  void load_with_byte_offset(Fragment &frag, LongIndex byte_offset) {
-    iterator_.load_with_byte_offset(frag, byte_offset);
+  void add_tile_offset(TensorCoord const &coord) {
+    iterator_.add_tile_offset({coord.column(), coord.row()});
   }
 
-  /// Loads a fragment from memory
-  CUTLASS_DEVICE
-  void load(Fragment &frag) {
-    load_with_pointer_offset(frag, 0);
+  /// Advances to the next tile in memory.
+  CUTLASS_HOST_DEVICE
+  RegularTileAccessIterator &operator++() {
+    ++iterator_;
+    return *this;
   }
 
-  /// Store a fragment to memory
-  CUTLASS_DEVICE
-  void store_with_pointer_offset(Fragment const &frag, Index pointer_offset) {
-    iterator_.store_with_pointer_offset(frag, pointer_offset);
-  }
-  
-  /// Store a fragment to memory
-  CUTLASS_DEVICE
-  void store_with_byte_offset(Fragment const &frag, LongIndex byte_offset) {
-    iterator_.store_with_byte_offset(frag, byte_offset);
-  }
+  /// Advances to the next tile in memory.
+  CUTLASS_HOST_DEVICE
+  RegularTileAccessIterator operator++(int) {
+    RegularTileAccessIterator prev(*this);
+    ++iterator_;
 
-  /// Store a fragment to memory
-  CUTLASS_DEVICE
-  void store(Fragment const &frag) {
-    store_with_pointer_offset(frag, 0);
+    return prev;
   }
 };
 
 ////////////////////////////////////////////////////////////////////////////////
 
-} // namespace threadblock
-} // namespace transform
-} // namespace cutlass
+}  // namespace threadblock
+}  // namespace transform
+}  // namespace cutlass
 
 ////////////////////////////////////////////////////////////////////////////////
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/transform/threadblock/predicated_vector_access_iterator.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/host/symm_complex.h`

 * *Files 26% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -24,394 +24,296 @@
  * DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR
  * SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER
  * CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,
  * OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
  * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
  *
  **************************************************************************************************/
-
 /*! \file
-    \brief Templates implementing computing the addresses of loading small
-    vectors from the global memory.
+    \brief Reference implementation for complex-valued SYMM update in host-side code.
+
+    
 */
 
 #pragma once
 
-#include "cutlass/cutlass.h"
-#include "cutlass/array.h"
-#include "cutlass/coord.h"
-#include "cutlass/layout/pitch_linear.h"
-#include "cutlass/layout/matrix.h"
-#include "cutlass/layout/tensor.h"
-#include "cutlass/matrix_coord.h"
-#include "cutlass/matrix_shape.h"
-#include "cutlass/tensor_ref.h"
-
-////////////////////////////////////////////////////////////////////////////////
+#include "cutlass/blas3.h"
+#include "cutlass/complex.h"
+#include "cutlass/numeric_conversion.h"
+#include "cutlass/tensor_view.h"
+#include "cutlass/gemm/gemm.h"
+#include <assert.h>
 
 namespace cutlass {
-namespace transform {
-namespace threadblock {
+namespace reference {
+namespace host {
 
-////////////////////////////////////////////////////////////////////////////////
+////////////////////////////////////////////////////////////////////////////////////////////////////
 
-/// PredicatedVectorAccessIterator
+/// Computes a general matrix product among matrices (tensors of rank=2) pointed to by TensorRef
+/// objects.
 ///
+/// Explicitly naming types needed by this template can be cumbersome, particularly for the
+/// accumulator type, so a function argument 'initial_accum' is exposed. Passing
+/// AccumulatorType(0) as the last function argument can be easier than naming all template
+/// arguments explicitly.
 template <
-    /// Shape of the vector accessed by the entire threadblock
-    typename Shape,
-    /// Shape of the vector accessed by the warp
-    typename WarpShape,
-    /// Type of Element
-    typename Element,
-    /// Layout of the vector
-    typename Layout,
-    /// Number of elements for each access
-    int ElementsPerAccess,
-    /// Support residual tile
-    bool EnableResidualAccess = false
+  typename ElementA,
+  typename LayoutA,
+  SideMode SideModeA,
+  FillMode FillModeA,
+  typename ElementB,
+  typename LayoutB,
+  typename ElementC,
+  typename LayoutC,
+  typename ScalarType,
+  typename ComputeType,
+  BlasMode BlasMode_ = BlasMode::kSymmetric,
+  typename InnerProductOp = multiply_add<ComputeType>,
+  typename ConvertOp = NumericConverter<ElementC, ScalarType>
 >
-class PredicatedVectorAccessIterator;
+void compute_symm_complex(
+  gemm::GemmCoord problem_size,
+  ScalarType alpha,
+  TensorRef<ElementA, LayoutA> tensor_a,
+  TensorRef<ElementB, LayoutB> tensor_b,
+  ScalarType beta,
+  TensorRef<ElementC, LayoutC> tensor_c,
+  TensorRef<ElementC, LayoutC> tensor_d,
+  ComputeType initial_accum,
+  int batch_count = 1,
+  int64_t batch_stride_A = 0,
+  int64_t batch_stride_B = 0,
+  int64_t batch_stride_C = 0,
+  int64_t batch_stride_D = 0) {
+  
+  static SideMode const kSideModeA = SideModeA;
+  static FillMode const kFillModeA = FillModeA;
+  static BlasMode const kBlasMode  = BlasMode_;
+
+  static_assert(
+    LayoutA::kRank == 2 &&
+    LayoutB::kRank == 2 &&
+    LayoutC::kRank == 2, "Tensors must be of rank 2");
+
+  static_assert(kSideModeA != SideMode::kInvalid
+                , "Side Mode can either be Left or Right.");
+
+  static_assert(
+    kFillModeA == FillMode::kLower || 
+    kFillModeA == FillMode::kUpper, 
+    "Fill Mode can either be Lower or Upper.");
+
+  using CompareOp_w_diag =  typename TrMatrixCompareOp<kFillModeA, DiagType::kNonUnit>::Type;
+  using CompareOp_wo_diag = typename TrMatrixCompareOp<kFillModeA, DiagType::kZero>::Type;
+
+  // Note: batch is ignored.
+  int const M = problem_size.m();
+  int const N = problem_size.n();
+  // Assuming correct k-dimension value is passed
+  int const K = problem_size.k();
+
+  // Blocking necessary to speedup reference implementation
+  int const Mblock = 16;
+  int const Nblock = 16;
+
+  ConvertOp convert_op;
+  InnerProductOp inner_product_op;
+  CompareOp_w_diag compare_op_1;
+  CompareOp_wo_diag compare_op_2;
+
+  for (int batch_idx = 0; batch_idx < batch_count; ++batch_idx) {
+
+    // Compute matrix product using blocks
+    for (int row_block = 0; row_block < M; row_block += Mblock) {
+      for (int col_block = 0; col_block < N; col_block += Nblock) {
+
+        ComputeType accum[Mblock][Nblock];
+
+        for (int j = 0; j < Nblock; j++) {
+          for (int i = 0; i < Mblock; i++) {
+            accum[i][j] = initial_accum;
+          }
+        }
+
+        for (int k_block = 0; k_block < K; ++k_block) {
+          for (int j = 0; j < Nblock; j++) {
+            for (int i = 0; i < Mblock; i++) {
+              int row = row_block + i;
+              int col = col_block + j;
+
+              if (row < M && col < N) 
+              {
+                ElementA a_1 = ElementA();
+                ElementB b_1 = ElementB();
+                ElementA a_2 = ElementA();
+                ElementB b_2 = ElementB();
+                
+                // A x B or B x A (with diagonal)
+                if (kSideModeA == SideMode::kLeft) {
+                  a_1 = (compare_op_1(row, k_block)) ? 
+                        (tensor_a.at(MatrixCoord(row, k_block))) : ElementA();
+                  b_1 = tensor_b.at(MatrixCoord(k_block, col));
+                } else if (kSideModeA == SideMode::kRight) {
+                  a_1 = tensor_b.at(MatrixCoord(row, k_block));
+                  b_1 = (compare_op_1(k_block, col)) ? 
+                        tensor_a.at(MatrixCoord(k_block, col)) : ElementA();
+                }
+                ComputeType compute_a_1 = ComputeType(a_1);
+                ComputeType compute_b_1 = ComputeType(b_1);
+
+                // The imaginary parts of the diagonal elements of 
+                // a complex data type are assumed and set to zero
+                if (kBlasMode == BlasMode::kHermitian && kSideModeA == SideMode::kLeft && row == k_block) {
+                  compute_a_1 = real(compute_a_1);
+                } else if (kBlasMode == BlasMode::kHermitian && kSideModeA == SideMode::kRight && k_block == col) {
+                  compute_b_1 = real(compute_b_1);
+                }
+
+                accum[i][j] = inner_product_op(compute_a_1, compute_b_1,  accum[i][j]);
+
+                // A^T x B or B x A^T (without diagonal)
+                if (kSideModeA == SideMode::kLeft) {
+                  a_2 = (compare_op_2(k_block, row)) ? 
+                        (tensor_a.at(MatrixCoord(k_block, row))) : ElementA();
+                  b_2 = tensor_b.at(MatrixCoord(k_block, col));
+                  if (kBlasMode == BlasMode::kHermitian)
+                    a_2 = conj(a_2);
+                } else if (kSideModeA == SideMode::kRight) {
+                  a_2 = tensor_b.at(MatrixCoord(row, k_block));
+                  b_2 = (compare_op_2(col, k_block)) ? 
+                        tensor_a.at(MatrixCoord(col, k_block)) : ElementA();
+                  if (kBlasMode == BlasMode::kHermitian)
+                    b_2 = conj(b_2);
+                }
+
+                ComputeType compute_a_2 = ComputeType(a_2);
+                ComputeType compute_b_2 = ComputeType(b_2);
+
+                accum[i][j] = inner_product_op(compute_a_2, compute_b_2, accum[i][j]);
+              }
+            }
+          }
+        }
+
+        for (int j = 0; j < Nblock; j++) {
+          for (int i = 0; i < Mblock; i++) {
+            int row = row_block + i;
+            int col = col_block + j;
+
+            MatrixCoord coord = MatrixCoord(row, col);
+
+            if (row < M && col < N) {
+
+              ScalarType c = tensor_c.at(coord);
+
+              tensor_d.at(coord) = convert_op(
+                alpha * ScalarType(accum[i][j]) + 
+                beta * c);
+            }
+          }
+        }
+
+      } // for (col_block)
+    } // for (row_block)
+
+    tensor_a.add_pointer_offset(batch_stride_A);
+    tensor_b.add_pointer_offset(batch_stride_B);
+    tensor_c.add_pointer_offset(batch_stride_C);
+    tensor_d.add_pointer_offset(batch_stride_D);
 
-////////////////////////////////////////////////////////////////////////////////
+  } // for (batch_idx)
+}
 
-/// Vector access iterator specialized for vectors, e.g. scale and bias
-/// Thread arrangements are for TensorOps
-///
-template <
-  typename Shape_, 
-  typename WarpShape_, 
-  typename Element_, 
-  int ElementsPerAccess, 
-  bool EnableResidualAccess
->
-class PredicatedVectorAccessIterator <
-  Shape_,
-  WarpShape_,
-  Element_,
-  layout::PitchLinear,
-  ElementsPerAccess,
-  EnableResidualAccess
-> {
-  public:
-
-  using Shape = Shape_;
-  using WarpShape = WarpShape_;
-  using Element = Element_;
-  using Layout = layout::PitchLinear;
-
-  using Index = typename Layout::Index;
-  using LongIndex = typename Layout::LongIndex;
-
-  using TensorRef = TensorRef<Element, Layout>;
-  using TensorView = TensorView<Element, Layout>;
-  using TensorCoord = typename Layout::TensorCoord;
-
-  using ConstPointer = const Element *;
-  using NonConstPointer = typename platform::remove_const<Element>::type *;
-
-//  static int const kElementsPerAccess = 128 / sizeof_bits<Element>::value;
-  static int const kElementsPerAccess = ElementsPerAccess;
-  static int const kThreads = 32;
-  static int const kRowsPerIteration = 8;
-  static int const kThreadsPerRow = kThreads / kRowsPerIteration;
-  static int const kThreadsPerRowMask = 0x3;
-  static int const kIterations = WarpShape::kContiguous / (kThreadsPerRow * kElementsPerAccess); 
-  static int const kWarpCountStrided = Shape::kStrided / WarpShape::kStrided;
-
-  using AccessType = AlignedArray<Element, kElementsPerAccess>;
-
- private:
-  /// Internal pointer type permits fast address arithmetic
-  using BytePointer = char *;
-
- private:
-  //
-  // Data members
-  //
-
-  /// Internal pointer to first access of tile
-  BytePointer pointer_;
-
-  /// Extent of tensor
-  TensorCoord extent_;
-
-  /// pointer offset of each thread
-  TensorCoord thread_offset_;
-
-  /// iteration index
-  LongIndex iteration_;
-
-  /// residual access
-  bool is_residual_;
-
-  /// residual offset of each thread
-  TensorCoord residual_offset_;
-
- public:
-  /// Constructs a vector access iterator
-  CUTLASS_HOST_DEVICE
-  PredicatedVectorAccessIterator(
-    /// Pointer to the start of the vector
-    ConstPointer pointer,
-    /// Extent of vector
-    TensorCoord extent,
-    /// ID of each participating thread
-    int thread_id,
-    /// ID of each participating warp
-    int warp_id,
-    /// Initial offset of threadblock
-    TensorCoord const &threadblock_offset)
-    : pointer_(reinterpret_cast<BytePointer>(
-                       const_cast<NonConstPointer>(pointer))),
-      extent_(extent),
-      is_residual_(false) {
-
-
-    int warp_offset = (warp_id / kWarpCountStrided) * WarpShape::kContiguous;
-
-    // Per-thread offset in logical coordinates of tensor
-
-    thread_offset_ = threadblock_offset + TensorCoord(warp_offset, 0) +
-        TensorCoord((thread_id & kThreadsPerRowMask) * kElementsPerAccess, 0);
-
-    set_iteration_index(0);
-
-    if(EnableResidualAccess) {
-      // compute residual offset
-      typename TensorCoord::Index residual_size = extent_.contiguous() % WarpShape::kContiguous;
-      if (residual_size) {
-        is_residual_ = true;
-        residual_offset_ = make_Coord(residual_size, 0);
-      }
-    }
-  }
-
-  /// Construct a PredicatedVectorAccessIterator with zero threadblock offset
-  CUTLASS_HOST_DEVICE
-  PredicatedVectorAccessIterator(
-    /// Pointer to start of vector
-    ConstPointer pointer,
-    /// Extent of vector
-    TensorCoord extent,
-    ///< ID of each participating thread
-    int thread_id,
-    /// ID of each participating warp
-    int warp_id)
-    : PredicatedVectorAccessIterator(pointer, extent, thread_id, warp_id,
-                                     make_Coord(0, 0)) {}
-
-
-  /// Overrides the internal iteration index
-  CUTLASS_HOST_DEVICE
-  void set_iteration_index(int index) {
-    iteration_ = index;
-  }
+////////////////////////////////////////////////////////////////////////////////////////////////////
 
-  /// Advances an iterator along logical dimensions of matrix in units of whole tiles
-  CUTLASS_DEVICE
-  void add_tile_offset(
-      TensorCoord const &tile_offset) {
-
-    thread_offset_ =
-        thread_offset_ +
-        TensorCoord(WarpShape::kContiguous * tile_offset.contiguous(), 0);
-  }
-
-  /// Returns a pointer
-  CUTLASS_HOST_DEVICE
-  AccessType *get() const {
-
-    return reinterpret_cast<AccessType *>(
-        pointer_ +
-        ((thread_offset_.contiguous() + iteration_ * kThreadsPerRow * kElementsPerAccess) 
-        * sizeof_bits<Element>::value / 8));
-  }
-
-  /// Increment and return an instance to self.
-  CUTLASS_HOST_DEVICE
-  PredicatedVectorAccessIterator &operator++() {
-    ++iteration_;
-    if(iteration_ >= kIterations)
-      iteration_ = 0; 
-
-    return *this;
-  }
-
-  /// Increment and return an instance to self.
-  CUTLASS_HOST_DEVICE
-  void advance() {
-    if(EnableResidualAccess && is_residual_) {
-      is_residual_ = false;
-      thread_offset_ += residual_offset_; 
-    }
-    else
-      add_tile_offset(TensorCoord(1, 0));
-  }
-
-  /// Increment and return an instance to self.
-  CUTLASS_HOST_DEVICE
-  PredicatedVectorAccessIterator operator++(int) {
-    PredicatedVectorAccessIterator self(*this);
-    operator++();
-    return self;
-  }
-
-  /// Returns whether access is valid or not
-  CUTLASS_HOST_DEVICE
-  bool valid() {
-    return ((thread_offset_.contiguous() + 
-              iteration_ * kThreadsPerRow * kElementsPerAccess) < extent_.contiguous());
-  }
-};
-
-////////////////////////////////////////////////////////////////////////////////
-
-/// Specialization of PredicatedVectorAccessIterator for row-major data.
-///
 template <
-  typename Shape_,
-  typename WarpShape_,
-  typename Element_,
-  int ElementsPerAccess,
-  bool EnableResidualAccess
+  typename ElementA,
+  typename LayoutA,
+  SideMode SideModeA,
+  FillMode FillModeA,
+  typename ElementB,
+  typename LayoutB,
+  typename ElementC,
+  typename LayoutC,
+  typename ScalarType,
+  typename ComputeType,
+  BlasMode BlasMode_ = cutlass::BlasMode::kSymmetric,
+  typename InnerProductOp = cutlass::arch::OpMultiplyAddComplex
 >
-class PredicatedVectorAccessIterator<
-  Shape_,
-  WarpShape_,
-  Element_,
-  layout::RowMajor,
-  ElementsPerAccess,
-  EnableResidualAccess
-> {
- public:
-
-  using Shape = Shape_;
-  using WarpShape = WarpShape_;
-  using Element = Element_;
-  using Layout = layout::RowMajor;
-
-  using Index = typename Layout::Index;
-  using LongIndex = typename Layout::LongIndex;
-
-  using TensorRef = TensorRef<Element, Layout>;
-  using TensorView = TensorView<Element, Layout>;
-  using TensorCoord = typename Layout::TensorCoord;
-
-  using ConstPointer = const Element *;
-  using NonConstPointer = typename platform::remove_const<Element>::type *;
-
-  using UnderlyingIterator = PredicatedVectorAccessIterator<
-      layout::PitchLinearShape<Shape::kColumn, Shape::kRow>, 
-      layout::PitchLinearShape<WarpShape::kColumn, WarpShape::kRow>, 
-      Element,
-      layout::PitchLinear,
-      ElementsPerAccess,
-      EnableResidualAccess>;
-
-  using AccessType = typename UnderlyingIterator::AccessType;
-  static int const kElementsPerAccess = UnderlyingIterator::kElementsPerAccess;
-  static int const kRowsPerIteration = UnderlyingIterator::kRowsPerIteration;
-  static int const kThreads = UnderlyingIterator::kThreads;
-  static int const kIterations = UnderlyingIterator::kIterations;
-
- private:
-  //
-  // Data members
-  //
-
-  /// Underlying pitch-linear tile iterator
-  UnderlyingIterator iterator_;
-
- public:
-  /// Constructs a TileIterator from its precomputed state, threadblock offset,
-  /// and thread ID
-  CUTLASS_HOST_DEVICE
-  PredicatedVectorAccessIterator(
-      ///< Pointer to the start of the vector
-      ConstPointer pointer,
-      ///< Extent of tensor
-      TensorCoord extent,
-      ///< ID of each participating thread
-      int thread_id,
-      ///< ID of each participating warp
-      int warp_id,
-      ///< Initial offset of threadblock
-      TensorCoord const &threadblock_offset)
-      : iterator_(pointer, layout::PitchLinearCoord(extent.column(), extent.row()),
-                  thread_id, warp_id,
-                  layout::PitchLinearCoord(threadblock_offset.column(),
-                                           threadblock_offset.row())) {}
-
-  /// Construct a PredicatedVectorAccessIterator with zero threadblock offset
-  CUTLASS_HOST_DEVICE
-  PredicatedVectorAccessIterator(
-      ConstPointer pointer,   ///< Pointer to the start of the vector
-      TensorCoord extent,     ///< Extent of tensor
-      int thread_id,          ///< ID of each participating thread
-      int warp_id             ///< ID of each participating warp
-      )
-      : PredicatedVectorAccessIterator(pointer, extent, thread_id, warp_id, 
-                                        make_Coord(0, 0)) {}
-
-  /// Overrides the internal iteration index
-  CUTLASS_HOST_DEVICE
-  void set_iteration_index(int index) { iterator_.set_iteration_index(index); }
-
-  /// Advances an iterator along logical dimensions of matrix in units of whole
-  /// tiles
-  CUTLASS_HOST_DEVICE
-  void add_tile_offset(TensorCoord const &tile_offset) {
-    iterator_.add_tile_offset({tile_offset.column(), tile_offset.row()});
-  }
-
-  /// Returns a pointer
-  CUTLASS_HOST_DEVICE
-  AccessType *get() const {
-    return reinterpret_cast<AccessType *>(iterator_.get());
-  }
+struct SymmComplex;
 
-  /// Advances to the next tile in memory.
-  ///
-  /// The first time this method is called, predicates are updated, and the
-  /// iterator's internal pointer is reverted to the first "steady state" tile.
-  /// Subsequent calls are lightweight and must only update the internal
-  /// pointer.
-  CUTLASS_HOST_DEVICE
-  PredicatedVectorAccessIterator &operator++() {
-    ++iterator_;
-    return *this;
-  }
+////////////////////////////////////////////////////////////////////////////////////////////////////
 
-  /// Advances to the next tile in memory.
-  ///
-  /// The first time this method is called, predicates are updated, and the
-  /// iterator's internal pointer is reverted to the first "steady state" tile.
-  /// Subsequent calls are lightweight and must only update the internal
-  /// pointer.
-  CUTLASS_HOST_DEVICE
-  PredicatedVectorAccessIterator operator++(int) {
-    PredicatedVectorAccessIterator self(*this);
-    operator++();
-    return self;
+/// Partial specialization for multiply-add
+template <typename ElementA, typename LayoutA,
+          SideMode SideModeA, FillMode FillModeA, 
+          typename ElementB, typename LayoutB,
+          typename ElementC, typename LayoutC,
+          typename ScalarType, typename ComputeType,
+          BlasMode BlasMode_>
+struct SymmComplex<ElementA, LayoutA, 
+                   SideModeA, FillModeA,
+                   ElementB, LayoutB,
+                   ElementC, LayoutC, ScalarType,
+                   ComputeType, BlasMode_,
+                   arch::OpMultiplyAddComplex> {
+
+  void operator()(gemm::GemmCoord problem_size, ScalarType alpha,
+                  TensorRef<ElementA, LayoutA> tensor_a,
+                  TensorRef<ElementB, LayoutB> tensor_b, ScalarType beta,
+                  TensorRef<ElementC, LayoutC> tensor_c,
+                  TensorRef<ElementC, LayoutC> tensor_d,
+                  ComputeType initial_accum = ComputeType(0)) {
+    static_assert(
+        LayoutA::kRank == 2 && LayoutC::kRank == 2,
+        "Tensors must be of rank 2");
+
+    compute_symm_complex<ElementA, LayoutA,
+                 SideModeA, FillModeA,
+                 ElementB, LayoutB,
+                 ElementC, LayoutC, 
+                 ScalarType, ComputeType, BlasMode_, multiply_add<ComputeType>>(
+                 problem_size, alpha, tensor_a, tensor_b, beta, tensor_c, tensor_d, initial_accum);
   }
+};
 
-  /// Increment and return an instance to self.
-  CUTLASS_HOST_DEVICE
-  void advance() {
-    iterator_.advance();
-  }
+////////////////////////////////////////////////////////////////////////////////////////////////////
 
-  /// Returns whether access is valid or not
-  CUTLASS_HOST_DEVICE
-  bool valid() {
-    return iterator_.valid();
+/// Partial specialization for gaussian multiply-add 
+template <typename ElementA, typename LayoutA,
+          SideMode SideModeA, FillMode FillModeA,
+          typename ElementB, typename LayoutB,
+          typename ElementC, typename LayoutC,
+          typename ScalarType, typename ComputeType,
+          BlasMode BlasMode_>
+struct SymmComplex<ElementA, LayoutA, 
+                   SideModeA, FillModeA, 
+                   ElementB, LayoutB,
+                   ElementC, LayoutC, ScalarType,
+                   ComputeType, BlasMode_,
+                   arch::OpMultiplyAddGaussianComplex> {
+
+  void operator()(gemm::GemmCoord problem_size, ScalarType alpha,
+                  TensorRef<ElementA, LayoutA> tensor_a,
+                  TensorRef<ElementB, LayoutB> tensor_b, ScalarType beta,
+                  TensorRef<ElementC, LayoutC> tensor_c,
+                  TensorRef<ElementC, LayoutC> tensor_d,
+                  ComputeType initial_accum = ComputeType(0)) {
+    static_assert(
+        LayoutA::kRank == 2 && LayoutC::kRank == 2,
+        "Tensors must be of rank 2");
+
+    compute_symm_complex<ElementA, LayoutA,
+                 SideModeA, FillModeA,
+                 ElementB, LayoutB,
+                 ElementC, LayoutC, 
+                 ScalarType, ComputeType, BlasMode_, multiply_add<ComputeType>>(
+                 problem_size, alpha, tensor_a, tensor_b, beta, tensor_c, tensor_d, initial_accum);
   }
 };
 
+////////////////////////////////////////////////////////////////////////////////////////////////////
 
-////////////////////////////////////////////////////////////////////////////////
-
-}  // namespace threadblock
-}  // namespace transform 
-}  // namespace cutlass
-
+} // namespace host
+} // namespace reference
+} // namespace cutlass
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/transform/threadblock/regular_scale_bias_vector_access_iterator.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/transform/threadblock/regular_scale_bias_vector_access_iterator.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/transform/threadblock/regular_tile_access_iterator.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/transform/threadblock/regular_tile_access_iterator.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/transform/threadblock/regular_tile_access_iterator_pitch_linear.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/transform/threadblock/regular_tile_iterator_pitch_linear_2dthreadtile.h`

 * *Files 18% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -25,384 +25,485 @@
  * SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER
  * CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,
  * OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
  * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
  *
  **************************************************************************************************/
 /*! \file
-    \brief Templates implementing computing the addresses of storing of tiles
-   from pitch-linear rank=2 tensors.
+    \brief Templates implementing loading of tiles from pitch-linear rank=2 tensors. 
+
+    This iterator uses masks to guard out-of-bounds accesses and visits the last "residue" tile
+    first, with the objective of minimizing predicate mask updates during steady-state operation.
+
+    A precomputed "Params" object minimizes the amount of state that must be stored in registers,
+    and integer addition is used to advance the pointer through memory.
 */
 
 #pragma once
 
 #include "cutlass/cutlass.h"
-#include "cutlass/array.h"
-#include "cutlass/layout/pitch_linear.h"
-#include "cutlass/layout/matrix.h"
-#include "cutlass/matrix_coord.h"
-#include "cutlass/matrix_shape.h"
 #include "cutlass/tensor_ref.h"
+#include "cutlass/layout/matrix.h"
+#include "cutlass/layout/pitch_linear.h"
 
-#include "cutlass/transform/threadblock/regular_tile_access_iterator.h"
+#include "regular_tile_iterator.h"
 
-////////////////////////////////////////////////////////////////////////////////
+/////////////////////////////////////////////////////////////////////////////////////////////////
 
 namespace cutlass {
 namespace transform {
 namespace threadblock {
 
-////////////////////////////////////////////////////////////////////////////////
-
-/// Tile iterator specialized for congruous arrangements for TensorOps
-///
-///
-/// Satisfies: ForwardTileIteratorConcept |
-///            ReadableContiguousTileIteratorConcept |
-///            WriteableContiguousTileIteratorConcept
-///
-template <typename Shape_, typename Element_, int AdvanceRank,
-          typename ThreadMap_, int Alignment>
-class RegularTileAccessIterator<
-    Shape_, Element_,
-    layout::PitchLinear,
-    AdvanceRank, ThreadMap_, Alignment> {
- public:
-  static_assert(
-      AdvanceRank == 0 || AdvanceRank == 1,
-      "Specialization for pitch-linear iterator may along advance along the "
-      "contiguous(rank=0) or strided(rank=1) dimension.");
+/////////////////////////////////////////////////////////////////////////////////////////////////
+template <
+  typename Shape,
+  typename Element,
+  typename Layout,
+  int AdvanceRank,
+  typename ThreadMap,
+  int Alignment = sizeof_bits<Element>::value * ThreadMap::kElementsPerAccess / 8
+>
+class RegularTileIterator2dThreadTile;
+
+
+/// Regular tile iterator specialized for pitch-linear + 2d thread-tiled threadmapping
+template <
+  typename Shape_,
+  typename Element_,
+  int AdvanceRank,
+  typename ThreadMap_,
+  int Alignment
+>
+class RegularTileIterator2dThreadTile<Shape_, Element_, layout::PitchLinear, AdvanceRank, ThreadMap_, Alignment> {
+public:
 
   using Shape = Shape_;
   using Element = Element_;
   using Layout = layout::PitchLinear;
   static int const kAdvanceRank = AdvanceRank;
+  using ThreadMap = ThreadMap_;
   static int const kAlignment = Alignment;
 
   using Index = typename Layout::Index;
   using LongIndex = typename Layout::LongIndex;
   using StrideIndex = typename Layout::Stride::Index;
 
   using TensorRef = TensorRef<Element, Layout>;
   using TensorCoord = typename Layout::TensorCoord;
 
-  using ThreadMap = ThreadMap_;
+  using Fragment = Array<Element, ThreadMap::Iterations::kCount * ThreadMap::ThreadAccessShape::kCount>;
+
+  static_assert(kAdvanceRank == 0 || kAdvanceRank == 1, 
+    "Advance rank may only be along the contiguous or strided dimensions.");
+
+private:
 
-  /// Element type per access
-  using AccessType = Array<Element, ThreadMap::kElementsPerAccess>;
+  //
+  // Types
+  //
+  
+  using AccessType = AlignedArray<Element, ThreadMap::ThreadAccessShape::kCount, kAlignment>;
 
- private:
   //
   // Data members
   //
 
-  /// Stride value
+  /// Pointer to memory
+  uint8_t *pointer_;
+
+  /// Stride quantity
   StrideIndex stride_;
 
-  /// Internal pointer to first access of tile
-  AccessType *pointer_;
+  /// Amount to increment pointer along strided dimension
+  LongIndex increment_strided_;
 
-  /// Internal byte offset
-  Index byte_offset_;
+  /// Amount to advance pointer between tiles
+  LongIndex increment_advance_;
 
-  /// Iteration in the contiguous dimension
-  int iteration_contiguous_;
+public:
 
-  /// Iteration in the strided dimension
-  int iteration_strided_;
+  CUTLASS_DEVICE
+  RegularTileIterator2dThreadTile(): pointer_(nullptr), increment_strided_(0), increment_advance_(0) { }
 
- public:
-  /// Construct a TileIterator with zero threadblock offset
-  CUTLASS_HOST_DEVICE
-  RegularTileAccessIterator(TensorRef ref,  ///< Pointer to start of tensor
-                            int thread_id   ///< ID of each participating thread
-                            )
-      : stride_(ref.stride(0) / ThreadMap::kElementsPerAccess),
-        byte_offset_(0) {
+  CUTLASS_DEVICE
+  RegularTileIterator2dThreadTile(
+    TensorRef const &ref, 
+    int thread_idx,
+    int interleave
+  ){ 
+    
+    TensorCoord t = ThreadMap::initial_offset(thread_idx);
+    long int offset = t[0] * interleave + t[1] * ref.stride()[0]/interleave;
+    pointer_ = reinterpret_cast<uint8_t *>(ref.data() + offset);
+
+    stride_ = ref.stride()[0] / interleave;
+    increment_strided_ = (ref.stride()[0] * sizeof_bits<Element>::value / 8) * ThreadMap::Delta::kStrided / interleave;
+
+    increment_advance_ = 
+      (kAdvanceRank == 0 ? 
+        Shape::kContiguous * sizeof_bits<Element>::value / 8 : 
+        Shape::kStrided * (ref.stride()[0] * sizeof_bits<Element>::value / 8) / interleave);
+  }
+
+  /// Loads a fragment
+  CUTLASS_DEVICE
+  void load_with_pointer_offset(Fragment &frag, Index pointer_offset) {
+
+    AccessType *frag_ptr = reinterpret_cast<AccessType *>(&frag);
+    uint8_t const *byte_pointer = pointer_ + pointer_offset * sizeof_bits<Element>::value / 8;
+
+    CUTLASS_PRAGMA_UNROLL
+    for (int s = 0; s < ThreadMap::Iterations::kStrided; ++s) {
+
+      AccessType const *access_ptr = reinterpret_cast<AccessType const *>(byte_pointer);
 
-    layout::PitchLinearCoord thread_offset_base = ThreadMap::initial_offset(thread_id);
+      CUTLASS_PRAGMA_UNROLL
+      for (int c = 0; c < ThreadMap::Iterations::kContiguous; ++c) {
 
-    // initialize pointer
-    pointer_ = reinterpret_cast<AccessType *>(ref.data() + ref.offset(thread_offset_base));
+          int idx = c + s * ThreadMap::Iterations::kContiguous;
+           frag_ptr[idx] = access_ptr[c * ThreadMap::Delta::kContiguous / ThreadMap::ThreadAccessShape::kStrided];
+        }
 
-    set_iteration_index(0);
+      if (s + 1 < ThreadMap::Iterations::kStrided) {
+        byte_pointer += increment_strided_;
+      }
+    }
   }
 
-  /// Overrides the internal iteration index
+  /// Loads a fragment
   CUTLASS_HOST_DEVICE
-  void set_iteration_index(int index) {
-    iteration_contiguous_ = index % ThreadMap::Iterations::kContiguous;
-    iteration_strided_ = index / ThreadMap::Iterations::kContiguous;
+  void load(Fragment &frag, TensorCoord const & tile_offset) {
+    load_with_pointer_offset(
+      frag, 
+      tile_offset.contiguous() * Shape::kContiguous / ThreadMap::kElementsPerAccess + 
+        tile_offset.strided() * Shape::kStrided * stride_
+    );
   }
 
-  /// Adds a pointer offset in units of Element
+  /// Loads a fragment
   CUTLASS_HOST_DEVICE
-  void add_pointer_offset(LongIndex pointer_offset) {
-    byte_offset_ += pointer_offset * sizeof(Element);
+  void load(Fragment &frag) {
+    load_with_pointer_offset(frag, 0);
   }
 
-  /// Returns a pointer
-  CUTLASS_DEVICE
-  AccessType *get() const {
+  /// Stores a fragment
+  CUTLASS_HOST_DEVICE
+  void store_with_pointer_offset(Fragment const &frag, Index pointer_offset) {
 
-    AccessType *access_ptr = pointer_;
+    AccessType const *frag_ptr = reinterpret_cast<AccessType const*>(&frag);
+    uint8_t *byte_pointer = pointer_ + pointer_offset * sizeof_bits<Element>::value / 8;
 
-    int access_offset = iteration_strided_ * ThreadMap::Delta::kStrided * stride_ +
-                        iteration_contiguous_ * ThreadMap::Delta::kContiguous /
-                            ThreadMap::kElementsPerAccess;
+    CUTLASS_PRAGMA_UNROLL
+    for (int s = 0; s < ThreadMap::Iterations::kStrided; ++s) {
 
-    char *access_byte_ptr =
-        reinterpret_cast<char *>(access_ptr + access_offset);
+      AccessType *access_ptr = reinterpret_cast<AccessType *>(byte_pointer);
 
-    return reinterpret_cast<AccessType *>(access_byte_ptr + byte_offset_);
-  }
+      CUTLASS_PRAGMA_UNROLL
+      for (int c = 0; c < ThreadMap::Iterations::kContiguous; ++c) {
 
-  /// Advances to the next tile in memory.
-  CUTLASS_HOST_DEVICE
-  RegularTileAccessIterator &operator++() {
-    ++iteration_contiguous_;
+          int idx = c + s * ThreadMap::Iterations::kContiguous;
+          access_ptr[c * ThreadMap::Delta::kContiguous / ThreadMap::ThreadAccessShape::kStrided] = frag_ptr[idx];
+      }
 
-    if (iteration_contiguous_ < ThreadMap::Iterations::kContiguous)
-      return *this;
+      if (s + 1 < ThreadMap::Iterations::kStrided) {
+        byte_pointer += increment_strided_;
+      }
+    }
+  }
 
-    // Enter here only if (iteration_contiguous_ ==
-    // ThreadMap::Iteration::kContiguous)
-    iteration_contiguous_ = 0;
-    ++iteration_strided_;
+  /// Stores a fragment
+  CUTLASS_HOST_DEVICE
+  void store(Fragment const &frag, TensorCoord const & tile_offset) {
+    store_with_pointer_offset(
+      frag,
+      tile_offset.contiguous() * Shape::kContiguous + tile_offset.strided() * Shape::kStrided * stride_
+    );
+  }
 
-    if (iteration_strided_ < ThreadMap::Iterations::kStrided) {
-      return *this;
-    }
+  /// Stores a fragment
+  CUTLASS_HOST_DEVICE
+  void store(Fragment const &frag) {
+    store_with_pointer_offset(frag, 0);
+  }
 
-    // Enter here only if (iteration_stride_ == ThreadMap::Iteration::kStrided)
-    // which means we enter the next tile.
-    iteration_strided_ = 0;
+  /// Advances the pointer
+  CUTLASS_HOST_DEVICE
+  RegularTileIterator2dThreadTile &operator++() {
+    pointer_ += increment_advance_;
+    return *this;
+  }
 
+  /// Advances the pointer
+  CUTLASS_HOST_DEVICE
+  RegularTileIterator2dThreadTile &operator--() {
+    pointer_ -= increment_advance_;
     return *this;
   }
 
-  /// Advances to the next tile in memory.
+  /// Adds a pointer offset in units of Element
   CUTLASS_HOST_DEVICE
-  RegularTileAccessIterator operator++(int) {
-    RegularTileAccessIterator prev(*this);
-    this->operator++();
-
-    return prev;
-  }
-
-  /// Adds a tile offset in the unit of tile.
-  /// In GEMM/Conv implementation, this is used to move in the k dimension in the shared memory.
-  /// Below layouts are the shared memory layouts.  Current SM50 SIMT kernels only use col major A and row major B.
-  ///   For row major A operand, k dimension is contiguous dimension;
-  ///   For col major A operand, k dimension is strided dimension;
-  ///   For row major B operand, k dimension is strided dimension;
-  ///   For col major B operand, k dimension is contiguous dimension.
-  /// Below two classes map col/row major to the pitch linear coordinates used
-  /// in this base class.
+  void add_pointer_offset(LongIndex pointer_offset) {
+    pointer_ += pointer_offset;
+  }
+
+  /// Adds a tile offset
   CUTLASS_DEVICE
   void add_tile_offset(TensorCoord const &coord) {
-    add_pointer_offset(coord.contiguous() * Shape::kContiguous +
-                       coord.strided() * Shape::kStrided * stride_ *
-                           ThreadMap::kElementsPerAccess);
+    int offset = sizeof_bits<Element>::value *
+        (coord.contiguous() * Shape::kContiguous + coord.strided() * Shape::kStrided * stride_) / 8;
+    add_pointer_offset(offset);
   }
+
 };
 
-////////////////////////////////////////////////////////////////////////////////
+/////////////////////////////////////////////////////////////////////////////////////////////////
 
-/// Tile iterator specialized for column major layouts
-///
-///
-/// Satisfies: ForwardTileIteratorConcept |
-///            ReadableContiguousTileIteratorConcept |
-///            WriteableContiguousTileIteratorConcept
-///
-template <typename Shape_, typename Element_, int AdvanceRank,
-          typename ThreadMap_, int Alignment>
-class RegularTileAccessIterator<
-    Shape_, Element_,
-    layout::ColumnMajor,
-    AdvanceRank, ThreadMap_, Alignment> {
- public:
-  static_assert(
-      AdvanceRank == 0 || AdvanceRank == 1,
-      "Specialization for pitch-linear iterator may along advance along the "
-      "contiguous(rank=0) or strided(rank=1) dimension.");
+/// Regular tile iterator specialized for interleaved layout + 2d thread-tiled threadmapping
+template <
+  typename Shape_,
+  typename Element_,
+  int AdvanceRank,
+  typename ThreadMap_,
+  int Alignment
+>
+class RegularTileIterator2dThreadTile<Shape_, Element_, layout::RowMajorInterleaved<4>, AdvanceRank, ThreadMap_, Alignment> {
+public:
 
   using Shape = Shape_;
   using Element = Element_;
-  using Layout = layout::ColumnMajor;
+  using Layout = layout::RowMajorInterleaved<4>;
   static int const kAdvanceRank = AdvanceRank;
+  using ThreadMap = ThreadMap_;
   static int const kAlignment = Alignment;
 
   using Index = typename Layout::Index;
   using LongIndex = typename Layout::LongIndex;
 
   using TensorRef = TensorRef<Element, Layout>;
   using TensorCoord = typename Layout::TensorCoord;
 
-  using ThreadMap = ThreadMap_;
+  using Fragment = Array<Element, ThreadMap::Iterations::kCount * ThreadMap::ThreadAccessShape::kCount>;
 
-  /// Underlying iterator type
-  using UnderlyingIterator = RegularTileAccessIterator<
-      layout::PitchLinearShape<Shape::kRow, Shape::kColumn>, Element,
-      layout::PitchLinear,
-      (kAdvanceRank == 0 ? 0 : 1), 
-      ThreadMap_>;
+  using Underlying = RegularTileIterator2dThreadTile<
+    layout::PitchLinearShape<Shape::kColumn, Shape::kRow>,
+    Element,
+    layout::PitchLinear,
+    (kAdvanceRank == 0 ? 1 : 0),
+    ThreadMap,
+    kAlignment
+  >;
+
+  static_assert(kAdvanceRank == 0 || kAdvanceRank == 1, 
+    "Advance rank may only be along the row or column dimensions.");
 
-  using AccessType = typename UnderlyingIterator::AccessType;
+private:
 
- private:
+  Underlying iterator_;
 
-  /// Underlying iterator
-  UnderlyingIterator iterator_;
+public:
+
+  CUTLASS_DEVICE
+  RegularTileIterator2dThreadTile() { }
 
- public:
-  /// Construct a TileIterator with zero threadblock offset
+  CUTLASS_DEVICE
+  RegularTileIterator2dThreadTile(
+    TensorRef const &ref, 
+    int thread_idx
+  ):
+    iterator_({ref.data(), ref.stride()}, thread_idx, 4) {
+
+  }
+
+  /// Loads a fragment
   CUTLASS_HOST_DEVICE
-  RegularTileAccessIterator(TensorRef ref,  ///< Pointer to start of tensor
-                            int thread_id   ///< ID of each participating thread
-                            )
-      : iterator_({ref.data(), ref.stride()}, thread_id) {}
+  void load_with_pointer_offset(Fragment &frag, Index pointer_offset) {
+    iterator_.load_with_pointer_offset(frag, pointer_offset);
+  }
 
-  /// Overrides the internal iteration index
+  /// Loads a fragment
   CUTLASS_HOST_DEVICE
-  void set_iteration_index(int index) { iterator_.set_iteration_index(index); }
+  void load(Fragment &frag, TensorCoord const & tile_offset) {
+    iterator_.load_with_pointer_offset(frag, {tile_offset.column(), tile_offset.row()});
+  }
 
-  /// Adds a pointer offset in units of Element
+  /// Loads a fragment
   CUTLASS_HOST_DEVICE
-  void add_pointer_offset(LongIndex pointer_offset) {
-    iterator_.add_pointer_offset(pointer_offset);
+  void load(Fragment &frag) {
+    iterator_.load_with_pointer_offset(frag, 0);
   }
 
-  /// Returns a pointer
+  /// Stores a fragment
   CUTLASS_HOST_DEVICE
-  AccessType *get() const {
-    return reinterpret_cast<AccessType *>(iterator_.get());
+  void store_with_pointer_offset(Fragment const &frag, Index pointer_offset) {
+    iterator_.store_with_pointer_offset(frag, pointer_offset);
   }
 
-  /// Adds a tile offset
-  CUTLASS_DEVICE
-  void add_tile_offset(TensorCoord const &coord) {
-    iterator_.add_tile_offset({coord.row(), coord.column()});
+  /// Stores a fragment
+  CUTLASS_HOST_DEVICE
+  void store(Fragment const &frag, TensorCoord const & tile_offset) {
+    iterator_.store_with_pointer_offset(frag, {tile_offset.column(), tile_offset.row()});
+  }
+
+  /// Stores a fragment
+  CUTLASS_HOST_DEVICE
+  void store(Fragment const &frag) {
+    iterator_.store_with_pointer_offset(frag, 0);
   }
 
-  /// Advances to the next tile in memory.
+  /// Advances the pointer
   CUTLASS_HOST_DEVICE
-  RegularTileAccessIterator &operator++() {
+  RegularTileIterator2dThreadTile &operator++() {
     ++iterator_;
     return *this;
   }
 
-  /// Advances to the next tile in memory.
+  /// Advances the pointer
   CUTLASS_HOST_DEVICE
-  RegularTileAccessIterator operator++(int) {
-    RegularTileAccessIterator prev(*this);
-    ++iterator_;
+  RegularTileIterator2dThreadTile &operator--() {
+    --iterator_;
+    return *this;
+  }
 
-    return prev;
+  /// Adds a pointer offset in units of Element
+  CUTLASS_HOST_DEVICE
+  void add_pointer_offset(LongIndex pointer_offset) {
+    iterator_.add_pointer_offset(pointer_offset);
   }
-};
 
+  /// Adds a tile offset
+  CUTLASS_DEVICE
+  void add_tile_offset(TensorCoord const &coord) {
+    iterator_.add_tile_offset({coord.column(), coord.row()});
+  }
 
-////////////////////////////////////////////////////////////////////////////////
+};
 
-/// Tile iterator specialized for row major layouts
-///
-///
-/// Satisfies: ForwardTileIteratorConcept |
-///            ReadableContiguousTileIteratorConcept |
-///            WriteableContiguousTileIteratorConcept
-///
-template <typename Shape_, typename Element_, int AdvanceRank,
-          typename ThreadMap_, int Alignment>
-class RegularTileAccessIterator<
-    Shape_, Element_,
-    layout::RowMajor,
-    AdvanceRank, ThreadMap_, Alignment> {
- public:
-  static_assert(
-      AdvanceRank == 0 || AdvanceRank == 1,
-      "Specialization for pitch-linear iterator may along advance along the "
-      "contiguous(rank=0) or strided(rank=1) dimension.");
+/////////////////////////////////////////////////////////////////////////////////////////////////
+
+/// Regular tile iterator specialized for interleaved layout + 2d thread-tiled threadmapping
+template <
+  typename Shape_,
+  typename Element_,
+  int AdvanceRank,
+  typename ThreadMap_,
+  int Alignment
+>
+class RegularTileIterator2dThreadTile<Shape_, Element_, layout::ColumnMajorInterleaved<4>, AdvanceRank, ThreadMap_, Alignment> {
+public:
 
   using Shape = Shape_;
   using Element = Element_;
-  using Layout = layout::RowMajor;
+  using Layout = layout::ColumnMajorInterleaved<4>;
   static int const kAdvanceRank = AdvanceRank;
+  using ThreadMap = ThreadMap_;
   static int const kAlignment = Alignment;
 
   using Index = typename Layout::Index;
   using LongIndex = typename Layout::LongIndex;
 
   using TensorRef = TensorRef<Element, Layout>;
   using TensorCoord = typename Layout::TensorCoord;
 
-  using ThreadMap = ThreadMap_;
+  using Fragment = Array<Element, ThreadMap::Iterations::kCount * ThreadMap::ThreadAccessShape::kCount>;
+  using PitchLinearThreadMap = PitchLinearStripminedThreadMap< layout::PitchLinearShape<Shape::kRow, Shape::kColumn>, 
+                                  ThreadMap::kThreads, ThreadMap::ThreadAccessShape::kCount >;
+                        
+
+  using Underlying = RegularTileIterator2dThreadTile<
+    layout::PitchLinearShape<Shape::kRow, Shape::kColumn>,
+    Element,
+    layout::PitchLinear,
+    (kAdvanceRank == 0 ? 0 : 1),
+    ThreadMap
+  >;
 
-  /// Underlying iterator type
-  using UnderlyingIterator = RegularTileAccessIterator<
-      layout::PitchLinearShape<Shape::kColumn, Shape::kRow>, Element,
-      layout::PitchLinear,
-      (kAdvanceRank == 0 ? 1 : 0), 
-      ThreadMap_>;
+  static_assert(kAdvanceRank == 0 || kAdvanceRank == 1, 
+    "Advance rank may only be along the row or column dimensions.");
 
-  using AccessType = typename UnderlyingIterator::AccessType;
+private:
 
- private:
+  Underlying iterator_;
 
-  /// Underlying iterator
-  UnderlyingIterator iterator_;
+public:
 
- public:
-  /// Construct a TileIterator with zero threadblock offset
+  CUTLASS_DEVICE
+  RegularTileIterator2dThreadTile() { }
+
+  CUTLASS_DEVICE
+  RegularTileIterator2dThreadTile(
+    TensorRef const &ref, 
+    int thread_idx
+  ):
+    iterator_({ref.data(), ref.stride()}, thread_idx, 4) {
+
+  }
+
+  /// Loads a fragment
   CUTLASS_HOST_DEVICE
-  RegularTileAccessIterator(TensorRef ref,  ///< Pointer to start of tensor
-                            int thread_id   ///< ID of each participating thread
-                            )
-      : iterator_({ref.data(), ref.stride()}, thread_id) {}
+  void load_with_pointer_offset(Fragment &frag, Index pointer_offset) {
+    iterator_.load_with_pointer_offset(frag, pointer_offset);
+  }
 
-  /// Overrides the internal iteration index
+  /// Loads a fragment
   CUTLASS_HOST_DEVICE
-  void set_iteration_index(int index) { iterator_.set_iteration_index(index); }
+  void load(Fragment &frag, TensorCoord const & tile_offset) {
+    iterator_.load_with_pointer_offset(frag, {tile_offset.row(), tile_offset.column()});
+  }
 
-  /// Adds a pointer offset in units of Element
+  /// Loads a fragment
   CUTLASS_HOST_DEVICE
-  void add_pointer_offset(LongIndex pointer_offset) {
-    iterator_.add_pointer_offset(pointer_offset);
+  void load(Fragment &frag) {
+    iterator_.load_with_pointer_offset(frag, 0);
   }
 
-  /// Returns a pointer
+  /// Stores a fragment
   CUTLASS_HOST_DEVICE
-  AccessType *get() const {
-    return reinterpret_cast<AccessType *>(iterator_.get());
+  void store_with_pointer_offset(Fragment const &frag, Index pointer_offset) {
+    iterator_.store_with_pointer_offset(frag, pointer_offset);
   }
 
-  /// Adds a tile offset
-  CUTLASS_DEVICE
-  void add_tile_offset(TensorCoord const &coord) {
-    iterator_.add_tile_offset({coord.column(), coord.row()});
+  /// Stores a fragment
+  CUTLASS_HOST_DEVICE
+  void store(Fragment const &frag, TensorCoord const & tile_offset) {
+    iterator_.store_with_pointer_offset(frag, {tile_offset.row(), tile_offset.column()});
+  }
+
+  /// Stores a fragment
+  CUTLASS_HOST_DEVICE
+  void store(Fragment const &frag) {
+    iterator_.store_with_pointer_offset(frag, 0);
   }
 
-  /// Advances to the next tile in memory.
+  /// Advances the pointer
   CUTLASS_HOST_DEVICE
-  RegularTileAccessIterator &operator++() {
+  RegularTileIterator2dThreadTile &operator++() {
     ++iterator_;
     return *this;
   }
 
-  /// Advances to the next tile in memory.
+  /// Advances the pointer
   CUTLASS_HOST_DEVICE
-  RegularTileAccessIterator operator++(int) {
-    RegularTileAccessIterator prev(*this);
-    ++iterator_;
+  RegularTileIterator2dThreadTile &operator--() {
+    --iterator_;
+    return *this;
+  }
 
-    return prev;
+  /// Adds a pointer offset in units of Element
+  CUTLASS_HOST_DEVICE
+  void add_pointer_offset(LongIndex pointer_offset) {
+    iterator_.add_pointer_offset(pointer_offset);
   }
+
+  /// Adds a tile offset
+  CUTLASS_DEVICE
+  void add_tile_offset(TensorCoord const &coord) {
+    iterator_.add_tile_offset({coord.row(), coord.column()});
+  }
+
 };
 
-////////////////////////////////////////////////////////////////////////////////
+/////////////////////////////////////////////////////////////////////////////////////////////////
 
-}  // namespace threadblock
-}  // namespace transform
-}  // namespace cutlass
+} // namespace threadblock
+} // namespace transform
+} // namespace cutlass
 
-////////////////////////////////////////////////////////////////////////////////
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/transform/threadblock/regular_tile_access_iterator_pitch_linear_direct_conv.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/transform/threadblock/regular_tile_access_iterator_pitch_linear_direct_conv.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/transform/threadblock/regular_tile_access_iterator_tensor_op.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/epilogue/warp/tile_iterator_simt.h`

 * *Files 22% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -25,796 +25,761 @@
  * SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER
  * CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,
  * OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
  * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
  *
  **************************************************************************************************/
 /*! \file
-    \brief Templates implementing computing the addresses of storing of tiles
-   from pitch-linear rank=2 tensors.
+    \brief 
 */
 
 #pragma once
 
 #include "cutlass/array.h"
-#include "cutlass/cutlass.h"
+#include "cutlass/layout/matrix.h"
 #include "cutlass/layout/pitch_linear.h"
-#include "cutlass/layout/tensor_op_multiplicand_sm75.h"
-#include "cutlass/matrix_coord.h"
-#include "cutlass/matrix_shape.h"
-#include "cutlass/tensor_ref.h"
-#include "cutlass/transform/threadblock/regular_tile_access_iterator.h"
 
-////////////////////////////////////////////////////////////////////////////////
+#include "cutlass/epilogue/warp/simt_policy.h"
+
+#define CUTLASS_SIMT_EPILOGUE_USE_SCALAR_STORES 1
+
+/////////////////////////////////////////////////////////////////////////////////////////////////
 
 namespace cutlass {
-namespace transform {
-namespace threadblock {
+namespace epilogue {
+namespace warp {
 
-////////////////////////////////////////////////////////////////////////////////
+/////////////////////////////////////////////////////////////////////////////////////////////////
 
-/// Tile iterator specialized for congruous arrangements for TensorOps
-///
-///
-/// Satisfies: ForwardTileIteratorConcept |
-///            ReadableContiguousTileIteratorConcept |
-///            WriteableContiguousTileIteratorConcept
-///
-template <typename Shape_, typename Element_, int AdvanceRank,
-          typename ThreadMap_, int Alignment>
-class RegularTileAccessIterator<
-    Shape_, Element_,
-    layout::TensorOpMultiplicandCongruous<sizeof_bits<Element_>::value,
-                                          int(128 / sizeof(Element_))>,
-    AdvanceRank, ThreadMap_, Alignment> {
- public:
-  static_assert(
-      AdvanceRank == 0 || AdvanceRank == 1,
-      "Specialization for pitch-linear iterator may along advance along the "
-      "contiguous(rank=0) or strided(rank=1) dimension.");
+/// Template for reading and writing tiles of accumulators to shared memory
+template <
+  typename WarpShape,     ///< shape of warp-level GEMM (concept: MatrixShape)
+  typename Operator,      ///< matrix multiply operation (concept: arch::Mma)
+  typename Element,       ///< data type of element to be written
+  typename Layout,        ///< target shared memory layout
+  typename MmaSimtPolicy          ///< policy defining lane arrangement (concept: MmaSimtPolicy)
+>
+class TileIteratorSimt;
+
+/////////////////////////////////////////////////////////////////////////////////////////////////
+
+/// Template for reading and writing tiles of accumulators to shared memory
+template <
+  typename WarpShape_,     ///< shape of warp-level GEMM (concept: GemmShape)
+  typename Operator_,      ///< matrix multiply operation (concept: arch::Mma)
+  typename Element_,       ///< data type of element to be written
+  typename MmaSimtPolicy_         ///< policy defining lane arrangement (concept: MmaSimtPolicy)
+>
+class TileIteratorSimt<WarpShape_, Operator_, Element_, layout::RowMajor, MmaSimtPolicy_> {
+public:
 
-  using Shape = Shape_;
+  using WarpShape = WarpShape_;
+  using Operator = Operator_;
   using Element = Element_;
-  using Layout =
-      layout::TensorOpMultiplicandCongruous<sizeof_bits<Element_>::value,
-                                            int(128 / sizeof(Element_))>;
-  static int const kAdvanceRank = AdvanceRank;
-  static int const kAlignment = Alignment;
-
-  using Index = typename Layout::Index;
-  using LongIndex = typename Layout::LongIndex;
-  using StrideIndex = typename Layout::Stride::Index;
-
-  using TensorRef = TensorRef<Element, Layout>;
-  using TensorCoord = typename Layout::TensorCoord;
-
-  using ThreadMap = ThreadMap_;
-
-  /// Internal details made public to facilitate introspection
-  struct Detail {
-    /// This iterator is specialized for an access size that is 128 bits in
-    /// length.
-    static int const kAccessSizeInBits = 128;
-
-    static_assert(sizeof_bits<Element_>::value *
-                          ThreadMap::kElementsPerAccess ==
-                      kAccessSizeInBits,
-                  "This iterator requires a policy whose access size is 128bs");
-
-    ///< Number of pointers
-    static int const kPointerCount =
-        (ThreadMap::Iterations::kStrided > 1 ? 2 : 1);
-  };
+  using Layout = layout::RowMajor;
 
-  /// Element type per access
-  using AccessType = Array<Element, Layout::kElementsPerAccess>;
+  using TensorRef = TensorRef<Element, Layout>;         ///< Tensor Reference object
+  using TensorCoord = MatrixCoord;                      ///< Logical coordinate in referenced tensor
+  using Index = typename TensorRef::Index;
+  using LongIndex = typename TensorRef::LongIndex;
+
+  using Policy = SimtPolicy<WarpShape, Operator, Layout, MmaSimtPolicy_>;
+
+  /// Shape of the tile in memory
+  using Shape = MatrixShape<
+    Policy::kRowsPerIteration,
+    WarpShape::kN
+  >;
+
+  /// This is the fragment size produced by one access of the iterator.
+  using Fragment = Array<
+    typename Operator::ElementC, 
+    Policy::kElementsPerIteration>;
+
+  /// This is the complete warp-level accumulator tile.
+  using AccumulatorTile = Array<
+    typename Operator::ElementC, 
+    Policy::kAccumulatorElementCount>;
+
+  /// Number of times this iterator can be incremented
+  static int const kIterations = Policy::kIterations;
+
+  /// Padding quantity
+  using Padding = MatrixShape<
+    0,
+    4 * Policy::kElementsPerAccess
+#if CUTLASS_SIMT_EPILOGUE_USE_SCALAR_STORES
+    + 1
+#endif
+  >;
+
+private:
+
+#if CUTLASS_SIMT_EPILOGUE_USE_SCALAR_STORES
+  /// Storage type for accessing memory
+  using AccessType = AlignedArray<
+    Element, 
+    1
+  >;
+
+#else
+  /// Storage type for accessing memory
+  using AccessType = AlignedArray<
+    Element, 
+    Policy::kElementsPerAccess
+  >;
+#endif
 
- private:
   //
   // Data members
   //
 
-  /// Stride value
-  StrideIndex stride_;
-
-  /// Internal pointer to first access of tile
-  AccessType *pointer_[Detail::kPointerCount];
+  /// Internal pointer to memory
+  AccessType *pointer_;
 
-  /// Internal byte offset
-  Index byte_offset_;
+  /// Internal layout object
+  Layout layout_;
 
-  /// Iteration in the contiguous dimension
-  int iteration_contiguous_;
+public:
 
-  /// Iteration in the strided dimension
-  int iteration_strided_;
+  /// Default constructor
+  CUTLASS_HOST_DEVICE
+  TileIteratorSimt(): pointer_(nullptr) { }
 
- public:
-  /// Construct a TileIterator with zero threadblock offset
+  /// Constructor from TensorRef
   CUTLASS_HOST_DEVICE
-  RegularTileAccessIterator(TensorRef ref,  ///< Pointer to start of tensor
-                            int thread_id   ///< ID of each participating thread
-                            )
-      : stride_(ref.stride(0) / Layout::kElementsPerAccess),
-        byte_offset_(0) {
-    layout::PitchLinearCoord thread_offset_base =
-        ThreadMap::initial_offset(thread_id);
+  TileIteratorSimt(
+    TensorRef const &ref,
+    unsigned lane_id
+  ):
+    pointer_(reinterpret_cast<AccessType *>(ref.data())),
+    layout_(ref.stride()[0] / AccessType::kElements) { 
 
-    CUTLASS_PRAGMA_UNROLL
-    for (int i = 0; i < Detail::kPointerCount; ++i) {
-      // This is the offset of a thread within a threadblock tile for a specific
-      // pointer (units of elements)
-      layout::PitchLinearCoord thread_offset_in_threadblock_tile =
-          thread_offset_base +
-          layout::PitchLinearCoord{
-              0, ThreadMap::Detail::WarpThreadArrangement::kStrided * i};
-
-      // initialize pointer
-      pointer_[i] = reinterpret_cast<AccessType *>(
-          ref.data() + ref.offset(thread_offset_in_threadblock_tile));
-    }
+    auto lane_layout = Policy::MmaSimtPolicy::get_lane_layout();
+    MatrixCoord lane_offset = lane_layout.inverse(lane_id);
 
-    set_iteration_index(0);
+    pointer_ += layout_({
+      lane_offset.row(),
+      lane_offset.column() * Policy::kElementsPerAccess / int(AccessType::kElements)
+    });
   }
 
-  /// Overrides the internal iteration index
+  /// Adds a pointer offset
   CUTLASS_HOST_DEVICE
-  void set_iteration_index(int index) {
-    iteration_contiguous_ = index % ThreadMap::Iterations::kContiguous;
-    iteration_strided_ = index / ThreadMap::Iterations::kContiguous;
+  TileIteratorSimt & add_pointer_offset(Index pointer_offset) {
+    pointer_ += pointer_offset / AccessType::kElements;
+    return *this;
   }
 
-  /// Adds a pointer offset in units of Element
+  ///< advances in units of whole tiles along the logical coordinate space of the tensor
   CUTLASS_HOST_DEVICE
-  void add_pointer_offset(LongIndex pointer_offset) {
-    byte_offset_ += pointer_offset * sizeof(Element);
+  TileIteratorSimt & add_tile_offset(TensorCoord const &tile_offset) {
+
+    pointer_ += layout_({
+      tile_offset.row() * Shape::kRow, 
+      (tile_offset.column() * Shape::kColumn / int(AccessType::kElements))
+    });
+
+    return *this;
   }
 
-  /// Returns a pointer
+  ///< advances in units of whole tiles along the logical coordinate space of the tensor
   CUTLASS_HOST_DEVICE
-  AccessType *get() const {
-    AccessType *access_ptr = pointer_[iteration_strided_ & 1];
-    int stride_idx = (iteration_strided_ & ~1);
+  TileIteratorSimt & operator+=(TensorCoord const &tile_offset) {
 
-    int access_offset = stride_idx * ThreadMap::Delta::kStrided * stride_ +
-                        iteration_contiguous_ * ThreadMap::Delta::kContiguous /
-                            ThreadMap::kElementsPerAccess;
+    add_tile_offset(tile_offset);
+    
+    return *this;
+  }
 
-    char *access_byte_ptr =
-        reinterpret_cast<char *>(access_ptr + access_offset);
-    return reinterpret_cast<AccessType *>(access_byte_ptr + byte_offset_);
+  /// Store
+  CUTLASS_HOST_DEVICE
+  void store_with_pointer_offset(Fragment const &frag, Index pointer_offset) {
+#if CUTLASS_SIMT_EPILOGUE_USE_SCALAR_STORES
+      // de-vectorized stores
+      using ScalarAccessType = AlignedArray<Element, 1>;
+      ScalarAccessType const *scalarFragPtr = reinterpret_cast<ScalarAccessType const *>(&frag);
+      ScalarAccessType *scalarPointer = reinterpret_cast<ScalarAccessType *>(pointer_) + pointer_offset;
+
+      CUTLASS_PRAGMA_UNROLL
+      for (int n = 0; n < Policy::kAccessesPerIteration; ++n) {
+        CUTLASS_PRAGMA_UNROLL
+        for (int s = 0; s < Policy::kElementsPerAccess; s++) {
+          scalarPointer[n * Policy::MmaSimtPolicy::WarpShape::kColumn * Policy::kElementsPerAccess + s] = scalarFragPtr[n * Policy::kElementsPerAccess + s];
+        }
+      }
+#else
+    // original vector stores
+    AccessType const *frag_ptr = reinterpret_cast<AccessType const *>(&frag);
+    CUTLASS_PRAGMA_UNROLL
+    for (int n = 0; n < Policy::kAccessesPerIteration; ++n) {
+      pointer_[n * Policy::MmaSimtPolicy::WarpShape::kColumn + pointer_offset / int(AccessType::kElements)] = frag_ptr[n];
+    }
+#endif
   }
 
-  /// Advances to the next tile in memory.
+  /// Store
   CUTLASS_HOST_DEVICE
-  RegularTileAccessIterator &operator++() {
-    ++iteration_contiguous_;
+  void store(Fragment const &frag) {
+    store_with_pointer_offset(frag, 0);
+  }
 
-    if (iteration_contiguous_ < ThreadMap::Iterations::kContiguous)
-      return *this;
+  /// Load
+  CUTLASS_HOST_DEVICE
+  void load_with_pointer_offset(Fragment &frag, Index pointer_offset) const {
 
-    // Enter here only if (iteration_contiguous_ ==
-    // ThreadMap::Iteration::kContiguous)
-    iteration_contiguous_ = 0;
-    ++iteration_strided_;
+    AccessType *frag_ptr = reinterpret_cast<AccessType *>(&frag);
 
-    if (iteration_strided_ < ThreadMap::Iterations::kStrided) {
-      return *this;
+    CUTLASS_PRAGMA_UNROLL
+    for (int n = 0; n < Policy::kAccessesPerIteration; ++n) {
+      frag_ptr[n] = pointer_[n * Policy::MmaSimtPolicy::WarpShape::kColumn + pointer_offset / int(AccessType::kElements)];
     }
-
-    // Enter here only if (iteration_strided_ == ThreadMap::Iteration::kStrided)
-    // which means we enter the next tile.
-    iteration_strided_ = 0;
-
-    return *this;
   }
 
-  /// Advances to the next tile in memory.
+  /// Load
   CUTLASS_HOST_DEVICE
-  RegularTileAccessIterator operator++(int) {
-    RegularTileAccessIterator prev(*this);
-    this->operator++();
-
-    return prev;
+  void load(Fragment &frag) const {
+    load_with_pointer_offset(frag, 0);
   }
 
-  /// Adds a tile offset
-  CUTLASS_DEVICE
-  void add_tile_offset(TensorCoord const &coord) {
-    add_pointer_offset(coord.contiguous() * Shape::kContiguous +
-                       coord.strided() * Shape::kStrided * stride_ *
-                           Layout::kElementsPerAccess);
+  /// Set smem base address
+  CUTLASS_HOST_DEVICE
+  void set_smem_base_address(Index address) {
   }
 };
 
-////////////////////////////////////////////////////////////////////////////////
+/////////////////////////////////////////////////////////////////////////////////////////////////
+
+/////////////////////////////////////////////////////////////////////////////////////////////////
 
-/// Tile Iterator specialized for column-major congruous TensorOp formats.
-///
-///
-/// Satisfies: ForwardTileIteratorConcept |
-///            ReadableContiguousTileIteratorConcept |
-///            WriteableContiguousTileIteratorConcept
-///
-template <typename Shape_, typename Element_, int AdvanceRank,
-          typename ThreadMap_, int Alignment>
-class RegularTileAccessIterator<
-    Shape_, Element_,
-    layout::ColumnMajorTensorOpMultiplicandCongruous<
-        sizeof_bits<Element_>::value, int(128 / sizeof(Element_))>,
-    AdvanceRank, ThreadMap_, Alignment> {
+/// Template for reading and writing tiles of accumulators to shared memory
+template <typename WarpShape_,     ///< shape of warp-level GEMM (concept: GemmShape)
+          typename Operator_,      ///< matrix multiply operation (concept: arch::Mma)
+          typename Element_,       ///< data type of element to be written
+          typename Layout_,         ///< target shared memory layout
+          typename MmaSimtPolicy_  ///< policy defining lane arrangement (concept: MmaSimtPolicy)
+          >
+class TileIteratorSimtDirectConv {
  public:
-  static_assert(
-      AdvanceRank == 0 || AdvanceRank == 1,
-      "Specialization for column-major iterator may along advance along the "
-      "columns(rank=0) or rows(rank=1) dimension.");
 
-  using Shape = Shape_;
+  using WarpShape = WarpShape_;
+  using Operator = Operator_;
   using Element = Element_;
-  using Layout = layout::ColumnMajorTensorOpMultiplicandCongruous<
-      sizeof_bits<Element_>::value, int(128 / sizeof(Element_))>;
-  static int const kAdvanceRank = AdvanceRank;
-  static int const kAlignment = Alignment;
-
-  using Index = typename Layout::Index;
-  using LongIndex = typename Layout::LongIndex;
-
-  using TensorRef = TensorRef<Element, Layout>;
-  using TensorCoord = typename Layout::TensorCoord;
-
-  using ThreadMap = ThreadMap_;
-
-  /// Underlying iterator type
-  using UnderlyingIterator = RegularTileAccessIterator<
-      layout::PitchLinearShape<Shape::kRow, Shape::kColumn>, Element,
-      layout::TensorOpMultiplicandCongruous<sizeof_bits<Element_>::value,
-                                            int(128 / sizeof(Element_))>,
-      (kAdvanceRank == 0 ? 0 : 1), ThreadMap_>;
+  using Layout = layout::RowMajor;
 
-  using AccessType = typename UnderlyingIterator::AccessType;
+  using TensorRef = TensorRef<Element, Layout>;  ///< Tensor Reference object
+  using TensorCoord = MatrixCoord;               ///< Logical coordinate in referenced tensor
+  using Index = typename TensorRef::Index;
+  using LongIndex = typename TensorRef::LongIndex;
+
+  using Policy = SimtPolicy<WarpShape, Operator, Layout, MmaSimtPolicy_>;
+
+  /// Shape of the tile in memory
+  using Shape = MatrixShape<Policy::kRowsPerIteration, WarpShape::kN>;
+
+  /// This is the fragment size produced by one access of the iterator.
+  using Fragment = Array<typename Operator::ElementC, Policy::kElementsPerIteration>;
+
+  /// This is the complete warp-level accumulator tile.
+  using AccumulatorTile = Array<typename Operator::ElementC, Policy::kAccumulatorElementCount>;
+
+  /// Number of times this iterator can be incremented
+  static int const kIterations = Policy::kIterations;
+
+  /// Padding quantity
+  using Padding = MatrixShape<0,
+                              0
+                              >;
+
+private:
+  /// Storage type for accessing memory
+  using AccessType = AlignedArray<
+    Element, 
+    Policy::kElementsPerAccess
+  >;
 
- private:
-  /// Underlying iterator
-  UnderlyingIterator iterator_;
+  //
+  // Data members
+  //
 
- public:
-  /// Construct a TileIterator with zero threadblock offset
-  CUTLASS_HOST_DEVICE
-  RegularTileAccessIterator(TensorRef ref,  ///< Pointer to start of tensor
-                            int thread_id   ///< ID of each participating thread
-                            )
-      : iterator_({ref.data(), ref.stride()}, thread_id) {}
+  /// Internal pointer to memory
+  AccessType *pointer_;
 
-  /// Overrides the internal iteration index
-  CUTLASS_HOST_DEVICE
-  void set_iteration_index(int index) { iterator_.set_iteration_index(index); }
+  /// Internal layout object
+  Layout layout_;
+
+  /// Base smem offset;
+  Index base_smem_address_;
 
-  /// Adds a pointer offset in units of Element
+ public:
+  /// Default constructor
   CUTLASS_HOST_DEVICE
-  void add_pointer_offset(LongIndex pointer_offset) {
-    iterator_.add_pointer_offset(pointer_offset);
-  }
+  TileIteratorSimtDirectConv() : pointer_(nullptr) {}
 
-  /// Returns a pointer
+  /// Constructor from TensorRef
   CUTLASS_HOST_DEVICE
-  AccessType *get() const {
-    return reinterpret_cast<AccessType *>(iterator_.get());
-  }
+  TileIteratorSimtDirectConv(
+    TensorRef const &ref,
+    unsigned lane_id
+  ):
+    pointer_(reinterpret_cast<AccessType *>(ref.data())),
+    layout_(ref.stride()[0] / AccessType::kElements) {
+
+    auto lane_layout = Policy::MmaSimtPolicy::get_lane_layout();
+    MatrixCoord lane_offset = lane_layout.inverse(lane_id);
 
-  /// Adds a tile offset
-  CUTLASS_DEVICE
-  void add_tile_offset(TensorCoord const &coord) {
-    iterator_.add_tile_offset({coord.row(), coord.column()});
+    pointer_ += layout_({
+      lane_offset.row(),
+      lane_offset.column() * Policy::kElementsPerAccess / int(AccessType::kElements)
+    });
   }
 
-  /// Advances to the next tile in memory.
+  /// Adds a pointer offset
   CUTLASS_HOST_DEVICE
-  RegularTileAccessIterator &operator++() {
-    ++iterator_;
+  TileIteratorSimtDirectConv & add_pointer_offset(Index pointer_offset) {
+    pointer_ += pointer_offset / AccessType::kElements;
     return *this;
   }
 
-  /// Advances to the next tile in memory.
+  ///< advances in units of whole tiles along the logical coordinate space of the tensor
   CUTLASS_HOST_DEVICE
-  RegularTileAccessIterator operator++(int) {
-    RegularTileAccessIterator prev(*this);
-    ++iterator_;
-
-    return prev;
-  }
-};
-
-////////////////////////////////////////////////////////////////////////////////
+  TileIteratorSimtDirectConv & add_tile_offset(TensorCoord const &tile_offset) {
 
-/// Tile Iterator specialized for row-major congruous TensorOp formats.
-///
-///
-/// Satisfies: ForwardTileIteratorConcept |
-///            ReadableContiguousTileIteratorConcept |
-///            WriteableContiguousTileIteratorConcept
-///
-template <typename Shape_, typename Element_, int AdvanceRank,
-          typename ThreadMap_, int Alignment>
-class RegularTileAccessIterator<
-    Shape_, Element_,
-    layout::RowMajorTensorOpMultiplicandCongruous<sizeof_bits<Element_>::value,
-                                                  int(128 / sizeof(Element_))>,
-    AdvanceRank, ThreadMap_, Alignment> {
- public:
-  static_assert(
-      AdvanceRank == 0 || AdvanceRank == 1,
-      "Specialization for row-major iterator may along advance along the "
-      "columns(rank=0) or rows(rank=1) dimension.");
+    pointer_ += layout_({
+      tile_offset.row() * Shape::kRow, 
+      (tile_offset.column() * Shape::kColumn / int(AccessType::kElements))
+    });
 
-  using Shape = Shape_;
-  using Element = Element_;
-  using Layout = layout::RowMajorTensorOpMultiplicandCongruous<
-      sizeof_bits<Element_>::value, int(128 / sizeof(Element_))>;
-  static int const kAdvanceRank = AdvanceRank;
-  static int const kAlignment = Alignment;
-
-  using Index = typename Layout::Index;
-  using LongIndex = typename Layout::LongIndex;
-
-  using TensorRef = TensorRef<Element, Layout>;
-  using TensorCoord = typename Layout::TensorCoord;
-
-  using ThreadMap = ThreadMap_;
-
-  /// Underlying iterator type
-  using UnderlyingIterator = RegularTileAccessIterator<
-      layout::PitchLinearShape<Shape::kColumn, Shape::kRow>, Element,
-      layout::TensorOpMultiplicandCongruous<sizeof_bits<Element_>::value,
-                                            int(128 / sizeof(Element_))>,
-      (kAdvanceRank == 0 ? 1 : 0), ThreadMap_>;
+    return *this;
+  }
 
-  using AccessType = typename UnderlyingIterator::AccessType;
+  ///< advances in units of whole tiles along the logical coordinate space of the tensor
+  CUTLASS_HOST_DEVICE
+  TileIteratorSimtDirectConv & operator+=(TensorCoord const &tile_offset) {
 
- private:
-  /// Underlying iterator
-  UnderlyingIterator iterator_;
+    add_tile_offset(tile_offset);
+    
+    return *this;
+  }
 
- public:
-  /// Construct a TileIterator with zero threadblock offset
+  /// Store
   CUTLASS_HOST_DEVICE
-  RegularTileAccessIterator(TensorRef ref,  ///< Pointer to start of tensor
-                            int thread_id   ///< ID of each participating thread
-                            )
-      : iterator_({ref.data(), ref.stride()}, thread_id) {}
+  void store_with_pointer_offset(Fragment const &frag, Index pointer_offset) {
 
-  /// Overrides the internal iteration index
-  CUTLASS_HOST_DEVICE
-  void set_iteration_index(int index) { iterator_.set_iteration_index(index); }
+    // original vector stores
+    AccessType const *frag_ptr = reinterpret_cast<AccessType const *>(&frag);
+    AccessType * load_pointer_ = reinterpret_cast<AccessType *>(reinterpret_cast<uint8_t *>(pointer_) + base_smem_address_);
+    CUTLASS_PRAGMA_UNROLL
+    for (int n = 0; n < Policy::kAccessesPerIteration; ++n) {
+      load_pointer_[n * Policy::MmaSimtPolicy::WarpShape::kColumn + pointer_offset / int(AccessType::kElements)] = frag_ptr[n];
+    }
+  }
 
-  /// Adds a pointer offset in units of Element
+  /// Store
   CUTLASS_HOST_DEVICE
-  void add_pointer_offset(LongIndex pointer_offset) {
-    iterator_.add_pointer_offset(pointer_offset);
+  void store(Fragment const &frag) {
+    store_with_pointer_offset(frag, 0);
   }
 
-  /// Returns a pointer
+  /// Load
   CUTLASS_HOST_DEVICE
-  AccessType *get() const {
-    return reinterpret_cast<AccessType *>(iterator_.get());
-  }
+  void load_with_pointer_offset(Fragment &frag, Index pointer_offset) const {
 
-  /// Adds a tile offset
-  CUTLASS_DEVICE
-  void add_tile_offset(TensorCoord const &coord) {
-    iterator_.add_tile_offset({coord.column(), coord.row()});
+    AccessType *frag_ptr = reinterpret_cast<AccessType *>(&frag);
+
+    CUTLASS_PRAGMA_UNROLL
+    for (int n = 0; n < Policy::kAccessesPerIteration; ++n) {
+      frag_ptr[n] = pointer_[n * Policy::MmaSimtPolicy::WarpShape::kColumn + pointer_offset / int(AccessType::kElements)];
+    }
   }
 
-  /// Advances to the next tile in memory.
+  /// Load
   CUTLASS_HOST_DEVICE
-  RegularTileAccessIterator &operator++() {
-    ++iterator_;
-    return *this;
+  void load(Fragment &frag) const {
+    load_with_pointer_offset(frag, 0);
   }
 
-  /// Advances to the next tile in memory.
+  /// Set smem base address
   CUTLASS_HOST_DEVICE
-  RegularTileAccessIterator operator++(int) {
-    RegularTileAccessIterator prev(*this);
-    ++iterator_;
-
-    return prev;
+  void set_smem_base_address(Index address){
+    base_smem_address_ = address;
   }
+
 };
 
-////////////////////////////////////////////////////////////////////////////////
+/////////////////////////////////////////////////////////////////////////////////////////////////
+/// Template for reading and writing tiles of accumulators to shared memory
+template <typename WarpShape_,               ///< shape of warp-level GEMM (concept: GemmShape)
+          typename ThreadOutputShape_,       /// Size of the matrix to load (concept: TensorNHWC)
+          typename ThreadBlockOutputShape_,  /// Size of the matrix to load (concept: TensorNHWC)
+          typename Operator_,                ///< matrix multi ply operation (concept: arch::Mma)
+          typename Element_,                 ///< data type of element to be written
+          typename Layout_,                  ///< target shared memory layout
+          typename MmaSimtPolicy_            ///< policy defining lane arrangement (concept: MmaSimtPolicy)
+          >
+class TileIteratorSimtDirect2dConv {
+ public:
+  using WarpShape = WarpShape_;
+  using ThreadOutputShape = ThreadOutputShape_;
+  using ThreadBlockOutputShape = ThreadBlockOutputShape_;
+  using Operator = Operator_;
+  using Element = Element_;
+  using Layout = layout::RowMajor;
+  using MmaSimtPolicy = MmaSimtPolicy_;
 
-/// Tile iterator specialized for crosswise arrangements for TensorOps
-///
-///
-/// Satisfies: ForwardTileIteratorConcept |
-///            ReadableContiguousTileIteratorConcept |
-///            WriteableContiguousTileIteratorConcept
-///
-template <typename Shape_, typename Element_, int AdvanceRank,
-          typename ThreadMap_, int Alignment, int Crosswise>
-class RegularTileAccessIterator<Shape_, Element_,
-                                layout::TensorOpMultiplicandCrosswise<
-                                    sizeof_bits<Element_>::value, Crosswise>,
-                                AdvanceRank, ThreadMap_, Alignment> {
- public:
-  static_assert(
-      AdvanceRank == 0 || AdvanceRank == 1,
-      "Specialization for pitch-linear iterator may along advance along the "
-      "contiguous(rank=0) or strided(rank=1) dimension.");
+  using TensorRef = TensorRef<Element, Layout>;  ///< Tensor Reference object
+  using TensorCoord = MatrixCoord;               ///< Logical coordinate in referenced tensor
+  using Index = typename TensorRef::Index;
+  using LongIndex = typename TensorRef::LongIndex;
 
-  using Shape = Shape_;
-  using Element = Element_;
-  using Layout =
-      layout::TensorOpMultiplicandCrosswise<sizeof_bits<Element_>::value,
-                                            Crosswise>;
-  static int const kAdvanceRank = AdvanceRank;
-  static int const kAlignment = Alignment;
-  static int const kCrosswise = Crosswise;
-
-  using Index = typename Layout::Index;
-  using LongIndex = typename Layout::LongIndex;
-  using StrideIndex = typename Layout::Stride::Index;
-
-  using TensorRef = TensorRef<Element, Layout>;
-  using TensorCoord = typename Layout::TensorCoord;
-
-  using ThreadMap = ThreadMap_;
-
-  static_assert(!(ThreadMap::Delta::kContiguous % kCrosswise),
-                "kCrosswise is the smallest unit in the contiguous dimension "
-                "for shared memory swizzling.");
-
-  /// Internal details made public to facilitate introspection
-  struct Detail {
-    /// This iterator is specialized for an access size that is 128 bits in
-    /// length.
-    static int const kAccessSizeInBits = 128;
-
-    static_assert(sizeof_bits<Element_>::value *
-                          ThreadMap::kElementsPerAccess ==
-                      kAccessSizeInBits,
-                  "This iterator requires a policy whose access size is 128bs");
-
-    /// Number of pointers
-    ///
-    /// Note:TN kblock32 layouts only needs 1 pointer, but strangely
-    /// reducing pointer count hurts perfomrnace
-    static int const kPointerCount =
-        (ThreadMap::Iterations::kStrided > 1 ? 2 : 1);
-  };
+  // Thread-level shape of a fragment
+  using ThreadShape = MatrixShape<ThreadOutputShape::kNHW, ThreadOutputShape::kC>;
 
-  /// Element type per access
-  using AccessType = Array<Element, Layout::kElementsPerAccess>;
+  static_assert(!(ThreadShape::kColumn % MmaSimtPolicy::LaneMmaShape::kN),
+                "Thread-level GEMM must be divisible by Policy::LaneMmaShape.");
 
- private:
-  //
-  // Data members
-  //
+  using ThreadTileCount = MatrixShape<ThreadBlockOutputShape::kH / ThreadOutputShape::kH,
+                                      ThreadBlockOutputShape::kW / ThreadOutputShape::kW>;
+
+  using Iterations =
+      MatrixShape<ThreadShape::kRow, ThreadShape::kColumn / MmaSimtPolicy::LaneMmaShape::kN>;
 
-  /// Total number of sections.  The memory is divided into stages.  One stage
-  /// can store one tile.  Stage is divided into sections.  Interleaved layout
-  /// can have multiple sections in a stage.  The rest layout only has one section
-  /// in a stage.
-  int sections_;
+  /// This is the complete warp-level accumulator tile.
+  using AccumulatorTile = typename Operator::FragmentC;
 
-  /// Sections that a stage has
-  int sections_per_stage_;
+  /// This is the fragment size produced by one access of the iterator.
+  using Fragment = AccumulatorTile;
 
-  /// Stride value
-  StrideIndex stride_;
+  /// Padding quantity
+  using Padding = MatrixShape<0, 0>;
 
-  /// Internal pointer to first access of tile
-  AccessType *pointer_[Detail::kPointerCount];
+ private:
+  // Storage type for accessing memory
+  using AccessType = AlignedArray<Element, MmaSimtPolicy::LaneMmaShape::kN>;
+  //
+  // Data members
+  //
 
-  /// Internal byte offset
-  Index byte_offset_;
+  /// Internal pointer to memory
+  AccessType *pointer_;
 
-  /// Iteration in the contiguous dimension
-  int iteration_contiguous_;
+  /// Internal layout object
+  Layout layout_;
 
-  /// Iteration in the strided dimension
-  int iteration_strided_;
+  /// Base smem offset;
+  Index base_smem_address_;
 
  public:
-  /// Construct a TileIterator with zero threadblock offset
+  /// Default constructor
   CUTLASS_HOST_DEVICE
-  RegularTileAccessIterator(TensorRef ref,  ///< Pointer to start of tensor
-                            int thread_id   ///< ID of each participating thread
-                            )
-      : sections_(ref.stride(0) / kCrosswise),
-        sections_per_stage_(Shape::kContiguous / kCrosswise),
-        // stride_ = kCrosswise x sections_ x kFactor
-        stride_(ref.stride(0) * Layout::kFactor / Layout::kElementsPerAccess),
-        byte_offset_(0) {
-    layout::PitchLinearCoord thread_offset_base =
-        ThreadMap::initial_offset(thread_id);
+  TileIteratorSimtDirect2dConv() : pointer_(nullptr) {}
 
-    CUTLASS_PRAGMA_UNROLL
-    for (int i = 0; i < Detail::kPointerCount; ++i) {
-      // This is the offset of a thread within a threadblock tile for a specific
-      // pointer (units of elements)
-      layout::PitchLinearCoord thread_offset_in_threadblock_tile =
-          thread_offset_base +
-          layout::PitchLinearCoord{
-              0, ThreadMap::Detail::WarpThreadArrangement::kStrided * i};
-      // initialize pointer
-      pointer_[i] = reinterpret_cast<AccessType *>(ref.data()) +
-                    ref.offset(thread_offset_in_threadblock_tile) /
-                        Layout::kElementsPerAccess;
-    }
+  /// Constructor from TensorRef
+  CUTLASS_HOST_DEVICE
+  TileIteratorSimtDirect2dConv(TensorRef const &ref, unsigned thread_id, unsigned lane_id)
+      : pointer_(reinterpret_cast<AccessType *>(ref.data())),
+        layout_(ref.stride()[0] / AccessType::kElements) {
+  
+    auto lane_layout = MmaSimtPolicy::get_lane_layout();
 
-    set_iteration_index(0);
-  }
+    MatrixCoord lane_offset = lane_layout.inverse(lane_id);
 
-  /// Overrides the internal iteration index
-  CUTLASS_HOST_DEVICE
-  void set_iteration_index(int index) {
-    iteration_contiguous_ = index % ThreadMap::Iterations::kContiguous;
-    iteration_strided_ = index / ThreadMap::Iterations::kContiguous;
+    // Get base HW offset of current threads
+    const int threadgroup = thread_id / (ThreadBlockOutputShape::kC / ThreadOutputShape::kC);
+    const int base_p = (threadgroup / (ThreadTileCount::kColumn)) * ThreadOutputShape::kH;
+    const int base_q = (threadgroup % (ThreadTileCount::kColumn)) * ThreadOutputShape::kW;
+
+    const int row_offset = base_p * ThreadBlockOutputShape::kW + base_q;
+
+    pointer_ += layout_(
+        {row_offset,
+         lane_offset.column() * MmaSimtPolicy::LaneMmaShape::kN / int(AccessType::kElements)});
   }
 
-  /// Adds a pointer offset in units of Element
+  /// Adds a pointer offset
   CUTLASS_HOST_DEVICE
-  void add_pointer_offset(LongIndex pointer_offset) {
-    byte_offset_ += pointer_offset * sizeof_bits<Element>::value / 8;
+  TileIteratorSimtDirect2dConv &add_pointer_offset(Index pointer_offset) {
+    pointer_ += pointer_offset / AccessType::kElements;
+    return *this;
   }
 
-  /// Returns a pointer
+  /// Store
   CUTLASS_HOST_DEVICE
-  AccessType *get() const {
-    AccessType *access_ptr = pointer_[iteration_strided_ & 1];
-    int stride_idx = (iteration_strided_ & ~1);
+  void store_with_pointer_offset(Fragment const &frag, Index pointer_offset) {
+    AccessType *storer_pointer_ =
+        reinterpret_cast<AccessType *>(reinterpret_cast<uint8_t *>(pointer_) + base_smem_address_);
+    AccessType const *frag_ptr = reinterpret_cast<AccessType const *>(&frag);
 
-    int access_offset =
-        stride_idx * ThreadMap::Delta::kStrided * stride_ / Layout::kFactor +
-        // kCrosswise elements in the contiguous dimension would span to a
-        // shared memory cache line.
-        iteration_contiguous_ * (ThreadMap::Delta::kContiguous / kCrosswise) *
-            Layout::TileShape::kContiguous;
-    char *access_byte_ptr =
-        reinterpret_cast<char *>(access_ptr + access_offset);
-    return reinterpret_cast<AccessType *>(access_byte_ptr + byte_offset_);
+    CUTLASS_PRAGMA_UNROLL
+    for (int h = 0; h < ThreadOutputShape::kH; ++h) {
+      CUTLASS_PRAGMA_UNROLL
+      for (int w = 0; w < ThreadOutputShape::kW; ++w) {
+        CUTLASS_PRAGMA_UNROLL
+        for (int col = 0; col < Iterations::kColumn; ++col) {
+          int offset = (w + h * ThreadBlockOutputShape::kW) *
+                           (ThreadBlockOutputShape::kC / AccessType::kElements) +
+                       col;
+          storer_pointer_[offset + pointer_offset / int(AccessType::kElements)] =
+              frag_ptr[w + h * ThreadOutputShape::kW + col];
+        }
+      }
+    }
   }
 
-  /// Advances to the next tile in memory.
+  /// Store
   CUTLASS_HOST_DEVICE
-  RegularTileAccessIterator &operator++() {
-    ++iteration_contiguous_;
+  void store(Fragment const &frag) { store_with_pointer_offset(frag, 0); }
 
-    if (iteration_contiguous_ < ThreadMap::Iterations::kContiguous)
-      return *this;
+  /// Set smem base address
+  CUTLASS_HOST_DEVICE
+  void set_smem_base_address(Index address) { base_smem_address_ = address; }
+};
+/////////////////////////////////////////////////////////////////////////////////////////////////
 
-    // Enter here only if (iteration_contiguous_ ==
-    // ThreadMap::Iteration::kContiguous)
-    iteration_contiguous_ = 0;
-    ++iteration_strided_;
+/// Template for reading and writing tiles of accumulators to shared memory
+template <
+  typename WarpShape_,        ///< shape of warp-level GEMM (concept: GemmShape)
+  typename Operator_,         ///< matrix multiply operation (concept: arch::Mma)
+  typename Element_,          ///< data type of element to be written
+  typename Layout_,            ///< target shared memory layout
+  typename MmaSimtPolicy_     ///< policy defining lane arrangement (concept: MmaSimtPolicy)
+>
+class TileIteratorSimtCanonical {
+public:
 
-    if (iteration_strided_ < ThreadMap::Iterations::kStrided) {
-      return *this;
-    }
+  using WarpShape = WarpShape_;
+  using Operator = Operator_;
+  using Element = Element_;
+  using Layout = Layout_;
 
-    // Enter here only if (iteration_strided_ == ThreadMap::Iteration::kStrided)
-    // which means we enter the next section.
-    iteration_strided_ = 0;
+  using TensorRef = TensorRef<Element, Layout>;         ///< Tensor Reference object
+  using TensorCoord = MatrixCoord;                      ///< Logical coordinate in referenced tensor
+  using Index = typename TensorRef::Index;
+  using LongIndex = typename TensorRef::LongIndex;
+
+  using Policy = SimtPolicy<WarpShape, Operator, Layout, MmaSimtPolicy_>;
+
+  /// Shape of the tile in memory
+  using Shape = MatrixShape<
+    Policy::kRowsPerIteration,
+    WarpShape::kN
+  >;
+
+  /// This is the fragment size produced by one access of the iterator.
+  using Fragment = Array<
+    typename Operator::ElementC, 
+    Policy::kElementsPerIteration>;
+
+  /// This is the complete warp-level accumulator tile.
+  using AccumulatorTile = Array<
+    typename Operator::ElementC, 
+    Policy::kAccumulatorElementCount>;
+
+  /// Number of times this iterator can be incremented
+  static int const kIterations = Policy::kIterations;
+
+  /// Padding quantity
+  using Padding = MatrixShape<
+    0,
+    4 * Policy::kElementsPerAccess + 1
+  >;
+
+private:
+
+  /// Storage type for accessing memory
+  using AccessType = AlignedArray<
+    Element, 
+    1
+  >;
 
-    return *this;
-  }
+  //
+  // Data members
+  //
 
-  /// Advances to the next tile in memory.
-  CUTLASS_HOST_DEVICE
-  RegularTileAccessIterator operator++(int) {
-    RegularTileAccessIterator prev(*this);
-    this->operator++();
-
-    return prev;
-  }
-
-  /// Adds a tile offset
-  CUTLASS_DEVICE
-  void add_tile_offset(TensorCoord const &coord) {
-    add_pointer_offset(coord.contiguous() * sections_per_stage_ * stride_ *
-                           ThreadMap::kElementsPerAccess / sections_ +
-                       coord.strided() * Shape::kStrided * stride_ *
-                           Layout::kElementsPerAccess / Layout::kFactor);
-  }
-};
+  /// Internal pointer to memory
+  AccessType *pointer_;
 
-////////////////////////////////////////////////////////////////////////////////
+  /// Internal layout object
+  Layout layout_;
 
-/// Tile Iterator specialized for column-major crosswise TensorOp formats.
-///
-///
-/// Satisfies: ForwardTileIteratorConcept |
-///            ReadableContiguousTileIteratorConcept |
-///            WriteableContiguousTileIteratorConcept
-///
-template <typename Shape_, typename Element_, int AdvanceRank,
-          typename ThreadMap_, int Alignment, int Crosswise>
-class RegularTileAccessIterator<
-    Shape_, Element_,
-    layout::ColumnMajorTensorOpMultiplicandCrosswise<
-        sizeof_bits<Element_>::value, Crosswise>,
-    AdvanceRank, ThreadMap_, Alignment> {
- public:
-  static_assert(
-      AdvanceRank == 0 || AdvanceRank == 1,
-      "Specialization for column-major iterator may along advance along the "
-      "columns(rank=0) or rows(rank=1) dimension.");
+  /// Guard to indicate whether the shape is divisible
+  bool divisible_;
 
-  using Shape = Shape_;
-  using Element = Element_;
-  using Layout = layout::ColumnMajorTensorOpMultiplicandCrosswise<
-      sizeof_bits<Element_>::value, Crosswise>;
-  static int const kAdvanceRank = AdvanceRank;
-  static int const kAlignment = Alignment;
-
-  using Index = typename Layout::Index;
-  using LongIndex = typename Layout::LongIndex;
-
-  using TensorRef = TensorRef<Element, Layout>;
-  using TensorCoord = typename Layout::TensorCoord;
-
-  using ThreadMap = ThreadMap_;
-
-  /// Underlying iterator type
-  using UnderlyingIterator = RegularTileAccessIterator<
-      layout::PitchLinearShape<Shape::kRow, Shape::kColumn>, Element,
-      layout::TensorOpMultiplicandCrosswise<sizeof_bits<Element_>::value,
-                                            Crosswise>,
-      (kAdvanceRank == 0 ? 0 : 1), ThreadMap_>;
+  /// Extent of the output tensor
+  MatrixCoord extent_;
 
-  using AccessType = typename UnderlyingIterator::AccessType;
+  /// Thread offset
+  MatrixCoord thread_offset_;
 
- private:
-  /// Underlying iterator
-  UnderlyingIterator iterator_;
+public:
 
- public:
-  /// Construct a TileIterator with zero threadblock offset
+  /// Default constructor
   CUTLASS_HOST_DEVICE
-  RegularTileAccessIterator(TensorRef ref,  ///< Pointer to start of tensor
-                            int thread_id   ///< ID of each participating thread
-                            )
-      : iterator_({ref.data(), ref.stride()}, thread_id) {}
+  TileIteratorSimtCanonical(): pointer_(nullptr) { }
 
-  /// Overrides the internal iteration index
+  /// Constructor from TensorRef
   CUTLASS_HOST_DEVICE
-  void set_iteration_index(int index) { iterator_.set_iteration_index(index); }
+  TileIteratorSimtCanonical(
+    TensorRef const &ref,
+    unsigned lane_id
+  ):
+    pointer_(reinterpret_cast<AccessType *>(ref.data())),
+    layout_(ref.stride()[0] / AccessType::kElements),
+    divisible_(true),
+    extent_(WarpShape::kM, WarpShape::kN) { 
 
-  /// Adds a pointer offset in units of Element
-  CUTLASS_HOST_DEVICE
-  void add_pointer_offset(LongIndex pointer_offset) {
-    iterator_.add_pointer_offset(pointer_offset);
+    auto lane_layout = Policy::MmaSimtPolicy::get_lane_layout();
+    MatrixCoord lane_offset = lane_layout.inverse(lane_id);
+
+    thread_offset_ = {
+      lane_offset.row() * Shape::kRow, 
+      lane_offset.column() * Policy::kElementsPerAccess
+    };
+
+    pointer_ += layout_({
+      lane_offset.row() * Shape::kRow,
+      lane_offset.column() * Policy::kElementsPerAccess / int(AccessType::kElements)
+    });
   }
 
-  /// Returns a pointer
+  /// Constructor from TensorRef
   CUTLASS_HOST_DEVICE
-  AccessType *get() const {
-    return reinterpret_cast<AccessType *>(iterator_.get());
-  }
+  TileIteratorSimtCanonical(
+    TensorRef const &ref,
+    TensorCoord const &extent,
+    unsigned lane_id
+  ):
+    pointer_(reinterpret_cast<AccessType *>(ref.data())),
+    layout_(ref.stride()[0] / AccessType::kElements),
+    divisible_(false),
+    extent_(extent) { 
 
-  /// Adds a tile offset
-  CUTLASS_DEVICE
-  void add_tile_offset(TensorCoord const &coord) {
-    iterator_.add_tile_offset({coord.row(), coord.column()});
+    auto lane_layout = Policy::MmaSimtPolicy::get_lane_layout();
+    MatrixCoord lane_offset = lane_layout.inverse(lane_id);
+
+    thread_offset_ = {
+      lane_offset.row() * Shape::kRow, 
+      lane_offset.column() * Policy::kElementsPerAccess
+    };
+
+    pointer_ += layout_({
+      lane_offset.row() * Shape::kRow,
+      lane_offset.column() * Policy::kElementsPerAccess / int(AccessType::kElements)
+    });
   }
 
-  /// Advances to the next tile in memory.
+  /// Adds a pointer offset
   CUTLASS_HOST_DEVICE
-  RegularTileAccessIterator &operator++() {
-    ++iterator_;
+  TileIteratorSimtCanonical & add_pointer_offset(Index pointer_offset) {
+    pointer_ += pointer_offset / AccessType::kElements;
     return *this;
   }
 
-  /// Advances to the next tile in memory.
+  ///< advances in units of whole tiles along the logical coordinate space of the tensor
   CUTLASS_HOST_DEVICE
-  RegularTileAccessIterator operator++(int) {
-    RegularTileAccessIterator prev(*this);
-    ++iterator_;
+  TileIteratorSimtCanonical & add_tile_offset(TensorCoord const &tile_offset) {
 
-    return prev;
-  }
-};
+    MatrixCoord coord_offset(
+      tile_offset.row(), 
+      tile_offset.column() * Shape::kColumn
+    );
 
-////////////////////////////////////////////////////////////////////////////////
+    thread_offset_ += coord_offset;
 
-/// Tile Iterator specialized for row-major crosswise TensorOp formats.
-///
-///
-/// Satisfies: ForwardTileIteratorConcept |
-///            ReadableContiguousTileIteratorConcept |
-///            WriteableContiguousTileIteratorConcept
-///
-template <typename Shape_, typename Element_, int AdvanceRank,
-          typename ThreadMap_, int Alignment, int Crosswise>
-class RegularTileAccessIterator<Shape_, Element_,
-                                layout::RowMajorTensorOpMultiplicandCrosswise<
-                                    sizeof_bits<Element_>::value, Crosswise>,
-                                AdvanceRank, ThreadMap_, Alignment> {
- public:
-  static_assert(
-      AdvanceRank == 0 || AdvanceRank == 1,
-      "Specialization for row-major iterator may along advance along the "
-      "columns(rank=0) or rows(rank=1) dimension.");
+    pointer_ += layout_({
+      coord_offset.row(), 
+      coord_offset.column()
+    });
 
-  using Shape = Shape_;
-  using Element = Element_;
-  using Layout = layout::RowMajorTensorOpMultiplicandCrosswise<
-      sizeof_bits<Element_>::value, Crosswise>;
-  static int const kAdvanceRank = AdvanceRank;
-  static int const kAlignment = Alignment;
-
-  using Index = typename Layout::Index;
-  using LongIndex = typename Layout::LongIndex;
-
-  using TensorRef = TensorRef<Element, Layout>;
-  using TensorCoord = typename Layout::TensorCoord;
-
-  using ThreadMap = ThreadMap_;
-
-  /// Underlying iterator type
-  using UnderlyingIterator = RegularTileAccessIterator<
-      layout::PitchLinearShape<Shape::kColumn, Shape::kRow>, Element,
-      layout::TensorOpMultiplicandCrosswise<sizeof_bits<Element_>::value,
-                                            Crosswise>,
-      (kAdvanceRank == 0 ? 1 : 0), ThreadMap_>;
+    return *this;
+  }
 
-  using AccessType = typename UnderlyingIterator::AccessType;
+  ///< advances in units of whole tiles along the logical coordinate space of the tensor
+  CUTLASS_HOST_DEVICE
+  TileIteratorSimtCanonical & operator+=(TensorCoord const &tile_offset) {
 
- private:
-  /// Underlying iterator
-  UnderlyingIterator iterator_;
+    add_tile_offset(tile_offset);
+    
+    return *this;
+  }
 
- public:
-  /// Construct a TileIterator with zero threadblock offset
+  /// Store
   CUTLASS_HOST_DEVICE
-  RegularTileAccessIterator(TensorRef ref,  ///< Pointer to start of tensor
-                            int thread_id   ///< ID of each participating thread
-                            )
-      : iterator_({ref.data(), ref.stride()}, thread_id) {}
+  void store_with_pointer_offset(Fragment const &frag, Index pointer_offset) {
 
-  /// Overrides the internal iteration index
-  CUTLASS_HOST_DEVICE
-  void set_iteration_index(int index) { iterator_.set_iteration_index(index); }
+    // de-vectorized stores
+    using ScalarAccessType = AlignedArray<Element, 1>;
+    ScalarAccessType const *scalarFragPtr = reinterpret_cast<ScalarAccessType const *>(&frag);
+    ScalarAccessType *scalarPointer = reinterpret_cast<ScalarAccessType *>(pointer_) + pointer_offset;
 
-  /// Adds a pointer offset in units of Element
-  CUTLASS_HOST_DEVICE
-  void add_pointer_offset(LongIndex pointer_offset) {
-    iterator_.add_pointer_offset(pointer_offset);
+    CUTLASS_PRAGMA_UNROLL
+    for (int n = 0; n < Policy::kAccessesPerIteration; ++n) {
+      CUTLASS_PRAGMA_UNROLL
+      for (int s = 0; s < Policy::kElementsPerAccess; s++) {
+        
+        int ptr_idx = n * Policy::MmaSimtPolicy::WarpShape::kColumn * Policy::kElementsPerAccess + s;
+        int frag_idx = n * Policy::kElementsPerAccess + s;
+        
+        int col = thread_offset_.column() + ptr_idx;
+
+        if (divisible_ || (thread_offset_.row() < extent_.row() && col < extent_.column())) {
+          scalarPointer[ptr_idx] = scalarFragPtr[frag_idx];
+        }
+      }
+    }
   }
 
-  /// Returns a pointer
+  /// Store
   CUTLASS_HOST_DEVICE
-  AccessType *get() const {
-    return reinterpret_cast<AccessType *>(iterator_.get());
+  void store(Fragment const &frag) {
+    store_with_pointer_offset(frag, 0);
   }
 
-  /// Adds a tile offset
-  CUTLASS_DEVICE
-  void add_tile_offset(TensorCoord const &coord) {
-    iterator_.add_tile_offset({coord.column(), coord.row()});
+  /// Load
+  CUTLASS_HOST_DEVICE
+  void load_with_pointer_offset(Fragment &frag, Index pointer_offset) const {
+
+      // de-vectorized loads
+      using ScalarAccessType = AlignedArray<Element, 1>;
+      ScalarAccessType *scalarFragPtr = reinterpret_cast<ScalarAccessType *>(&frag);
+      ScalarAccessType const *scalarPointer = reinterpret_cast<ScalarAccessType const*>(pointer_) + pointer_offset;
+
+      CUTLASS_PRAGMA_UNROLL
+      for (int n = 0; n < Policy::kAccessesPerIteration; ++n) {
+        CUTLASS_PRAGMA_UNROLL
+        for (int s = 0; s < Policy::kElementsPerAccess; s++) {
+          
+          int ptr_idx = n * Policy::MmaSimtPolicy::WarpShape::kColumn * Policy::kElementsPerAccess + s;
+          int frag_idx = n * Policy::kElementsPerAccess + s;
+          
+          int col = thread_offset_.column() + ptr_idx;
+
+          if (divisible_ || (thread_offset_.row() < extent_.row() && col < extent_.column())) {
+            scalarFragPtr[frag_idx] = scalarPointer[ptr_idx];
+          }
+        }
+      }
   }
 
-  /// Advances to the next tile in memory.
+  /// Load
   CUTLASS_HOST_DEVICE
-  RegularTileAccessIterator &operator++() {
-    ++iterator_;
-    return *this;
+  void load(Fragment &frag) const {
+    load_with_pointer_offset(frag, 0);
   }
 
-  /// Advances to the next tile in memory.
   CUTLASS_HOST_DEVICE
-  RegularTileAccessIterator operator++(int) {
-    RegularTileAccessIterator prev(*this);
-    ++iterator_;
+  TileIteratorSimtCanonical & operator++() {
+    return add_tile_offset({1, 0});
+  }
 
-    return prev;
+  /// Set smem base address
+  CUTLASS_HOST_DEVICE
+  void set_smem_base_address(Index address) {
   }
 };
 
-////////////////////////////////////////////////////////////////////////////////
 
-}  // namespace threadblock
-}  // namespace transform
-}  // namespace cutlass
+} // namespace warp
+} // namespace epilogue
+} // namespace cutlass
 
-////////////////////////////////////////////////////////////////////////////////
+/////////////////////////////////////////////////////////////////////////////////////////////////
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/transform/threadblock/regular_tile_access_iterator_tensor_op_sm80.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/transform/threadblock/regular_tile_access_iterator_tensor_op_sm80.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/transform/threadblock/regular_tile_iterator.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/transform/threadblock/regular_tile_iterator.h`

 * *Files 2% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/transform/threadblock/regular_tile_iterator_pitch_linear.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/transform/threadblock/regular_tile_iterator_pitch_linear.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/transform/threadblock/regular_tile_iterator_tensor_op.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/transform/threadblock/regular_tile_iterator_tensor_op.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/transform/threadblock/regular_tile_iterator_tensor_op_sm70.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/transform/threadblock/regular_tile_iterator_tensor_op_sm70.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/transform/threadblock/vector_iterator.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/transform/threadblock/vector_iterator.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/uint128.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/uint128.h`

 * *Files 2% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -50,15 +50,15 @@
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
 namespace cutlass {
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
 /// Optionally enable GCC's built-in type
-#if defined(__x86_64) && !defined(__CUDA_ARCH__) && defined(__GNUC__)
+#if (defined(__x86_64) || defined (__aarch64__)) && !defined(__CUDA_ARCH__) && defined(__GNUC__)
 #define CUTLASS_UINT128_NATIVE
 #elif defined(_MSC_VER) && defined(_M_AMD64) && !defined(__CUDA_ARCH__)
 #define CUTLASS_INT128_ARITHMETIC
 #include <intrin.h>
 #if _MSC_VER >= 1920
 #define CUTLASS_INT128_ARITHMETIC_DIV
 #include <immintrin.h>
@@ -73,14 +73,16 @@
   /// Size of one part of the uint's storage in bits
   int const kPartSize = sizeof_bits<uint64_t>::value;
 
   struct hilo {
     uint64_t lo;
     uint64_t hi;
 
+    hilo() = default;
+
     CUTLASS_HOST_DEVICE hilo(uint64_t lo_, uint64_t hi_):lo(lo_), hi(hi_) {}
   };
 
   // Use a union to store either low and high parts or, if present, a built-in 128b integer type.
   union {
     struct hilo hilo_;
 
@@ -90,16 +92,15 @@
   };
 
   //
   // Methods
   //
 
   /// Default ctor
-  CUTLASS_HOST_DEVICE
-  uint128_t(): hilo_(0, 0) { }
+  uint128_t() = default;
 
   /// Constructor from uint64
   CUTLASS_HOST_DEVICE
   uint128_t(uint64_t lo_): hilo_(lo_, 0) { }
 
   /// Constructor from two 64b unsigned integers
   CUTLASS_HOST_DEVICE
@@ -218,14 +219,15 @@
     quotient = uint64_t(native / divisor);
     remainder = uint64_t(native % divisor);
 #elif defined(CUTLASS_INT128_ARITHMETIC_DIV)
     // implemented using MSVC's arithmetic intrinsics
     quotient = _udiv128(hilo_.hi, hilo_.lo, divisor, &remainder);
 #else
     // TODO - not implemented
+    CUTLASS_UNUSED(remainder);
     CUTLASS_UNUSED(divisor);
     exception();
 #endif
     return quotient;
   }
 
   /// Left-shifts a 128b unsigned integer
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/wmma_array.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/include/cutlass/wmma_array.h`

 * *Files 8% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -37,39 +37,54 @@
 
 #include "cutlass/arch/wmma.h"
 
 #if defined(CUTLASS_ARCH_WMMA_ENABLED)
 
 #include "cutlass/cutlass.h"
 #include "cutlass/array.h"
+#include "cutlass/functional.h"
 
 namespace cutlass {
 
 ////////////////////////////////////////////////////////////////////////////////////////////////////
 
 /// Wmma array type (WmmaFragmentArray holds elements of of type nvcuda::wmma::fragment)
 template <
   /// Element type
   typename T,
   /// Number of elements in the array
   int N
 >
 class WmmaFragmentArray: public Array<T, N, true> {
 public:
+
   /// Efficient clear method (override Array::clear())
   CUTLASS_HOST_DEVICE
-  void clear() {
-
-    for(int i=0; i<Array<T, N, true>::kElements; i++) {
-
+  void clear()
+  {
+    for(int i = 0; i < Array<T, N, true>::kElements; i++)
+    {
       nvcuda::wmma::fill_fragment((*this)[i], (typename T::element_type)0);
+    }
+  }
 
+  CUTLASS_HOST_DEVICE
+  WmmaFragmentArray<T, N>& operator+=(const WmmaFragmentArray<T, N>& rhs)
+  {
+    using element_type = typename T::element_type;
+    plus<T> add;
+
+    for (int i = 0; i < Array<T, N, true>::kElements; i++)
+    {
+      (*this)[i] = add((*this)[i], rhs[i]);
     }
 
+    return *this;
   }
+
 };
 
 ////////////////////////////////////////////////////////////////////////////////////////////////////
 
 } // namespace cutlass
 
 ///////////////////////////////////////////////////////////////////////////////////////////////////
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/common/cutlass_unit_test.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/common/cutlass_unit_test.h`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/common/filter_architecture.cpp` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/common/filter_architecture.cpp`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -74,14 +74,15 @@
   test_filters[] = {
     { "SM50*",                      50, kMaxDevice},
     { "SM60*",                      60, kMaxDevice},
     { "SM61*",                      61, kMaxDevice},
     { "SM70*",                      70, 75},
     { "SM75*",                      75, kMaxDevice},
     { "SM80*",                      80, kMaxDevice},
+    { "SM90*",                      90, kMaxDevice},
     { 0, 0, false }
   };
 
   // Set negative test filters
   std::stringstream ss;
   ss << "-";
   for (int i = 0, j = 0; test_filters[i].filter; ++i) {
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/conv/device/cache_testbed_output.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/conv/device/cache_testbed_output.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_dgrad_implicit_gemm_cf32nhwc_cf32nhwc_cf32nhwc_simt_f32_sm50.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_dgrad_implicit_gemm_cf32nhwc_cf32nhwc_cf32nhwc_simt_f32_sm50.cu`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_dgrad_implicit_gemm_cf32nhwc_cf32nhwc_cf32nhwc_simt_f32_sm80.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_dgrad_implicit_gemm_cf32nhwc_cf32nhwc_cf32nhwc_simt_f32_sm80.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_dgrad_implicit_gemm_f16nhwc_f16nhwc_f16nhwc_tensor_op_f16_sm80.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_dgrad_implicit_gemm_f16nhwc_f16nhwc_f16nhwc_tensor_op_f16_sm80.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_dgrad_implicit_gemm_f16nhwc_f16nhwc_f32nhwc_tensor_op_f32_sm70.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_dgrad_implicit_gemm_f16nhwc_f16nhwc_f32nhwc_tensor_op_f32_sm70.cu`

 * *Files 2% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_dgrad_implicit_gemm_f16nhwc_f16nhwc_f32nhwc_tensor_op_f32_sm75.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_dgrad_implicit_gemm_f16nhwc_f16nhwc_f32nhwc_tensor_op_f32_sm75.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_dgrad_implicit_gemm_f16nhwc_f16nhwc_f32nhwc_tensor_op_f32_sm80.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_dgrad_implicit_gemm_f16nhwc_f16nhwc_f32nhwc_tensor_op_f32_sm80.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_dgrad_implicit_gemm_f32nhwc_f32nhwc_f32nhwc_simt_f32_sm80.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_dgrad_implicit_gemm_f32nhwc_f32nhwc_f32nhwc_simt_f32_sm80.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_dgrad_implicit_gemm_tf32nhwc_tf32nhwc_f32nhwc_tensor_op_f32_sm80.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_dgrad_implicit_gemm_tf32nhwc_tf32nhwc_f32nhwc_tensor_op_f32_sm80.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_fprop_few_channels_f16nhwc_f16nhwc_f16nhwc_tensor_op_f32_sm80.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_fprop_few_channels_f16nhwc_f16nhwc_f16nhwc_tensor_op_f32_sm80.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_fprop_fixed_channels_f16nhwc_f16nhwc_f16nhwc_tensor_op_f32_sm80.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_fprop_fixed_channels_f16nhwc_f16nhwc_f16nhwc_tensor_op_f32_sm80.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_fprop_implicit_gemm_cf32nhwc_cf32nhwc_cf32nhwc_simt_f32_sm50.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_fprop_implicit_gemm_cf32nhwc_cf32nhwc_cf32nhwc_simt_f32_sm50.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_fprop_implicit_gemm_cf32nhwc_cf32nhwc_cf32nhwc_simt_f32_sm80.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_fprop_implicit_gemm_cf32nhwc_cf32nhwc_cf32nhwc_simt_f32_sm80.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_fprop_implicit_gemm_f16nhwc_f16nhwc_f16nhwc_simt_f16_sm60.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_fprop_implicit_gemm_f16nhwc_f16nhwc_f16nhwc_simt_f16_sm60.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_fprop_implicit_gemm_f16nhwc_f16nhwc_f16nhwc_tensor_op_f16_sm80.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_strided_dgrad_implicit_gemm_f16nhwc_f16nhwc_f32nhwc_tensor_op_f32_sm80.cu`

 * *Files 11% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -31,320 +31,340 @@
 /*! \file
     \brief Tests for device-wide Implicit GEMM interface
 */
 
 #include "../../common/cutlass_unit_test.h"
 #include "cutlass/cutlass.h"
 
-
-#include "cutlass/conv/kernel/default_conv2d_fprop.h"
+#include "cutlass/conv/kernel/default_conv2d_dgrad.h"
 #include "cutlass/conv/device/implicit_gemm_convolution.h"
 
 #include "conv2d_testbed.h"
 
 #if defined(CUTLASS_ARCH_MMA_SM80_SUPPORTED)
 
 ////////////////////////////////////////////////////////////////////////////////
+//                           Strided Dgrad (Analytic)
+////////////////////////////////////////////////////////////////////////////////
+TEST(SM80_Device_Conv2d_Strided_Dgrad_Analytic_ImplicitGemm_f16nhwc_f16nhwc_f32nhwc_tensor_op_f32,
+  128x128_32x3_64x64x32) {
 
-TEST(SM80_Device_Conv2d_Fprop_Analytic_ImplicitGemm_f16nhwc_f16nhwc_f16nhwc_tensor_op_f16,
-  128x128_64x3_64x64x64) {
- 
   /// Conv operation element types for the Gemm equivalent (ImplicitGemm)
   using ElementA           = cutlass::half_t;
   using ElementB           = cutlass::half_t;
-  using ElementC           = cutlass::half_t;
-  using ElementAccumulator = cutlass::half_t;
-  using ElementCompute     = cutlass::half_t;
+  using ElementC           = float;
+  using ElementAccumulator = float;
+  using ElementCompute     = float;
 
   /// Device-level Conv2d instance
-  using Conv2dFpropKernel = typename cutlass::conv::kernel::DefaultConv2dFprop<
+  using Conv2dDgradKernel = typename cutlass::conv::kernel::DefaultConv2dDgrad<
     ElementA, cutlass::layout::TensorNHWC,
     ElementB, cutlass::layout::TensorNHWC,
     ElementC, cutlass::layout::TensorNHWC,
     ElementAccumulator,
     cutlass::arch::OpClassTensorOp,
     cutlass::arch::Sm80,
-    cutlass::gemm::GemmShape<128, 128, 64>,
-    cutlass::gemm::GemmShape<64, 64, 64>,
+    cutlass::gemm::GemmShape<128, 128, 32>,
+    cutlass::gemm::GemmShape<64, 64, 32>,
+    cutlass::gemm::GemmShape<16, 8, 16>,
+    cutlass::epilogue::thread::LinearCombination<
+      ElementC,
+      128 / cutlass::sizeof_bits<ElementC>::value,
+      ElementAccumulator,
+      ElementCompute
+    >,
+    cutlass::conv::threadblock::StridedDgradIdentityThreadblockSwizzle<>,
+    3,
+    cutlass::arch::OpMultiplyAdd,
+    cutlass::conv::IteratorAlgorithm::kAnalytic,
+    cutlass::conv::StrideSupport::kStrided
+  >::Kernel;
+
+  using Conv2dDgrad = cutlass::conv::device::ImplicitGemmConvolution<Conv2dDgradKernel>;
+
+
+  test::conv::device::Conv2dProblemVector problem_size_list;
+
+
+// run specific problem size in the unit test first
+    problem_size_list.push_back(cutlass::conv::Conv2dProblemSize(
+      {1, 4, 4, 8},     // input size (NHWC)
+      {8, 3, 3, 8},     // filter size (KRSC)
+      {0, 0, 0, 0},         // padding (pad_h, _, pad_w, _)
+      {3, 3},               // stride (stride_h, stride_w)
+      {1, 1}                // dilation (dilation_h, dilation_w)
+    ));
+
+  /// Run all unit test sizes with device-level Conv2d instance
+  EXPECT_TRUE(test::conv::device::TestAllConv2d<Conv2dDgrad>(problem_size_list));
+}
+
+////////////////////////////////////////////////////////////////////////////////
+TEST(SM80_Device_Conv2d_Strided_Dgrad_Analytic_ImplicitGemm_f16nhwc_f16nhwc_f32nhwc_tensor_op_f32,
+  128x256_32x3_64x64x32) {
+
+  /// Conv operation element types for the Gemm equivalent (ImplicitGemm)
+  using ElementA           = cutlass::half_t;
+  using ElementB           = cutlass::half_t;
+  using ElementC           = float;
+  using ElementAccumulator = float;
+  using ElementCompute     = float;
+
+  /// Device-level Conv2d instance
+  using Conv2dDgradKernel = typename cutlass::conv::kernel::DefaultConv2dDgrad<
+    ElementA, cutlass::layout::TensorNHWC,
+    ElementB, cutlass::layout::TensorNHWC,
+    ElementC, cutlass::layout::TensorNHWC,
+    ElementAccumulator,
+    cutlass::arch::OpClassTensorOp,
+    cutlass::arch::Sm80,
+    cutlass::gemm::GemmShape<128, 256, 32>,
+    cutlass::gemm::GemmShape<64, 64, 32>,
     cutlass::gemm::GemmShape<16, 8, 16>,
     cutlass::epilogue::thread::LinearCombination<
       ElementC,
       128 / cutlass::sizeof_bits<ElementC>::value,
       ElementAccumulator,
       ElementCompute
     >,
-    cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>,
+    cutlass::conv::threadblock::StridedDgradIdentityThreadblockSwizzle<>,
     3,
     cutlass::arch::OpMultiplyAdd,
-    cutlass::conv::IteratorAlgorithm::kAnalytic
+    cutlass::conv::IteratorAlgorithm::kAnalytic,
+    cutlass::conv::StrideSupport::kStrided
   >::Kernel;
 
-  using Conv2dFprop = cutlass::conv::device::ImplicitGemmConvolution<Conv2dFpropKernel>;
-  
+  using Conv2dDgrad = cutlass::conv::device::ImplicitGemmConvolution<Conv2dDgradKernel>;
+
   /// Run all unit test sizes with device-level Conv2d instance
-  EXPECT_TRUE(test::conv::device::TestAllConv2d<Conv2dFprop>());
+  EXPECT_TRUE(test::conv::device::TestAllConv2d<Conv2dDgrad>());
 }
 
 ////////////////////////////////////////////////////////////////////////////////
+TEST(SM80_Device_Conv2d_Strided_Dgrad_Analytic_ImplicitGemm_f16nhwc_f16nhwc_f32nhwc_tensor_op_f32,
+  128x256_64x3_64x64x64) {
 
-TEST(SM80_Device_Conv2d_Fprop_Optimized_ImplicitGemm_f16nhwc_f16nhwc_f16nhwc_tensor_op_f16,
-  128x128_64x3_64x64x64) {
- 
   /// Conv operation element types for the Gemm equivalent (ImplicitGemm)
   using ElementA           = cutlass::half_t;
   using ElementB           = cutlass::half_t;
-  using ElementC           = cutlass::half_t;
-  using ElementAccumulator = cutlass::half_t;
-  using ElementCompute     = cutlass::half_t;
+  using ElementC           = float;
+  using ElementAccumulator = float;
+  using ElementCompute     = float;
 
   /// Device-level Conv2d instance
-  using Conv2dFpropKernel = typename cutlass::conv::kernel::DefaultConv2dFprop<
+  using Conv2dDgradKernel = typename cutlass::conv::kernel::DefaultConv2dDgrad<
     ElementA, cutlass::layout::TensorNHWC,
     ElementB, cutlass::layout::TensorNHWC,
     ElementC, cutlass::layout::TensorNHWC,
     ElementAccumulator,
     cutlass::arch::OpClassTensorOp,
     cutlass::arch::Sm80,
-    cutlass::gemm::GemmShape<128, 128, 64>,
+    cutlass::gemm::GemmShape<128, 256, 64>,
     cutlass::gemm::GemmShape<64, 64, 64>,
     cutlass::gemm::GemmShape<16, 8, 16>,
     cutlass::epilogue::thread::LinearCombination<
       ElementC,
       128 / cutlass::sizeof_bits<ElementC>::value,
       ElementAccumulator,
       ElementCompute
     >,
-    cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>,
+    cutlass::conv::threadblock::StridedDgradIdentityThreadblockSwizzle<>,
     3,
     cutlass::arch::OpMultiplyAdd,
-    cutlass::conv::IteratorAlgorithm::kOptimized
+    cutlass::conv::IteratorAlgorithm::kAnalytic,
+    cutlass::conv::StrideSupport::kStrided
   >::Kernel;
 
-  using Conv2dFprop = cutlass::conv::device::ImplicitGemmConvolution<Conv2dFpropKernel>;
-  
+  using Conv2dDgrad = cutlass::conv::device::ImplicitGemmConvolution<Conv2dDgradKernel>;
+
   /// Run all unit test sizes with device-level Conv2d instance
-  EXPECT_TRUE(test::conv::device::TestAllConv2d<Conv2dFprop>());
+  EXPECT_TRUE(test::conv::device::TestAllConv2d<Conv2dDgrad>());
 }
 
 ////////////////////////////////////////////////////////////////////////////////
 
-TEST(SM80_Device_Conv2d_Fprop_Analytic_ImplicitGemm_f16nhwc_f16nhwc_f16nhwc_tensor_op_f16_align2,
-  128x128_64x3_64x64x64) {
+TEST(SM80_Device_Conv2d_Strided_Dgrad_Analytic_ImplicitGemm_f16nhwc_f16nhwc_f32nhwc_tensor_op_f32_align4,
+  128x128_32x3_64x64x32) {
 
   /// Conv operation element types for the Gemm equivalent (ImplicitGemm)
   using ElementA           = cutlass::half_t;
   using ElementB           = cutlass::half_t;
-  using ElementC           = cutlass::half_t;
-  using ElementAccumulator = cutlass::half_t;
-  using ElementCompute     = cutlass::half_t;
+  using ElementC           = float;
+  using ElementAccumulator = float;
+  using ElementCompute     = float;
 
   /// Device-level Conv2d instance
-  using Conv2dFpropKernel = typename cutlass::conv::kernel::DefaultConv2dFprop<
+  using Conv2dDgradKernel = typename cutlass::conv::kernel::DefaultConv2dDgrad<
     ElementA, cutlass::layout::TensorNHWC,
     ElementB, cutlass::layout::TensorNHWC,
     ElementC, cutlass::layout::TensorNHWC,
     ElementAccumulator,
     cutlass::arch::OpClassTensorOp,
     cutlass::arch::Sm80,
-    cutlass::gemm::GemmShape<128, 128, 64>,
-    cutlass::gemm::GemmShape<64, 64, 64>,
+    cutlass::gemm::GemmShape<128, 128, 32>,
+    cutlass::gemm::GemmShape<64, 64, 32>,
     cutlass::gemm::GemmShape<16, 8, 16>,
     cutlass::epilogue::thread::LinearCombination<
       ElementC,
-      8,
+      4,
       ElementAccumulator,
       ElementCompute
     >,
-    cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>,
+    cutlass::conv::threadblock::StridedDgradIdentityThreadblockSwizzle<>,
     3,
     cutlass::arch::OpMultiplyAdd,
     cutlass::conv::IteratorAlgorithm::kAnalytic,
     cutlass::conv::StrideSupport::kStrided,
-    2,
-    2
+    4,
+    4
   >::Kernel;
 
-  using Conv2dFprop = cutlass::conv::device::ImplicitGemmConvolution<Conv2dFpropKernel>;
+  using Conv2dDgrad = cutlass::conv::device::ImplicitGemmConvolution<Conv2dDgradKernel>;
 
   test::conv::device::Conv2dProblemVector problem_size_list;
 
   // run specific problem size in the unit test first
   problem_size_list.push_back(cutlass::conv::Conv2dProblemSize(
     {1, 4, 4, 12},     // input size (NHWC)
     {8, 3, 3, 12},     // filter size (KRSC)
     {0, 0, 0, 0},      // padding (pad_h, _, pad_w, _)
     {3, 3},            // stride (stride_h, stride_w)
     {1, 1}             // dilation (dilation_h, dilation_w)
   ));
 
-  // run specific problem size in the unit test first
-  problem_size_list.push_back(cutlass::conv::Conv2dProblemSize(
-    {1, 4, 4, 14},     // input size (NHWC)
-    {8, 3, 3, 14},     // filter size (KRSC)
-    {0, 0, 0, 0},      // padding (pad_h, _, pad_w, _)
-    {3, 3},            // stride (stride_h, stride_w)
-    {1, 1}             // dilation (dilation_h, dilation_w)
-  ));
-
-  // run specific problem size in the unit test first
-  problem_size_list.push_back(cutlass::conv::Conv2dProblemSize(
-    {1, 23, 56, 98},     // input size (NHWC)
-    {128, 3, 3, 98},     // filter size (KRSC)
-    {4, 0, 5, 0},      // padding (pad_h, _, pad_w, _)
-    {3, 3},            // stride (stride_h, stride_w)
-    {1, 1}             // dilation (dilation_h, dilation_w)
-  ));
-
-
   /// Run all unit test sizes with device-level Conv2d instance
-  EXPECT_TRUE(test::conv::device::TestAllConv2d<Conv2dFprop>(problem_size_list));
+  EXPECT_TRUE(test::conv::device::TestAllConv2d<Conv2dDgrad>(problem_size_list));
 }
 
 ////////////////////////////////////////////////////////////////////////////////
 
-TEST(SM80_Device_Conv2d_Fprop_Optimized_ImplicitGemm_f16nhwc_f16nhwc_f16nhwc_tensor_op_f16_align2,
-  128x128_64x3_64x64x64) {
+////////////////////////////////////////////////////////////////////////////////
+//                           Strided Dgrad (Optimized)
+////////////////////////////////////////////////////////////////////////////////
+
+TEST(SM80_Device_Conv2d_Strided_Dgrad_Optimized_ImplicitGemm_f16nhwc_f16nhwc_f32nhwc_tensor_op_f32,
+  128x128_32x3_64x64x32) {
 
   /// Conv operation element types for the Gemm equivalent (ImplicitGemm)
   using ElementA           = cutlass::half_t;
   using ElementB           = cutlass::half_t;
-  using ElementC           = cutlass::half_t;
-  using ElementAccumulator = cutlass::half_t;
-  using ElementCompute     = cutlass::half_t;
+  using ElementC           = float;
+  using ElementAccumulator = float;
+  using ElementCompute     = float;
 
   /// Device-level Conv2d instance
-  using Conv2dFpropKernel = typename cutlass::conv::kernel::DefaultConv2dFprop<
+  using Conv2dDgradKernel = typename cutlass::conv::kernel::DefaultConv2dDgrad<
     ElementA, cutlass::layout::TensorNHWC,
     ElementB, cutlass::layout::TensorNHWC,
     ElementC, cutlass::layout::TensorNHWC,
     ElementAccumulator,
     cutlass::arch::OpClassTensorOp,
     cutlass::arch::Sm80,
-    cutlass::gemm::GemmShape<128, 128, 64>,
-    cutlass::gemm::GemmShape<64, 64, 64>,
+    cutlass::gemm::GemmShape<128, 128, 32>,
+    cutlass::gemm::GemmShape<64, 64, 32>,
     cutlass::gemm::GemmShape<16, 8, 16>,
     cutlass::epilogue::thread::LinearCombination<
       ElementC,
-      8,
+      128 / cutlass::sizeof_bits<ElementC>::value,
       ElementAccumulator,
       ElementCompute
     >,
-    cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>,
+    cutlass::conv::threadblock::StridedDgradIdentityThreadblockSwizzle<>,
     3,
     cutlass::arch::OpMultiplyAdd,
     cutlass::conv::IteratorAlgorithm::kOptimized,
-    cutlass::conv::StrideSupport::kStrided,
-    2,
-    2
+    cutlass::conv::StrideSupport::kStrided
   >::Kernel;
 
-  using Conv2dFprop = cutlass::conv::device::ImplicitGemmConvolution<Conv2dFpropKernel>;
-
-  test::conv::device::Conv2dProblemVector problem_size_list;
+  using Conv2dDgrad = cutlass::conv::device::ImplicitGemmConvolution<Conv2dDgradKernel>;
 
-  // run specific problem size in the unit test first
-  problem_size_list.push_back(cutlass::conv::Conv2dProblemSize(
-    {1, 4, 4, 12},     // input size (NHWC)
-    {8, 3, 3, 12},     // filter size (KRSC)
-    {0, 0, 0, 0},      // padding (pad_h, _, pad_w, _)
-    {3, 3},            // stride (stride_h, stride_w)
-    {1, 1}             // dilation (dilation_h, dilation_w)
-  ));
-
-  // run specific problem size in the unit test first
-  problem_size_list.push_back(cutlass::conv::Conv2dProblemSize(
-    {1, 4, 4, 14},     // input size (NHWC)
-    {8, 3, 3, 14},     // filter size (KRSC)
-    {0, 0, 0, 0},      // padding (pad_h, _, pad_w, _)
-    {3, 3},            // stride (stride_h, stride_w)
-    {1, 1}             // dilation (dilation_h, dilation_w)
-  ));
 
-  // run specific problem size in the unit test first
-  problem_size_list.push_back(cutlass::conv::Conv2dProblemSize(
-    {1, 23, 56, 98},     // input size (NHWC)
-    {128, 3, 3, 98},     // filter size (KRSC)
-    {4, 0, 5, 0},      // padding (pad_h, _, pad_w, _)
-    {3, 3},            // stride (stride_h, stride_w)
-    {1, 1}             // dilation (dilation_h, dilation_w)
-  ));
+  test::conv::device::Conv2dProblemVector problem_size_list;
 
+ // run specific problem size in the unit test first
+    problem_size_list.push_back(cutlass::conv::Conv2dProblemSize(
+      {1, 56, 56, 8},   // input size (NHWC)
+      {8, 1, 1, 8},   // filter size (KRSC)
+      {0, 0, 0, 0},       // padding (pad_h, _, pad_w, _)
+      {2, 2},             // stride (stride_h, stride_w)
+      {1, 1}              // dilation (dilation_h, dilation_w)
+    ));
+
+    problem_size_list.push_back(cutlass::conv::Conv2dProblemSize(
+      {1, 55, 55, 8},   // input size (NHWC)
+      {8, 1, 1, 8},   // filter size (KRSC)
+      {0, 0, 0, 0},       // padding (pad_h, _, pad_w, _)
+      {2, 2},             // stride (stride_h, stride_w)
+      {1, 1}              // dilation (dilation_h, dilation_w)
+    ));
 
   /// Run all unit test sizes with device-level Conv2d instance
-  EXPECT_TRUE(test::conv::device::TestAllConv2d<Conv2dFprop>(problem_size_list));
+  EXPECT_TRUE(test::conv::device::TestAllConv2d<Conv2dDgrad>(problem_size_list));
 }
 
 ////////////////////////////////////////////////////////////////////////////////
 
-TEST(SM80_Device_Conv2d_Fprop_Optimized_ImplicitGemm_f16nhwc_f16nhwc_f16nhwc_tensor_op_f16_align4,
-  128x128_64x3_64x64x64) {
+TEST(SM80_Device_Conv2d_Strided_Dgrad_Optimized_ImplicitGemm_f16nhwc_f16nhwc_f32nhwc_tensor_op_f32_align4,
+  128x128_32x3_64x64x32) {
 
   /// Conv operation element types for the Gemm equivalent (ImplicitGemm)
   using ElementA           = cutlass::half_t;
   using ElementB           = cutlass::half_t;
-  using ElementC           = cutlass::half_t;
-  using ElementAccumulator = cutlass::half_t;
-  using ElementCompute     = cutlass::half_t;
+  using ElementC           = float;
+  using ElementAccumulator = float;
+  using ElementCompute     = float;
 
   /// Device-level Conv2d instance
-  using Conv2dFpropKernel = typename cutlass::conv::kernel::DefaultConv2dFprop<
+  using Conv2dDgradKernel = typename cutlass::conv::kernel::DefaultConv2dDgrad<
     ElementA, cutlass::layout::TensorNHWC,
     ElementB, cutlass::layout::TensorNHWC,
     ElementC, cutlass::layout::TensorNHWC,
     ElementAccumulator,
     cutlass::arch::OpClassTensorOp,
     cutlass::arch::Sm80,
-    cutlass::gemm::GemmShape<128, 128, 64>,
-    cutlass::gemm::GemmShape<64, 64, 64>,
+    cutlass::gemm::GemmShape<128, 128, 32>,
+    cutlass::gemm::GemmShape<64, 64, 32>,
     cutlass::gemm::GemmShape<16, 8, 16>,
     cutlass::epilogue::thread::LinearCombination<
       ElementC,
-      8,
+      4,
       ElementAccumulator,
       ElementCompute
     >,
-    cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>,
+    cutlass::conv::threadblock::StridedDgradIdentityThreadblockSwizzle<>,
     3,
     cutlass::arch::OpMultiplyAdd,
     cutlass::conv::IteratorAlgorithm::kOptimized,
     cutlass::conv::StrideSupport::kStrided,
     4,
     4
   >::Kernel;
 
-  using Conv2dFprop = cutlass::conv::device::ImplicitGemmConvolution<Conv2dFpropKernel>;
+  using Conv2dDgrad = cutlass::conv::device::ImplicitGemmConvolution<Conv2dDgradKernel>;
+
 
   test::conv::device::Conv2dProblemVector problem_size_list;
 
   // run specific problem size in the unit test first
   problem_size_list.push_back(cutlass::conv::Conv2dProblemSize(
-    {1, 4, 4, 12},     // input size (NHWC)
-    {8, 3, 3, 12},     // filter size (KRSC)
-    {0, 0, 0, 0},      // padding (pad_h, _, pad_w, _)
-    {3, 3},            // stride (stride_h, stride_w)
-    {1, 1}             // dilation (dilation_h, dilation_w)
+    {1, 56, 56, 12},   // input size (NHWC)
+    {8, 1, 1, 12},     // filter size (KRSC)
+    {0, 0, 0, 0},     // padding (pad_h, _, pad_w, _)
+    {2, 2},           // stride (stride_h, stride_w)
+    {1, 1}            // dilation (dilation_h, dilation_w)
   ));
 
-  // run specific problem size in the unit test first
   problem_size_list.push_back(cutlass::conv::Conv2dProblemSize(
-    {1, 4, 4, 28},     // input size (NHWC)
-    {8, 3, 3, 28},     // filter size (KRSC)
-    {0, 0, 0, 0},      // padding (pad_h, _, pad_w, _)
-    {3, 3},            // stride (stride_h, stride_w)
-    {1, 1}             // dilation (dilation_h, dilation_w)
+    {1, 55, 55, 12},   // input size (NHWC)
+    {8, 1, 1, 12},     // filter size (KRSC)
+    {0, 0, 0, 0},     // padding (pad_h, _, pad_w, _)
+    {2, 2},           // stride (stride_h, stride_w)
+    {1, 1}            // dilation (dilation_h, dilation_w)
   ));
 
-  // run specific problem size in the unit test first
-  problem_size_list.push_back(cutlass::conv::Conv2dProblemSize(
-    {1, 23, 56, 100},     // input size (NHWC)
-    {128, 3, 3, 100},     // filter size (KRSC)
-    {4, 0, 5, 0},      // padding (pad_h, _, pad_w, _)
-    {3, 3},            // stride (stride_h, stride_w)
-    {1, 1}             // dilation (dilation_h, dilation_w)
-  ));
-  
   /// Run all unit test sizes with device-level Conv2d instance
-  EXPECT_TRUE(test::conv::device::TestAllConv2d<Conv2dFprop>(problem_size_list));
+  EXPECT_TRUE(test::conv::device::TestAllConv2d<Conv2dDgrad>(problem_size_list));
 }
 
 ////////////////////////////////////////////////////////////////////////////////
+
 #endif  // CUTLASS_ARCH_MMA_SM80_SUPPORTED
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_fprop_implicit_gemm_f16nhwc_f16nhwc_f16nhwc_tensor_op_f32_sm80.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_fprop_implicit_gemm_f16nhwc_f16nhwc_f16nhwc_tensor_op_f32_sm80.cu`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_fprop_implicit_gemm_f16nhwc_f16nhwc_f32nhwc_tensor_op_f32_sm70.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_fprop_implicit_gemm_f16nhwc_f16nhwc_f32nhwc_tensor_op_f32_sm70.cu`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_fprop_implicit_gemm_f16nhwc_f16nhwc_f32nhwc_tensor_op_f32_sm75.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_fprop_implicit_gemm_f16nhwc_f16nhwc_f32nhwc_tensor_op_f32_sm75.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_fprop_implicit_gemm_f16nhwc_f16nhwc_f32nhwc_tensor_op_f32_sm80.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_fprop_implicit_gemm_f16nhwc_f16nhwc_f32nhwc_tensor_op_f32_sm80.cu`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_fprop_implicit_gemm_f32nhwc_f32nhwc_f32nhwc_simt_f32_sm50.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_fprop_implicit_gemm_f32nhwc_f32nhwc_f32nhwc_simt_f32_sm50.cu`

 * *Files 4% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_fprop_implicit_gemm_f32nhwc_f32nhwc_f32nhwc_simt_f32_sm80.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_fprop_implicit_gemm_f32nhwc_f32nhwc_f32nhwc_simt_f32_sm80.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_fprop_implicit_gemm_qf32nhwc_qf32nhwc_qf32nhwc_simt_f32_sm50.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_fprop_implicit_gemm_qf32nhwc_qf32nhwc_qf32nhwc_simt_f32_sm50.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_fprop_implicit_gemm_s4ncxhwx_s4cxrskx_s4ncxhwx_tensor_op_s32_sm75.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_fprop_implicit_gemm_s4ncxhwx_s4cxrskx_s4ncxhwx_tensor_op_s32_sm75.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_fprop_implicit_gemm_s4ncxhwx_s4cxrskx_s4ncxhwx_tensor_op_s32_sm80.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_fprop_implicit_gemm_s4ncxhwx_s4cxrskx_s4ncxhwx_tensor_op_s32_sm80.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_fprop_implicit_gemm_s4nhwc_s4nhwc_s32nhwc_tensor_op_s32_sm75.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_fprop_implicit_gemm_s4nhwc_s4nhwc_s32nhwc_tensor_op_s32_sm75.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_fprop_implicit_gemm_s4nhwc_s4nhwc_s32nhwc_tensor_op_s32_sm80.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_fprop_implicit_gemm_s4nhwc_s4nhwc_s32nhwc_tensor_op_s32_sm80.cu`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_fprop_implicit_gemm_s8ncxhwx_s8cxrskx_s8ncxhwx_tensor_op_s32_sm75.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_fprop_implicit_gemm_s8ncxhwx_s8cxrskx_s8ncxhwx_tensor_op_s32_sm75.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_fprop_implicit_gemm_s8ncxhwx_s8cxrskx_s8ncxhwx_tensor_op_s32_sm80.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_fprop_implicit_gemm_s8ncxhwx_s8cxrskx_s8ncxhwx_tensor_op_s32_sm80.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_fprop_implicit_gemm_s8nhwc_s8nhwc_s32nhwc_tensor_op_s32_sm75.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_fprop_implicit_gemm_s8nhwc_s8nhwc_s32nhwc_tensor_op_s32_sm75.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_fprop_implicit_gemm_s8nhwc_s8nhwc_s32nhwc_tensor_op_s32_sm80.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_fprop_implicit_gemm_s8nhwc_s8nhwc_s32nhwc_tensor_op_s32_sm80.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_fprop_implicit_gemm_tf32nhwc_tf32nhwc_f32nhwc_tensor_op_f32_sm80.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_fprop_implicit_gemm_tf32nhwc_tf32nhwc_f32nhwc_tensor_op_f32_sm80.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_fprop_with_broadcast_sm70.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_fprop_with_broadcast_sm70.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_fprop_with_broadcast_sm75.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_fprop_with_broadcast_sm75.cu`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_fprop_with_reduction_sm75.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_fprop_with_reduction_sm75.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_problems.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_problems.h`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -772,14 +772,37 @@
       {1, 1},                                         // stride (stride_h, stride_w)
       {1, 1},                                         // dilation (dilation_h, dilation_w)
       cutlass::conv::Mode::kCrossCorrelation,
       1,                                              // split_k_slices
       2                                               // groups
     ));
 
+    // Larger problem sizes
+    
+    default_single_group_sizes.push_back(cutlass::conv::Conv2dProblemSize(
+      {1, 56, 56, 696},                               // input size  (NHWC)
+      {768, 3, 3, 232},                               // filter size (KRSC)
+      {1, 1, 1, 1},                                   // padding (pad_h, _, pad_w, _)
+      {2, 2},                                         // stride (stride_h, stride_w)
+      {1, 1},                                         // dilation (dilation_h, dilation_w)
+      cutlass::conv::Mode::kCrossCorrelation,
+      1,                                              // split_k_slices
+      3                                               // groups
+    ));
+    default_single_group_sizes.push_back(cutlass::conv::Conv2dProblemSize(
+      {1, 14, 14, 1392},                              // input size  (NHWC)
+      {1536, 3, 3, 232},                              // filter size (KRSC)
+      {1, 1, 1, 1},                                   // padding (pad_h, _, pad_w, _)
+      {1, 1},                                         // stride (stride_h, stride_w)
+      {1, 1},                                         // dilation (dilation_h, dilation_w)
+      cutlass::conv::Mode::kCrossCorrelation,
+      1,                                              // split_k_slices
+      3                                               // groups
+    ));
+
     ////////////////////////////////////////////////////////////////////////////////////
     // One CTA calculate multiple groups: CTA::N % k_per_group = 0
     ////////////////////////////////////////////////////////////////////////////////////
 
     // 2 groups per CTA
     default_multiple_group_sizes.push_back(cutlass::conv::Conv2dProblemSize(
       {1, 8, 8, threadblock_k * 4},                   // input size  (NHWC)
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_strided_dgrad_implicit_gemm_f16nhwc_f16nhwc_f32nhwc_tensor_op_f32_sm80.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_wgrad_implicit_gemm_f16nhwc_f16nhwc_f32nhwc_tensor_op_f32_sm75.cu`

 * *Files 25% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -31,340 +31,165 @@
 /*! \file
     \brief Tests for device-wide Implicit GEMM interface
 */
 
 #include "../../common/cutlass_unit_test.h"
 #include "cutlass/cutlass.h"
 
-#include "cutlass/conv/kernel/default_conv2d_dgrad.h"
+#include "cutlass/conv/kernel/default_conv2d_wgrad.h"
 #include "cutlass/conv/device/implicit_gemm_convolution.h"
 
 #include "conv2d_testbed.h"
 
-#if defined(CUTLASS_ARCH_MMA_SM80_SUPPORTED)
+#if defined(CUTLASS_ARCH_MMA_SM75_SUPPORTED)
 
 ////////////////////////////////////////////////////////////////////////////////
-//                           Strided Dgrad (Analytic)
-////////////////////////////////////////////////////////////////////////////////
-TEST(SM80_Device_Conv2d_Strided_Dgrad_Analytic_ImplicitGemm_f16nhwc_f16nhwc_f32nhwc_tensor_op_f32,
-  128x128_32x3_64x64x32) {
 
+TEST(SM75_Device_Conv2d_Wgrad_Analytic_ImplicitGemm_f16nhwc_f16nhwc_f32nhwc_tensor_op_f32,
+  128x128_32x2_64x64x32) {
+  
   /// Conv operation element types for the Gemm equivalent (ImplicitGemm)
   using ElementA           = cutlass::half_t;
   using ElementB           = cutlass::half_t;
   using ElementC           = float;
   using ElementAccumulator = float;
   using ElementCompute     = float;
 
-  /// Device-level Conv2d instance
-  using Conv2dDgradKernel = typename cutlass::conv::kernel::DefaultConv2dDgrad<
+  using Conv2dWgradKernel = typename cutlass::conv::kernel::DefaultConv2dWgrad<
     ElementA, cutlass::layout::TensorNHWC,
     ElementB, cutlass::layout::TensorNHWC,
     ElementC, cutlass::layout::TensorNHWC,
     ElementAccumulator,
     cutlass::arch::OpClassTensorOp,
-    cutlass::arch::Sm80,
+    cutlass::arch::Sm75,
     cutlass::gemm::GemmShape<128, 128, 32>,
     cutlass::gemm::GemmShape<64, 64, 32>,
-    cutlass::gemm::GemmShape<16, 8, 16>,
-    cutlass::epilogue::thread::LinearCombination<
-      ElementC,
-      128 / cutlass::sizeof_bits<ElementC>::value,
-      ElementAccumulator,
-      ElementCompute
-    >,
-    cutlass::conv::threadblock::StridedDgradIdentityThreadblockSwizzle<>,
-    3,
-    cutlass::arch::OpMultiplyAdd,
-    cutlass::conv::IteratorAlgorithm::kAnalytic,
-    cutlass::conv::StrideSupport::kStrided
-  >::Kernel;
-
-  using Conv2dDgrad = cutlass::conv::device::ImplicitGemmConvolution<Conv2dDgradKernel>;
-
-
-  test::conv::device::Conv2dProblemVector problem_size_list;
-
-
-// run specific problem size in the unit test first
-    problem_size_list.push_back(cutlass::conv::Conv2dProblemSize(
-      {1, 4, 4, 8},     // input size (NHWC)
-      {8, 3, 3, 8},     // filter size (KRSC)
-      {0, 0, 0, 0},         // padding (pad_h, _, pad_w, _)
-      {3, 3},               // stride (stride_h, stride_w)
-      {1, 1}                // dilation (dilation_h, dilation_w)
-    ));
-
-  /// Run all unit test sizes with device-level Conv2d instance
-  EXPECT_TRUE(test::conv::device::TestAllConv2d<Conv2dDgrad>(problem_size_list));
-}
-
-////////////////////////////////////////////////////////////////////////////////
-TEST(SM80_Device_Conv2d_Strided_Dgrad_Analytic_ImplicitGemm_f16nhwc_f16nhwc_f32nhwc_tensor_op_f32,
-  128x256_32x3_64x64x32) {
-
-  /// Conv operation element types for the Gemm equivalent (ImplicitGemm)
-  using ElementA           = cutlass::half_t;
-  using ElementB           = cutlass::half_t;
-  using ElementC           = float;
-  using ElementAccumulator = float;
-  using ElementCompute     = float;
-
-  /// Device-level Conv2d instance
-  using Conv2dDgradKernel = typename cutlass::conv::kernel::DefaultConv2dDgrad<
-    ElementA, cutlass::layout::TensorNHWC,
-    ElementB, cutlass::layout::TensorNHWC,
-    ElementC, cutlass::layout::TensorNHWC,
-    ElementAccumulator,
-    cutlass::arch::OpClassTensorOp,
-    cutlass::arch::Sm80,
-    cutlass::gemm::GemmShape<128, 256, 32>,
-    cutlass::gemm::GemmShape<64, 64, 32>,
-    cutlass::gemm::GemmShape<16, 8, 16>,
+    cutlass::gemm::GemmShape<16, 8, 8>,
     cutlass::epilogue::thread::LinearCombination<
       ElementC,
       128 / cutlass::sizeof_bits<ElementC>::value,
       ElementAccumulator,
       ElementCompute
     >,
-    cutlass::conv::threadblock::StridedDgradIdentityThreadblockSwizzle<>,
-    3,
-    cutlass::arch::OpMultiplyAdd,
-    cutlass::conv::IteratorAlgorithm::kAnalytic,
-    cutlass::conv::StrideSupport::kStrided
+    cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>,
+    2,
+    cutlass::arch::OpMultiplyAdd
   >::Kernel;
 
-  using Conv2dDgrad = cutlass::conv::device::ImplicitGemmConvolution<Conv2dDgradKernel>;
+  using Conv2dWgrad = cutlass::conv::device::ImplicitGemmConvolution<Conv2dWgradKernel>;
 
   /// Run all unit test sizes with device-level Conv2d instance
-  EXPECT_TRUE(test::conv::device::TestAllConv2d<Conv2dDgrad>());
+  EXPECT_TRUE(test::conv::device::TestAllConv2d<Conv2dWgrad>());
 }
 
 ////////////////////////////////////////////////////////////////////////////////
-TEST(SM80_Device_Conv2d_Strided_Dgrad_Analytic_ImplicitGemm_f16nhwc_f16nhwc_f32nhwc_tensor_op_f32,
-  128x256_64x3_64x64x64) {
 
+TEST(SM75_Device_Conv2d_Wgrad_Analytic_ImplicitGemm_f16nhwc_f16nhwc_f32nhwc_tensor_op_f32_align4,
+  128x128_32x2_64x64x32) {
+  
   /// Conv operation element types for the Gemm equivalent (ImplicitGemm)
   using ElementA           = cutlass::half_t;
   using ElementB           = cutlass::half_t;
   using ElementC           = float;
   using ElementAccumulator = float;
   using ElementCompute     = float;
 
-  /// Device-level Conv2d instance
-  using Conv2dDgradKernel = typename cutlass::conv::kernel::DefaultConv2dDgrad<
+  using Conv2dWgradKernel = typename cutlass::conv::kernel::DefaultConv2dWgrad<
     ElementA, cutlass::layout::TensorNHWC,
     ElementB, cutlass::layout::TensorNHWC,
     ElementC, cutlass::layout::TensorNHWC,
     ElementAccumulator,
     cutlass::arch::OpClassTensorOp,
-    cutlass::arch::Sm80,
-    cutlass::gemm::GemmShape<128, 256, 64>,
-    cutlass::gemm::GemmShape<64, 64, 64>,
-    cutlass::gemm::GemmShape<16, 8, 16>,
-    cutlass::epilogue::thread::LinearCombination<
-      ElementC,
-      128 / cutlass::sizeof_bits<ElementC>::value,
-      ElementAccumulator,
-      ElementCompute
-    >,
-    cutlass::conv::threadblock::StridedDgradIdentityThreadblockSwizzle<>,
-    3,
-    cutlass::arch::OpMultiplyAdd,
-    cutlass::conv::IteratorAlgorithm::kAnalytic,
-    cutlass::conv::StrideSupport::kStrided
-  >::Kernel;
-
-  using Conv2dDgrad = cutlass::conv::device::ImplicitGemmConvolution<Conv2dDgradKernel>;
-
-  /// Run all unit test sizes with device-level Conv2d instance
-  EXPECT_TRUE(test::conv::device::TestAllConv2d<Conv2dDgrad>());
-}
-
-////////////////////////////////////////////////////////////////////////////////
-
-TEST(SM80_Device_Conv2d_Strided_Dgrad_Analytic_ImplicitGemm_f16nhwc_f16nhwc_f32nhwc_tensor_op_f32_align4,
-  128x128_32x3_64x64x32) {
-
-  /// Conv operation element types for the Gemm equivalent (ImplicitGemm)
-  using ElementA           = cutlass::half_t;
-  using ElementB           = cutlass::half_t;
-  using ElementC           = float;
-  using ElementAccumulator = float;
-  using ElementCompute     = float;
-
-  /// Device-level Conv2d instance
-  using Conv2dDgradKernel = typename cutlass::conv::kernel::DefaultConv2dDgrad<
-    ElementA, cutlass::layout::TensorNHWC,
-    ElementB, cutlass::layout::TensorNHWC,
-    ElementC, cutlass::layout::TensorNHWC,
-    ElementAccumulator,
-    cutlass::arch::OpClassTensorOp,
-    cutlass::arch::Sm80,
+    cutlass::arch::Sm75,
     cutlass::gemm::GemmShape<128, 128, 32>,
     cutlass::gemm::GemmShape<64, 64, 32>,
-    cutlass::gemm::GemmShape<16, 8, 16>,
+    cutlass::gemm::GemmShape<16, 8, 8>,
     cutlass::epilogue::thread::LinearCombination<
       ElementC,
       4,
       ElementAccumulator,
       ElementCompute
     >,
-    cutlass::conv::threadblock::StridedDgradIdentityThreadblockSwizzle<>,
-    3,
+    cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>,
+    2,
     cutlass::arch::OpMultiplyAdd,
     cutlass::conv::IteratorAlgorithm::kAnalytic,
     cutlass::conv::StrideSupport::kStrided,
     4,
     4
   >::Kernel;
 
-  using Conv2dDgrad = cutlass::conv::device::ImplicitGemmConvolution<Conv2dDgradKernel>;
+  using Conv2dWgrad = cutlass::conv::device::ImplicitGemmConvolution<Conv2dWgradKernel>;
 
   test::conv::device::Conv2dProblemVector problem_size_list;
 
   // run specific problem size in the unit test first
   problem_size_list.push_back(cutlass::conv::Conv2dProblemSize(
     {1, 4, 4, 12},     // input size (NHWC)
     {8, 3, 3, 12},     // filter size (KRSC)
     {0, 0, 0, 0},      // padding (pad_h, _, pad_w, _)
     {3, 3},            // stride (stride_h, stride_w)
     {1, 1}             // dilation (dilation_h, dilation_w)
   ));
 
   /// Run all unit test sizes with device-level Conv2d instance
-  EXPECT_TRUE(test::conv::device::TestAllConv2d<Conv2dDgrad>(problem_size_list));
+  EXPECT_TRUE(test::conv::device::TestAllConv2d<Conv2dWgrad>(problem_size_list));
 }
 
 ////////////////////////////////////////////////////////////////////////////////
 
-////////////////////////////////////////////////////////////////////////////////
-//                           Strided Dgrad (Optimized)
-////////////////////////////////////////////////////////////////////////////////
-
-TEST(SM80_Device_Conv2d_Strided_Dgrad_Optimized_ImplicitGemm_f16nhwc_f16nhwc_f32nhwc_tensor_op_f32,
-  128x128_32x3_64x64x32) {
-
+TEST(SM75_Device_Conv2d_Wgrad_Optimized_ImplicitGemm_f16nhwc_f16nhwc_f32nhwc_tensor_op_f32_align4,
+  128x128_32x2_64x64x32) {
+  
   /// Conv operation element types for the Gemm equivalent (ImplicitGemm)
   using ElementA           = cutlass::half_t;
   using ElementB           = cutlass::half_t;
   using ElementC           = float;
   using ElementAccumulator = float;
   using ElementCompute     = float;
 
-  /// Device-level Conv2d instance
-  using Conv2dDgradKernel = typename cutlass::conv::kernel::DefaultConv2dDgrad<
+  using Conv2dWgradKernel = typename cutlass::conv::kernel::DefaultConv2dWgrad<
     ElementA, cutlass::layout::TensorNHWC,
     ElementB, cutlass::layout::TensorNHWC,
     ElementC, cutlass::layout::TensorNHWC,
     ElementAccumulator,
     cutlass::arch::OpClassTensorOp,
-    cutlass::arch::Sm80,
+    cutlass::arch::Sm75,
     cutlass::gemm::GemmShape<128, 128, 32>,
     cutlass::gemm::GemmShape<64, 64, 32>,
-    cutlass::gemm::GemmShape<16, 8, 16>,
-    cutlass::epilogue::thread::LinearCombination<
-      ElementC,
-      128 / cutlass::sizeof_bits<ElementC>::value,
-      ElementAccumulator,
-      ElementCompute
-    >,
-    cutlass::conv::threadblock::StridedDgradIdentityThreadblockSwizzle<>,
-    3,
-    cutlass::arch::OpMultiplyAdd,
-    cutlass::conv::IteratorAlgorithm::kOptimized,
-    cutlass::conv::StrideSupport::kStrided
-  >::Kernel;
-
-  using Conv2dDgrad = cutlass::conv::device::ImplicitGemmConvolution<Conv2dDgradKernel>;
-
-
-  test::conv::device::Conv2dProblemVector problem_size_list;
-
- // run specific problem size in the unit test first
-    problem_size_list.push_back(cutlass::conv::Conv2dProblemSize(
-      {1, 56, 56, 8},   // input size (NHWC)
-      {8, 1, 1, 8},   // filter size (KRSC)
-      {0, 0, 0, 0},       // padding (pad_h, _, pad_w, _)
-      {2, 2},             // stride (stride_h, stride_w)
-      {1, 1}              // dilation (dilation_h, dilation_w)
-    ));
-
-    problem_size_list.push_back(cutlass::conv::Conv2dProblemSize(
-      {1, 55, 55, 8},   // input size (NHWC)
-      {8, 1, 1, 8},   // filter size (KRSC)
-      {0, 0, 0, 0},       // padding (pad_h, _, pad_w, _)
-      {2, 2},             // stride (stride_h, stride_w)
-      {1, 1}              // dilation (dilation_h, dilation_w)
-    ));
-
-  /// Run all unit test sizes with device-level Conv2d instance
-  EXPECT_TRUE(test::conv::device::TestAllConv2d<Conv2dDgrad>(problem_size_list));
-}
-
-////////////////////////////////////////////////////////////////////////////////
-
-TEST(SM80_Device_Conv2d_Strided_Dgrad_Optimized_ImplicitGemm_f16nhwc_f16nhwc_f32nhwc_tensor_op_f32_align4,
-  128x128_32x3_64x64x32) {
-
-  /// Conv operation element types for the Gemm equivalent (ImplicitGemm)
-  using ElementA           = cutlass::half_t;
-  using ElementB           = cutlass::half_t;
-  using ElementC           = float;
-  using ElementAccumulator = float;
-  using ElementCompute     = float;
-
-  /// Device-level Conv2d instance
-  using Conv2dDgradKernel = typename cutlass::conv::kernel::DefaultConv2dDgrad<
-    ElementA, cutlass::layout::TensorNHWC,
-    ElementB, cutlass::layout::TensorNHWC,
-    ElementC, cutlass::layout::TensorNHWC,
-    ElementAccumulator,
-    cutlass::arch::OpClassTensorOp,
-    cutlass::arch::Sm80,
-    cutlass::gemm::GemmShape<128, 128, 32>,
-    cutlass::gemm::GemmShape<64, 64, 32>,
-    cutlass::gemm::GemmShape<16, 8, 16>,
+    cutlass::gemm::GemmShape<16, 8, 8>,
     cutlass::epilogue::thread::LinearCombination<
       ElementC,
       4,
       ElementAccumulator,
       ElementCompute
     >,
-    cutlass::conv::threadblock::StridedDgradIdentityThreadblockSwizzle<>,
-    3,
+    cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>,
+    2,
     cutlass::arch::OpMultiplyAdd,
     cutlass::conv::IteratorAlgorithm::kOptimized,
     cutlass::conv::StrideSupport::kStrided,
     4,
     4
   >::Kernel;
 
-  using Conv2dDgrad = cutlass::conv::device::ImplicitGemmConvolution<Conv2dDgradKernel>;
-
+  using Conv2dWgrad = cutlass::conv::device::ImplicitGemmConvolution<Conv2dWgradKernel>;
 
   test::conv::device::Conv2dProblemVector problem_size_list;
 
   // run specific problem size in the unit test first
   problem_size_list.push_back(cutlass::conv::Conv2dProblemSize(
-    {1, 56, 56, 12},   // input size (NHWC)
-    {8, 1, 1, 12},     // filter size (KRSC)
-    {0, 0, 0, 0},     // padding (pad_h, _, pad_w, _)
-    {2, 2},           // stride (stride_h, stride_w)
-    {1, 1}            // dilation (dilation_h, dilation_w)
-  ));
-
-  problem_size_list.push_back(cutlass::conv::Conv2dProblemSize(
-    {1, 55, 55, 12},   // input size (NHWC)
-    {8, 1, 1, 12},     // filter size (KRSC)
-    {0, 0, 0, 0},     // padding (pad_h, _, pad_w, _)
-    {2, 2},           // stride (stride_h, stride_w)
-    {1, 1}            // dilation (dilation_h, dilation_w)
+    {1, 4, 4, 12},     // input size (NHWC)
+    {8, 3, 3, 12},     // filter size (KRSC)
+    {0, 0, 0, 0},      // padding (pad_h, _, pad_w, _)
+    {3, 3},            // stride (stride_h, stride_w)
+    {1, 1}             // dilation (dilation_h, dilation_w)
   ));
 
   /// Run all unit test sizes with device-level Conv2d instance
-  EXPECT_TRUE(test::conv::device::TestAllConv2d<Conv2dDgrad>(problem_size_list));
+  EXPECT_TRUE(test::conv::device::TestAllConv2d<Conv2dWgrad>(problem_size_list));
 }
 
 ////////////////////////////////////////////////////////////////////////////////
 
-#endif  // CUTLASS_ARCH_MMA_SM80_SUPPORTED
+#endif  // CUTLASS_ARCH_MMA_SM75_SUPPORTED
+
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_strided_dgrad_implicit_gemm_tf32nhwc_tf32nhwc_f32nhwc_tensor_op_f32_sm80.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_strided_dgrad_implicit_gemm_tf32nhwc_tf32nhwc_f32nhwc_tensor_op_f32_sm80.cu`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_testbed.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_testbed.h`

 * *Files 5% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -188,15 +188,15 @@
   }
 
   bool sufficient() const {
     //
     // Determine SMEM requirements and waive if not satisfied
     //
 
-    int smem_size = int(sizeof(typename Conv2d::ImplicitGemmKernel::SharedStorage));
+    int smem_size = int(sizeof(typename Conv2d::UnderlyingKernel::SharedStorage));
 
     cudaDeviceProp properties;
     int device_idx;
     cudaError_t result = cudaGetDevice(&device_idx);
 
     if (result != cudaSuccess) {
       throw std::runtime_error("cudaGetDevice() API call failed.");
@@ -204,15 +204,15 @@
 
     result = cudaGetDeviceProperties(&properties, device_idx);
 
     if (result != cudaSuccess) {
       throw std::runtime_error("cudaGetDeviceProperties() failed");
     }
 
-    if (properties.sharedMemPerMultiprocessor < smem_size) {
+    if (properties.sharedMemPerBlockOptin < smem_size) {
       return false;
     }
 
     return true;
   }
 
   /// Executes one test
@@ -301,23 +301,23 @@
 
       typename ReductionDevice::Arguments reduction_args(
         cutlass::conv::implicit_gemm_problem_size(kConvolutionalOperator, problem_size).mn(),
         problem_size.split_k_slices,
         cutlass::conv::implicit_gemm_tensor_c_size(kConvolutionalOperator, problem_size),
         {
           reinterpret_cast<ElementAccumulator*> (workspace.get()),
-          ReductionStrideIndex(tensor_C.stride()[Conv2d::ImplicitGemmKernel::kTensorCStrideIdx])
+          ReductionStrideIndex(tensor_C.stride()[Conv2d::UnderlyingKernel::kTensorCStrideIdx])
         },
         {
           tensor_D_computed.device_data(),
-          ReductionStrideIndex(tensor_C.stride()[Conv2d::ImplicitGemmKernel::kTensorCStrideIdx])
+          ReductionStrideIndex(tensor_C.stride()[Conv2d::UnderlyingKernel::kTensorCStrideIdx])
         },
         {
           tensor_C.device_data(),
-          ReductionStrideIndex(tensor_C.stride()[Conv2d::ImplicitGemmKernel::kTensorCStrideIdx])
+          ReductionStrideIndex(tensor_C.stride()[Conv2d::UnderlyingKernel::kTensorCStrideIdx])
         },
         // apply alpha, beta to obtain the following equation alpha * ReduceAdd(A * B) + beta * C 
         {alpha, beta} 
       );
 
       status = reduction_op.initialize(reduction_args, nullptr);
 
@@ -633,43 +633,43 @@
     //
     // Procedurally disable certain cases
     //
   
     // CUTLASS DGRAD's *unity* stride specialization only support stride {1, 1} 
     if ((ImplicitGemm::kConvolutionalOperator == 
           cutlass::conv::Operator::kDgrad) && 
-        (ImplicitGemm::ImplicitGemmKernel::Mma::IteratorA::kStrideSupport == 
+        (ImplicitGemm::UnderlyingKernel::Mma::IteratorA::kStrideSupport == 
           cutlass::conv::StrideSupport::kUnity)) {
       if (!((conv_problem.stride_h == 1) && (conv_problem.stride_w == 1))) {
         continue;
       }
     }
 
     // Fixed channels algorithm requires channel count to match access size
-    if (ImplicitGemm::ImplicitGemmKernel::Mma::IteratorA::kIteratorAlgorithm ==
+    if (ImplicitGemm::UnderlyingKernel::Mma::IteratorA::kIteratorAlgorithm ==
         cutlass::conv::IteratorAlgorithm::kFixedChannels) {
-      if (conv_problem.C != ImplicitGemm::ImplicitGemmKernel::Mma::IteratorA::AccessType::kElements) {
+      if (conv_problem.C != ImplicitGemm::UnderlyingKernel::Mma::IteratorA::AccessType::kElements) {
         continue;
       }
     }
 
     // Few channels algorithm requires channel count to match access size
-    if (ImplicitGemm::ImplicitGemmKernel::Mma::IteratorA::kIteratorAlgorithm ==
+    if (ImplicitGemm::UnderlyingKernel::Mma::IteratorA::kIteratorAlgorithm ==
         cutlass::conv::IteratorAlgorithm::kFewChannels) {
-      if (conv_problem.C % ImplicitGemm::ImplicitGemmKernel::Mma::IteratorA::AccessType::kElements) {
+      if (conv_problem.C % ImplicitGemm::UnderlyingKernel::Mma::IteratorA::AccessType::kElements) {
         continue;
       }
     }
 
     // CUTLASS DGRAD's *strided* stride specialization supports all stride {stride_h, stride_w} 
     // Although strided dgrad works for all stride combinations, we are only going 
     // to run strided dgrad for non-unity strides 
     if ((ImplicitGemm::kConvolutionalOperator == 
           cutlass::conv::Operator::kDgrad) && 
-        (ImplicitGemm::ImplicitGemmKernel::Mma::IteratorA::kStrideSupport == 
+        (ImplicitGemm::UnderlyingKernel::Mma::IteratorA::kStrideSupport == 
           cutlass::conv::StrideSupport::kStrided)) {
        if (((conv_problem.stride_h == 1) && (conv_problem.stride_w == 1))) {
          continue;
        }
     }
     
     //
@@ -700,31 +700,31 @@
     if (CutlassUnitTestProblemCount() && 
         testbed.tested_problem_count > CutlassUnitTestProblemCount()) {
       return true;
     }
   }
 
   // Small-channels convolution can't run here.
-  if (ImplicitGemm::ImplicitGemmKernel::Mma::IteratorA::kIteratorAlgorithm ==
+  if (ImplicitGemm::UnderlyingKernel::Mma::IteratorA::kIteratorAlgorithm ==
         cutlass::conv::IteratorAlgorithm::kFixedChannels) {
 
     return true;
   }
 
   // Small-channels convolution can't run here.
-  if (ImplicitGemm::ImplicitGemmKernel::Mma::IteratorA::kIteratorAlgorithm ==
+  if (ImplicitGemm::UnderlyingKernel::Mma::IteratorA::kIteratorAlgorithm ==
         cutlass::conv::IteratorAlgorithm::kFewChannels) {
 
     return true;
   }
 
   // CUTLASS DGRAD's *strided* specialization does not support split-k mode 
   if ((ImplicitGemm::kConvolutionalOperator == 
           cutlass::conv::Operator::kDgrad) && 
-      (ImplicitGemm::ImplicitGemmKernel::Mma::IteratorA::kStrideSupport == 
+      (ImplicitGemm::UnderlyingKernel::Mma::IteratorA::kStrideSupport == 
         cutlass::conv::StrideSupport::kStrided)) {
 
     passed = testbed.run(
       cutlass::conv::Conv2dProblemSize(
       {1, 56, 56, 8},   // input size (NHWC)
       {8, 1, 1, 8},     // filter size (KRSC)
       {0, 0, 0, 0},     // padding (pad_h, _, pad_w, _)
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_testbed_interleaved.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_testbed_interleaved.h`

 * *Files 2% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -253,23 +253,23 @@
 
       typename ReductionDevice::Arguments reduction_args(
         cutlass::conv::implicit_gemm_problem_size(kConvolutionalOperator, problem_size).mn(),
         problem_size.split_k_slices,
         cutlass::conv::implicit_gemm_tensor_c_size(kConvolutionalOperator, problem_size),
         {
           reinterpret_cast<ElementAccumulator*> (workspace.get()),
-          ReductionStrideIndex(tensor_C.stride()[Conv2d::ImplicitGemmKernel::kTensorCStrideIdx])
+          ReductionStrideIndex(tensor_C.stride()[Conv2d::UnderlyingKernel::kTensorCStrideIdx])
         },
         {
           tensor_D_computed.device_data(),
-          ReductionStrideIndex(tensor_C.stride()[Conv2d::ImplicitGemmKernel::kTensorCStrideIdx])
+          ReductionStrideIndex(tensor_C.stride()[Conv2d::UnderlyingKernel::kTensorCStrideIdx])
         },
         {
           tensor_C.device_data(),
-          ReductionStrideIndex(tensor_C.stride()[Conv2d::ImplicitGemmKernel::kTensorCStrideIdx])
+          ReductionStrideIndex(tensor_C.stride()[Conv2d::UnderlyingKernel::kTensorCStrideIdx])
         },
         // apply alpha, beta to obtain the following equation alpha * ReduceAdd(A * B) + beta * C 
         {alpha, beta}
       );
 
       status = reduction_op.initialize(reduction_args, nullptr);
 
@@ -532,15 +532,15 @@
       //
       // Procedurally disable certain cases
       //
   
       // CUTLASS DGRAD's unity stride specialization only support stride {1, 1} 
       if ((ImplicitGemm::kConvolutionalOperator == 
             cutlass::conv::Operator::kDgrad) && 
-          (ImplicitGemm::ImplicitGemmKernel::Mma::IteratorA::kStrideSupport == 
+          (ImplicitGemm::UnderlyingKernel::Mma::IteratorA::kStrideSupport == 
             cutlass::conv::StrideSupport::kUnity)) {
         if (!((conv_problem.stride_h == 1) && (conv_problem.stride_w == 1))) {
           continue;
         }
       }
 
       //
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_wgrad_implicit_gemm_cf32nhwc_cf32nhwc_cf32nhwc_simt_f32_sm50.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_wgrad_implicit_gemm_cf32nhwc_cf32nhwc_cf32nhwc_simt_f32_sm50.cu`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_wgrad_implicit_gemm_cf32nhwc_cf32nhwc_cf32nhwc_simt_f32_sm80.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_wgrad_implicit_gemm_cf32nhwc_cf32nhwc_cf32nhwc_simt_f32_sm80.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_wgrad_implicit_gemm_f16nhwc_f16nhwc_f16nhwc_tensor_op_f16_sm80.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_wgrad_implicit_gemm_f16nhwc_f16nhwc_f16nhwc_tensor_op_f16_sm80.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_wgrad_implicit_gemm_f16nhwc_f16nhwc_f32nhwc_tensor_op_f32_sm70.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_wgrad_implicit_gemm_f16nhwc_f16nhwc_f32nhwc_tensor_op_f32_sm70.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_wgrad_implicit_gemm_f16nhwc_f16nhwc_f32nhwc_tensor_op_f32_sm75.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_wgrad_implicit_gemm_tf32nhwc_tf32nhwc_f32nhwc_tensor_op_f32_sm80.cu`

 * *Files 16% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -36,160 +36,108 @@
 #include "cutlass/cutlass.h"
 
 #include "cutlass/conv/kernel/default_conv2d_wgrad.h"
 #include "cutlass/conv/device/implicit_gemm_convolution.h"
 
 #include "conv2d_testbed.h"
 
-#if defined(CUTLASS_ARCH_MMA_SM75_SUPPORTED)
+
+#if defined(CUTLASS_ARCH_MMA_SM80_SUPPORTED)
 
 ////////////////////////////////////////////////////////////////////////////////
 
-TEST(SM75_Device_Conv2d_Wgrad_Analytic_ImplicitGemm_f16nhwc_f16nhwc_f32nhwc_tensor_op_f32,
-  128x128_32x2_64x64x32) {
-  
+TEST(SM80_Device_Conv2d_Wgrad_Optimized_ImplicitGemm_tf32nhwc_tf32nhwc_f32nhwc_tensor_op_f32,
+  128x128_32x3_64x64x32) {
+
   /// Conv operation element types for the Gemm equivalent (ImplicitGemm)
-  using ElementA           = cutlass::half_t;
-  using ElementB           = cutlass::half_t;
+  using ElementA           = cutlass::tfloat32_t;
+  using ElementB           = cutlass::tfloat32_t;
   using ElementC           = float;
   using ElementAccumulator = float;
   using ElementCompute     = float;
 
+  /// Device-level Conv2d instance
   using Conv2dWgradKernel = typename cutlass::conv::kernel::DefaultConv2dWgrad<
     ElementA, cutlass::layout::TensorNHWC,
     ElementB, cutlass::layout::TensorNHWC,
     ElementC, cutlass::layout::TensorNHWC,
     ElementAccumulator,
     cutlass::arch::OpClassTensorOp,
-    cutlass::arch::Sm75,
-    cutlass::gemm::GemmShape<128, 128, 32>,
-    cutlass::gemm::GemmShape<64, 64, 32>,
+    cutlass::arch::Sm80,
+    cutlass::gemm::GemmShape<128, 128, 16>,
+    cutlass::gemm::GemmShape<64, 64, 16>,
     cutlass::gemm::GemmShape<16, 8, 8>,
     cutlass::epilogue::thread::LinearCombination<
       ElementC,
       128 / cutlass::sizeof_bits<ElementC>::value,
       ElementAccumulator,
       ElementCompute
     >,
     cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>,
-    2,
+    3,
     cutlass::arch::OpMultiplyAdd
   >::Kernel;
 
   using Conv2dWgrad = cutlass::conv::device::ImplicitGemmConvolution<Conv2dWgradKernel>;
 
   /// Run all unit test sizes with device-level Conv2d instance
   EXPECT_TRUE(test::conv::device::TestAllConv2d<Conv2dWgrad>());
 }
 
 ////////////////////////////////////////////////////////////////////////////////
 
-TEST(SM75_Device_Conv2d_Wgrad_Analytic_ImplicitGemm_f16nhwc_f16nhwc_f32nhwc_tensor_op_f32_align4,
-  128x128_32x2_64x64x32) {
-  
-  /// Conv operation element types for the Gemm equivalent (ImplicitGemm)
-  using ElementA           = cutlass::half_t;
-  using ElementB           = cutlass::half_t;
-  using ElementC           = float;
-  using ElementAccumulator = float;
-  using ElementCompute     = float;
-
-  using Conv2dWgradKernel = typename cutlass::conv::kernel::DefaultConv2dWgrad<
-    ElementA, cutlass::layout::TensorNHWC,
-    ElementB, cutlass::layout::TensorNHWC,
-    ElementC, cutlass::layout::TensorNHWC,
-    ElementAccumulator,
-    cutlass::arch::OpClassTensorOp,
-    cutlass::arch::Sm75,
-    cutlass::gemm::GemmShape<128, 128, 32>,
-    cutlass::gemm::GemmShape<64, 64, 32>,
-    cutlass::gemm::GemmShape<16, 8, 8>,
-    cutlass::epilogue::thread::LinearCombination<
-      ElementC,
-      4,
-      ElementAccumulator,
-      ElementCompute
-    >,
-    cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>,
-    2,
-    cutlass::arch::OpMultiplyAdd,
-    cutlass::conv::IteratorAlgorithm::kAnalytic,
-    cutlass::conv::StrideSupport::kStrided,
-    4,
-    4
-  >::Kernel;
-
-  using Conv2dWgrad = cutlass::conv::device::ImplicitGemmConvolution<Conv2dWgradKernel>;
-
-  test::conv::device::Conv2dProblemVector problem_size_list;
+TEST(SM80_Device_Conv2d_Wgrad_Optimized_ImplicitGemm_tf32nhwc_tf32nhwc_f32nhwc_tensor_op_f32_align1,
+  128x128_32x3_64x64x32) {
 
-  // run specific problem size in the unit test first
-  problem_size_list.push_back(cutlass::conv::Conv2dProblemSize(
-    {1, 4, 4, 12},     // input size (NHWC)
-    {8, 3, 3, 12},     // filter size (KRSC)
-    {0, 0, 0, 0},      // padding (pad_h, _, pad_w, _)
-    {3, 3},            // stride (stride_h, stride_w)
-    {1, 1}             // dilation (dilation_h, dilation_w)
-  ));
-
-  /// Run all unit test sizes with device-level Conv2d instance
-  EXPECT_TRUE(test::conv::device::TestAllConv2d<Conv2dWgrad>(problem_size_list));
-}
-
-////////////////////////////////////////////////////////////////////////////////
-
-TEST(SM75_Device_Conv2d_Wgrad_Optimized_ImplicitGemm_f16nhwc_f16nhwc_f32nhwc_tensor_op_f32_align4,
-  128x128_32x2_64x64x32) {
-  
   /// Conv operation element types for the Gemm equivalent (ImplicitGemm)
-  using ElementA           = cutlass::half_t;
-  using ElementB           = cutlass::half_t;
+  using ElementA           = cutlass::tfloat32_t;
+  using ElementB           = cutlass::tfloat32_t;
   using ElementC           = float;
   using ElementAccumulator = float;
   using ElementCompute     = float;
 
+  /// Device-level Conv2d instance
   using Conv2dWgradKernel = typename cutlass::conv::kernel::DefaultConv2dWgrad<
     ElementA, cutlass::layout::TensorNHWC,
     ElementB, cutlass::layout::TensorNHWC,
     ElementC, cutlass::layout::TensorNHWC,
     ElementAccumulator,
     cutlass::arch::OpClassTensorOp,
-    cutlass::arch::Sm75,
+    cutlass::arch::Sm80,
     cutlass::gemm::GemmShape<128, 128, 32>,
     cutlass::gemm::GemmShape<64, 64, 32>,
     cutlass::gemm::GemmShape<16, 8, 8>,
     cutlass::epilogue::thread::LinearCombination<
       ElementC,
-      4,
+      32 / cutlass::sizeof_bits<ElementC>::value,
       ElementAccumulator,
       ElementCompute
     >,
     cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>,
-    2,
+    3,
     cutlass::arch::OpMultiplyAdd,
     cutlass::conv::IteratorAlgorithm::kOptimized,
-    cutlass::conv::StrideSupport::kStrided,
-    4,
-    4
+    cutlass::conv::StrideSupport::kUnity,
+    1,
+    1    
   >::Kernel;
 
   using Conv2dWgrad = cutlass::conv::device::ImplicitGemmConvolution<Conv2dWgradKernel>;
 
   test::conv::device::Conv2dProblemVector problem_size_list;
 
   // run specific problem size in the unit test first
   problem_size_list.push_back(cutlass::conv::Conv2dProblemSize(
-    {1, 4, 4, 12},     // input size (NHWC)
-    {8, 3, 3, 12},     // filter size (KRSC)
-    {0, 0, 0, 0},      // padding (pad_h, _, pad_w, _)
-    {3, 3},            // stride (stride_h, stride_w)
+    {1, 8, 8, 1},  // input size (NHWC)
+    {1, 3, 3, 1},      // filter size (KRSC)
+    {1, 1, 1, 1},      // padding (pad_h, _, pad_w, _)
+    {1, 1},            // stride (stride_h, stride_w)
     {1, 1}             // dilation (dilation_h, dilation_w)
   ));
 
   /// Run all unit test sizes with device-level Conv2d instance
   EXPECT_TRUE(test::conv::device::TestAllConv2d<Conv2dWgrad>(problem_size_list));
 }
 
 ////////////////////////////////////////////////////////////////////////////////
 
-#endif  // CUTLASS_ARCH_MMA_SM75_SUPPORTED
-
+#endif  // CUTLASS_ARCH_MMA_SM80_SUPPORTED
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_wgrad_implicit_gemm_f16nhwc_f16nhwc_f32nhwc_tensor_op_f32_sm80.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_wgrad_implicit_gemm_f16nhwc_f16nhwc_f32nhwc_tensor_op_f32_sm80.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_wgrad_implicit_gemm_f32nhwc_f32nhwc_f32nhwc_simt_f32_sm80.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_wgrad_implicit_gemm_f32nhwc_f32nhwc_f32nhwc_simt_f32_sm80.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_wgrad_implicit_gemm_tf32nhwc_tf32nhwc_f32nhwc_tensor_op_f32_sm80.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/conv/device/conv3d_dgrad_implicit_gemm_tf32ndhwc_tf32ndhwc_f32ndhwc_tensor_op_f32_sm80.cu`

 * *Files 12% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -31,113 +31,98 @@
 /*! \file
     \brief Tests for device-wide Implicit GEMM interface
 */
 
 #include "../../common/cutlass_unit_test.h"
 #include "cutlass/cutlass.h"
 
-#include "cutlass/conv/kernel/default_conv2d_wgrad.h"
+#include "cutlass/conv/kernel/default_conv3d_dgrad.h"
 #include "cutlass/conv/device/implicit_gemm_convolution.h"
 
-#include "conv2d_testbed.h"
-
+#include "conv3d_testbed.h"
 
 #if defined(CUTLASS_ARCH_MMA_SM80_SUPPORTED)
 
 ////////////////////////////////////////////////////////////////////////////////
 
-TEST(SM80_Device_Conv2d_Wgrad_Optimized_ImplicitGemm_tf32nhwc_tf32nhwc_f32nhwc_tensor_op_f32,
+TEST(SM80_Device_Conv3d_Dgrad_Analytic_ImplicitGemm_tf32ndhwc_tf32ndhwc_f32ndhwc_tensor_op_f32,
   128x128_32x3_64x64x32) {
 
   /// Conv operation element types for the Gemm equivalent (ImplicitGemm)
   using ElementA           = cutlass::tfloat32_t;
   using ElementB           = cutlass::tfloat32_t;
   using ElementC           = float;
   using ElementAccumulator = float;
   using ElementCompute     = float;
 
   /// Device-level Conv2d instance
-  using Conv2dWgradKernel = typename cutlass::conv::kernel::DefaultConv2dWgrad<
-    ElementA, cutlass::layout::TensorNHWC,
-    ElementB, cutlass::layout::TensorNHWC,
-    ElementC, cutlass::layout::TensorNHWC,
+  using Conv3dDgradKernel = typename cutlass::conv::kernel::DefaultConv3dDgrad<
+    ElementA, cutlass::layout::TensorNDHWC,
+    ElementB, cutlass::layout::TensorNDHWC,
+    ElementC, cutlass::layout::TensorNDHWC,
     ElementAccumulator,
     cutlass::arch::OpClassTensorOp,
     cutlass::arch::Sm80,
     cutlass::gemm::GemmShape<128, 128, 16>,
     cutlass::gemm::GemmShape<64, 64, 16>,
     cutlass::gemm::GemmShape<16, 8, 8>,
     cutlass::epilogue::thread::LinearCombination<
       ElementC,
       128 / cutlass::sizeof_bits<ElementC>::value,
       ElementAccumulator,
       ElementCompute
     >,
     cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>,
     3,
-    cutlass::arch::OpMultiplyAdd
+    cutlass::arch::OpMultiplyAdd,
+    cutlass::conv::IteratorAlgorithm::kAnalytic
   >::Kernel;
 
-  using Conv2dWgrad = cutlass::conv::device::ImplicitGemmConvolution<Conv2dWgradKernel>;
+  using Conv3dDgrad = cutlass::conv::device::ImplicitGemmConvolution<Conv3dDgradKernel>;
 
-  /// Run all unit test sizes with device-level Conv2d instance
-  EXPECT_TRUE(test::conv::device::TestAllConv2d<Conv2dWgrad>());
+  /// Run all unit test sizes with device-level Conv3d instance
+  EXPECT_TRUE(test::conv::device::TestAllConv3d<Conv3dDgrad>());
 }
 
 ////////////////////////////////////////////////////////////////////////////////
 
-TEST(SM80_Device_Conv2d_Wgrad_Optimized_ImplicitGemm_tf32nhwc_tf32nhwc_f32nhwc_tensor_op_f32_align1,
+TEST(SM80_Device_Conv3d_Dgrad_Optimized_ImplicitGemm_tf32ndhwc_tf32ndhwc_f32ndhwc_tensor_op_f32,
   128x128_32x3_64x64x32) {
 
   /// Conv operation element types for the Gemm equivalent (ImplicitGemm)
   using ElementA           = cutlass::tfloat32_t;
   using ElementB           = cutlass::tfloat32_t;
   using ElementC           = float;
   using ElementAccumulator = float;
   using ElementCompute     = float;
 
   /// Device-level Conv2d instance
-  using Conv2dWgradKernel = typename cutlass::conv::kernel::DefaultConv2dWgrad<
-    ElementA, cutlass::layout::TensorNHWC,
-    ElementB, cutlass::layout::TensorNHWC,
-    ElementC, cutlass::layout::TensorNHWC,
+  using Conv3dDgradKernel = typename cutlass::conv::kernel::DefaultConv3dDgrad<
+    ElementA, cutlass::layout::TensorNDHWC,
+    ElementB, cutlass::layout::TensorNDHWC,
+    ElementC, cutlass::layout::TensorNDHWC,
     ElementAccumulator,
     cutlass::arch::OpClassTensorOp,
     cutlass::arch::Sm80,
-    cutlass::gemm::GemmShape<128, 128, 32>,
-    cutlass::gemm::GemmShape<64, 64, 32>,
+    cutlass::gemm::GemmShape<128, 128, 16>,
+    cutlass::gemm::GemmShape<64, 64, 16>,
     cutlass::gemm::GemmShape<16, 8, 8>,
     cutlass::epilogue::thread::LinearCombination<
       ElementC,
-      32 / cutlass::sizeof_bits<ElementC>::value,
+      128 / cutlass::sizeof_bits<ElementC>::value,
       ElementAccumulator,
       ElementCompute
     >,
     cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>,
     3,
     cutlass::arch::OpMultiplyAdd,
     cutlass::conv::IteratorAlgorithm::kOptimized,
-    cutlass::conv::StrideSupport::kUnity,
-    1,
-    1    
+    cutlass::conv::StrideSupport::kUnity
   >::Kernel;
 
-  using Conv2dWgrad = cutlass::conv::device::ImplicitGemmConvolution<Conv2dWgradKernel>;
+  using Conv3dDgrad = cutlass::conv::device::ImplicitGemmConvolution<Conv3dDgradKernel>;
 
-  test::conv::device::Conv2dProblemVector problem_size_list;
-
-  // run specific problem size in the unit test first
-  problem_size_list.push_back(cutlass::conv::Conv2dProblemSize(
-    {1, 8, 8, 1},  // input size (NHWC)
-    {1, 3, 3, 1},      // filter size (KRSC)
-    {1, 1, 1, 1},      // padding (pad_h, _, pad_w, _)
-    {1, 1},            // stride (stride_h, stride_w)
-    {1, 1}             // dilation (dilation_h, dilation_w)
-  ));
-
-  /// Run all unit test sizes with device-level Conv2d instance
-  EXPECT_TRUE(test::conv::device::TestAllConv2d<Conv2dWgrad>(problem_size_list));
+  /// Run all unit test sizes with device-level Conv3d instance
+  EXPECT_TRUE(test::conv::device::TestAllConv3d<Conv3dDgrad>());
 }
-
 ////////////////////////////////////////////////////////////////////////////////
-
 #endif  // CUTLASS_ARCH_MMA_SM80_SUPPORTED
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_with_broadcast_testbed.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_with_broadcast_testbed.h`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -249,15 +249,15 @@
   }
 
   bool sufficient() const {
     //
     // Determine SMEM requirements and waive if not satisfied
     //
 
-    int smem_size = int(sizeof(typename Conv2d::ImplicitGemmKernel::SharedStorage));
+    int smem_size = int(sizeof(typename Conv2d::UnderlyingKernel::SharedStorage));
 
     cudaDeviceProp properties;
     int device_idx;
     cudaError_t result = cudaGetDevice(&device_idx);
 
     if (result != cudaSuccess) {
       throw std::runtime_error("cudaGetDevice() API call failed.");
@@ -265,15 +265,15 @@
 
     result = cudaGetDeviceProperties(&properties, device_idx);
 
     if (result != cudaSuccess) {
       throw std::runtime_error("cudaGetDeviceProperties() failed");
     }
 
-    if (properties.sharedMemPerMultiprocessor < smem_size) {
+    if (properties.sharedMemPerBlockOptin < smem_size) {
       return false;
     }
 
     return true;
   }
 
   /// Executes one test
@@ -553,26 +553,26 @@
       //
       // Procedurally disable certain cases
       //
   
       // CUTLASS DGRAD's *unity* stride specialization only support stride {1, 1} 
       if ((ImplicitGemm::kConvolutionalOperator == 
             cutlass::conv::Operator::kDgrad) && 
-          (ImplicitGemm::ImplicitGemmKernel::Mma::IteratorA::kStrideSupport == 
+          (ImplicitGemm::UnderlyingKernel::Mma::IteratorA::kStrideSupport == 
             cutlass::conv::StrideSupport::kUnity)) {
         if (!((conv_problem.stride_h == 1) && (conv_problem.stride_w == 1))) {
           continue;
         }
       }
 
 #if 0 // relax restrictions on analytic strided dgrad
       // CUTLASS DGRAD's *strided* specialization only support stride >= {2, 2} 
       if ((ImplicitGemm::kConvolutionalOperator == 
             cutlass::conv::Operator::kDgrad) && 
-          (ImplicitGemm::ImplicitGemmKernel::Mma::IteratorA::kStrideSupport == 
+          (ImplicitGemm::UnderlyingKernel::Mma::IteratorA::kStrideSupport == 
             cutlass::conv::StrideSupport::kStrided)) {
          if (((conv_problem.stride_h == 1) && (conv_problem.stride_w == 1))) {
            continue;
          }
       }
 #endif
       
@@ -601,15 +601,15 @@
       }
     }
   }
 
   // CUTLASS DGRAD's *strided* specialization does not support split-k mode 
   if ((ImplicitGemm::kConvolutionalOperator == 
           cutlass::conv::Operator::kDgrad) && 
-      (ImplicitGemm::ImplicitGemmKernel::Mma::IteratorA::kStrideSupport == 
+      (ImplicitGemm::UnderlyingKernel::Mma::IteratorA::kStrideSupport == 
         cutlass::conv::StrideSupport::kStrided)) {
 
     passed = testbed.run(
       cutlass::conv::Conv2dProblemSize(
       {1, 56, 56, 8},   // input size (NHWC)
       {8, 1, 1, 8},     // filter size (KRSC)
       {0, 0, 0, 0},     // padding (pad_h, _, pad_w, _)
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_with_reduction_testbed.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_with_reduction_testbed.h`

 * *Files 2% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -178,15 +178,15 @@
   }
 
   bool sufficient() const {
     //
     // Determine SMEM requirements and waive if not satisfied
     //
 
-    int smem_size = int(sizeof(typename Conv2d::ImplicitGemmKernel::SharedStorage));
+    int smem_size = int(sizeof(typename Conv2d::UnderlyingKernel::SharedStorage));
 
     cudaDeviceProp properties;
     int device_idx;
     cudaError_t result = cudaGetDevice(&device_idx);
 
     if (result != cudaSuccess) {
       throw std::runtime_error("cudaGetDevice() API call failed.");
@@ -194,15 +194,15 @@
 
     result = cudaGetDeviceProperties(&properties, device_idx);
 
     if (result != cudaSuccess) {
       throw std::runtime_error("cudaGetDeviceProperties() failed");
     }
 
-    if (properties.sharedMemPerMultiprocessor < smem_size) {
+    if (properties.sharedMemPerBlockOptin < smem_size) {
       return false;
     }
 
     return true;
   }
 
   /// Executes one test
@@ -512,26 +512,26 @@
       //
       // Procedurally disable certain cases
       //
   
       // CUTLASS DGRAD's *unity* stride specialization only support stride {1, 1} 
       if ((ImplicitGemm::kConvolutionalOperator == 
             cutlass::conv::Operator::kDgrad) && 
-          (ImplicitGemm::ImplicitGemmKernel::Mma::IteratorA::kStrideSupport == 
+          (ImplicitGemm::UnderlyingKernel::Mma::IteratorA::kStrideSupport == 
             cutlass::conv::StrideSupport::kUnity)) {
         if (!((conv_problem.stride_h == 1) && (conv_problem.stride_w == 1))) {
           continue;
         }
       }
 
 #if 0 // relax restrictions on analytic strided dgrad
       // CUTLASS DGRAD's *strided* specialization only support stride >= {2, 2} 
       if ((ImplicitGemm::kConvolutionalOperator == 
             cutlass::conv::Operator::kDgrad) && 
-          (ImplicitGemm::ImplicitGemmKernel::Mma::IteratorA::kStrideSupport == 
+          (ImplicitGemm::UnderlyingKernel::Mma::IteratorA::kStrideSupport == 
             cutlass::conv::StrideSupport::kStrided)) {
          if (((conv_problem.stride_h == 1) && (conv_problem.stride_w == 1))) {
            continue;
          }
       }
 #endif
       
@@ -560,15 +560,15 @@
       }
     }
   }
 
   // CUTLASS DGRAD's *strided* specialization does not support split-k mode 
   if ((ImplicitGemm::kConvolutionalOperator == 
           cutlass::conv::Operator::kDgrad) && 
-      (ImplicitGemm::ImplicitGemmKernel::Mma::IteratorA::kStrideSupport == 
+      (ImplicitGemm::UnderlyingKernel::Mma::IteratorA::kStrideSupport == 
         cutlass::conv::StrideSupport::kStrided)) {
 
     passed = testbed.run(
       cutlass::conv::Conv2dProblemSize(
       {1, 56, 56, 8},   // input size (NHWC)
       {8, 1, 1, 8},     // filter size (KRSC)
       {0, 0, 0, 0},     // padding (pad_h, _, pad_w, _)
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/conv/device/conv3d_dgrad_implicit_gemm_f16ndhwc_f16ndhwc_f32ndhwc_tensor_op_f32_sm80.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/conv/device/conv3d_dgrad_implicit_gemm_f16ndhwc_f16ndhwc_f32ndhwc_tensor_op_f32_sm80.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/conv/device/conv3d_dgrad_implicit_gemm_tf32ndhwc_tf32ndhwc_f32ndhwc_tensor_op_f32_sm80.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/conv/device/conv3d_wgrad_implicit_gemm_tf32ndhwc_tf32ndhwc_f32ndhwc_tensor_op_f32_sm80.cu`

 * *Files 10% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -31,35 +31,35 @@
 /*! \file
     \brief Tests for device-wide Implicit GEMM interface
 */
 
 #include "../../common/cutlass_unit_test.h"
 #include "cutlass/cutlass.h"
 
-#include "cutlass/conv/kernel/default_conv3d_dgrad.h"
+#include "cutlass/conv/kernel/default_conv3d_wgrad.h"
 #include "cutlass/conv/device/implicit_gemm_convolution.h"
 
 #include "conv3d_testbed.h"
 
 #if defined(CUTLASS_ARCH_MMA_SM80_SUPPORTED)
 
 ////////////////////////////////////////////////////////////////////////////////
 
-TEST(SM80_Device_Conv3d_Dgrad_Analytic_ImplicitGemm_tf32ndhwc_tf32ndhwc_f32ndhwc_tensor_op_f32,
+TEST(SM80_Device_Conv3d_Wgrad_Analytic_ImplicitGemm_tf32ndhwc_tf32ndhwc_f32ndhwc_tensor_op_f32,
   128x128_32x3_64x64x32) {
 
   /// Conv operation element types for the Gemm equivalent (ImplicitGemm)
   using ElementA           = cutlass::tfloat32_t;
   using ElementB           = cutlass::tfloat32_t;
   using ElementC           = float;
   using ElementAccumulator = float;
   using ElementCompute     = float;
 
   /// Device-level Conv2d instance
-  using Conv3dDgradKernel = typename cutlass::conv::kernel::DefaultConv3dDgrad<
+  using Conv3dWgradKernel = typename cutlass::conv::kernel::DefaultConv3dWgrad<
     ElementA, cutlass::layout::TensorNDHWC,
     ElementB, cutlass::layout::TensorNDHWC,
     ElementC, cutlass::layout::TensorNDHWC,
     ElementAccumulator,
     cutlass::arch::OpClassTensorOp,
     cutlass::arch::Sm80,
     cutlass::gemm::GemmShape<128, 128, 16>,
@@ -69,38 +69,36 @@
       ElementC,
       128 / cutlass::sizeof_bits<ElementC>::value,
       ElementAccumulator,
       ElementCompute
     >,
     cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>,
     3,
-    cutlass::arch::OpMultiplyAdd,
-    cutlass::conv::IteratorAlgorithm::kAnalytic
+    cutlass::arch::OpMultiplyAdd
   >::Kernel;
 
-  using Conv3dDgrad = cutlass::conv::device::ImplicitGemmConvolution<Conv3dDgradKernel>;
+  using Conv3dWgrad = cutlass::conv::device::ImplicitGemmConvolution<Conv3dWgradKernel>;
 
   /// Run all unit test sizes with device-level Conv3d instance
-  EXPECT_TRUE(test::conv::device::TestAllConv3d<Conv3dDgrad>());
+  EXPECT_TRUE(test::conv::device::TestAllConv3d<Conv3dWgrad>());
 }
 
 ////////////////////////////////////////////////////////////////////////////////
-
-TEST(SM80_Device_Conv3d_Dgrad_Optimized_ImplicitGemm_tf32ndhwc_tf32ndhwc_f32ndhwc_tensor_op_f32,
+TEST(SM80_Device_Conv3d_Wgrad_Optimized_ImplicitGemm_tf32ndhwc_tf32ndhwc_f32ndhwc_tensor_op_f32,
   128x128_32x3_64x64x32) {
 
   /// Conv operation element types for the Gemm equivalent (ImplicitGemm)
   using ElementA           = cutlass::tfloat32_t;
   using ElementB           = cutlass::tfloat32_t;
   using ElementC           = float;
   using ElementAccumulator = float;
   using ElementCompute     = float;
 
   /// Device-level Conv2d instance
-  using Conv3dDgradKernel = typename cutlass::conv::kernel::DefaultConv3dDgrad<
+  using Conv3dWgradKernel = typename cutlass::conv::kernel::DefaultConv3dWgrad<
     ElementA, cutlass::layout::TensorNDHWC,
     ElementB, cutlass::layout::TensorNDHWC,
     ElementC, cutlass::layout::TensorNDHWC,
     ElementAccumulator,
     cutlass::arch::OpClassTensorOp,
     cutlass::arch::Sm80,
     cutlass::gemm::GemmShape<128, 128, 16>,
@@ -111,18 +109,18 @@
       128 / cutlass::sizeof_bits<ElementC>::value,
       ElementAccumulator,
       ElementCompute
     >,
     cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>,
     3,
     cutlass::arch::OpMultiplyAdd,
-    cutlass::conv::IteratorAlgorithm::kOptimized,
-    cutlass::conv::StrideSupport::kUnity
+    cutlass::conv::IteratorAlgorithm::kOptimized
   >::Kernel;
 
-  using Conv3dDgrad = cutlass::conv::device::ImplicitGemmConvolution<Conv3dDgradKernel>;
+  using Conv3dWgrad = cutlass::conv::device::ImplicitGemmConvolution<Conv3dWgradKernel>;
 
   /// Run all unit test sizes with device-level Conv3d instance
-  EXPECT_TRUE(test::conv::device::TestAllConv3d<Conv3dDgrad>());
+  EXPECT_TRUE(test::conv::device::TestAllConv3d<Conv3dWgrad>());
 }
+
 ////////////////////////////////////////////////////////////////////////////////
 #endif  // CUTLASS_ARCH_MMA_SM80_SUPPORTED
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/conv/device/conv3d_fprop_implicit_gemm_f16ndhwc_f16ndhwc_f32ndhwc_tensor_op_f32_sm75.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/conv/device/conv3d_fprop_implicit_gemm_f16ndhwc_f16ndhwc_f32ndhwc_tensor_op_f32_sm75.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/conv/device/conv3d_fprop_implicit_gemm_f16ndhwc_f16ndhwc_f32ndhwc_tensor_op_f32_sm80.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/conv/device/conv3d_fprop_implicit_gemm_f16ndhwc_f16ndhwc_f32ndhwc_tensor_op_f32_sm80.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/conv/device/conv3d_fprop_implicit_gemm_tf32ndhwc_tf32ndhwc_f32ndhwc_tensor_op_f32_sm80.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/conv/device/conv3d_fprop_implicit_gemm_tf32ndhwc_tf32ndhwc_f32ndhwc_tensor_op_f32_sm80.cu`

 * *Files 2% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/conv/device/conv3d_problems.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/conv/device/conv3d_problems.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/conv/device/conv3d_testbed.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/conv/device/conv3d_testbed.h`

 * *Files 2% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -180,15 +180,15 @@
   }
 
   bool sufficient() const {
     //
     // Determine SMEM requirements and waive if not satisfied
     //
 
-    int smem_size = int(sizeof(typename Conv3d::ImplicitGemmKernel::SharedStorage));
+    int smem_size = int(sizeof(typename Conv3d::UnderlyingKernel::SharedStorage));
 
     cudaDeviceProp properties;
     int device_idx;
     cudaError_t result = cudaGetDevice(&device_idx);
 
     if (result != cudaSuccess) {
       throw std::runtime_error("cudaGetDevice() API call failed.");
@@ -196,15 +196,15 @@
 
     result = cudaGetDeviceProperties(&properties, device_idx);
 
     if (result != cudaSuccess) {
       throw std::runtime_error("cudaGetDeviceProperties() failed");
     }
 
-    if (properties.sharedMemPerMultiprocessor < smem_size) {
+    if (properties.sharedMemPerBlockOptin < smem_size) {
       return false;
     }
 
     return true;
   }
 
 
@@ -290,23 +290,23 @@
 
       typename ReductionDevice::Arguments reduction_args(
         cutlass::conv::implicit_gemm_problem_size(kConvolutionalOperator, problem_size).mn(),
         problem_size.split_k_slices,
         cutlass::conv::implicit_gemm_tensor_c_size(kConvolutionalOperator, problem_size),
         {
           reinterpret_cast<ElementAccumulator*> (workspace.get()),
-          ReductionStrideIndex(tensor_C.stride()[Conv3d::ImplicitGemmKernel::kTensorCStrideIdx])
+          ReductionStrideIndex(tensor_C.stride()[Conv3d::UnderlyingKernel::kTensorCStrideIdx])
         },
         {
           tensor_D_computed.device_data(),
-          ReductionStrideIndex(tensor_C.stride()[Conv3d::ImplicitGemmKernel::kTensorCStrideIdx])
+          ReductionStrideIndex(tensor_C.stride()[Conv3d::UnderlyingKernel::kTensorCStrideIdx])
         },
         {
           tensor_C.device_data(),
-          ReductionStrideIndex(tensor_C.stride()[Conv3d::ImplicitGemmKernel::kTensorCStrideIdx])
+          ReductionStrideIndex(tensor_C.stride()[Conv3d::UnderlyingKernel::kTensorCStrideIdx])
         },
         // apply alpha, beta to obtain the following equation alpha * ReduceAdd(A * B) + beta * C 
         {alpha, beta}
       );
 
       status = reduction_op.initialize(reduction_args, nullptr);
 
@@ -569,17 +569,17 @@
       //
       // Procedurally disable certain cases
       //
   
       // CUTLASS DGRAD's unity stride specialization only support stride {1, 1, 1} 
       if ((ImplicitGemm::kConvolutionalOperator == 
             cutlass::conv::Operator::kDgrad) && 
-          ((ImplicitGemm::ImplicitGemmKernel::Mma::IteratorA::kStrideSupport == 
+          ((ImplicitGemm::UnderlyingKernel::Mma::IteratorA::kStrideSupport == 
             cutlass::conv::StrideSupport::kUnity) ||
-           (ImplicitGemm::ImplicitGemmKernel::Mma::IteratorB::kStrideSupport == 
+           (ImplicitGemm::UnderlyingKernel::Mma::IteratorB::kStrideSupport == 
             cutlass::conv::StrideSupport::kUnity))) {
         if (!((conv_problem.stride_d == 1) &&
               (conv_problem.stride_h == 1) && 
               (conv_problem.stride_w == 1))
           ) {
           continue;
         }
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/conv/device/conv3d_wgrad_implicit_gemm_f16ndhwc_f16ndhwc_f32ndhwc_tensor_op_f32_sm75.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/conv/device/conv3d_wgrad_implicit_gemm_f16ndhwc_f16ndhwc_f32ndhwc_tensor_op_f32_sm75.cu`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/conv/device/conv3d_wgrad_implicit_gemm_f16ndhwc_f16ndhwc_f32ndhwc_tensor_op_f32_sm80.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/conv/device/conv3d_wgrad_implicit_gemm_f16ndhwc_f16ndhwc_f32ndhwc_tensor_op_f32_sm80.cu`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/conv/device/conv3d_wgrad_implicit_gemm_tf32ndhwc_tf32ndhwc_f32ndhwc_tensor_op_f32_sm80.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/symm_f64_f64_tensor_op_f64_sm90.cu`

 * *Files 15% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -25,102 +25,111 @@
  * SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER
  * CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,
  * OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
  * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
  *
  **************************************************************************************************/
 /*! \file
-    \brief Tests for device-wide Implicit GEMM interface
+    \brief Tests for device-wide SYMM interface
+  
 */
 
-#include "../../common/cutlass_unit_test.h"
-#include "cutlass/cutlass.h"
-
-#include "cutlass/conv/kernel/default_conv3d_wgrad.h"
-#include "cutlass/conv/device/implicit_gemm_convolution.h"
-
-#include "conv3d_testbed.h"
-
-#if defined(CUTLASS_ARCH_MMA_SM80_SUPPORTED)
-
-////////////////////////////////////////////////////////////////////////////////
-
-TEST(SM80_Device_Conv3d_Wgrad_Analytic_ImplicitGemm_tf32ndhwc_tf32ndhwc_f32ndhwc_tensor_op_f32,
-  128x128_32x3_64x64x32) {
+#include <iostream>
 
-  /// Conv operation element types for the Gemm equivalent (ImplicitGemm)
-  using ElementA           = cutlass::tfloat32_t;
-  using ElementB           = cutlass::tfloat32_t;
-  using ElementC           = float;
-  using ElementAccumulator = float;
-  using ElementCompute     = float;
-
-  /// Device-level Conv2d instance
-  using Conv3dWgradKernel = typename cutlass::conv::kernel::DefaultConv3dWgrad<
-    ElementA, cutlass::layout::TensorNDHWC,
-    ElementB, cutlass::layout::TensorNDHWC,
-    ElementC, cutlass::layout::TensorNDHWC,
+#include "../../common/cutlass_unit_test.h"
+#include "cutlass/blas3.h"
+#include "cutlass/gemm/device/symm.h"
+#include "cutlass/util/host_tensor.h"
+#include "cutlass/util/reference/host/symm.h"
+#include "cutlass/util/reference/host/tensor_compare.h"
+#include "cutlass/util/reference/host/tensor_copy.h"
+#include "cutlass/util/reference/host/tensor_fill.h"
+#include "cutlass/util/tensor_view_io.h"
+
+#include "testbed_symm_universal.h"
+
+#if defined(CUTLASS_ARCH_MMA_SM90_SUPPORTED)
+
+/////////////////////////////////////////////////////////////////////////////////////////////////
+
+TEST(SM90_Device_Symm_f64n_f64n_rs_l_tensor_op_f64, 32x32x16_16x16x16) {
+
+  using ElementA = double;
+  using LayoutA = cutlass::layout::ColumnMajor;
+  using ElementB = double;
+  using LayoutB = cutlass::layout::ColumnMajor;
+  using ElementC = double;
+  using LayoutC = cutlass::layout::ColumnMajor;
+  using ElementAccumulator = double;
+
+  using Symm = cutlass::gemm::device::Symm<
+    ElementA,
+    LayoutA,
+    cutlass::SideMode::kRight,
+    cutlass::FillMode::kLower,
+    ElementB,
+    LayoutB,
+    ElementC,
+    LayoutC,
     ElementAccumulator,
     cutlass::arch::OpClassTensorOp,
-    cutlass::arch::Sm80,
-    cutlass::gemm::GemmShape<128, 128, 16>,
-    cutlass::gemm::GemmShape<64, 64, 16>,
-    cutlass::gemm::GemmShape<16, 8, 8>,
+    cutlass::arch::Sm90,
+    cutlass::gemm::GemmShape<32, 32, 16>,
+    cutlass::gemm::GemmShape<16, 16, 16>,
+    cutlass::gemm::GemmShape<16, 8, 4>,
     cutlass::epilogue::thread::LinearCombination<
       ElementC,
-      128 / cutlass::sizeof_bits<ElementC>::value,
+      1,
       ElementAccumulator,
-      ElementCompute
+      ElementAccumulator
     >,
     cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>,
-    3,
-    cutlass::arch::OpMultiplyAdd
-  >::Kernel;
+    4
+  >;
 
-  using Conv3dWgrad = cutlass::conv::device::ImplicitGemmConvolution<Conv3dWgradKernel>;
+  EXPECT_TRUE(test::gemm::device::TestAllSymmUniversal<Symm>());
 
-  /// Run all unit test sizes with device-level Conv3d instance
-  EXPECT_TRUE(test::conv::device::TestAllConv3d<Conv3dWgrad>());
 }
 
-////////////////////////////////////////////////////////////////////////////////
-TEST(SM80_Device_Conv3d_Wgrad_Optimized_ImplicitGemm_tf32ndhwc_tf32ndhwc_f32ndhwc_tensor_op_f32,
-  128x128_32x3_64x64x32) {
-
-  /// Conv operation element types for the Gemm equivalent (ImplicitGemm)
-  using ElementA           = cutlass::tfloat32_t;
-  using ElementB           = cutlass::tfloat32_t;
-  using ElementC           = float;
-  using ElementAccumulator = float;
-  using ElementCompute     = float;
-
-  /// Device-level Conv2d instance
-  using Conv3dWgradKernel = typename cutlass::conv::kernel::DefaultConv3dWgrad<
-    ElementA, cutlass::layout::TensorNDHWC,
-    ElementB, cutlass::layout::TensorNDHWC,
-    ElementC, cutlass::layout::TensorNDHWC,
+/////////////////////////////////////////////////////////////////////////////////////////////////
+
+TEST(SM90_Device_Symm_f64t_f64t_ls_l_tensor_op_f64, 128x128x16_32x64x16) {
+
+  using ElementA = double;
+  using LayoutA = cutlass::layout::RowMajor;
+  using ElementB = double;
+  using LayoutB = cutlass::layout::RowMajor;
+  using ElementC = double;
+  using LayoutC = cutlass::layout::RowMajor;
+  using ElementAccumulator = double;
+
+  using Symm = cutlass::gemm::device::Symm<
+    ElementA,
+    LayoutA,
+    cutlass::SideMode::kLeft,
+    cutlass::FillMode::kLower,
+    ElementB,
+    LayoutB,
+    ElementC,
+    LayoutC,
     ElementAccumulator,
     cutlass::arch::OpClassTensorOp,
-    cutlass::arch::Sm80,
+    cutlass::arch::Sm90,
     cutlass::gemm::GemmShape<128, 128, 16>,
-    cutlass::gemm::GemmShape<64, 64, 16>,
-    cutlass::gemm::GemmShape<16, 8, 8>,
+    cutlass::gemm::GemmShape<32, 64, 16>,
+    cutlass::gemm::GemmShape<16, 8, 4>,
     cutlass::epilogue::thread::LinearCombination<
       ElementC,
-      128 / cutlass::sizeof_bits<ElementC>::value,
+      1,
       ElementAccumulator,
-      ElementCompute
+      ElementAccumulator
     >,
     cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>,
-    3,
-    cutlass::arch::OpMultiplyAdd,
-    cutlass::conv::IteratorAlgorithm::kOptimized
-  >::Kernel;
+    3
+  >;
 
-  using Conv3dWgrad = cutlass::conv::device::ImplicitGemmConvolution<Conv3dWgradKernel>;
+  EXPECT_TRUE(test::gemm::device::TestAllSymmUniversal<Symm>());
 
-  /// Run all unit test sizes with device-level Conv3d instance
-  EXPECT_TRUE(test::conv::device::TestAllConv3d<Conv3dWgrad>());
 }
 
-////////////////////////////////////////////////////////////////////////////////
-#endif  // CUTLASS_ARCH_MMA_SM80_SUPPORTED
+/////////////////////////////////////////////////////////////////////////////////////////////////
+#endif // #if defined(CUTLASS_ARCH_MMA_SM90_SUPPORTED)
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/conv/device/depthwise_conv2d_direct_conv_testbed.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/conv/device/depthwise_conv2d_direct_conv_testbed.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/conv/device/depthwise_conv2d_fprop_direct_conv_f16nhwc_f16nhwc_f16nhwc_simt_f16_sm60.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/conv/device/depthwise_conv2d_fprop_direct_conv_f16nhwc_f16nhwc_f16nhwc_simt_f16_sm60.cu`

 * *Files 0% similar despite different names*

```diff
@@ -324,15 +324,14 @@
   using Direct2dConv = cutlass::conv::device::DirectConvolution<DepthwiseDirect2dConv>;
 
   /// Run all unit test sizes with device-level Conv2d instance
   EXPECT_TRUE(test::conv::device::TestSpecificDepthwiseDirectConv2d<Direct2dConv>(
       DepthwiseFpropProblemSizes_filter5x5()));
 }
 
-#if 0
 ////////////////////////////////////////////////////////////////////////////////
 TEST(
     SM60_Device_Depthwise_conv2d_Fprop_Direct_Conv_Optimized_f16nhwc_f16nhwc_f16nhwc_simt_f16,
     64x32_3_16x32_5x37) {
 
   using ElementInputA = cutlass::half_t;
   using ElementInputB = cutlass::half_t;
@@ -421,9 +420,7 @@
 
   using Direct2dConv = cutlass::conv::device::DirectConvolution<DepthwiseDirect2dConv>;
 
   /// Run all unit test sizes with device-level Conv2d instance
   EXPECT_TRUE(test::conv::device::TestSpecificDepthwiseDirectConv2d<Direct2dConv>(
       DepthwiseFpropProblemSizes_filter5x37()));
 }
-#endif
-
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/conv/device/depthwise_conv2d_fprop_direct_conv_fixed_stride_dilation_f16nhwc_f16nhwc_f16nhwc_simt_f16_sm60.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/conv/device/depthwise_conv2d_fprop_direct_conv_fixed_stride_dilation_f16nhwc_f16nhwc_f16nhwc_simt_f16_sm60.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/conv/device/depthwise_conv2d_fprop_implicit_gemm_f16nhwc_f16nhwc_f16nhwc_simt_f16_sm60.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/conv/device/depthwise_conv2d_fprop_implicit_gemm_f16nhwc_f16nhwc_f16nhwc_simt_f16_sm60.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/conv/device/depthwise_fprop_implicit_gemm_f16nhwc_f16nhwc_f16nhwc_simt_f16_sm60.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/conv/device/depthwise_fprop_implicit_gemm_f16nhwc_f16nhwc_f16nhwc_simt_f16_sm60.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/core/array.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/core/array.cu`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/core/bfloat16.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/core/bfloat16.cu`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/core/complex.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/core/complex.cu`

 * *Files 2% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/core/float8.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/core/float8.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/core/functional.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/core/functional.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/core/half.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/core/half.cu`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/core/matrix.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/core/matrix.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/core/matrix_coord.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/core/matrix_coord.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/core/predicate_vector.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/core/predicate_vector.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/core/quaternion.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/core/quaternion.cu`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/core/tensor_ref.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/core/tensor_ref.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/core/tensor_view.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/core/tensor_view.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/core/test_unit_core.cpp` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/core/test_unit_core.cpp`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/core/tfloat32.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/core/tfloat32.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/cute/ampere/cp_async.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/cute/ampere/cp_async.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/cute/ampere/ldsm.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/cute/ampere/ldsm.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/cute/core/bitfield.cpp` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/cute/core/bitfield.cpp`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/cute/core/coalesce.cpp` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/cute/core/coalesce.cpp`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/cute/core/compare.cpp` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/cute/core/compare.cpp`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/cute/core/complement.cpp` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/cute/core/complement.cpp`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/cute/core/composition.cpp` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/cute/core/composition.cpp`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/cute/core/inverse_left.cpp` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/cute/core/inverse_left.cpp`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/cute/core/inverse_right.cpp` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/cute/core/inverse_right.cpp`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/cute/core/logical_divide.cpp` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/cute/core/logical_divide.cpp`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/cute/core/logical_product.cpp` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/cute/core/logical_product.cpp`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/cute/core/mixedbits.cpp` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/cute/core/mixedbits.cpp`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/cute/core/transform.cpp` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/cute/core/transform.cpp`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/cute/core/tuple.cpp` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/cute/core/tuple.cpp`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/cute/hopper/stsm.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/cute/hopper/stsm.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/cute/hopper/tma_load.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/cute/hopper/tma_load.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/cute/hopper/tma_store.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/cute/hopper/tma_store.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/cute/layout/layout_operator.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/cute/layout/layout_operator.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/epilogue/thread/activation.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/epilogue/thread/activation.cu`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -30,14 +30,15 @@
  **************************************************************************************************/
 /*! \file
     \brief Unit tests for thread-level GEMM
 */
 
 #include "../../common/cutlass_unit_test.h"
 
+#include "cutlass/layout/layout.h"
 #include "cutlass/epilogue/thread/activation.h"
 
 #include "cutlass/util/host_tensor.h"
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
 template <typename T, int N, typename Func>
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/epilogue/thread/linear_combination.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/epilogue/thread/linear_combination.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/epilogue/thread/linear_combination_planar_complex.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/epilogue/thread/linear_combination_planar_complex.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/epilogue/threadblock/epilogue_planar_complex.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/epilogue/threadblock/epilogue_planar_complex.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/epilogue/threadblock/epilogue_simt.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/epilogue/threadblock/epilogue_simt.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/epilogue/threadblock/epilogue_simt_sm60.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/epilogue/threadblock/epilogue_simt_sm60.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/epilogue/threadblock/epilogue_simt_sm61.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/epilogue/threadblock/epilogue_simt_sm61.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/epilogue/threadblock/epilogue_tensor_op.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/epilogue/threadblock/epilogue_tensor_op.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/epilogue/threadblock/epilogue_volta_tensor_op.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/epilogue/threadblock/epilogue_volta_tensor_op.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/epilogue/threadblock/epilogue_with_reduction_tensor_op.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/epilogue/threadblock/epilogue_with_reduction_tensor_op.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/epilogue/threadblock/epilogue_with_reduction_testbed.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/epilogue/threadblock/epilogue_with_reduction_testbed.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/epilogue/threadblock/epilogue_wmma_tensor_op_sm70.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/epilogue/threadblock/epilogue_wmma_tensor_op_sm70.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/epilogue/threadblock/output_tile_threadmap.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/epilogue/threadblock/output_tile_threadmap.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/epilogue/threadblock/predicated_tile_iterator.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/epilogue/threadblock/predicated_tile_iterator.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/epilogue/threadblock/testbed.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/epilogue/threadblock/testbed.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/epilogue/threadblock/testbed_planar_complex.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/epilogue/threadblock/testbed_planar_complex.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/epilogue/warp/fragment_iterator_tensor_op.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/epilogue/warp/fragment_iterator_tensor_op.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/epilogue/warp/fragment_iterator_volta_tensor_op.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/epilogue/warp/fragment_iterator_volta_tensor_op.cu`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/epilogue/warp/fragment_iterator_wmma_tensor_op.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/epilogue/warp/fragment_iterator_wmma_tensor_op.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_b1t_b1n_s32n_tensor_op_s32_sm75.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_b1t_b1n_s32n_tensor_op_s32_sm75.cu`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -67,15 +67,15 @@
       cutlass::gemm::GemmShape<8, 8, 128>,
       cutlass::epilogue::thread::LinearCombination<
           ElementOutput, 128 / cutlass::sizeof_bits<ElementOutput>::value,
           ElementAccumulator, ElementCompute>,
       cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>, 2, 128, 128,
       false, cutlass::arch::OpXorPopc>;
 
-  EXPECT_TRUE(test::gemm::device::TestAllGemm<Gemm>());
+  EXPECT_TRUE(test::gemm::device::TestAllGemmBasic<Gemm>());
 }
 
 TEST(SM75_Device_Gemm_b1t_b1n_s32n_tensor_op_s32, 256x128x512_64x64x512) {
 
   using ElementOutput = int32_t;
   using ElementAccumulator = int32_t;
   using ElementCompute = int32_t;
@@ -89,15 +89,15 @@
       cutlass::gemm::GemmShape<8, 8, 128>,
       cutlass::epilogue::thread::LinearCombination<
           ElementOutput, 128 / cutlass::sizeof_bits<ElementOutput>::value,
           ElementAccumulator, ElementCompute>,
       cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>, 2, 128, 128,
       false, cutlass::arch::OpXorPopc>;
 
-  EXPECT_TRUE(test::gemm::device::TestAllGemm<Gemm>());
+  EXPECT_TRUE(test::gemm::device::TestAllGemmBasic<Gemm>());
 }
 
 TEST(SM75_Device_Gemm_b1t_b1n_s32n_tensor_op_s32, 128x128x512_64x64x512) {
 
   using ElementOutput = int32_t;
   using ElementAccumulator = int32_t;
   using ElementCompute = int32_t;
@@ -111,15 +111,15 @@
       cutlass::gemm::GemmShape<8, 8, 128>,
       cutlass::epilogue::thread::LinearCombination<
           ElementOutput, 128 / cutlass::sizeof_bits<ElementOutput>::value,
           ElementAccumulator, ElementCompute>,
       cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>, 2, 128, 128,
       false, cutlass::arch::OpXorPopc>;
 
-  EXPECT_TRUE(test::gemm::device::TestAllGemm<Gemm>());
+  EXPECT_TRUE(test::gemm::device::TestAllGemmBasic<Gemm>());
 }
 
 TEST(SM75_Device_Gemm_b1t_b1n_s32n_tensor_op_s32, 64x256x512_64x64x512) {
 
   using ElementOutput = int32_t;
   using ElementAccumulator = int32_t;
   using ElementCompute = int32_t;
@@ -133,15 +133,15 @@
       cutlass::gemm::GemmShape<8, 8, 128>,
       cutlass::epilogue::thread::LinearCombination<
           ElementOutput, 128 / cutlass::sizeof_bits<ElementOutput>::value,
           ElementAccumulator, ElementCompute>,
       cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>, 2, 128, 128,
       false, cutlass::arch::OpXorPopc>;
 
-  EXPECT_TRUE(test::gemm::device::TestAllGemm<Gemm>());
+  EXPECT_TRUE(test::gemm::device::TestAllGemmBasic<Gemm>());
 }
 
 TEST(SM75_Device_Gemm_b1t_b1n_s32n_tensor_op_s32, 256x64x512_64x64x512) {
 
   using ElementOutput = int32_t;
   using ElementAccumulator = int32_t;
   using ElementCompute = int32_t;
@@ -155,15 +155,15 @@
       cutlass::gemm::GemmShape<8, 8, 128>,
       cutlass::epilogue::thread::LinearCombination<
           ElementOutput, 128 / cutlass::sizeof_bits<ElementOutput>::value,
           ElementAccumulator, ElementCompute>,
       cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>, 2, 128, 128,
       false, cutlass::arch::OpXorPopc>;
 
-  EXPECT_TRUE(test::gemm::device::TestAllGemm<Gemm>());
+  EXPECT_TRUE(test::gemm::device::TestAllGemmBasic<Gemm>());
 }
 
 TEST(SM75_Device_Gemm_b1t_b1n_s32n_tensor_op_s32, 64x128x512_32x64x512) {
 
   using ElementOutput = int32_t;
   using ElementAccumulator = int32_t;
   using ElementCompute = int32_t;
@@ -177,15 +177,15 @@
       cutlass::gemm::GemmShape<8, 8, 128>,
       cutlass::epilogue::thread::LinearCombination<
           ElementOutput, 128 / cutlass::sizeof_bits<ElementOutput>::value,
           ElementAccumulator, ElementCompute>,
       cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>, 2, 128, 128,
       false, cutlass::arch::OpXorPopc>;
 
-  EXPECT_TRUE(test::gemm::device::TestAllGemm<Gemm>());
+  EXPECT_TRUE(test::gemm::device::TestAllGemmBasic<Gemm>());
 }
 
 TEST(SM75_Device_Gemm_b1t_b1n_s32n_tensor_op_s32, 128x64x512_64x32x512) {
 
   using ElementOutput = int32_t;
   using ElementAccumulator = int32_t;
   using ElementCompute = int32_t;
@@ -199,15 +199,15 @@
       cutlass::gemm::GemmShape<8, 8, 128>,
       cutlass::epilogue::thread::LinearCombination<
           ElementOutput, 128 / cutlass::sizeof_bits<ElementOutput>::value,
           ElementAccumulator, ElementCompute>,
       cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>, 2, 128, 128,
       false, cutlass::arch::OpXorPopc>;
 
-  EXPECT_TRUE(test::gemm::device::TestAllGemm<Gemm>());
+  EXPECT_TRUE(test::gemm::device::TestAllGemmBasic<Gemm>());
 }
 
 TEST(SM75_Device_Gemm_b1t_b1n_s32n_tensor_op_s32, 64x64x512_32x32x512) {
 
   using ElementOutput = int32_t;
   using ElementAccumulator = int32_t;
   using ElementCompute = int32_t;
@@ -221,13 +221,13 @@
       cutlass::gemm::GemmShape<8, 8, 128>,
       cutlass::epilogue::thread::LinearCombination<
           ElementOutput, 128 / cutlass::sizeof_bits<ElementOutput>::value,
           ElementAccumulator, ElementCompute>,
       cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>, 2, 128, 128,
       false, cutlass::arch::OpXorPopc>;
 
-  EXPECT_TRUE(test::gemm::device::TestAllGemm<Gemm>());
+  EXPECT_TRUE(test::gemm::device::TestAllGemmBasic<Gemm>());
 }
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
 #endif
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_b1t_b1n_s32n_tensor_op_s32_sm80.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_b1t_b1n_s32n_tensor_op_s32_sm80.cu`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /**************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -66,15 +66,15 @@
       cutlass::gemm::GemmShape<16, 8, 256>,
       cutlass::epilogue::thread::LinearCombination<
           ElementOutput, 128 / cutlass::sizeof_bits<ElementOutput>::value,
           ElementAccumulator, ElementCompute>,
       cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>, 3, 128, 128,
       false, cutlass::arch::OpXorPopc>;
 
-  EXPECT_TRUE(test::gemm::device::TestAllGemm<Gemm>());
+  EXPECT_TRUE(test::gemm::device::TestAllGemmBasic<Gemm>());
 }
 
 TEST(SM80_Device_Gemm_XOR_b1t_b1n_s32n_tensor_op_s32, 256x128x1024_64x64x1024) {
   using ElementOutput = int32_t;
   using ElementAccumulator = int32_t;
   using ElementCompute = int32_t;
 
@@ -86,15 +86,15 @@
       cutlass::gemm::GemmShape<64, 64, 1024>, cutlass::gemm::GemmShape<16, 8, 256>,
       cutlass::epilogue::thread::LinearCombination<
           ElementOutput, 128 / cutlass::sizeof_bits<ElementOutput>::value,
           ElementAccumulator, ElementCompute>,
       cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>, 3, 128, 128,
       false, cutlass::arch::OpXorPopc>;
 
-  EXPECT_TRUE(test::gemm::device::TestAllGemm<Gemm>());
+  EXPECT_TRUE(test::gemm::device::TestAllGemmBasic<Gemm>());
 }
 
 TEST(SM80_Device_Gemm_XOR_b1t_b1n_s32n_tensor_op_s32, 128x128x1024_64x64x1024) {
   using ElementOutput = int32_t;
   using ElementAccumulator = int32_t;
   using ElementCompute = int32_t;
 
@@ -107,15 +107,15 @@
       cutlass::gemm::GemmShape<16, 8, 256>,
       cutlass::epilogue::thread::LinearCombination<
           ElementOutput, 128 / cutlass::sizeof_bits<ElementOutput>::value,
           ElementAccumulator, ElementCompute>,
       cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>, 3, 128, 128,
       false, cutlass::arch::OpXorPopc>;
 
-  EXPECT_TRUE(test::gemm::device::TestAllGemm<Gemm>());
+  EXPECT_TRUE(test::gemm::device::TestAllGemmBasic<Gemm>());
 }
 
 TEST(SM80_Device_Gemm_XOR_b1t_b1n_s32n_tensor_op_s32, 256x64x1024_64x64x1024) {
   using ElementOutput = int32_t;
   using ElementAccumulator = int32_t;
   using ElementCompute = int32_t;
 
@@ -127,15 +127,15 @@
       cutlass::gemm::GemmShape<64, 64, 1024>, cutlass::gemm::GemmShape<16, 8, 256>,
       cutlass::epilogue::thread::LinearCombination<
           ElementOutput, 128 / cutlass::sizeof_bits<ElementOutput>::value,
           ElementAccumulator, ElementCompute>,
       cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>, 3, 128, 128,
       false, cutlass::arch::OpXorPopc>;
 
-  EXPECT_TRUE(test::gemm::device::TestAllGemm<Gemm>());
+  EXPECT_TRUE(test::gemm::device::TestAllGemmBasic<Gemm>());
 }
 
 TEST(SM80_Device_Gemm_XOR_b1t_b1n_s32n_tensor_op_s32, 64x256x1024_64x64x1024) {
   using ElementOutput = int32_t;
   using ElementAccumulator = int32_t;
   using ElementCompute = int32_t;
 
@@ -147,15 +147,15 @@
       cutlass::gemm::GemmShape<64, 64, 1024>, cutlass::gemm::GemmShape<16, 8, 256>,
       cutlass::epilogue::thread::LinearCombination<
           ElementOutput, 128 / cutlass::sizeof_bits<ElementOutput>::value,
           ElementAccumulator, ElementCompute>,
       cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>, 3, 128, 128,
       false, cutlass::arch::OpXorPopc>;
 
-  EXPECT_TRUE(test::gemm::device::TestAllGemm<Gemm>());
+  EXPECT_TRUE(test::gemm::device::TestAllGemmBasic<Gemm>());
 }
 
 TEST(SM80_Device_Gemm_XOR_b1t_b1n_s32n_tensor_op_s32, 64x128x1024_32x64x1024) {
   using ElementOutput = int32_t;
   using ElementAccumulator = int32_t;
   using ElementCompute = int32_t;
 
@@ -167,15 +167,15 @@
       cutlass::gemm::GemmShape<32, 64, 1024>, cutlass::gemm::GemmShape<16, 8, 256>,
       cutlass::epilogue::thread::LinearCombination<
           ElementOutput, 128 / cutlass::sizeof_bits<ElementOutput>::value,
           ElementAccumulator, ElementCompute>,
       cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>, 3, 128, 128,
       false, cutlass::arch::OpXorPopc>;
 
-  EXPECT_TRUE(test::gemm::device::TestAllGemm<Gemm>());
+  EXPECT_TRUE(test::gemm::device::TestAllGemmBasic<Gemm>());
 }
 
 TEST(SM80_Device_Gemm_XOR_b1t_b1n_s32n_tensor_op_s32, 128x64x1024_64x32x1024) {
   using ElementOutput = int32_t;
   using ElementAccumulator = int32_t;
   using ElementCompute = int32_t;
 
@@ -187,15 +187,15 @@
       cutlass::gemm::GemmShape<64, 32, 1024>, cutlass::gemm::GemmShape<16, 8, 256>,
       cutlass::epilogue::thread::LinearCombination<
           ElementOutput, 128 / cutlass::sizeof_bits<ElementOutput>::value,
           ElementAccumulator, ElementCompute>,
       cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>, 3, 128, 128,
       false, cutlass::arch::OpXorPopc>;
 
-  EXPECT_TRUE(test::gemm::device::TestAllGemm<Gemm>());
+  EXPECT_TRUE(test::gemm::device::TestAllGemmBasic<Gemm>());
 }
 
 TEST(SM80_Device_Gemm_XOR_b1t_b1n_s32n_tensor_op_s32, 64x64x1024_32x32x1024) {
   using ElementOutput = int32_t;
   using ElementAccumulator = int32_t;
   using ElementCompute = int32_t;
 
@@ -207,15 +207,15 @@
       cutlass::gemm::GemmShape<32, 32, 1024>, cutlass::gemm::GemmShape<16, 8, 256>,
       cutlass::epilogue::thread::LinearCombination<
           ElementOutput, 128 / cutlass::sizeof_bits<ElementOutput>::value,
           ElementAccumulator, ElementCompute>,
       cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>, 4, 128, 128,
       false, cutlass::arch::OpXorPopc>;
 
-  EXPECT_TRUE(test::gemm::device::TestAllGemm<Gemm>());
+  EXPECT_TRUE(test::gemm::device::TestAllGemmBasic<Gemm>());
 }
 
 TEST(SM80_Device_Gemm_XOR_b1t_b1n_s32n_tensor_op_s32, 128x256x512_64x64x512) {
   using ElementOutput = int32_t;
   using ElementAccumulator = int32_t;
   using ElementCompute = int32_t;
 
@@ -227,15 +227,15 @@
       cutlass::gemm::GemmShape<64, 64, 512>, cutlass::gemm::GemmShape<16, 8, 256>,
       cutlass::epilogue::thread::LinearCombination<
           ElementOutput, 128 / cutlass::sizeof_bits<ElementOutput>::value,
           ElementAccumulator, ElementCompute>,
       cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>, 3, 128, 128,
       false, cutlass::arch::OpXorPopc>;
 
-  EXPECT_TRUE(test::gemm::device::TestAllGemm<Gemm>());
+  EXPECT_TRUE(test::gemm::device::TestAllGemmBasic<Gemm>());
 }
 
 TEST(SM80_Device_Gemm_XOR_b1t_b1n_s32n_tensor_op_s32, 256x128x512_64x64x512) {
   using ElementOutput = int32_t;
   using ElementAccumulator = int32_t;
   using ElementCompute = int32_t;
 
@@ -247,15 +247,15 @@
       cutlass::gemm::GemmShape<64, 64, 512>, cutlass::gemm::GemmShape<16, 8, 256>,
       cutlass::epilogue::thread::LinearCombination<
           ElementOutput, 128 / cutlass::sizeof_bits<ElementOutput>::value,
           ElementAccumulator, ElementCompute>,
       cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>, 3, 128, 128,
       false, cutlass::arch::OpXorPopc>;
 
-  EXPECT_TRUE(test::gemm::device::TestAllGemm<Gemm>());
+  EXPECT_TRUE(test::gemm::device::TestAllGemmBasic<Gemm>());
 }
 
 TEST(SM80_Device_Gemm_XOR_b1t_b1n_s32n_tensor_op_s32, 128x128x512_64x64x512) {
   using ElementOutput = int32_t;
   using ElementAccumulator = int32_t;
   using ElementCompute = int32_t;
 
@@ -267,15 +267,15 @@
       cutlass::gemm::GemmShape<64, 64, 512>, cutlass::gemm::GemmShape<16, 8, 256>,
       cutlass::epilogue::thread::LinearCombination<
           ElementOutput, 128 / cutlass::sizeof_bits<ElementOutput>::value,
           ElementAccumulator, ElementCompute>,
       cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>, 3, 128, 128,
       false, cutlass::arch::OpXorPopc>;
 
-  EXPECT_TRUE(test::gemm::device::TestAllGemm<Gemm>());
+  EXPECT_TRUE(test::gemm::device::TestAllGemmBasic<Gemm>());
 }
 
 TEST(SM80_Device_Gemm_XOR_b1t_b1n_s32n_tensor_op_s32, 256x64x512_64x64x512) {
   using ElementOutput = int32_t;
   using ElementAccumulator = int32_t;
   using ElementCompute = int32_t;
 
@@ -287,15 +287,15 @@
       cutlass::gemm::GemmShape<64, 64, 512>, cutlass::gemm::GemmShape<16, 8, 256>,
       cutlass::epilogue::thread::LinearCombination<
           ElementOutput, 128 / cutlass::sizeof_bits<ElementOutput>::value,
           ElementAccumulator, ElementCompute>,
       cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>, 3, 128, 128,
       false, cutlass::arch::OpXorPopc>;
 
-  EXPECT_TRUE(test::gemm::device::TestAllGemm<Gemm>());
+  EXPECT_TRUE(test::gemm::device::TestAllGemmBasic<Gemm>());
 }
 
 TEST(SM80_Device_Gemm_XOR_b1t_b1n_s32n_tensor_op_s32, 64x256x512_64x64x512) {
   using ElementOutput = int32_t;
   using ElementAccumulator = int32_t;
   using ElementCompute = int32_t;
 
@@ -307,15 +307,15 @@
       cutlass::gemm::GemmShape<64, 64, 512>, cutlass::gemm::GemmShape<16, 8, 256>,
       cutlass::epilogue::thread::LinearCombination<
           ElementOutput, 128 / cutlass::sizeof_bits<ElementOutput>::value,
           ElementAccumulator, ElementCompute>,
       cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>, 3, 128, 128,
       false, cutlass::arch::OpXorPopc>;
 
-  EXPECT_TRUE(test::gemm::device::TestAllGemm<Gemm>());
+  EXPECT_TRUE(test::gemm::device::TestAllGemmBasic<Gemm>());
 }
 
 TEST(SM80_Device_Gemm_XOR_b1t_b1n_s32n_tensor_op_s32, 64x128x512_32x64x512) {
   using ElementOutput = int32_t;
   using ElementAccumulator = int32_t;
   using ElementCompute = int32_t;
 
@@ -327,15 +327,15 @@
       cutlass::gemm::GemmShape<32, 64, 512>, cutlass::gemm::GemmShape<16, 8, 256>,
       cutlass::epilogue::thread::LinearCombination<
           ElementOutput, 128 / cutlass::sizeof_bits<ElementOutput>::value,
           ElementAccumulator, ElementCompute>,
       cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>, 4, 128, 128,
       false, cutlass::arch::OpXorPopc>;
 
-  EXPECT_TRUE(test::gemm::device::TestAllGemm<Gemm>());
+  EXPECT_TRUE(test::gemm::device::TestAllGemmBasic<Gemm>());
 }
 
 TEST(SM80_Device_Gemm_XOR_b1t_b1n_s32n_tensor_op_s32, 128x64x512_64x32x512) {
   using ElementOutput = int32_t;
   using ElementAccumulator = int32_t;
   using ElementCompute = int32_t;
 
@@ -347,15 +347,15 @@
       cutlass::gemm::GemmShape<64, 32, 512>, cutlass::gemm::GemmShape<16, 8, 256>,
       cutlass::epilogue::thread::LinearCombination<
           ElementOutput, 128 / cutlass::sizeof_bits<ElementOutput>::value,
           ElementAccumulator, ElementCompute>,
       cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>, 4, 128, 128,
       false, cutlass::arch::OpXorPopc>;
 
-  EXPECT_TRUE(test::gemm::device::TestAllGemm<Gemm>());
+  EXPECT_TRUE(test::gemm::device::TestAllGemmBasic<Gemm>());
 }
 
 TEST(SM80_Device_Gemm_XOR_b1t_b1n_s32n_tensor_op_s32, 64x64x512_32x32x512) {
   using ElementOutput = int32_t;
   using ElementAccumulator = int32_t;
   using ElementCompute = int32_t;
 
@@ -367,13 +367,13 @@
       cutlass::gemm::GemmShape<32, 32, 512>, cutlass::gemm::GemmShape<16, 8, 256>,
       cutlass::epilogue::thread::LinearCombination<
           ElementOutput, 128 / cutlass::sizeof_bits<ElementOutput>::value,
           ElementAccumulator, ElementCompute>,
       cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>, 6, 128, 128,
       false, cutlass::arch::OpXorPopc>;
 
-  EXPECT_TRUE(test::gemm::device::TestAllGemm<Gemm>());
+  EXPECT_TRUE(test::gemm::device::TestAllGemmBasic<Gemm>());
 }
 
 ////////////////////////////////////////////////////////////////////////////////
 
 #endif  // #if defined(CUTLASS_ARCH_MMA_SM80_SUPPORTED)
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_b1t_b1n_s32n_wmma_tensor_op_s32_sm75.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_b1t_b1n_s32n_wmma_tensor_op_s32_sm75.cu`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -79,15 +79,15 @@
       ElementAccumulator
     >,
     cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>,
     2, 128, 128, false, 
     cutlass::arch::OpXorPopc
   >;
 
-  EXPECT_TRUE(test::gemm::device::TestAllGemm<Gemm>());
+  EXPECT_TRUE(test::gemm::device::TestAllGemmBasic<Gemm>());
 }
 
 TEST(SM75_Device_Gemm_b1t_b1n_s32n_wmma_tensor_op_s32, 256x128x512_64x64x512_8x8x128) {
 
   using ElementOutput = int32_t;
   using ElementAccumulator = int32_t;
   using ElementCompute = int32_t;
@@ -110,15 +110,15 @@
           128 / cutlass::sizeof_bits<ElementOutput>::value,
           ElementAccumulator, 
           ElementCompute>,
       cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>, 
       2, 128, 128, false, 
       cutlass::arch::OpXorPopc>;
 
-  EXPECT_TRUE(test::gemm::device::TestAllGemm<Gemm>());
+  EXPECT_TRUE(test::gemm::device::TestAllGemmBasic<Gemm>());
 }
 
 TEST(SM75_Device_Gemm_b1t_b1n_s32n_wmma_tensor_op_s32, 128x128x512_64x64x512_8x8x128) {
 
   using ElementOutput = int32_t;
   using ElementAccumulator = int32_t;
   using ElementCompute = int32_t;
@@ -141,15 +141,15 @@
           128 / cutlass::sizeof_bits<ElementOutput>::value,
           ElementAccumulator, 
           ElementCompute>,
       cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>, 
       2, 128, 128, false, 
       cutlass::arch::OpXorPopc>;
 
-  EXPECT_TRUE(test::gemm::device::TestAllGemm<Gemm>());
+  EXPECT_TRUE(test::gemm::device::TestAllGemmBasic<Gemm>());
 }
 
 TEST(SM75_Device_Gemm_b1t_b1n_s32n_wmma_tensor_op_s32, 64x128x512_32x64x512_8x8x128) {
 
   using ElementOutput = int32_t;
   using ElementAccumulator = int32_t;
   using ElementCompute = int32_t;
@@ -172,15 +172,15 @@
           128 / cutlass::sizeof_bits<ElementOutput>::value,
           ElementAccumulator, 
           ElementCompute>,
       cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>, 
       2, 128, 128, false, 
       cutlass::arch::OpXorPopc>;
 
-  EXPECT_TRUE(test::gemm::device::TestAllGemm<Gemm>());
+  EXPECT_TRUE(test::gemm::device::TestAllGemmBasic<Gemm>());
 }
 
 TEST(SM75_Device_Gemm_b1t_b1n_s32n_wmma_tensor_op_s32, 128x64x512_64x32x512_8x8x128) {
 
   using ElementOutput = int32_t;
   using ElementAccumulator = int32_t;
   using ElementCompute = int32_t;
@@ -203,15 +203,15 @@
           128 / cutlass::sizeof_bits<ElementOutput>::value,
           ElementAccumulator, 
           ElementCompute>,
       cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>, 
       2, 128, 128, false, 
       cutlass::arch::OpXorPopc>;
 
-  EXPECT_TRUE(test::gemm::device::TestAllGemm<Gemm>());
+  EXPECT_TRUE(test::gemm::device::TestAllGemmBasic<Gemm>());
 }
 
 TEST(SM75_Device_Gemm_b1t_b1n_s32n_wmma_tensor_op_s32, 64x64x512_32x32x512_8x8x128) {
 
   using ElementOutput = int32_t;
   using ElementAccumulator = int32_t;
   using ElementCompute = int32_t;
@@ -234,10 +234,10 @@
           128 / cutlass::sizeof_bits<ElementOutput>::value,
           ElementAccumulator, 
           ElementCompute>,
       cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>, 
       2, 128, 128, false, 
       cutlass::arch::OpXorPopc>;
 
-  EXPECT_TRUE(test::gemm::device::TestAllGemm<Gemm>());
+  EXPECT_TRUE(test::gemm::device::TestAllGemmBasic<Gemm>());
 }
 #endif //CUTLASS_SUBBYTE_INTEGER_MATRIX_MULTIPLY_ENABLED
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_b1t_b1n_s32t_tensor_op_s32_sm75.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_b1t_b1n_s32t_tensor_op_s32_sm75.cu`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -67,15 +67,15 @@
       cutlass::gemm::GemmShape<8, 8, 128>,
       cutlass::epilogue::thread::LinearCombination<
           ElementOutput, 128 / cutlass::sizeof_bits<ElementOutput>::value,
           ElementAccumulator, ElementCompute>,
       cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>, 2, 128, 128,
       false, cutlass::arch::OpXorPopc>;
 
-  EXPECT_TRUE(test::gemm::device::TestAllGemm<Gemm>());
+  EXPECT_TRUE(test::gemm::device::TestAllGemmBasic<Gemm>());
 }
 
 TEST(SM75_Device_Gemm_b1t_b1n_s32t_tensor_op_s32, 256x128x512_64x64x512) {
 
   using ElementOutput = int32_t;
   using ElementAccumulator = int32_t;
   using ElementCompute = int32_t;
@@ -89,15 +89,15 @@
       cutlass::gemm::GemmShape<8, 8, 128>,
       cutlass::epilogue::thread::LinearCombination<
           ElementOutput, 128 / cutlass::sizeof_bits<ElementOutput>::value,
           ElementAccumulator, ElementCompute>,
       cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>, 2, 128, 128,
       false, cutlass::arch::OpXorPopc>;
 
-  EXPECT_TRUE(test::gemm::device::TestAllGemm<Gemm>());
+  EXPECT_TRUE(test::gemm::device::TestAllGemmBasic<Gemm>());
 }
 
 TEST(SM75_Device_Gemm_b1t_b1n_s32t_tensor_op_s32, 128x128x512_64x64x512) {
 
   using ElementOutput = int32_t;
   using ElementAccumulator = int32_t;
   using ElementCompute = int32_t;
@@ -111,15 +111,15 @@
       cutlass::gemm::GemmShape<8, 8, 128>,
       cutlass::epilogue::thread::LinearCombination<
           ElementOutput, 128 / cutlass::sizeof_bits<ElementOutput>::value,
           ElementAccumulator, ElementCompute>,
       cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>, 2, 128, 128,
       false, cutlass::arch::OpXorPopc>;
 
-  EXPECT_TRUE(test::gemm::device::TestAllGemm<Gemm>());
+  EXPECT_TRUE(test::gemm::device::TestAllGemmBasic<Gemm>());
 }
 
 TEST(SM75_Device_Gemm_b1t_b1n_s32t_tensor_op_s32, 64x256x512_64x64x512) {
 
   using ElementOutput = int32_t;
   using ElementAccumulator = int32_t;
   using ElementCompute = int32_t;
@@ -133,15 +133,15 @@
       cutlass::gemm::GemmShape<8, 8, 128>,
       cutlass::epilogue::thread::LinearCombination<
           ElementOutput, 128 / cutlass::sizeof_bits<ElementOutput>::value,
           ElementAccumulator, ElementCompute>,
       cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>, 2, 128, 128,
       false, cutlass::arch::OpXorPopc>;
 
-  EXPECT_TRUE(test::gemm::device::TestAllGemm<Gemm>());
+  EXPECT_TRUE(test::gemm::device::TestAllGemmBasic<Gemm>());
 }
 
 TEST(SM75_Device_Gemm_b1t_b1n_s32t_tensor_op_s32, 256x64x512_64x64x512) {
 
   using ElementOutput = int32_t;
   using ElementAccumulator = int32_t;
   using ElementCompute = int32_t;
@@ -155,15 +155,15 @@
       cutlass::gemm::GemmShape<8, 8, 128>,
       cutlass::epilogue::thread::LinearCombination<
           ElementOutput, 128 / cutlass::sizeof_bits<ElementOutput>::value,
           ElementAccumulator, ElementCompute>,
       cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>, 2, 128, 128,
       false, cutlass::arch::OpXorPopc>;
 
-  EXPECT_TRUE(test::gemm::device::TestAllGemm<Gemm>());
+  EXPECT_TRUE(test::gemm::device::TestAllGemmBasic<Gemm>());
 }
 TEST(SM75_Device_Gemm_b1t_b1n_s32t_tensor_op_s32, 64x128x512_32x64x512) {
 
   using ElementOutput = int32_t;
   using ElementAccumulator = int32_t;
   using ElementCompute = int32_t;
 
@@ -176,15 +176,15 @@
       cutlass::gemm::GemmShape<8, 8, 128>,
       cutlass::epilogue::thread::LinearCombination<
           ElementOutput, 128 / cutlass::sizeof_bits<ElementOutput>::value,
           ElementAccumulator, ElementCompute>,
       cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>, 2, 128, 128,
       false, cutlass::arch::OpXorPopc>;
 
-  EXPECT_TRUE(test::gemm::device::TestAllGemm<Gemm>());
+  EXPECT_TRUE(test::gemm::device::TestAllGemmBasic<Gemm>());
 }
 
 TEST(SM75_Device_Gemm_b1t_b1n_s32t_tensor_op_s32, 128x64x512_64x32x512) {
 
   using ElementOutput = int32_t;
   using ElementAccumulator = int32_t;
   using ElementCompute = int32_t;
@@ -198,15 +198,15 @@
       cutlass::gemm::GemmShape<8, 8, 128>,
       cutlass::epilogue::thread::LinearCombination<
           ElementOutput, 128 / cutlass::sizeof_bits<ElementOutput>::value,
           ElementAccumulator, ElementCompute>,
       cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>, 2, 128, 128,
       false, cutlass::arch::OpXorPopc>;
 
-  EXPECT_TRUE(test::gemm::device::TestAllGemm<Gemm>());
+  EXPECT_TRUE(test::gemm::device::TestAllGemmBasic<Gemm>());
 }
 
 TEST(SM75_Device_Gemm_b1t_b1n_s32t_tensor_op_s32, 64x64x512_32x32x512) {
 
   using ElementOutput = int32_t;
   using ElementAccumulator = int32_t;
   using ElementCompute = int32_t;
@@ -220,13 +220,13 @@
       cutlass::gemm::GemmShape<8, 8, 128>,
       cutlass::epilogue::thread::LinearCombination<
           ElementOutput, 128 / cutlass::sizeof_bits<ElementOutput>::value,
           ElementAccumulator, ElementCompute>,
       cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>, 2, 128, 128,
       false, cutlass::arch::OpXorPopc>;
 
-  EXPECT_TRUE(test::gemm::device::TestAllGemm<Gemm>());
+  EXPECT_TRUE(test::gemm::device::TestAllGemmBasic<Gemm>());
 }
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
 #endif
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_b1t_b1n_s32t_tensor_op_s32_sm80.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_b1t_b1n_s32t_tensor_op_s32_sm80.cu`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /**************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_b1t_b1n_s32t_wmma_tensor_op_s32_sm75.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_b1t_b1n_s32t_wmma_tensor_op_s32_sm75.cu`

 * *Files 2% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -79,15 +79,15 @@
       ElementAccumulator
     >,
     cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>,
     2, 128, 128, false, 
     cutlass::arch::OpXorPopc
   >;
 
-  EXPECT_TRUE(test::gemm::device::TestAllGemm<Gemm>());
+  EXPECT_TRUE(test::gemm::device::TestAllGemmBasic<Gemm>());
 }
 
 TEST(SM75_Device_Gemm_b1t_b1n_s32t_wmma_tensor_op_s32, 256x128x512_64x64x512_8x8x128) {
 
   using ElementOutput = int32_t;
   using ElementAccumulator = int32_t;
   using ElementCompute = int32_t;
@@ -110,15 +110,15 @@
           128 / cutlass::sizeof_bits<ElementOutput>::value,
           ElementAccumulator, 
           ElementCompute>,
       cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>, 
       2, 128, 128, false, 
       cutlass::arch::OpXorPopc>;
 
-  EXPECT_TRUE(test::gemm::device::TestAllGemm<Gemm>());
+  EXPECT_TRUE(test::gemm::device::TestAllGemmBasic<Gemm>());
 }
 
 TEST(SM75_Device_Gemm_b1t_b1n_s32t_wmma_tensor_op_s32, 128x128x512_64x64x512_8x8x128) {
 
   using ElementOutput = int32_t;
   using ElementAccumulator = int32_t;
   using ElementCompute = int32_t;
@@ -141,15 +141,15 @@
           128 / cutlass::sizeof_bits<ElementOutput>::value,
           ElementAccumulator, 
           ElementCompute>,
       cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>, 
       2, 128, 128, false, 
       cutlass::arch::OpXorPopc>;
 
-  EXPECT_TRUE(test::gemm::device::TestAllGemm<Gemm>());
+  EXPECT_TRUE(test::gemm::device::TestAllGemmBasic<Gemm>());
 }
 
 TEST(SM75_Device_Gemm_b1t_b1n_s32t_wmma_tensor_op_s32, 64x128x512_32x64x512_8x8x128) {
 
   using ElementOutput = int32_t;
   using ElementAccumulator = int32_t;
   using ElementCompute = int32_t;
@@ -172,15 +172,15 @@
           128 / cutlass::sizeof_bits<ElementOutput>::value,
           ElementAccumulator, 
           ElementCompute>,
       cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>, 
       2, 128, 128, false, 
       cutlass::arch::OpXorPopc>;
 
-  EXPECT_TRUE(test::gemm::device::TestAllGemm<Gemm>());
+  EXPECT_TRUE(test::gemm::device::TestAllGemmBasic<Gemm>());
 }
 
 TEST(SM75_Device_Gemm_b1t_b1n_s32t_wmma_tensor_op_s32, 128x64x512_64x32x512_8x8x128) {
 
   using ElementOutput = int32_t;
   using ElementAccumulator = int32_t;
   using ElementCompute = int32_t;
@@ -203,15 +203,15 @@
           128 / cutlass::sizeof_bits<ElementOutput>::value,
           ElementAccumulator, 
           ElementCompute>,
       cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>, 
       2, 128, 128, false, 
       cutlass::arch::OpXorPopc>;
 
-  EXPECT_TRUE(test::gemm::device::TestAllGemm<Gemm>());
+  EXPECT_TRUE(test::gemm::device::TestAllGemmBasic<Gemm>());
 }
 
 TEST(SM75_Device_Gemm_b1t_b1n_s32t_wmma_tensor_op_s32, 64x64x512_32x32x512_8x8x128) {
 
   using ElementOutput = int32_t;
   using ElementAccumulator = int32_t;
   using ElementCompute = int32_t;
@@ -234,10 +234,10 @@
           128 / cutlass::sizeof_bits<ElementOutput>::value,
           ElementAccumulator, 
           ElementCompute>,
       cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>, 
       2, 128, 128, false, 
       cutlass::arch::OpXorPopc>;
 
-  EXPECT_TRUE(test::gemm::device::TestAllGemm<Gemm>());
+  EXPECT_TRUE(test::gemm::device::TestAllGemmBasic<Gemm>());
 }
 #endif //CUTLASS_SUBBYTE_INTEGER_MATRIX_MULTIPLY_ENABLED
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_bf16n_bf16n_f32t_tensor_op_f32_sm80.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_bf16n_bf16n_f32t_tensor_op_f32_sm80.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_bf16t_bf16t_bf16t_tensor_op_f32_sm80.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_bf16t_bf16t_bf16t_tensor_op_f32_sm80.cu`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -61,15 +61,15 @@
       cutlass::gemm::GemmShape<128, 256, 64>,
       cutlass::gemm::GemmShape<64, 64, 64>, cutlass::gemm::GemmShape<16, 8, 16>,
       cutlass::epilogue::thread::LinearCombination<
           ElementOutput, 128 / cutlass::sizeof_bits<ElementOutput>::value,
           ElementAccumulator, ElementAccumulator>,
       cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>, 3>;
 
-  EXPECT_TRUE(test::gemm::device::TestAllGemm<Gemm>());
+  EXPECT_TRUE(test::gemm::device::TestAllGemmBasic<Gemm>());
 }
 
 TEST(SM80_Device_Gemm_bf16t_bf16t_bf16t_tensor_op_f32, 256x128x64_64x64x64) {
   using ElementOutput = cutlass::bfloat16_t;
   using ElementAccumulator = float;
 
   using Gemm = cutlass::gemm::device::Gemm<
@@ -79,15 +79,15 @@
       cutlass::gemm::GemmShape<256, 128, 64>,
       cutlass::gemm::GemmShape<64, 64, 64>, cutlass::gemm::GemmShape<16, 8, 16>,
       cutlass::epilogue::thread::LinearCombination<
           ElementOutput, 128 / cutlass::sizeof_bits<ElementOutput>::value,
           ElementAccumulator, ElementAccumulator>,
       cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>, 3>;
 
-  EXPECT_TRUE(test::gemm::device::TestAllGemm<Gemm>());
+  EXPECT_TRUE(test::gemm::device::TestAllGemmBasic<Gemm>());
 }
 
 TEST(SM80_Device_Gemm_bf16t_bf16t_bf16t_tensor_op_f32, 128x128x64_64x64x64) {
   using ElementOutput = cutlass::bfloat16_t;
   using ElementAccumulator = float;
 
   using Gemm = cutlass::gemm::device::Gemm<
@@ -97,15 +97,15 @@
       cutlass::gemm::GemmShape<128, 128, 64>,
       cutlass::gemm::GemmShape<64, 64, 64>, cutlass::gemm::GemmShape<16, 8, 16>,
       cutlass::epilogue::thread::LinearCombination<
           ElementOutput, 128 / cutlass::sizeof_bits<ElementOutput>::value,
           ElementAccumulator, ElementAccumulator>,
       cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>, 3>;
 
-  EXPECT_TRUE(test::gemm::device::TestAllGemm<Gemm>());
+  EXPECT_TRUE(test::gemm::device::TestAllGemmBasic<Gemm>());
 }
 
 TEST(SM80_Device_Gemm_bf16t_bf16t_bf16t_tensor_op_f32, 256x64x64_64x64x64) {
   using ElementOutput = cutlass::bfloat16_t;
   using ElementAccumulator = float;
 
   using Gemm = cutlass::gemm::device::Gemm<
@@ -115,15 +115,15 @@
       cutlass::gemm::GemmShape<256, 64, 64>,
       cutlass::gemm::GemmShape<64, 64, 64>, cutlass::gemm::GemmShape<16, 8, 16>,
       cutlass::epilogue::thread::LinearCombination<
           ElementOutput, 128 / cutlass::sizeof_bits<ElementOutput>::value,
           ElementAccumulator, ElementAccumulator>,
       cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>, 3>;
 
-  EXPECT_TRUE(test::gemm::device::TestAllGemm<Gemm>());
+  EXPECT_TRUE(test::gemm::device::TestAllGemmBasic<Gemm>());
 }
 
 TEST(SM80_Device_Gemm_bf16t_bf16t_bf16t_tensor_op_f32, 64x256x64_64x64x64) {
   using ElementOutput = cutlass::bfloat16_t;
   using ElementAccumulator = float;
 
   using Gemm = cutlass::gemm::device::Gemm<
@@ -133,15 +133,15 @@
       cutlass::gemm::GemmShape<64, 256, 64>,
       cutlass::gemm::GemmShape<64, 64, 64>, cutlass::gemm::GemmShape<16, 8, 16>,
       cutlass::epilogue::thread::LinearCombination<
           ElementOutput, 128 / cutlass::sizeof_bits<ElementOutput>::value,
           ElementAccumulator, ElementAccumulator>,
       cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>, 3>;
 
-  EXPECT_TRUE(test::gemm::device::TestAllGemm<Gemm>());
+  EXPECT_TRUE(test::gemm::device::TestAllGemmBasic<Gemm>());
 }
 
 TEST(SM80_Device_Gemm_bf16t_bf16t_bf16t_tensor_op_f32, 64x128x64_32x64x64) {
   using ElementOutput = cutlass::bfloat16_t;
   using ElementAccumulator = float;
 
   using Gemm = cutlass::gemm::device::Gemm<
@@ -151,15 +151,15 @@
       cutlass::gemm::GemmShape<64, 128, 64>,
       cutlass::gemm::GemmShape<32, 64, 64>, cutlass::gemm::GemmShape<16, 8, 16>,
       cutlass::epilogue::thread::LinearCombination<
           ElementOutput, 128 / cutlass::sizeof_bits<ElementOutput>::value,
           ElementAccumulator, ElementAccumulator>,
       cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>, 4>;
 
-  EXPECT_TRUE(test::gemm::device::TestAllGemm<Gemm>());
+  EXPECT_TRUE(test::gemm::device::TestAllGemmBasic<Gemm>());
 }
 
 TEST(SM80_Device_Gemm_bf16t_bf16t_bf16t_tensor_op_f32, 128x64x64_64x32x64) {
   using ElementOutput = cutlass::bfloat16_t;
   using ElementAccumulator = float;
 
   using Gemm = cutlass::gemm::device::Gemm<
@@ -169,15 +169,15 @@
       cutlass::gemm::GemmShape<128, 64, 64>,
       cutlass::gemm::GemmShape<64, 32, 64>, cutlass::gemm::GemmShape<16, 8, 16>,
       cutlass::epilogue::thread::LinearCombination<
           ElementOutput, 128 / cutlass::sizeof_bits<ElementOutput>::value,
           ElementAccumulator, ElementAccumulator>,
       cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>, 4>;
 
-  EXPECT_TRUE(test::gemm::device::TestAllGemm<Gemm>());
+  EXPECT_TRUE(test::gemm::device::TestAllGemmBasic<Gemm>());
 }
 
 TEST(SM80_Device_Gemm_bf16t_bf16t_bf16t_tensor_op_f32, 64x64x64_32x32x64) {
   using ElementOutput = cutlass::bfloat16_t;
   using ElementAccumulator = float;
 
   using Gemm = cutlass::gemm::device::Gemm<
@@ -187,15 +187,15 @@
       cutlass::gemm::GemmShape<64, 64, 64>,
       cutlass::gemm::GemmShape<32, 32, 64>, cutlass::gemm::GemmShape<16, 8, 16>,
       cutlass::epilogue::thread::LinearCombination<
           ElementOutput, 128 / cutlass::sizeof_bits<ElementOutput>::value,
           ElementAccumulator, ElementAccumulator>,
       cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>, 6>;
 
-  EXPECT_TRUE(test::gemm::device::TestAllGemm<Gemm>());
+  EXPECT_TRUE(test::gemm::device::TestAllGemmBasic<Gemm>());
 }
 
 TEST(SM80_Device_Gemm_bf16t_bf16t_bf16t_tensor_op_f32, 128x256x32_64x64x32) {
   using ElementOutput = cutlass::bfloat16_t;
   using ElementAccumulator = float;
 
   using Gemm = cutlass::gemm::device::Gemm<
@@ -205,15 +205,15 @@
       cutlass::gemm::GemmShape<128, 256, 32>,
       cutlass::gemm::GemmShape<64, 64, 32>, cutlass::gemm::GemmShape<16, 8, 16>,
       cutlass::epilogue::thread::LinearCombination<
           ElementOutput, 128 / cutlass::sizeof_bits<ElementOutput>::value,
           ElementAccumulator, ElementAccumulator>,
       cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>, 3>;
 
-  EXPECT_TRUE(test::gemm::device::TestAllGemm<Gemm>());
+  EXPECT_TRUE(test::gemm::device::TestAllGemmBasic<Gemm>());
 }
 
 TEST(SM80_Device_Gemm_bf16t_bf16t_bf16t_tensor_op_f32, 256x128x32_64x64x32) {
   using ElementOutput = cutlass::bfloat16_t;
   using ElementAccumulator = float;
 
   using Gemm = cutlass::gemm::device::Gemm<
@@ -223,15 +223,15 @@
       cutlass::gemm::GemmShape<256, 128, 32>,
       cutlass::gemm::GemmShape<64, 64, 32>, cutlass::gemm::GemmShape<16, 8, 16>,
       cutlass::epilogue::thread::LinearCombination<
           ElementOutput, 128 / cutlass::sizeof_bits<ElementOutput>::value,
           ElementAccumulator, ElementAccumulator>,
       cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>, 3>;
 
-  EXPECT_TRUE(test::gemm::device::TestAllGemm<Gemm>());
+  EXPECT_TRUE(test::gemm::device::TestAllGemmBasic<Gemm>());
 }
 
 TEST(SM80_Device_Gemm_bf16t_bf16t_bf16t_tensor_op_f32, 128x128x32_64x64x32) {
   using ElementOutput = cutlass::bfloat16_t;
   using ElementAccumulator = float;
 
   using Gemm = cutlass::gemm::device::Gemm<
@@ -241,15 +241,15 @@
       cutlass::gemm::GemmShape<128, 128, 32>,
       cutlass::gemm::GemmShape<64, 64, 32>, cutlass::gemm::GemmShape<16, 8, 16>,
       cutlass::epilogue::thread::LinearCombination<
           ElementOutput, 128 / cutlass::sizeof_bits<ElementOutput>::value,
           ElementAccumulator, ElementAccumulator>,
       cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>, 4>;
 
-  EXPECT_TRUE(test::gemm::device::TestAllGemm<Gemm>());
+  EXPECT_TRUE(test::gemm::device::TestAllGemmBasic<Gemm>());
 }
 
 TEST(SM80_Device_Gemm_bf16t_bf16t_bf16t_tensor_op_f32, 256x64x32_64x64x32) {
   using ElementOutput = cutlass::bfloat16_t;
   using ElementAccumulator = float;
 
   using Gemm = cutlass::gemm::device::Gemm<
@@ -259,15 +259,15 @@
       cutlass::gemm::GemmShape<256, 64, 32>,
       cutlass::gemm::GemmShape<64, 64, 32>, cutlass::gemm::GemmShape<16, 8, 16>,
       cutlass::epilogue::thread::LinearCombination<
           ElementOutput, 128 / cutlass::sizeof_bits<ElementOutput>::value,
           ElementAccumulator, ElementAccumulator>,
       cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>, 4>;
 
-  EXPECT_TRUE(test::gemm::device::TestAllGemm<Gemm>());
+  EXPECT_TRUE(test::gemm::device::TestAllGemmBasic<Gemm>());
 }
 
 TEST(SM80_Device_Gemm_bf16t_bf16t_bf16t_tensor_op_f32, 64x256x32_64x64x32) {
   using ElementOutput = cutlass::bfloat16_t;
   using ElementAccumulator = float;
 
   using Gemm = cutlass::gemm::device::Gemm<
@@ -277,15 +277,15 @@
       cutlass::gemm::GemmShape<64, 256, 32>,
       cutlass::gemm::GemmShape<64, 64, 32>, cutlass::gemm::GemmShape<16, 8, 16>,
       cutlass::epilogue::thread::LinearCombination<
           ElementOutput, 128 / cutlass::sizeof_bits<ElementOutput>::value,
           ElementAccumulator, ElementAccumulator>,
       cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>, 4>;
 
-  EXPECT_TRUE(test::gemm::device::TestAllGemm<Gemm>());
+  EXPECT_TRUE(test::gemm::device::TestAllGemmBasic<Gemm>());
 }
 
 TEST(SM80_Device_Gemm_bf16t_bf16t_bf16t_tensor_op_f32, 64x128x32_32x64x32) {
   using ElementOutput = cutlass::bfloat16_t;
   using ElementAccumulator = float;
 
   using Gemm = cutlass::gemm::device::Gemm<
@@ -295,15 +295,15 @@
       cutlass::gemm::GemmShape<64, 128, 32>,
       cutlass::gemm::GemmShape<32, 64, 32>, cutlass::gemm::GemmShape<16, 8, 16>,
       cutlass::epilogue::thread::LinearCombination<
           ElementOutput, 128 / cutlass::sizeof_bits<ElementOutput>::value,
           ElementAccumulator, ElementAccumulator>,
       cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>, 6>;
 
-  EXPECT_TRUE(test::gemm::device::TestAllGemm<Gemm>());
+  EXPECT_TRUE(test::gemm::device::TestAllGemmBasic<Gemm>());
 }
 
 TEST(SM80_Device_Gemm_bf16t_bf16t_bf16t_tensor_op_f32, 128x64x32_64x32x32) {
   using ElementOutput = cutlass::bfloat16_t;
   using ElementAccumulator = float;
 
   using Gemm = cutlass::gemm::device::Gemm<
@@ -313,15 +313,15 @@
       cutlass::gemm::GemmShape<128, 64, 32>,
       cutlass::gemm::GemmShape<64, 32, 32>, cutlass::gemm::GemmShape<16, 8, 16>,
       cutlass::epilogue::thread::LinearCombination<
           ElementOutput, 128 / cutlass::sizeof_bits<ElementOutput>::value,
           ElementAccumulator, ElementAccumulator>,
       cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>, 6>;
 
-  EXPECT_TRUE(test::gemm::device::TestAllGemm<Gemm>());
+  EXPECT_TRUE(test::gemm::device::TestAllGemmBasic<Gemm>());
 }
 
 TEST(SM80_Device_Gemm_bf16t_bf16t_bf16t_tensor_op_f32, 64x64x32_32x32x32) {
   using ElementOutput = cutlass::bfloat16_t;
   using ElementAccumulator = float;
 
   using Gemm = cutlass::gemm::device::Gemm<
@@ -331,13 +331,13 @@
       cutlass::gemm::GemmShape<64, 64, 32>,
       cutlass::gemm::GemmShape<32, 32, 32>, cutlass::gemm::GemmShape<16, 8, 16>,
       cutlass::epilogue::thread::LinearCombination<
           ElementOutput, 128 / cutlass::sizeof_bits<ElementOutput>::value,
           ElementAccumulator, ElementAccumulator>,
       cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>, 10>;
 
-  EXPECT_TRUE(test::gemm::device::TestAllGemm<Gemm>());
+  EXPECT_TRUE(test::gemm::device::TestAllGemmBasic<Gemm>());
 }
 
 ////////////////////////////////////////////////////////////////////////////////
 
 #endif  // #if defined(CUTLASS_ARCH_MMA_SM80_SUPPORTED)
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_cf32n_cf32t_cf32t_tensor_op_tf32_f32_sm80.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_cf32n_cf32t_cf32t_tensor_op_tf32_f32_sm80.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -52,15 +52,15 @@
 
 #if defined(CUTLASS_ARCH_MMA_SM80_SUPPORTED)
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 //  Operands data type: complex<float>
 //  Rounding: float -> tfloat32_t (half_ulp_truncate)
 //  Instruction operand data type: tfloat32_t (real part) and  tfloat32_t (imaginary part)
-//  Math instruction: MMA.1688.F32.TF32
+//  Math instruction: mma.sync.aligned.m16n8k8.f32.tf32.tf32.f32
 //  Instruction output/accumulation data type: f32 (real part) and f32 (imaginary part)
 //  Output data type: complex<float>
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
 
 TEST(SM80_Device_Gemm_cf32n_cf32t_cf32t_tensor_op_tf32_f32, 32x32x16_16x16x16) {
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_cf32t_cf32n_cf32t_tensor_op_tf32_f32_sm80.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_cf32t_cf32n_cf32t_tensor_op_tf32_f32_sm80.cu`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -52,15 +52,15 @@
 
 #if defined(CUTLASS_ARCH_MMA_SM80_SUPPORTED)
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 //  Operands data type: complex<float>
 //  Rounding: float -> tfloat32_t (round to nearest)
 //  Instruction operand data type: tfloat32_t (real part) and  tfloat32_t (imaginary part)
-//  Math instruction: MMA.1688.F32.TF32
+//  Math instruction: mma.sync.aligned.m16n8k8.f32.tf32.tf32.f32
 //  Instruction output/accumulation data type: f32 (real part) and f32 (imaginary part)
 //  Output data type: complex<float>
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
 TEST(SM80_Device_Gemm_cf32t_cf32n_cf32t_tensor_op_tf32_f32, 32x32x16_16x16x16) {
 
   using Element = cutlass::complex<float>;
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_cf64n_cf64t_cf64t_tensor_op_f64_gaussian_sm80.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_cf64n_cf64t_cf64t_tensor_op_f64_gaussian_sm80.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_cf64n_cf64t_cf64t_tensor_op_f64_gaussian_sm90.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_cf64n_cf64t_cf64t_tensor_op_f64_gaussian_sm90.cu`

 * *Files 2% similar despite different names*

```diff
@@ -46,15 +46,15 @@
 #include "cutlass/util/reference/host/tensor_fill.h"
 #include "cutlass/util/tensor_view_io.h"
 
 #include "testbed_complex.h"
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
-#if defined(CUTLASS_ARCH_MMA_SM90_F64_MMA_ENABLED)
+#if defined(CUTLASS_ARCH_MMA_SM90_SUPPORTED)
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
 TEST(SM90_Device_Gemm_cf64n_cf64t_cf64t_tensor_op_f64_gaussian, 32x32x16_16x16x16) {
 
   using Element = cutlass::complex<double>; 
 
@@ -189,10 +189,10 @@
   >;
 
   EXPECT_TRUE(test::gemm::device::TestAllGemmComplex<Gemm>());
 }
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
-#endif // #if defined(CUTLASS_ARCH_MMA_SM90_F64_MMA_ENABLED)
+#endif // #if defined(CUTLASS_ARCH_MMA_SM90_SUPPORTED)
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_cf64n_cf64t_cf64t_tensor_op_f64_sm80.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_cf64n_cf64t_cf64t_tensor_op_f64_sm80.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_cf64n_cf64t_cf64t_tensor_op_f64_sm90.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_cf64n_cf64t_cf64t_tensor_op_f64_sm90.cu`

 * *Files 4% similar despite different names*

```diff
@@ -46,15 +46,15 @@
 #include "cutlass/util/reference/host/tensor_fill.h"
 #include "cutlass/util/tensor_view_io.h"
 
 #include "testbed_complex.h"
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
-#if defined(CUTLASS_ARCH_MMA_SM90_F64_MMA_ENABLED)
+#if defined(CUTLASS_ARCH_MMA_SM90_SUPPORTED)
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
 TEST(SM90_Device_Gemm_cf64n_cf64t_cf64t_tensor_op_f64, 32x32x16_16x16x16) {
 
   using Element = cutlass::complex<double>;
 
@@ -243,10 +243,10 @@
   >;
 
   EXPECT_TRUE(test::gemm::device::TestAllGemmComplex<Gemm>());
 }
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
-#endif // #if defined(CUTLASS_ARCH_MMA_SM90_F64_MMA_ENABLED)
+#endif // #if defined(CUTLASS_ARCH_MMA_SM90_SUPPORTED)
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_cf64t_cf64n_cf64t_tensor_op_f64_gaussian_sm80.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_cf64t_cf64n_cf64t_tensor_op_f64_gaussian_sm80.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_cf64t_cf64n_cf64t_tensor_op_f64_gaussian_sm90.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_cf64t_cf64n_cf64t_tensor_op_f64_gaussian_sm90.cu`

 * *Files 2% similar despite different names*

```diff
@@ -46,15 +46,15 @@
 #include "cutlass/util/reference/host/tensor_fill.h"
 #include "cutlass/util/tensor_view_io.h"
 
 #include "testbed_complex.h"
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
-#if defined(CUTLASS_ARCH_MMA_SM90_F64_MMA_ENABLED)
+#if defined(CUTLASS_ARCH_MMA_SM90_SUPPORTED)
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
 TEST(SM90_Device_Gemm_cf64t_cf64n_cf64t_tensor_op_f64_gaussian, 32x32x8_16x16x8) {
   
   using Element = cutlass::complex<double>;
 
@@ -187,11 +187,11 @@
 
   EXPECT_TRUE(test::gemm::device::TestAllGemmComplex<Gemm>());
 }
 
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
-#endif // #if defined(CUTLASS_ARCH_MMA_SM90_F64_MMA_ENABLED)
+#endif // #if defined(CUTLASS_ARCH_MMA_SM90_SUPPORTED)
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_cf64t_cf64n_cf64t_tensor_op_f64_sm80.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_cf64t_cf64n_cf64t_tensor_op_f64_sm80.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_cf64t_cf64n_cf64t_tensor_op_f64_sm90.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_cf64t_cf64n_cf64t_tensor_op_f64_sm90.cu`

 * *Files 1% similar despite different names*

```diff
@@ -46,15 +46,15 @@
 #include "cutlass/util/reference/host/tensor_fill.h"
 #include "cutlass/util/tensor_view_io.h"
 
 #include "testbed_complex.h"
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
-#if defined(CUTLASS_ARCH_MMA_SM90_F64_MMA_ENABLED)
+#if defined(CUTLASS_ARCH_MMA_SM90_SUPPORTED)
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
 TEST(SM90_Device_Gemm_cf64t_cf64n_cf64t_tensor_op_f64, 32x32x8_16x16x8) {
   
   using Element = cutlass::complex<double>;
 
@@ -295,11 +295,11 @@
   >;
 
   EXPECT_TRUE(test::gemm::device::TestAllGemmComplex<Gemm>());
 }
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
-#endif // #if defined(CUTLASS_ARCH_MMA_SM90_F64_MMA_ENABLED)
+#endif // #if defined(CUTLASS_ARCH_MMA_SM90_SUPPORTED)
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16n_f16n_f16n_direct_store_tensor_op_f32_sm80.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16n_f16n_f16n_direct_store_tensor_op_f32_sm80.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16n_f16n_f16n_wmma_tensor_op_f16_sm70.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16n_f16n_f16n_wmma_tensor_op_f16_sm70.cu`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16n_f16n_f16n_wmma_tensor_op_f32_sm70.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16n_f16n_f16n_wmma_tensor_op_f32_sm70.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16n_f16n_f16t_tensor_op_f32_sm75.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16n_f16n_f16t_tensor_op_f32_sm75.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16n_f16n_f16t_tensor_op_f32_sm80.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16n_f16n_f16t_tensor_op_f32_sm80.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16n_f16n_f16t_tensor_op_f32_sparse_sm80.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16n_f16n_f16t_tensor_op_f32_sparse_sm80.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16n_f16n_f16t_volta_tensor_op_f32_sm70.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16n_f16n_f16t_volta_tensor_op_f32_sm70.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16n_f16n_f16t_wmma_tensor_op_f16_sm70.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16n_f16n_f16t_wmma_tensor_op_f16_sm70.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16n_f16n_f16t_wmma_tensor_op_f32_sm70.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16n_f16n_f16t_wmma_tensor_op_f32_sm70.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16n_f16n_f32n_tensor_op_f32_sm75.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16n_f16n_f32n_tensor_op_f32_sm75.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16n_f16n_f32n_tensor_op_f32_sm80.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16n_f16n_f32n_tensor_op_f32_sm80.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16n_f16n_f32n_wmma_tensor_op_f32_sm70.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16n_f16n_f32n_wmma_tensor_op_f32_sm70.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16n_f16n_f32t_tensor_op_f32_sm75.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16n_f16n_f32t_tensor_op_f32_sm75.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16n_f16n_f32t_tensor_op_f32_sm80.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16n_f16n_f32t_tensor_op_f32_sm80.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16n_f16n_f32t_tensor_op_f32_sparse_sm80.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16n_f16n_f32t_tensor_op_f32_sparse_sm80.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16n_f16n_f32t_volta_tensor_op_f32_sm70.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16n_f16n_f32t_volta_tensor_op_f32_sm70.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16n_f16n_f32t_wmma_tensor_op_f32_sm70.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16n_f16n_f32t_wmma_tensor_op_f32_sm70.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16n_f16t_f16n_wmma_tensor_op_f16_sm70.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16n_f16t_f16n_wmma_tensor_op_f16_sm70.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16n_f16t_f16n_wmma_tensor_op_f32_sm70.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16n_f16t_f16n_wmma_tensor_op_f32_sm70.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16n_f16t_f16t_tensor_op_f16_slicedk_sm75.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16n_f16t_f16t_tensor_op_f16_slicedk_sm75.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16n_f16t_f16t_tensor_op_f16_slicedk_sm80.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16t_f16n_f16t_tensor_op_f16_slicedk_sm80.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -26,14 +26,15 @@
  * CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,
  * OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
  * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
  *
  **************************************************************************************************/
 /*! \file
     \brief Tests for device-wide GEMM interface
+
 */
 
 #include <iostream>
 
 #include "cutlass/cutlass.h"
 #include "cutlass/gemm/device/gemm.h"
 
@@ -48,24 +49,24 @@
 
 #include "testbed.h"
 
 #if defined(CUTLASS_ARCH_MMA_SM80_SUPPORTED)
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
-TEST(SM80_Device_Gemm_f16n_f16t_f16t_tensor_op_f16_sliced_k, 128x64x64_64x64x32) {
+TEST(SM80_Device_Gemm_f16t_f16n_f16t_tensor_op_f16_sliced_k, 128x64x64_64x64x32) {
 
   using ElementOutput = cutlass::half_t;
   using ElementAccumulator = cutlass::half_t;
 
   using Gemm = cutlass::gemm::device::Gemm<
     cutlass::half_t,
-    cutlass::layout::ColumnMajor,
-    cutlass::half_t,
     cutlass::layout::RowMajor,
+    cutlass::half_t,
+    cutlass::layout::ColumnMajor,
     ElementOutput,
     cutlass::layout::RowMajor,
     ElementAccumulator,
     cutlass::arch::OpClassTensorOp,
     cutlass::arch::Sm80,
     cutlass::gemm::GemmShape<128, 64, 64>,
     cutlass::gemm::GemmShape<64, 64, 32>,
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16n_f16t_f16t_tensor_op_f16_sm75.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16n_f16t_f16t_tensor_op_f16_sm75.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16n_f16t_f16t_tensor_op_f16_sm80.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16n_f16t_f16t_tensor_op_f16_sm80.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16n_f16t_f16t_tensor_op_f16_sparse_sm80.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16n_f16t_f16t_tensor_op_f16_sparse_sm80.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16n_f16t_f16t_tensor_op_f32_sm80.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16n_f16t_f16t_tensor_op_f32_sm80.cu`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16n_f16t_f16t_volta_tensor_op_f16_sm70.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16n_f16t_f16t_volta_tensor_op_f16_sm70.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16n_f16t_f16t_wmma_tensor_op_f16_sm70.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16n_f16t_f16t_wmma_tensor_op_f16_sm70.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16n_f16t_f16t_wmma_tensor_op_f32_sm70.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16n_f16t_f16t_wmma_tensor_op_f32_sm70.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16n_f16t_f32n_wmma_tensor_op_f32_sm70.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16n_f16t_f32n_wmma_tensor_op_f32_sm70.cu`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16n_f16t_f32t_tensor_op_f32_sm75.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16n_f16t_f32t_tensor_op_f32_sm75.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16n_f16t_f32t_tensor_op_f32_sm80.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16t_f16n_f32t_tensor_op_f32_sm80.cu`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -42,304 +42,303 @@
 #include "cutlass/util/reference/host/tensor_compare.h"
 #include "cutlass/util/reference/host/tensor_copy.h"
 #include "cutlass/util/reference/host/tensor_fill.h"
 #include "cutlass/util/tensor_view_io.h"
 
 #include "testbed.h"
 
-#if (CUTLASS_ARCH_MMA_SM80_SUPPORTED)
- 
+#if defined(CUTLASS_ARCH_MMA_SM80_SUPPORTED)
+
 ////////////////////////////////////////////////////////////////////////////////
 
-CUTLASS_TEST_L1(SM80_Device_Gemm_f16n_f16t_f32t_tensor_op_f32, 128x256x64_64x64x64, {
+TEST(SM80_Device_Gemm_f16t_f16n_f32t_tensor_op_f32, 128x256x64_64x64x64) {
   using ElementOutput = float;
   using ElementAccumulator = float;
 
   using Gemm = cutlass::gemm::device::Gemm<
-      cutlass::half_t, cutlass::layout::ColumnMajor, cutlass::half_t,
-      cutlass::layout::RowMajor, ElementOutput, cutlass::layout::RowMajor,
+      cutlass::half_t, cutlass::layout::RowMajor, cutlass::half_t,
+      cutlass::layout::ColumnMajor, ElementOutput, cutlass::layout::RowMajor,
       ElementAccumulator, cutlass::arch::OpClassTensorOp, cutlass::arch::Sm80,
       cutlass::gemm::GemmShape<128, 256, 64>,
       cutlass::gemm::GemmShape<64, 64, 64>, cutlass::gemm::GemmShape<16, 8, 16>,
       cutlass::epilogue::thread::LinearCombination<
           ElementOutput, 128 / cutlass::sizeof_bits<ElementOutput>::value,
           ElementAccumulator, ElementAccumulator>,
       cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>, 3>;
 
   EXPECT_TRUE(test::gemm::device::TestAllGemm<Gemm>());
-} )
+}
 
-CUTLASS_TEST_L1(SM80_Device_Gemm_f16n_f16t_f32t_tensor_op_f32, 256x128x64_64x64x64, {
+TEST(SM80_Device_Gemm_f16t_f16n_f32t_tensor_op_f32, 256x128x64_64x64x64) {
   using ElementOutput = float;
   using ElementAccumulator = float;
 
   using Gemm = cutlass::gemm::device::Gemm<
-      cutlass::half_t, cutlass::layout::ColumnMajor, cutlass::half_t,
-      cutlass::layout::RowMajor, ElementOutput, cutlass::layout::RowMajor,
+      cutlass::half_t, cutlass::layout::RowMajor, cutlass::half_t,
+      cutlass::layout::ColumnMajor, ElementOutput, cutlass::layout::RowMajor,
       ElementAccumulator, cutlass::arch::OpClassTensorOp, cutlass::arch::Sm80,
       cutlass::gemm::GemmShape<256, 128, 64>,
       cutlass::gemm::GemmShape<64, 64, 64>, cutlass::gemm::GemmShape<16, 8, 16>,
       cutlass::epilogue::thread::LinearCombination<
           ElementOutput, 128 / cutlass::sizeof_bits<ElementOutput>::value,
           ElementAccumulator, ElementAccumulator>,
       cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>, 3>;
 
   EXPECT_TRUE(test::gemm::device::TestAllGemm<Gemm>());
-} )
+}
 
-CUTLASS_TEST_L1(SM80_Device_Gemm_f16n_f16t_f32t_tensor_op_f32, 128x128x64_64x64x64, {
+TEST(SM80_Device_Gemm_f16t_f16n_f32t_tensor_op_f32, 128x128x64_64x64x64) {
   using ElementOutput = float;
   using ElementAccumulator = float;
 
   using Gemm = cutlass::gemm::device::Gemm<
-      cutlass::half_t, cutlass::layout::ColumnMajor, cutlass::half_t,
-      cutlass::layout::RowMajor, ElementOutput, cutlass::layout::RowMajor,
+      cutlass::half_t, cutlass::layout::RowMajor, cutlass::half_t,
+      cutlass::layout::ColumnMajor, ElementOutput, cutlass::layout::RowMajor,
       ElementAccumulator, cutlass::arch::OpClassTensorOp, cutlass::arch::Sm80,
       cutlass::gemm::GemmShape<128, 128, 64>,
       cutlass::gemm::GemmShape<64, 64, 64>, cutlass::gemm::GemmShape<16, 8, 16>,
       cutlass::epilogue::thread::LinearCombination<
           ElementOutput, 128 / cutlass::sizeof_bits<ElementOutput>::value,
           ElementAccumulator, ElementAccumulator>,
       cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>, 3>;
 
   EXPECT_TRUE(test::gemm::device::TestAllGemm<Gemm>());
-} )
+}
 
-CUTLASS_TEST_L1(SM80_Device_Gemm_f16n_f16t_f32t_tensor_op_f32, 256x64x64_64x64x64, {
+TEST(SM80_Device_Gemm_f16t_f16n_f32t_tensor_op_f32, 256x64x64_64x64x64) {
   using ElementOutput = float;
   using ElementAccumulator = float;
 
   using Gemm = cutlass::gemm::device::Gemm<
-      cutlass::half_t, cutlass::layout::ColumnMajor, cutlass::half_t,
-      cutlass::layout::RowMajor, ElementOutput, cutlass::layout::RowMajor,
+      cutlass::half_t, cutlass::layout::RowMajor, cutlass::half_t,
+      cutlass::layout::ColumnMajor, ElementOutput, cutlass::layout::RowMajor,
       ElementAccumulator, cutlass::arch::OpClassTensorOp, cutlass::arch::Sm80,
       cutlass::gemm::GemmShape<256, 64, 64>,
       cutlass::gemm::GemmShape<64, 64, 64>, cutlass::gemm::GemmShape<16, 8, 16>,
       cutlass::epilogue::thread::LinearCombination<
           ElementOutput, 128 / cutlass::sizeof_bits<ElementOutput>::value,
           ElementAccumulator, ElementAccumulator>,
       cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>, 3>;
 
   EXPECT_TRUE(test::gemm::device::TestAllGemm<Gemm>());
-} )
+}
 
-CUTLASS_TEST_L1(SM80_Device_Gemm_f16n_f16t_f32t_tensor_op_f32, 64x256x64_64x64x64, {
+TEST(SM80_Device_Gemm_f16t_f16n_f32t_tensor_op_f32, 64x256x64_64x64x64) {
   using ElementOutput = float;
   using ElementAccumulator = float;
 
   using Gemm = cutlass::gemm::device::Gemm<
-      cutlass::half_t, cutlass::layout::ColumnMajor, cutlass::half_t,
-      cutlass::layout::RowMajor, ElementOutput, cutlass::layout::RowMajor,
+      cutlass::half_t, cutlass::layout::RowMajor, cutlass::half_t,
+      cutlass::layout::ColumnMajor, ElementOutput, cutlass::layout::RowMajor,
       ElementAccumulator, cutlass::arch::OpClassTensorOp, cutlass::arch::Sm80,
       cutlass::gemm::GemmShape<64, 256, 64>,
       cutlass::gemm::GemmShape<64, 64, 64>, cutlass::gemm::GemmShape<16, 8, 16>,
       cutlass::epilogue::thread::LinearCombination<
           ElementOutput, 128 / cutlass::sizeof_bits<ElementOutput>::value,
           ElementAccumulator, ElementAccumulator>,
       cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>, 3>;
 
   EXPECT_TRUE(test::gemm::device::TestAllGemm<Gemm>());
-} )
+}
 
-CUTLASS_TEST_L1(SM80_Device_Gemm_f16n_f16t_f32t_tensor_op_f32, 64x128x64_32x64x64, {
+TEST(SM80_Device_Gemm_f16t_f16n_f32t_tensor_op_f32, 64x128x64_32x64x64) {
   using ElementOutput = float;
   using ElementAccumulator = float;
 
   using Gemm = cutlass::gemm::device::Gemm<
-      cutlass::half_t, cutlass::layout::ColumnMajor, cutlass::half_t,
-      cutlass::layout::RowMajor, ElementOutput, cutlass::layout::RowMajor,
+      cutlass::half_t, cutlass::layout::RowMajor, cutlass::half_t,
+      cutlass::layout::ColumnMajor, ElementOutput, cutlass::layout::RowMajor,
       ElementAccumulator, cutlass::arch::OpClassTensorOp, cutlass::arch::Sm80,
       cutlass::gemm::GemmShape<64, 128, 64>,
       cutlass::gemm::GemmShape<32, 64, 64>, cutlass::gemm::GemmShape<16, 8, 16>,
       cutlass::epilogue::thread::LinearCombination<
           ElementOutput, 128 / cutlass::sizeof_bits<ElementOutput>::value,
           ElementAccumulator, ElementAccumulator>,
       cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>, 4>;
 
   EXPECT_TRUE(test::gemm::device::TestAllGemm<Gemm>());
-} )
+}
 
-CUTLASS_TEST_L1(SM80_Device_Gemm_f16n_f16t_f32t_tensor_op_f32, 128x64x64_64x32x64, {
+TEST(SM80_Device_Gemm_f16t_f16n_f32t_tensor_op_f32, 128x64x64_64x32x64) {
   using ElementOutput = float;
   using ElementAccumulator = float;
 
   using Gemm = cutlass::gemm::device::Gemm<
-      cutlass::half_t, cutlass::layout::ColumnMajor, cutlass::half_t,
-      cutlass::layout::RowMajor, ElementOutput, cutlass::layout::RowMajor,
+      cutlass::half_t, cutlass::layout::RowMajor, cutlass::half_t,
+      cutlass::layout::ColumnMajor, ElementOutput, cutlass::layout::RowMajor,
       ElementAccumulator, cutlass::arch::OpClassTensorOp, cutlass::arch::Sm80,
       cutlass::gemm::GemmShape<128, 64, 64>,
       cutlass::gemm::GemmShape<64, 32, 64>, cutlass::gemm::GemmShape<16, 8, 16>,
       cutlass::epilogue::thread::LinearCombination<
           ElementOutput, 128 / cutlass::sizeof_bits<ElementOutput>::value,
           ElementAccumulator, ElementAccumulator>,
       cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>, 4>;
 
   EXPECT_TRUE(test::gemm::device::TestAllGemm<Gemm>());
-} )
+}
 
-CUTLASS_TEST_L1(SM80_Device_Gemm_f16n_f16t_f32t_tensor_op_f32, 64x64x64_32x32x64, {
+TEST(SM80_Device_Gemm_f16t_f16n_f32t_tensor_op_f32, 64x64x64_32x32x64) {
   using ElementOutput = float;
   using ElementAccumulator = float;
 
   using Gemm = cutlass::gemm::device::Gemm<
-      cutlass::half_t, cutlass::layout::ColumnMajor, cutlass::half_t,
-      cutlass::layout::RowMajor, ElementOutput, cutlass::layout::RowMajor,
+      cutlass::half_t, cutlass::layout::RowMajor, cutlass::half_t,
+      cutlass::layout::ColumnMajor, ElementOutput, cutlass::layout::RowMajor,
       ElementAccumulator, cutlass::arch::OpClassTensorOp, cutlass::arch::Sm80,
       cutlass::gemm::GemmShape<64, 64, 64>,
       cutlass::gemm::GemmShape<32, 32, 64>, cutlass::gemm::GemmShape<16, 8, 16>,
       cutlass::epilogue::thread::LinearCombination<
           ElementOutput, 128 / cutlass::sizeof_bits<ElementOutput>::value,
           ElementAccumulator, ElementAccumulator>,
       cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>, 6>;
 
   EXPECT_TRUE(test::gemm::device::TestAllGemm<Gemm>());
-} )
+}
 
-CUTLASS_TEST_L1(SM80_Device_Gemm_f16n_f16t_f32t_tensor_op_f32, 128x256x32_64x64x32, {
+TEST(SM80_Device_Gemm_f16t_f16n_f32t_tensor_op_f32, 128x256x32_64x64x32) {
   using ElementOutput = float;
   using ElementAccumulator = float;
 
   using Gemm = cutlass::gemm::device::Gemm<
-      cutlass::half_t, cutlass::layout::ColumnMajor, cutlass::half_t,
-      cutlass::layout::RowMajor, ElementOutput, cutlass::layout::RowMajor,
+      cutlass::half_t, cutlass::layout::RowMajor, cutlass::half_t,
+      cutlass::layout::ColumnMajor, ElementOutput, cutlass::layout::RowMajor,
       ElementAccumulator, cutlass::arch::OpClassTensorOp, cutlass::arch::Sm80,
       cutlass::gemm::GemmShape<128, 256, 32>,
       cutlass::gemm::GemmShape<64, 64, 32>, cutlass::gemm::GemmShape<16, 8, 16>,
       cutlass::epilogue::thread::LinearCombination<
           ElementOutput, 128 / cutlass::sizeof_bits<ElementOutput>::value,
           ElementAccumulator, ElementAccumulator>,
       cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>, 3>;
 
   EXPECT_TRUE(test::gemm::device::TestAllGemm<Gemm>());
-} )
+}
 
-CUTLASS_TEST_L1(SM80_Device_Gemm_f16n_f16t_f32t_tensor_op_f32, 256x128x32_64x64x32, {
+TEST(SM80_Device_Gemm_f16t_f16n_f32t_tensor_op_f32, 256x128x32_64x64x32) {
   using ElementOutput = float;
   using ElementAccumulator = float;
 
   using Gemm = cutlass::gemm::device::Gemm<
-      cutlass::half_t, cutlass::layout::ColumnMajor, cutlass::half_t,
-      cutlass::layout::RowMajor, ElementOutput, cutlass::layout::RowMajor,
+      cutlass::half_t, cutlass::layout::RowMajor, cutlass::half_t,
+      cutlass::layout::ColumnMajor, ElementOutput, cutlass::layout::RowMajor,
       ElementAccumulator, cutlass::arch::OpClassTensorOp, cutlass::arch::Sm80,
       cutlass::gemm::GemmShape<256, 128, 32>,
       cutlass::gemm::GemmShape<64, 64, 32>, cutlass::gemm::GemmShape<16, 8, 16>,
       cutlass::epilogue::thread::LinearCombination<
           ElementOutput, 128 / cutlass::sizeof_bits<ElementOutput>::value,
           ElementAccumulator, ElementAccumulator>,
       cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>, 3>;
 
   EXPECT_TRUE(test::gemm::device::TestAllGemm<Gemm>());
-} )
+}
 
-CUTLASS_TEST_L1(SM80_Device_Gemm_f16n_f16t_f32t_tensor_op_f32, 128x128x32_64x64x32, {
+TEST(SM80_Device_Gemm_f16t_f16n_f32t_tensor_op_f32, 128x128x32_64x64x32) {
   using ElementOutput = float;
   using ElementAccumulator = float;
 
   using Gemm = cutlass::gemm::device::Gemm<
-      cutlass::half_t, cutlass::layout::ColumnMajor, cutlass::half_t,
-      cutlass::layout::RowMajor, ElementOutput, cutlass::layout::RowMajor,
+      cutlass::half_t, cutlass::layout::RowMajor, cutlass::half_t,
+      cutlass::layout::ColumnMajor, ElementOutput, cutlass::layout::RowMajor,
       ElementAccumulator, cutlass::arch::OpClassTensorOp, cutlass::arch::Sm80,
       cutlass::gemm::GemmShape<128, 128, 32>,
       cutlass::gemm::GemmShape<64, 64, 32>, cutlass::gemm::GemmShape<16, 8, 16>,
       cutlass::epilogue::thread::LinearCombination<
           ElementOutput, 128 / cutlass::sizeof_bits<ElementOutput>::value,
           ElementAccumulator, ElementAccumulator>,
       cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>, 4>;
 
   EXPECT_TRUE(test::gemm::device::TestAllGemm<Gemm>());
-} )
+}
 
-CUTLASS_TEST_L1(SM80_Device_Gemm_f16n_f16t_f32t_tensor_op_f32, 256x64x32_64x64x32, {
+TEST(SM80_Device_Gemm_f16t_f16n_f32t_tensor_op_f32, 256x64x32_64x64x32) {
   using ElementOutput = float;
   using ElementAccumulator = float;
 
   using Gemm = cutlass::gemm::device::Gemm<
-      cutlass::half_t, cutlass::layout::ColumnMajor, cutlass::half_t,
-      cutlass::layout::RowMajor, ElementOutput, cutlass::layout::RowMajor,
+      cutlass::half_t, cutlass::layout::RowMajor, cutlass::half_t,
+      cutlass::layout::ColumnMajor, ElementOutput, cutlass::layout::RowMajor,
       ElementAccumulator, cutlass::arch::OpClassTensorOp, cutlass::arch::Sm80,
       cutlass::gemm::GemmShape<256, 64, 32>,
       cutlass::gemm::GemmShape<64, 64, 32>, cutlass::gemm::GemmShape<16, 8, 16>,
       cutlass::epilogue::thread::LinearCombination<
           ElementOutput, 128 / cutlass::sizeof_bits<ElementOutput>::value,
           ElementAccumulator, ElementAccumulator>,
       cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>, 4>;
 
   EXPECT_TRUE(test::gemm::device::TestAllGemm<Gemm>());
-} )
+}
 
-CUTLASS_TEST_L1(SM80_Device_Gemm_f16n_f16t_f32t_tensor_op_f32, 64x256x32_64x64x32, {
+TEST(SM80_Device_Gemm_f16t_f16n_f32t_tensor_op_f32, 64x256x32_64x64x32) {
   using ElementOutput = float;
   using ElementAccumulator = float;
 
   using Gemm = cutlass::gemm::device::Gemm<
-      cutlass::half_t, cutlass::layout::ColumnMajor, cutlass::half_t,
-      cutlass::layout::RowMajor, ElementOutput, cutlass::layout::RowMajor,
+      cutlass::half_t, cutlass::layout::RowMajor, cutlass::half_t,
+      cutlass::layout::ColumnMajor, ElementOutput, cutlass::layout::RowMajor,
       ElementAccumulator, cutlass::arch::OpClassTensorOp, cutlass::arch::Sm80,
       cutlass::gemm::GemmShape<64, 256, 32>,
       cutlass::gemm::GemmShape<64, 64, 32>, cutlass::gemm::GemmShape<16, 8, 16>,
       cutlass::epilogue::thread::LinearCombination<
           ElementOutput, 128 / cutlass::sizeof_bits<ElementOutput>::value,
           ElementAccumulator, ElementAccumulator>,
       cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>, 4>;
 
   EXPECT_TRUE(test::gemm::device::TestAllGemm<Gemm>());
-} )
+}
 
-CUTLASS_TEST_L1(SM80_Device_Gemm_f16n_f16t_f32t_tensor_op_f32, 64x128x32_32x64x32, {
+TEST(SM80_Device_Gemm_f16t_f16n_f32t_tensor_op_f32, 64x128x32_32x64x32) {
   using ElementOutput = float;
   using ElementAccumulator = float;
 
   using Gemm = cutlass::gemm::device::Gemm<
-      cutlass::half_t, cutlass::layout::ColumnMajor, cutlass::half_t,
-      cutlass::layout::RowMajor, ElementOutput, cutlass::layout::RowMajor,
+      cutlass::half_t, cutlass::layout::RowMajor, cutlass::half_t,
+      cutlass::layout::ColumnMajor, ElementOutput, cutlass::layout::RowMajor,
       ElementAccumulator, cutlass::arch::OpClassTensorOp, cutlass::arch::Sm80,
       cutlass::gemm::GemmShape<64, 128, 32>,
       cutlass::gemm::GemmShape<32, 64, 32>, cutlass::gemm::GemmShape<16, 8, 16>,
       cutlass::epilogue::thread::LinearCombination<
           ElementOutput, 128 / cutlass::sizeof_bits<ElementOutput>::value,
           ElementAccumulator, ElementAccumulator>,
       cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>, 6>;
 
   EXPECT_TRUE(test::gemm::device::TestAllGemm<Gemm>());
-} )
+}
 
-CUTLASS_TEST_L1(SM80_Device_Gemm_f16n_f16t_f32t_tensor_op_f32, 128x64x32_64x32x32, {
+TEST(SM80_Device_Gemm_f16t_f16n_f32t_tensor_op_f32, 128x64x32_64x32x32) {
   using ElementOutput = float;
   using ElementAccumulator = float;
 
   using Gemm = cutlass::gemm::device::Gemm<
-      cutlass::half_t, cutlass::layout::ColumnMajor, cutlass::half_t,
-      cutlass::layout::RowMajor, ElementOutput, cutlass::layout::RowMajor,
+      cutlass::half_t, cutlass::layout::RowMajor, cutlass::half_t,
+      cutlass::layout::ColumnMajor, ElementOutput, cutlass::layout::RowMajor,
       ElementAccumulator, cutlass::arch::OpClassTensorOp, cutlass::arch::Sm80,
       cutlass::gemm::GemmShape<128, 64, 32>,
-      cutlass::gemm::GemmShape<64, 32, 32>,
-      cutlass::gemm::GemmShape<16, 8, 16>,
+      cutlass::gemm::GemmShape<64, 32, 32>, cutlass::gemm::GemmShape<16, 8, 16>,
       cutlass::epilogue::thread::LinearCombination<
           ElementOutput, 128 / cutlass::sizeof_bits<ElementOutput>::value,
           ElementAccumulator, ElementAccumulator>,
       cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>, 6>;
 
   EXPECT_TRUE(test::gemm::device::TestAllGemm<Gemm>());
-} )
+}
 
-CUTLASS_TEST_L1(SM80_Device_Gemm_f16n_f16t_f32t_tensor_op_f32, 64x64x32_32x32x32, {
+TEST(SM80_Device_Gemm_f16t_f16n_f32t_tensor_op_f32, 64x64x32_32x32x32) {
   using ElementOutput = float;
   using ElementAccumulator = float;
 
   using Gemm = cutlass::gemm::device::Gemm<
-      cutlass::half_t, cutlass::layout::ColumnMajor, cutlass::half_t,
-      cutlass::layout::RowMajor, ElementOutput, cutlass::layout::RowMajor,
+      cutlass::half_t, cutlass::layout::RowMajor, cutlass::half_t,
+      cutlass::layout::ColumnMajor, ElementOutput, cutlass::layout::RowMajor,
       ElementAccumulator, cutlass::arch::OpClassTensorOp, cutlass::arch::Sm80,
       cutlass::gemm::GemmShape<64, 64, 32>,
       cutlass::gemm::GemmShape<32, 32, 32>, cutlass::gemm::GemmShape<16, 8, 16>,
       cutlass::epilogue::thread::LinearCombination<
           ElementOutput, 128 / cutlass::sizeof_bits<ElementOutput>::value,
           ElementAccumulator, ElementAccumulator>,
       cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>, 10>;
 
   EXPECT_TRUE(test::gemm::device::TestAllGemm<Gemm>());
-} )
+}
 
 ////////////////////////////////////////////////////////////////////////////////
 
 #endif  // CUTLASS_ARCH_MMA_SM80_SUPPORTED
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16n_f16t_f32t_tensor_op_f32_sparse_sm80.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16n_f16t_f32t_tensor_op_f32_sparse_sm80.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16n_f16t_f32t_volta_tensor_op_f32_sm70.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16n_f16t_f32t_volta_tensor_op_f32_sm70.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16n_f16t_f32t_wmma_tensor_op_f32_sm70.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16n_f16t_f32t_wmma_tensor_op_f32_sm70.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16t_f16n_f16n_singlestage_wmma_tensor_op_f16_sm70.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16t_f16n_f16n_singlestage_wmma_tensor_op_f16_sm70.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16t_f16n_f16n_wmma_tensor_op_f16_sm70.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16t_f16n_f16n_wmma_tensor_op_f16_sm70.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16t_f16n_f16n_wmma_tensor_op_f32_sm70.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16t_f16n_f16n_wmma_tensor_op_f32_sm70.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16t_f16n_f16t_singlestage_wmma_tensor_op_f16_sm70.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16t_f16n_f16t_singlestage_wmma_tensor_op_f16_sm70.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16t_f16n_f16t_tensor_op_f16_broadcast_sm80.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16t_f16n_f16t_tensor_op_f16_broadcast_sm80.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16t_f16n_f16t_tensor_op_f16_slicedk_sm75.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16t_f16n_f16t_tensor_op_f16_slicedk_sm75.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16t_f16n_f16t_tensor_op_f16_slicedk_sm80.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16n_f16t_f16t_tensor_op_f16_slicedk_sm80.cu`

 * *Files 2% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -26,15 +26,14 @@
  * CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,
  * OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
  * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
  *
  **************************************************************************************************/
 /*! \file
     \brief Tests for device-wide GEMM interface
-
 */
 
 #include <iostream>
 
 #include "cutlass/cutlass.h"
 #include "cutlass/gemm/device/gemm.h"
 
@@ -49,24 +48,24 @@
 
 #include "testbed.h"
 
 #if defined(CUTLASS_ARCH_MMA_SM80_SUPPORTED)
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
-TEST(SM80_Device_Gemm_f16t_f16n_f16t_tensor_op_f16_sliced_k, 128x64x64_64x64x32) {
+TEST(SM80_Device_Gemm_f16n_f16t_f16t_tensor_op_f16_sliced_k, 128x64x64_64x64x32) {
 
   using ElementOutput = cutlass::half_t;
   using ElementAccumulator = cutlass::half_t;
 
   using Gemm = cutlass::gemm::device::Gemm<
     cutlass::half_t,
-    cutlass::layout::RowMajor,
-    cutlass::half_t,
     cutlass::layout::ColumnMajor,
+    cutlass::half_t,
+    cutlass::layout::RowMajor,
     ElementOutput,
     cutlass::layout::RowMajor,
     ElementAccumulator,
     cutlass::arch::OpClassTensorOp,
     cutlass::arch::Sm80,
     cutlass::gemm::GemmShape<128, 64, 64>,
     cutlass::gemm::GemmShape<64, 64, 32>,
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16t_f16n_f16t_tensor_op_f16_sm75.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16t_f16n_f16t_tensor_op_f16_sm75.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16t_f16n_f16t_tensor_op_f16_sm80.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16t_f16n_f16t_tensor_op_f16_sm80.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16t_f16n_f16t_tensor_op_f16_sparse_sm80.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16t_f16n_f16t_tensor_op_f16_sparse_sm80.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16t_f16n_f16t_volta_tensor_op_f16_sm70.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16t_f16n_f16t_volta_tensor_op_f16_sm70.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16t_f16n_f16t_wmma_tensor_op_f16_sm70.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16t_f16n_f16t_wmma_tensor_op_f16_sm70.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16t_f16n_f16t_wmma_tensor_op_f32_sm70.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16t_f16n_f16t_wmma_tensor_op_f32_sm70.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16t_f16n_f32n_wmma_tensor_op_f32_sm70.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16t_f16n_f32n_wmma_tensor_op_f32_sm70.cu`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16t_f16n_f32t_singlestage_wmma_tensor_op_f32_sm70.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16t_f16n_f32t_singlestage_wmma_tensor_op_f32_sm70.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16t_f16n_f32t_tensor_op_f32_sm75.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16t_f16n_f32t_tensor_op_f32_sm75.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16t_f16n_f32t_tensor_op_f32_sm80.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16n_f16t_f32t_tensor_op_f32_sm80.cu`

 * *Files 2% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -41,304 +41,344 @@
 #include "cutlass/util/reference/host/gemm.h"
 #include "cutlass/util/reference/host/tensor_compare.h"
 #include "cutlass/util/reference/host/tensor_copy.h"
 #include "cutlass/util/reference/host/tensor_fill.h"
 #include "cutlass/util/tensor_view_io.h"
 
 #include "testbed.h"
+#include "testbed_universal.h"
 
-#if defined(CUTLASS_ARCH_MMA_SM80_SUPPORTED)
-
+#if (CUTLASS_ARCH_MMA_SM80_SUPPORTED)
+ 
 ////////////////////////////////////////////////////////////////////////////////
 
-TEST(SM80_Device_Gemm_f16t_f16n_f32t_tensor_op_f32, 128x256x64_64x64x64) {
+CUTLASS_TEST_L1(SM80_Device_Gemm_f16n_f16t_f32t_tensor_op_f32, 128x256x64_64x64x64, {
   using ElementOutput = float;
   using ElementAccumulator = float;
 
   using Gemm = cutlass::gemm::device::Gemm<
-      cutlass::half_t, cutlass::layout::RowMajor, cutlass::half_t,
-      cutlass::layout::ColumnMajor, ElementOutput, cutlass::layout::RowMajor,
+      cutlass::half_t, cutlass::layout::ColumnMajor, cutlass::half_t,
+      cutlass::layout::RowMajor, ElementOutput, cutlass::layout::RowMajor,
       ElementAccumulator, cutlass::arch::OpClassTensorOp, cutlass::arch::Sm80,
       cutlass::gemm::GemmShape<128, 256, 64>,
       cutlass::gemm::GemmShape<64, 64, 64>, cutlass::gemm::GemmShape<16, 8, 16>,
       cutlass::epilogue::thread::LinearCombination<
           ElementOutput, 128 / cutlass::sizeof_bits<ElementOutput>::value,
           ElementAccumulator, ElementAccumulator>,
       cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>, 3>;
 
   EXPECT_TRUE(test::gemm::device::TestAllGemm<Gemm>());
-}
+} )
 
-TEST(SM80_Device_Gemm_f16t_f16n_f32t_tensor_op_f32, 256x128x64_64x64x64) {
+CUTLASS_TEST_L1(SM80_Device_Gemm_f16n_f16t_f32t_tensor_op_f32, 256x128x64_64x64x64, {
   using ElementOutput = float;
   using ElementAccumulator = float;
 
   using Gemm = cutlass::gemm::device::Gemm<
-      cutlass::half_t, cutlass::layout::RowMajor, cutlass::half_t,
-      cutlass::layout::ColumnMajor, ElementOutput, cutlass::layout::RowMajor,
+      cutlass::half_t, cutlass::layout::ColumnMajor, cutlass::half_t,
+      cutlass::layout::RowMajor, ElementOutput, cutlass::layout::RowMajor,
       ElementAccumulator, cutlass::arch::OpClassTensorOp, cutlass::arch::Sm80,
       cutlass::gemm::GemmShape<256, 128, 64>,
       cutlass::gemm::GemmShape<64, 64, 64>, cutlass::gemm::GemmShape<16, 8, 16>,
       cutlass::epilogue::thread::LinearCombination<
           ElementOutput, 128 / cutlass::sizeof_bits<ElementOutput>::value,
           ElementAccumulator, ElementAccumulator>,
       cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>, 3>;
 
   EXPECT_TRUE(test::gemm::device::TestAllGemm<Gemm>());
-}
+} )
 
-TEST(SM80_Device_Gemm_f16t_f16n_f32t_tensor_op_f32, 128x128x64_64x64x64) {
+CUTLASS_TEST_L1(SM80_Device_Gemm_f16n_f16t_f32t_tensor_op_f32, 128x128x64_64x64x64, {
   using ElementOutput = float;
   using ElementAccumulator = float;
 
   using Gemm = cutlass::gemm::device::Gemm<
-      cutlass::half_t, cutlass::layout::RowMajor, cutlass::half_t,
-      cutlass::layout::ColumnMajor, ElementOutput, cutlass::layout::RowMajor,
+      cutlass::half_t, cutlass::layout::ColumnMajor, cutlass::half_t,
+      cutlass::layout::RowMajor, ElementOutput, cutlass::layout::RowMajor,
       ElementAccumulator, cutlass::arch::OpClassTensorOp, cutlass::arch::Sm80,
       cutlass::gemm::GemmShape<128, 128, 64>,
       cutlass::gemm::GemmShape<64, 64, 64>, cutlass::gemm::GemmShape<16, 8, 16>,
       cutlass::epilogue::thread::LinearCombination<
           ElementOutput, 128 / cutlass::sizeof_bits<ElementOutput>::value,
           ElementAccumulator, ElementAccumulator>,
       cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>, 3>;
 
   EXPECT_TRUE(test::gemm::device::TestAllGemm<Gemm>());
-}
+} )
+
+CUTLASS_TEST_L1(SM80_Device_Gemm_f16n_f16t_f32t_tensor_op_f32, 128x128x64_64x64x64_sk, {
+  using ElementOutput = float;
+  using ElementAccumulator = float;
+
+  using Gemm = cutlass::gemm::device::GemmUniversal<
+      cutlass::half_t, cutlass::layout::ColumnMajor,
+      cutlass::half_t, cutlass::layout::RowMajor,
+      ElementOutput, cutlass::layout::RowMajor,
+      ElementAccumulator, cutlass::arch::OpClassTensorOp, cutlass::arch::Sm80,
+      cutlass::gemm::GemmShape<128, 128, 64>,
+      cutlass::gemm::GemmShape<64, 64, 64>, cutlass::gemm::GemmShape<16, 8, 16>,
+      cutlass::epilogue::thread::LinearCombination<
+          ElementOutput, 128 / cutlass::sizeof_bits<ElementOutput>::value,
+          ElementAccumulator, ElementAccumulator>,
+      cutlass::gemm::threadblock::ThreadblockSwizzleStreamK, 3>;
+
+  EXPECT_TRUE(test::gemm::device::TestAllGemmUniversal<Gemm>());
+} )
+
+CUTLASS_TEST_L1(SM80_Device_Gemm_f16n_f16t_f32n_tensor_op_f32, 128x128x64_64x64x64_sk, {
+  using ElementOutput = float;
+  using ElementAccumulator = float;
+
+  using Gemm = cutlass::gemm::device::GemmUniversal<
+      cutlass::half_t, cutlass::layout::ColumnMajor,
+      cutlass::half_t, cutlass::layout::RowMajor,
+      ElementOutput, cutlass::layout::ColumnMajor,
+      ElementAccumulator, cutlass::arch::OpClassTensorOp, cutlass::arch::Sm80,
+      cutlass::gemm::GemmShape<128, 128, 64>,
+      cutlass::gemm::GemmShape<64, 64, 64>, cutlass::gemm::GemmShape<16, 8, 16>,
+      cutlass::epilogue::thread::LinearCombination<
+          ElementOutput, 128 / cutlass::sizeof_bits<ElementOutput>::value,
+          ElementAccumulator, ElementAccumulator>,
+      cutlass::gemm::threadblock::ThreadblockSwizzleStreamK, 3>;
+
+  EXPECT_TRUE(test::gemm::device::TestAllGemmUniversal<Gemm>());
+} )
 
-TEST(SM80_Device_Gemm_f16t_f16n_f32t_tensor_op_f32, 256x64x64_64x64x64) {
+CUTLASS_TEST_L1(SM80_Device_Gemm_f16n_f16t_f32t_tensor_op_f32, 256x64x64_64x64x64, {
   using ElementOutput = float;
   using ElementAccumulator = float;
 
   using Gemm = cutlass::gemm::device::Gemm<
-      cutlass::half_t, cutlass::layout::RowMajor, cutlass::half_t,
-      cutlass::layout::ColumnMajor, ElementOutput, cutlass::layout::RowMajor,
+      cutlass::half_t, cutlass::layout::ColumnMajor, cutlass::half_t,
+      cutlass::layout::RowMajor, ElementOutput, cutlass::layout::RowMajor,
       ElementAccumulator, cutlass::arch::OpClassTensorOp, cutlass::arch::Sm80,
       cutlass::gemm::GemmShape<256, 64, 64>,
       cutlass::gemm::GemmShape<64, 64, 64>, cutlass::gemm::GemmShape<16, 8, 16>,
       cutlass::epilogue::thread::LinearCombination<
           ElementOutput, 128 / cutlass::sizeof_bits<ElementOutput>::value,
           ElementAccumulator, ElementAccumulator>,
       cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>, 3>;
 
   EXPECT_TRUE(test::gemm::device::TestAllGemm<Gemm>());
-}
+} )
 
-TEST(SM80_Device_Gemm_f16t_f16n_f32t_tensor_op_f32, 64x256x64_64x64x64) {
+CUTLASS_TEST_L1(SM80_Device_Gemm_f16n_f16t_f32t_tensor_op_f32, 64x256x64_64x64x64, {
   using ElementOutput = float;
   using ElementAccumulator = float;
 
   using Gemm = cutlass::gemm::device::Gemm<
-      cutlass::half_t, cutlass::layout::RowMajor, cutlass::half_t,
-      cutlass::layout::ColumnMajor, ElementOutput, cutlass::layout::RowMajor,
+      cutlass::half_t, cutlass::layout::ColumnMajor, cutlass::half_t,
+      cutlass::layout::RowMajor, ElementOutput, cutlass::layout::RowMajor,
       ElementAccumulator, cutlass::arch::OpClassTensorOp, cutlass::arch::Sm80,
       cutlass::gemm::GemmShape<64, 256, 64>,
       cutlass::gemm::GemmShape<64, 64, 64>, cutlass::gemm::GemmShape<16, 8, 16>,
       cutlass::epilogue::thread::LinearCombination<
           ElementOutput, 128 / cutlass::sizeof_bits<ElementOutput>::value,
           ElementAccumulator, ElementAccumulator>,
       cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>, 3>;
 
   EXPECT_TRUE(test::gemm::device::TestAllGemm<Gemm>());
-}
+} )
 
-TEST(SM80_Device_Gemm_f16t_f16n_f32t_tensor_op_f32, 64x128x64_32x64x64) {
+CUTLASS_TEST_L1(SM80_Device_Gemm_f16n_f16t_f32t_tensor_op_f32, 64x128x64_32x64x64, {
   using ElementOutput = float;
   using ElementAccumulator = float;
 
   using Gemm = cutlass::gemm::device::Gemm<
-      cutlass::half_t, cutlass::layout::RowMajor, cutlass::half_t,
-      cutlass::layout::ColumnMajor, ElementOutput, cutlass::layout::RowMajor,
+      cutlass::half_t, cutlass::layout::ColumnMajor, cutlass::half_t,
+      cutlass::layout::RowMajor, ElementOutput, cutlass::layout::RowMajor,
       ElementAccumulator, cutlass::arch::OpClassTensorOp, cutlass::arch::Sm80,
       cutlass::gemm::GemmShape<64, 128, 64>,
       cutlass::gemm::GemmShape<32, 64, 64>, cutlass::gemm::GemmShape<16, 8, 16>,
       cutlass::epilogue::thread::LinearCombination<
           ElementOutput, 128 / cutlass::sizeof_bits<ElementOutput>::value,
           ElementAccumulator, ElementAccumulator>,
       cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>, 4>;
 
   EXPECT_TRUE(test::gemm::device::TestAllGemm<Gemm>());
-}
+} )
 
-TEST(SM80_Device_Gemm_f16t_f16n_f32t_tensor_op_f32, 128x64x64_64x32x64) {
+CUTLASS_TEST_L1(SM80_Device_Gemm_f16n_f16t_f32t_tensor_op_f32, 128x64x64_64x32x64, {
   using ElementOutput = float;
   using ElementAccumulator = float;
 
   using Gemm = cutlass::gemm::device::Gemm<
-      cutlass::half_t, cutlass::layout::RowMajor, cutlass::half_t,
-      cutlass::layout::ColumnMajor, ElementOutput, cutlass::layout::RowMajor,
+      cutlass::half_t, cutlass::layout::ColumnMajor, cutlass::half_t,
+      cutlass::layout::RowMajor, ElementOutput, cutlass::layout::RowMajor,
       ElementAccumulator, cutlass::arch::OpClassTensorOp, cutlass::arch::Sm80,
       cutlass::gemm::GemmShape<128, 64, 64>,
       cutlass::gemm::GemmShape<64, 32, 64>, cutlass::gemm::GemmShape<16, 8, 16>,
       cutlass::epilogue::thread::LinearCombination<
           ElementOutput, 128 / cutlass::sizeof_bits<ElementOutput>::value,
           ElementAccumulator, ElementAccumulator>,
       cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>, 4>;
 
   EXPECT_TRUE(test::gemm::device::TestAllGemm<Gemm>());
-}
+} )
 
-TEST(SM80_Device_Gemm_f16t_f16n_f32t_tensor_op_f32, 64x64x64_32x32x64) {
+CUTLASS_TEST_L1(SM80_Device_Gemm_f16n_f16t_f32t_tensor_op_f32, 64x64x64_32x32x64, {
   using ElementOutput = float;
   using ElementAccumulator = float;
 
   using Gemm = cutlass::gemm::device::Gemm<
-      cutlass::half_t, cutlass::layout::RowMajor, cutlass::half_t,
-      cutlass::layout::ColumnMajor, ElementOutput, cutlass::layout::RowMajor,
+      cutlass::half_t, cutlass::layout::ColumnMajor, cutlass::half_t,
+      cutlass::layout::RowMajor, ElementOutput, cutlass::layout::RowMajor,
       ElementAccumulator, cutlass::arch::OpClassTensorOp, cutlass::arch::Sm80,
       cutlass::gemm::GemmShape<64, 64, 64>,
       cutlass::gemm::GemmShape<32, 32, 64>, cutlass::gemm::GemmShape<16, 8, 16>,
       cutlass::epilogue::thread::LinearCombination<
           ElementOutput, 128 / cutlass::sizeof_bits<ElementOutput>::value,
           ElementAccumulator, ElementAccumulator>,
       cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>, 6>;
 
   EXPECT_TRUE(test::gemm::device::TestAllGemm<Gemm>());
-}
+} )
 
-TEST(SM80_Device_Gemm_f16t_f16n_f32t_tensor_op_f32, 128x256x32_64x64x32) {
+CUTLASS_TEST_L1(SM80_Device_Gemm_f16n_f16t_f32t_tensor_op_f32, 128x256x32_64x64x32, {
   using ElementOutput = float;
   using ElementAccumulator = float;
 
   using Gemm = cutlass::gemm::device::Gemm<
-      cutlass::half_t, cutlass::layout::RowMajor, cutlass::half_t,
-      cutlass::layout::ColumnMajor, ElementOutput, cutlass::layout::RowMajor,
+      cutlass::half_t, cutlass::layout::ColumnMajor, cutlass::half_t,
+      cutlass::layout::RowMajor, ElementOutput, cutlass::layout::RowMajor,
       ElementAccumulator, cutlass::arch::OpClassTensorOp, cutlass::arch::Sm80,
       cutlass::gemm::GemmShape<128, 256, 32>,
       cutlass::gemm::GemmShape<64, 64, 32>, cutlass::gemm::GemmShape<16, 8, 16>,
       cutlass::epilogue::thread::LinearCombination<
           ElementOutput, 128 / cutlass::sizeof_bits<ElementOutput>::value,
           ElementAccumulator, ElementAccumulator>,
       cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>, 3>;
 
   EXPECT_TRUE(test::gemm::device::TestAllGemm<Gemm>());
-}
+} )
 
-TEST(SM80_Device_Gemm_f16t_f16n_f32t_tensor_op_f32, 256x128x32_64x64x32) {
+CUTLASS_TEST_L1(SM80_Device_Gemm_f16n_f16t_f32t_tensor_op_f32, 256x128x32_64x64x32, {
   using ElementOutput = float;
   using ElementAccumulator = float;
 
   using Gemm = cutlass::gemm::device::Gemm<
-      cutlass::half_t, cutlass::layout::RowMajor, cutlass::half_t,
-      cutlass::layout::ColumnMajor, ElementOutput, cutlass::layout::RowMajor,
+      cutlass::half_t, cutlass::layout::ColumnMajor, cutlass::half_t,
+      cutlass::layout::RowMajor, ElementOutput, cutlass::layout::RowMajor,
       ElementAccumulator, cutlass::arch::OpClassTensorOp, cutlass::arch::Sm80,
       cutlass::gemm::GemmShape<256, 128, 32>,
       cutlass::gemm::GemmShape<64, 64, 32>, cutlass::gemm::GemmShape<16, 8, 16>,
       cutlass::epilogue::thread::LinearCombination<
           ElementOutput, 128 / cutlass::sizeof_bits<ElementOutput>::value,
           ElementAccumulator, ElementAccumulator>,
       cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>, 3>;
 
   EXPECT_TRUE(test::gemm::device::TestAllGemm<Gemm>());
-}
+} )
 
-TEST(SM80_Device_Gemm_f16t_f16n_f32t_tensor_op_f32, 128x128x32_64x64x32) {
+CUTLASS_TEST_L1(SM80_Device_Gemm_f16n_f16t_f32t_tensor_op_f32, 128x128x32_64x64x32, {
   using ElementOutput = float;
   using ElementAccumulator = float;
 
   using Gemm = cutlass::gemm::device::Gemm<
-      cutlass::half_t, cutlass::layout::RowMajor, cutlass::half_t,
-      cutlass::layout::ColumnMajor, ElementOutput, cutlass::layout::RowMajor,
+      cutlass::half_t, cutlass::layout::ColumnMajor, cutlass::half_t,
+      cutlass::layout::RowMajor, ElementOutput, cutlass::layout::RowMajor,
       ElementAccumulator, cutlass::arch::OpClassTensorOp, cutlass::arch::Sm80,
       cutlass::gemm::GemmShape<128, 128, 32>,
       cutlass::gemm::GemmShape<64, 64, 32>, cutlass::gemm::GemmShape<16, 8, 16>,
       cutlass::epilogue::thread::LinearCombination<
           ElementOutput, 128 / cutlass::sizeof_bits<ElementOutput>::value,
           ElementAccumulator, ElementAccumulator>,
       cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>, 4>;
 
   EXPECT_TRUE(test::gemm::device::TestAllGemm<Gemm>());
-}
+} )
 
-TEST(SM80_Device_Gemm_f16t_f16n_f32t_tensor_op_f32, 256x64x32_64x64x32) {
+CUTLASS_TEST_L1(SM80_Device_Gemm_f16n_f16t_f32t_tensor_op_f32, 256x64x32_64x64x32, {
   using ElementOutput = float;
   using ElementAccumulator = float;
 
   using Gemm = cutlass::gemm::device::Gemm<
-      cutlass::half_t, cutlass::layout::RowMajor, cutlass::half_t,
-      cutlass::layout::ColumnMajor, ElementOutput, cutlass::layout::RowMajor,
+      cutlass::half_t, cutlass::layout::ColumnMajor, cutlass::half_t,
+      cutlass::layout::RowMajor, ElementOutput, cutlass::layout::RowMajor,
       ElementAccumulator, cutlass::arch::OpClassTensorOp, cutlass::arch::Sm80,
       cutlass::gemm::GemmShape<256, 64, 32>,
       cutlass::gemm::GemmShape<64, 64, 32>, cutlass::gemm::GemmShape<16, 8, 16>,
       cutlass::epilogue::thread::LinearCombination<
           ElementOutput, 128 / cutlass::sizeof_bits<ElementOutput>::value,
           ElementAccumulator, ElementAccumulator>,
       cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>, 4>;
 
   EXPECT_TRUE(test::gemm::device::TestAllGemm<Gemm>());
-}
+} )
 
-TEST(SM80_Device_Gemm_f16t_f16n_f32t_tensor_op_f32, 64x256x32_64x64x32) {
+CUTLASS_TEST_L1(SM80_Device_Gemm_f16n_f16t_f32t_tensor_op_f32, 64x256x32_64x64x32, {
   using ElementOutput = float;
   using ElementAccumulator = float;
 
   using Gemm = cutlass::gemm::device::Gemm<
-      cutlass::half_t, cutlass::layout::RowMajor, cutlass::half_t,
-      cutlass::layout::ColumnMajor, ElementOutput, cutlass::layout::RowMajor,
+      cutlass::half_t, cutlass::layout::ColumnMajor, cutlass::half_t,
+      cutlass::layout::RowMajor, ElementOutput, cutlass::layout::RowMajor,
       ElementAccumulator, cutlass::arch::OpClassTensorOp, cutlass::arch::Sm80,
       cutlass::gemm::GemmShape<64, 256, 32>,
       cutlass::gemm::GemmShape<64, 64, 32>, cutlass::gemm::GemmShape<16, 8, 16>,
       cutlass::epilogue::thread::LinearCombination<
           ElementOutput, 128 / cutlass::sizeof_bits<ElementOutput>::value,
           ElementAccumulator, ElementAccumulator>,
       cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>, 4>;
 
   EXPECT_TRUE(test::gemm::device::TestAllGemm<Gemm>());
-}
+} )
 
-TEST(SM80_Device_Gemm_f16t_f16n_f32t_tensor_op_f32, 64x128x32_32x64x32) {
+CUTLASS_TEST_L1(SM80_Device_Gemm_f16n_f16t_f32t_tensor_op_f32, 64x128x32_32x64x32, {
   using ElementOutput = float;
   using ElementAccumulator = float;
 
   using Gemm = cutlass::gemm::device::Gemm<
-      cutlass::half_t, cutlass::layout::RowMajor, cutlass::half_t,
-      cutlass::layout::ColumnMajor, ElementOutput, cutlass::layout::RowMajor,
+      cutlass::half_t, cutlass::layout::ColumnMajor, cutlass::half_t,
+      cutlass::layout::RowMajor, ElementOutput, cutlass::layout::RowMajor,
       ElementAccumulator, cutlass::arch::OpClassTensorOp, cutlass::arch::Sm80,
       cutlass::gemm::GemmShape<64, 128, 32>,
       cutlass::gemm::GemmShape<32, 64, 32>, cutlass::gemm::GemmShape<16, 8, 16>,
       cutlass::epilogue::thread::LinearCombination<
           ElementOutput, 128 / cutlass::sizeof_bits<ElementOutput>::value,
           ElementAccumulator, ElementAccumulator>,
       cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>, 6>;
 
   EXPECT_TRUE(test::gemm::device::TestAllGemm<Gemm>());
-}
+} )
 
-TEST(SM80_Device_Gemm_f16t_f16n_f32t_tensor_op_f32, 128x64x32_64x32x32) {
+CUTLASS_TEST_L1(SM80_Device_Gemm_f16n_f16t_f32t_tensor_op_f32, 128x64x32_64x32x32, {
   using ElementOutput = float;
   using ElementAccumulator = float;
 
   using Gemm = cutlass::gemm::device::Gemm<
-      cutlass::half_t, cutlass::layout::RowMajor, cutlass::half_t,
-      cutlass::layout::ColumnMajor, ElementOutput, cutlass::layout::RowMajor,
+      cutlass::half_t, cutlass::layout::ColumnMajor, cutlass::half_t,
+      cutlass::layout::RowMajor, ElementOutput, cutlass::layout::RowMajor,
       ElementAccumulator, cutlass::arch::OpClassTensorOp, cutlass::arch::Sm80,
       cutlass::gemm::GemmShape<128, 64, 32>,
-      cutlass::gemm::GemmShape<64, 32, 32>, cutlass::gemm::GemmShape<16, 8, 16>,
+      cutlass::gemm::GemmShape<64, 32, 32>,
+      cutlass::gemm::GemmShape<16, 8, 16>,
       cutlass::epilogue::thread::LinearCombination<
           ElementOutput, 128 / cutlass::sizeof_bits<ElementOutput>::value,
           ElementAccumulator, ElementAccumulator>,
       cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>, 6>;
 
   EXPECT_TRUE(test::gemm::device::TestAllGemm<Gemm>());
-}
+} )
 
-TEST(SM80_Device_Gemm_f16t_f16n_f32t_tensor_op_f32, 64x64x32_32x32x32) {
+CUTLASS_TEST_L1(SM80_Device_Gemm_f16n_f16t_f32t_tensor_op_f32, 64x64x32_32x32x32, {
   using ElementOutput = float;
   using ElementAccumulator = float;
 
   using Gemm = cutlass::gemm::device::Gemm<
-      cutlass::half_t, cutlass::layout::RowMajor, cutlass::half_t,
-      cutlass::layout::ColumnMajor, ElementOutput, cutlass::layout::RowMajor,
+      cutlass::half_t, cutlass::layout::ColumnMajor, cutlass::half_t,
+      cutlass::layout::RowMajor, ElementOutput, cutlass::layout::RowMajor,
       ElementAccumulator, cutlass::arch::OpClassTensorOp, cutlass::arch::Sm80,
       cutlass::gemm::GemmShape<64, 64, 32>,
       cutlass::gemm::GemmShape<32, 32, 32>, cutlass::gemm::GemmShape<16, 8, 16>,
       cutlass::epilogue::thread::LinearCombination<
           ElementOutput, 128 / cutlass::sizeof_bits<ElementOutput>::value,
           ElementAccumulator, ElementAccumulator>,
       cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>, 10>;
 
   EXPECT_TRUE(test::gemm::device::TestAllGemm<Gemm>());
-}
+} )
 
 ////////////////////////////////////////////////////////////////////////////////
 
 #endif  // CUTLASS_ARCH_MMA_SM80_SUPPORTED
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16t_f16n_f32t_tensor_op_f32_sparse_sm80.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16t_f16n_f32t_tensor_op_f32_sparse_sm80.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16t_f16n_f32t_volta_tensor_op_f32_sm70.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16t_f16n_f32t_volta_tensor_op_f32_sm70.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16t_f16n_f32t_wmma_tensor_op_f32_sm70.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16t_f16n_f32t_wmma_tensor_op_f32_sm70.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16t_f16t_f16n_wmma_tensor_op_f16_sm70.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16t_f16t_f16n_wmma_tensor_op_f16_sm70.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16t_f16t_f16n_wmma_tensor_op_f32_sm70.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16t_f16t_f16n_wmma_tensor_op_f32_sm70.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16t_f16t_f16t_wmma_tensor_op_f16_sm70.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16t_f16t_f16t_wmma_tensor_op_f16_sm70.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16t_f16t_f16t_wmma_tensor_op_f32_sm70.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16t_f16t_f16t_wmma_tensor_op_f32_sm70.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16t_f16t_f32n_tensor_op_f32_sm75.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16t_f16t_f32n_tensor_op_f32_sm75.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16t_f16t_f32n_tensor_op_f32_sm80.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16t_f16t_f32n_tensor_op_f32_sm80.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16t_f16t_f32n_wmma_tensor_op_f32_sm70.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16t_f16t_f32n_wmma_tensor_op_f32_sm70.cu`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16t_f16t_f32t_tensor_op_f32_sm75.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16t_f16t_f32t_tensor_op_f32_sm75.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16t_f16t_f32t_tensor_op_f32_sm80.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16t_f16t_f32t_tensor_op_f32_sm80.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16t_f16t_f32t_tensor_op_f32_sparse_sm80.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16t_f16t_f32t_tensor_op_f32_sparse_sm80.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16t_f16t_f32t_volta_tensor_op_f32_sm70.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16t_f16t_f32t_volta_tensor_op_f32_sm70.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16t_f16t_f32t_wmma_tensor_op_f32_sm70.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16t_f16t_f32t_wmma_tensor_op_f32_sm70.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f32n_f32n_f32t_tensor_op_bf16_f32_sm80.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f32n_f32n_f32t_tensor_op_bf16_f32_sm80.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f32n_f32n_f32t_tensor_op_f32_sm80.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f32n_f32n_f32t_tensor_op_f32_sm80.cu`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f32n_f32n_f32t_tensor_op_f32_sparse_sm80.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f32n_f32n_f32t_tensor_op_f32_sparse_sm80.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f32n_f32t_f32t_tensor_op_f32_sparse_sm80.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f32n_f32t_f32t_tensor_op_f32_sparse_sm80.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f32t_f32n_f32t_tensor_op_f32_sparse_sm80.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f32t_f32n_f32t_tensor_op_f32_sparse_sm80.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f32t_f32t_f32t_tensor_op_f32_sparse_sm80.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f32t_f32t_f32t_tensor_op_f32_sparse_sm80.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f64n_f64t_f64t_tensor_op_f64_sm80.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f64n_f64t_f64t_tensor_op_f64_sm80.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f64n_f64t_f64t_tensor_op_f64_sm90.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f64t_f64n_f64t_tensor_op_f64_sm90.cu`

 * *Files 2% similar despite different names*

```diff
@@ -42,29 +42,29 @@
 #include "cutlass/util/reference/host/tensor_compare.h"
 #include "cutlass/util/reference/host/tensor_copy.h"
 #include "cutlass/util/reference/host/tensor_fill.h"
 #include "cutlass/util/tensor_view_io.h"
 
 #include "testbed.h"
 
-#if defined(CUTLASS_ARCH_MMA_SM90_F64_MMA_ENABLED)
+#if defined(CUTLASS_ARCH_MMA_SM90_SUPPORTED)
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
-TEST(SM90_Device_Gemm_f64n_f64t_f64t_tensor_op_f64, 32x32x16_16x16x16_16x8x4) {
+TEST(SM90_Device_Gemm_f64t_f64n_f64t_tensor_op_f64, 32x32x16_16x16x16_16x8x4) {
 
   using ElementOutput = double;
   using ElementAccumulator = double;
   using ElementCompute = double;
 
   using Gemm = cutlass::gemm::device::Gemm<
     double,
-    cutlass::layout::ColumnMajor,
-    double,
     cutlass::layout::RowMajor,
+    double,
+    cutlass::layout::ColumnMajor,
     ElementOutput,
     cutlass::layout::RowMajor,
     ElementAccumulator,
     cutlass::arch::OpClassTensorOp,
     cutlass::arch::Sm90,
     cutlass::gemm::GemmShape<32, 32, 16>,
     cutlass::gemm::GemmShape<16, 16, 16>,
@@ -78,27 +78,28 @@
     cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>,
     4
   >;
 
   EXPECT_TRUE(test::gemm::device::TestAllGemm<Gemm>());
 }
 
+
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
-TEST(SM90_Device_Gemm_f64n_f64t_f64t_tensor_op_f64, 64x64x16_32x32x16_16x8x4) {
+TEST(SM90_Device_Gemm_f64t_f64n_f64t_tensor_op_f64, 64x64x16_32x32x16_16x8x4) {
 
   using ElementOutput = double;
   using ElementAccumulator = double;
   using ElementCompute = double;
 
   using Gemm = cutlass::gemm::device::Gemm<
     double,
-    cutlass::layout::ColumnMajor,
-    double,
     cutlass::layout::RowMajor,
+    double,
+    cutlass::layout::ColumnMajor,
     ElementOutput,
     cutlass::layout::RowMajor,
     ElementAccumulator,
     cutlass::arch::OpClassTensorOp,
     cutlass::arch::Sm90,
     cutlass::gemm::GemmShape<64, 64, 16>,
     cutlass::gemm::GemmShape<32, 32, 16>,
@@ -114,32 +115,32 @@
   >;
 
   EXPECT_TRUE(test::gemm::device::TestAllGemm<Gemm>());
 }
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
-TEST(SM90_Device_Gemm_f64n_f64t_f64t_tensor_op_f64, 128x64x16_64x32x16_16x8x4) {
+TEST(SM90_Device_Gemm_f64t_f64n_f64t_tensor_op_f64, 64x128x16_32x64x16_16x8x4) {
 
   using ElementOutput = double;
   using ElementAccumulator = double;
   using ElementCompute = double;
 
   using Gemm = cutlass::gemm::device::Gemm<
     double,
-    cutlass::layout::ColumnMajor,
-    double,
     cutlass::layout::RowMajor,
+    double,
+    cutlass::layout::ColumnMajor,
     ElementOutput,
     cutlass::layout::RowMajor,
     ElementAccumulator,
     cutlass::arch::OpClassTensorOp,
     cutlass::arch::Sm90,
-    cutlass::gemm::GemmShape<128, 64, 16>,
-    cutlass::gemm::GemmShape<64, 32, 16>,
+    cutlass::gemm::GemmShape<64, 128, 16>,
+    cutlass::gemm::GemmShape<32, 64, 16>,
     cutlass::gemm::GemmShape<16, 8, 4>,
     cutlass::epilogue::thread::LinearCombination<
       ElementOutput,
       1,
       ElementAccumulator,
       ElementCompute
     >,
@@ -148,59 +149,59 @@
   >;
 
   EXPECT_TRUE(test::gemm::device::TestAllGemm<Gemm>());
 }
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
-TEST(SM90_Device_Gemm_f64n_f64t_f64t_tensor_op_f64, 64x128x16_32x64x16_16x8x4) {
+TEST(SM90_Device_Gemm_f64t_f64n_f64t_tensor_op_f64, 128x64x16_64x32x16_16x8x4) {
 
   using ElementOutput = double;
   using ElementAccumulator = double;
   using ElementCompute = double;
 
   using Gemm = cutlass::gemm::device::Gemm<
     double,
-    cutlass::layout::ColumnMajor,
-    double,
     cutlass::layout::RowMajor,
+    double,
+    cutlass::layout::ColumnMajor,
     ElementOutput,
     cutlass::layout::RowMajor,
     ElementAccumulator,
     cutlass::arch::OpClassTensorOp,
     cutlass::arch::Sm90,
-    cutlass::gemm::GemmShape<64, 128, 16>,
-    cutlass::gemm::GemmShape<32, 64, 16>,
+    cutlass::gemm::GemmShape<128, 64, 16>,
+    cutlass::gemm::GemmShape<64, 32, 16>,
     cutlass::gemm::GemmShape<16, 8, 4>,
     cutlass::epilogue::thread::LinearCombination<
       ElementOutput,
       1,
       ElementAccumulator,
       ElementCompute
     >,
     cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>,
-    3
+    4
   >;
-  
+
   EXPECT_TRUE(test::gemm::device::TestAllGemm<Gemm>());
 }
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
-TEST(SM90_Device_Gemm_f64n_f64t_f64t_tensor_op_f64, 128x128x16_32x64x16_16x8x4) {
+TEST(SM90_Device_Gemm_f64t_f64n_f64t_tensor_op_f64, 128x128x16_32x64x16_16x8x4) {
 
   using ElementOutput = double;
   using ElementAccumulator = double;
   using ElementCompute = double;
 
   using Gemm = cutlass::gemm::device::Gemm<
     double,
-    cutlass::layout::ColumnMajor,
-    double,
     cutlass::layout::RowMajor,
+    double,
+    cutlass::layout::ColumnMajor,
     ElementOutput,
     cutlass::layout::RowMajor,
     ElementAccumulator,
     cutlass::arch::OpClassTensorOp,
     cutlass::arch::Sm90,
     cutlass::gemm::GemmShape<128, 128, 16>,
     cutlass::gemm::GemmShape<32, 64, 16>,
@@ -213,11 +214,10 @@
     >,
     cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>,
     3
   >;
 
   EXPECT_TRUE(test::gemm::device::TestAllGemm<Gemm>());
 }
-
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
-#endif // if defined(CUTLASS_ARCH_MMA_SM90_F64_MMA_ENABLED)
+#endif // if (CUTLASS_ARCH_MMA_SM90_SUPPORTED)
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f64t_f64n_f64t_tensor_op_f64_sm80.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f64t_f64n_f64t_tensor_op_f64_sm80.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f64t_f64n_f64t_tensor_op_f64_sm90.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f64n_f64t_f64t_tensor_op_f64_sm90.cu`

 * *Files 3% similar despite different names*

```diff
@@ -42,29 +42,29 @@
 #include "cutlass/util/reference/host/tensor_compare.h"
 #include "cutlass/util/reference/host/tensor_copy.h"
 #include "cutlass/util/reference/host/tensor_fill.h"
 #include "cutlass/util/tensor_view_io.h"
 
 #include "testbed.h"
 
-#if defined(CUTLASS_ARCH_MMA_SM90_F64_MMA_ENABLED)
+#if defined(CUTLASS_ARCH_MMA_SM90_SUPPORTED)
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
-TEST(SM90_Device_Gemm_f64t_f64n_f64t_tensor_op_f64, 32x32x16_16x16x16_16x8x4) {
+TEST(SM90_Device_Gemm_f64n_f64t_f64t_tensor_op_f64, 32x32x16_16x16x16_16x8x4) {
 
   using ElementOutput = double;
   using ElementAccumulator = double;
   using ElementCompute = double;
 
   using Gemm = cutlass::gemm::device::Gemm<
     double,
-    cutlass::layout::RowMajor,
-    double,
     cutlass::layout::ColumnMajor,
+    double,
+    cutlass::layout::RowMajor,
     ElementOutput,
     cutlass::layout::RowMajor,
     ElementAccumulator,
     cutlass::arch::OpClassTensorOp,
     cutlass::arch::Sm90,
     cutlass::gemm::GemmShape<32, 32, 16>,
     cutlass::gemm::GemmShape<16, 16, 16>,
@@ -78,28 +78,27 @@
     cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>,
     4
   >;
 
   EXPECT_TRUE(test::gemm::device::TestAllGemm<Gemm>());
 }
 
-
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
-TEST(SM90_Device_Gemm_f64t_f64n_f64t_tensor_op_f64, 64x64x16_32x32x16_16x8x4) {
+TEST(SM90_Device_Gemm_f64n_f64t_f64t_tensor_op_f64, 64x64x16_32x32x16_16x8x4) {
 
   using ElementOutput = double;
   using ElementAccumulator = double;
   using ElementCompute = double;
 
   using Gemm = cutlass::gemm::device::Gemm<
     double,
-    cutlass::layout::RowMajor,
-    double,
     cutlass::layout::ColumnMajor,
+    double,
+    cutlass::layout::RowMajor,
     ElementOutput,
     cutlass::layout::RowMajor,
     ElementAccumulator,
     cutlass::arch::OpClassTensorOp,
     cutlass::arch::Sm90,
     cutlass::gemm::GemmShape<64, 64, 16>,
     cutlass::gemm::GemmShape<32, 32, 16>,
@@ -115,32 +114,32 @@
   >;
 
   EXPECT_TRUE(test::gemm::device::TestAllGemm<Gemm>());
 }
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
-TEST(SM90_Device_Gemm_f64t_f64n_f64t_tensor_op_f64, 64x128x16_32x64x16_16x8x4) {
+TEST(SM90_Device_Gemm_f64n_f64t_f64t_tensor_op_f64, 128x64x16_64x32x16_16x8x4) {
 
   using ElementOutput = double;
   using ElementAccumulator = double;
   using ElementCompute = double;
 
   using Gemm = cutlass::gemm::device::Gemm<
     double,
-    cutlass::layout::RowMajor,
-    double,
     cutlass::layout::ColumnMajor,
+    double,
+    cutlass::layout::RowMajor,
     ElementOutput,
     cutlass::layout::RowMajor,
     ElementAccumulator,
     cutlass::arch::OpClassTensorOp,
     cutlass::arch::Sm90,
-    cutlass::gemm::GemmShape<64, 128, 16>,
-    cutlass::gemm::GemmShape<32, 64, 16>,
+    cutlass::gemm::GemmShape<128, 64, 16>,
+    cutlass::gemm::GemmShape<64, 32, 16>,
     cutlass::gemm::GemmShape<16, 8, 4>,
     cutlass::epilogue::thread::LinearCombination<
       ElementOutput,
       1,
       ElementAccumulator,
       ElementCompute
     >,
@@ -149,59 +148,59 @@
   >;
 
   EXPECT_TRUE(test::gemm::device::TestAllGemm<Gemm>());
 }
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
-TEST(SM90_Device_Gemm_f64t_f64n_f64t_tensor_op_f64, 128x64x16_64x32x16_16x8x4) {
+TEST(SM90_Device_Gemm_f64n_f64t_f64t_tensor_op_f64, 64x128x16_32x64x16_16x8x4) {
 
   using ElementOutput = double;
   using ElementAccumulator = double;
   using ElementCompute = double;
 
   using Gemm = cutlass::gemm::device::Gemm<
     double,
-    cutlass::layout::RowMajor,
-    double,
     cutlass::layout::ColumnMajor,
+    double,
+    cutlass::layout::RowMajor,
     ElementOutput,
     cutlass::layout::RowMajor,
     ElementAccumulator,
     cutlass::arch::OpClassTensorOp,
     cutlass::arch::Sm90,
-    cutlass::gemm::GemmShape<128, 64, 16>,
-    cutlass::gemm::GemmShape<64, 32, 16>,
+    cutlass::gemm::GemmShape<64, 128, 16>,
+    cutlass::gemm::GemmShape<32, 64, 16>,
     cutlass::gemm::GemmShape<16, 8, 4>,
     cutlass::epilogue::thread::LinearCombination<
       ElementOutput,
       1,
       ElementAccumulator,
       ElementCompute
     >,
     cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>,
-    4
+    3
   >;
-
+  
   EXPECT_TRUE(test::gemm::device::TestAllGemm<Gemm>());
 }
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
-TEST(SM90_Device_Gemm_f64t_f64n_f64t_tensor_op_f64, 128x128x16_32x64x16_16x8x4) {
+TEST(SM90_Device_Gemm_f64n_f64t_f64t_tensor_op_f64, 128x128x16_32x64x16_16x8x4) {
 
   using ElementOutput = double;
   using ElementAccumulator = double;
   using ElementCompute = double;
 
   using Gemm = cutlass::gemm::device::Gemm<
     double,
-    cutlass::layout::RowMajor,
-    double,
     cutlass::layout::ColumnMajor,
+    double,
+    cutlass::layout::RowMajor,
     ElementOutput,
     cutlass::layout::RowMajor,
     ElementAccumulator,
     cutlass::arch::OpClassTensorOp,
     cutlass::arch::Sm90,
     cutlass::gemm::GemmShape<128, 128, 16>,
     cutlass::gemm::GemmShape<32, 64, 16>,
@@ -214,10 +213,11 @@
     >,
     cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>,
     3
   >;
 
   EXPECT_TRUE(test::gemm::device::TestAllGemm<Gemm>());
 }
+
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
-#endif // if (CUTLASS_ARCH_MMA_SM90_F64_MMA_ENABLED)
+#endif // if defined(CUTLASS_ARCH_MMA_SM90_SUPPORTED)
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_grouped_scheduler_sm80.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_grouped_scheduler_sm80.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_grouped_sm80.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_grouped_sm80.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_planar_complex_f16_f16_f32_tensor_op_sm70.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_planar_complex_f16_f16_f32_tensor_op_sm70.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_planar_complex_f16_f16_f32_tensor_op_sm75.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_planar_complex_f16_f16_f32_tensor_op_sm75.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_planar_complex_f16_f16_f32_tensor_op_sm80.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_planar_complex_f16_f16_f32_tensor_op_sm80.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_s4n_s4t_s4n_tensor_op_s32_sm75.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_s4n_s4t_s4n_tensor_op_s32_sm75.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_s4n_s4t_s4n_tensor_op_s32_sm80.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_s4n_s4t_s4n_tensor_op_s32_sm80.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_s4t_s4n_s32n_tensor_op_s32_sm75.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_s4t_s4n_s32n_tensor_op_s32_sm75.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_s4t_s4n_s32n_tensor_op_s32_sm80.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_s4t_s4n_s32n_tensor_op_s32_sm80.cu`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /**************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_s4t_s4n_s32n_wmma_tensor_op_s32_sm75.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_s4t_s4n_s32n_wmma_tensor_op_s32_sm75.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_s4t_s4n_s32t_tensor_op_s32_sm75.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_s4t_s4n_s32t_tensor_op_s32_sm75.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_s4t_s4n_s32t_tensor_op_s32_sm80.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_s4t_s4n_s32t_tensor_op_s32_sm80.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /**************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_s4t_s4n_s32t_tensor_op_s32_sparse_sm80.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_s4t_s4n_s32t_tensor_op_s32_sparse_sm80.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /**************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_s4t_s4n_s32t_wmma_tensor_op_s32_sm75.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_s4t_s4n_s32t_wmma_tensor_op_s32_sm75.cu`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_s4t_s4n_s4n_tensor_op_s32_sm75.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_s4t_s4n_s4t_tensor_op_s32_sm75.cu`

 * *Files 2% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -48,27 +48,27 @@
 
 #include "testbed.h"
 
 #if defined(CUTLASS_ARCH_MMA_SM75_SUPPORTED)
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
-TEST(SM75_Device_Gemm_s4t_s4n_s4n_tensor_op_s32, 128x256x128_64x64x128) {
+TEST(SM75_Device_Gemm_s4t_s4n_s4t_tensor_op_s32, 128x256x128_64x64x128) {
 
   using ElementOutput = cutlass::int4b_t;
   using ElementAccumulator = int32_t;
   using ElementCompute = float;
 
   using Gemm = cutlass::gemm::device::Gemm<
     cutlass::int4b_t,
     cutlass::layout::RowMajor,
     cutlass::int4b_t,
     cutlass::layout::ColumnMajor,
     ElementOutput,
-    cutlass::layout::ColumnMajor,
+    cutlass::layout::RowMajor,
     ElementAccumulator,
     cutlass::arch::OpClassTensorOp,
     cutlass::arch::Sm75,
     cutlass::gemm::GemmShape<128, 256, 128>,
     cutlass::gemm::GemmShape<64, 64, 128>,
     cutlass::gemm::GemmShape<8, 8, 32>,
     cutlass::epilogue::thread::LinearCombinationClamp<
@@ -77,30 +77,30 @@
       ElementAccumulator,
       ElementCompute
     >,
     cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>,
     2
   >;
 
-  EXPECT_TRUE(test::gemm::device::TestAllGemm<Gemm>());
+  EXPECT_TRUE(test::gemm::device::TestAllGemmBasic<Gemm>());
 }
 
-TEST(SM75_Device_Gemm_s4t_s4n_s4n_tensor_op_s32, 256x128x128_64x64x128) {
+TEST(SM75_Device_Gemm_s4t_s4n_s4t_tensor_op_s32, 256x128x128_64x64x128) {
 
   using ElementOutput = cutlass::int4b_t;
   using ElementAccumulator = int32_t;
   using ElementCompute = float;
 
   using Gemm = cutlass::gemm::device::Gemm<
     cutlass::int4b_t,
     cutlass::layout::RowMajor,
     cutlass::int4b_t,
     cutlass::layout::ColumnMajor,
     ElementOutput,
-    cutlass::layout::ColumnMajor,
+    cutlass::layout::RowMajor,
     ElementAccumulator,
     cutlass::arch::OpClassTensorOp,
     cutlass::arch::Sm75,
     cutlass::gemm::GemmShape<256, 128, 128>,
     cutlass::gemm::GemmShape<64, 64, 128>,
     cutlass::gemm::GemmShape<8, 8, 32>,
     cutlass::epilogue::thread::LinearCombinationClamp<
@@ -109,30 +109,30 @@
       ElementAccumulator,
       ElementCompute
     >,
     cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>,
     2
   >;
 
-  EXPECT_TRUE(test::gemm::device::TestAllGemm<Gemm>());
+  EXPECT_TRUE(test::gemm::device::TestAllGemmBasic<Gemm>());
 }
 
-TEST(SM75_Device_Gemm_s4t_s4n_s4n_tensor_op_s32, 128x128x128_64x64x128) {
+TEST(SM75_Device_Gemm_s4t_s4n_s4t_tensor_op_s32, 128x128x128_64x64x128) {
 
   using ElementOutput = cutlass::int4b_t;
   using ElementAccumulator = int32_t;
   using ElementCompute = float;
 
   using Gemm = cutlass::gemm::device::Gemm<
     cutlass::int4b_t,
     cutlass::layout::RowMajor,
     cutlass::int4b_t,
     cutlass::layout::ColumnMajor,
     ElementOutput,
-    cutlass::layout::ColumnMajor,
+    cutlass::layout::RowMajor,
     ElementAccumulator,
     cutlass::arch::OpClassTensorOp,
     cutlass::arch::Sm75,
     cutlass::gemm::GemmShape<128, 128, 128>,
     cutlass::gemm::GemmShape<64, 64, 128>,
     cutlass::gemm::GemmShape<8, 8, 32>,
     cutlass::epilogue::thread::LinearCombinationClamp<
@@ -141,30 +141,30 @@
       ElementAccumulator,
       ElementCompute
     >,
     cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>,
     2
   >;
 
-  EXPECT_TRUE(test::gemm::device::TestAllGemm<Gemm>());
+  EXPECT_TRUE(test::gemm::device::TestAllGemmBasic<Gemm>());
 }
 
-TEST(SM75_Device_Gemm_s4t_s4n_s4n_tensor_op_s32, 64x256x128_64x64x128) {
+TEST(SM75_Device_Gemm_s4t_s4n_s4t_tensor_op_s32, 64x256x128_64x64x128) {
 
   using ElementOutput = cutlass::int4b_t;
   using ElementAccumulator = int32_t;
   using ElementCompute = float;
 
   using Gemm = cutlass::gemm::device::Gemm<
     cutlass::int4b_t,
     cutlass::layout::RowMajor,
     cutlass::int4b_t,
     cutlass::layout::ColumnMajor,
     ElementOutput,
-    cutlass::layout::ColumnMajor,
+    cutlass::layout::RowMajor,
     ElementAccumulator,
     cutlass::arch::OpClassTensorOp,
     cutlass::arch::Sm75,
     cutlass::gemm::GemmShape<64, 256, 128>,
     cutlass::gemm::GemmShape<64, 64, 128>,
     cutlass::gemm::GemmShape<8, 8, 32>,
     cutlass::epilogue::thread::LinearCombinationClamp<
@@ -173,30 +173,30 @@
       ElementAccumulator,
       ElementCompute
     >,
     cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>,
     2
   >;
 
-  EXPECT_TRUE(test::gemm::device::TestAllGemm<Gemm>());
+  EXPECT_TRUE(test::gemm::device::TestAllGemmBasic<Gemm>());
 }
 
-TEST(SM75_Device_Gemm_s4t_s4n_s4n_tensor_op_s32, 256x64x128_64x64x128) {
+TEST(SM75_Device_Gemm_s4t_s4n_s4t_tensor_op_s32, 256x64x128_64x64x128) {
 
   using ElementOutput = cutlass::int4b_t;
   using ElementAccumulator = int32_t;
   using ElementCompute = float;
 
   using Gemm = cutlass::gemm::device::Gemm<
     cutlass::int4b_t,
     cutlass::layout::RowMajor,
     cutlass::int4b_t,
     cutlass::layout::ColumnMajor,
     ElementOutput,
-    cutlass::layout::ColumnMajor,
+    cutlass::layout::RowMajor,
     ElementAccumulator,
     cutlass::arch::OpClassTensorOp,
     cutlass::arch::Sm75,
     cutlass::gemm::GemmShape<256, 64, 128>,
     cutlass::gemm::GemmShape<64, 64, 128>,
     cutlass::gemm::GemmShape<8, 8, 32>,
     cutlass::epilogue::thread::LinearCombinationClamp<
@@ -205,30 +205,29 @@
       ElementAccumulator,
       ElementCompute
     >,
     cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>,
     2
   >;
 
-  EXPECT_TRUE(test::gemm::device::TestAllGemm<Gemm>());
+  EXPECT_TRUE(test::gemm::device::TestAllGemmBasic<Gemm>());
 }
-
-TEST(SM75_Device_Gemm_s4t_s4n_s4n_tensor_op_s32, 64x128x128_32x64x128) {
+TEST(SM75_Device_Gemm_s4t_s4n_s4t_tensor_op_s32, 64x128x128_32x64x128) {
 
   using ElementOutput = cutlass::int4b_t;
   using ElementAccumulator = int32_t;
   using ElementCompute = float;
 
   using Gemm = cutlass::gemm::device::Gemm<
     cutlass::int4b_t,
     cutlass::layout::RowMajor,
     cutlass::int4b_t,
     cutlass::layout::ColumnMajor,
     ElementOutput,
-    cutlass::layout::ColumnMajor,
+    cutlass::layout::RowMajor,
     ElementAccumulator,
     cutlass::arch::OpClassTensorOp,
     cutlass::arch::Sm75,
     cutlass::gemm::GemmShape<64, 128, 128>,
     cutlass::gemm::GemmShape<32, 64, 128>,
     cutlass::gemm::GemmShape<8, 8, 32>,
     cutlass::epilogue::thread::LinearCombinationClamp<
@@ -237,30 +236,30 @@
       ElementAccumulator,
       ElementCompute
     >,
     cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>,
     2
   >;
 
-  EXPECT_TRUE(test::gemm::device::TestAllGemm<Gemm>());
+  EXPECT_TRUE(test::gemm::device::TestAllGemmBasic<Gemm>());
 }
 
-TEST(SM75_Device_Gemm_s4t_s4n_s4n_tensor_op_s32, 128x64x128_64x32x128) {
+TEST(SM75_Device_Gemm_s4t_s4n_s4t_tensor_op_s32, 128x64x128_64x32x128) {
 
   using ElementOutput = cutlass::int4b_t;
   using ElementAccumulator = int32_t;
   using ElementCompute = float;
 
   using Gemm = cutlass::gemm::device::Gemm<
     cutlass::int4b_t,
     cutlass::layout::RowMajor,
     cutlass::int4b_t,
     cutlass::layout::ColumnMajor,
     ElementOutput,
-    cutlass::layout::ColumnMajor,
+    cutlass::layout::RowMajor,
     ElementAccumulator,
     cutlass::arch::OpClassTensorOp,
     cutlass::arch::Sm75,
     cutlass::gemm::GemmShape<128, 64, 128>,
     cutlass::gemm::GemmShape<64, 32, 128>,
     cutlass::gemm::GemmShape<8, 8, 32>,
     cutlass::epilogue::thread::LinearCombinationClamp<
@@ -269,30 +268,30 @@
       ElementAccumulator,
       ElementCompute
     >,
     cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>,
     2
   >;
 
-  EXPECT_TRUE(test::gemm::device::TestAllGemm<Gemm>());
+  EXPECT_TRUE(test::gemm::device::TestAllGemmBasic<Gemm>());
 }
 
-TEST(SM75_Device_Gemm_s4t_s4n_s4n_tensor_op_s32, 64x64x128_32x32x128) {
+TEST(SM75_Device_Gemm_s4t_s4n_s4t_tensor_op_s32, 64x64x128_32x32x128) {
 
   using ElementOutput = cutlass::int4b_t;
   using ElementAccumulator = int32_t;
   using ElementCompute = float;
 
   using Gemm = cutlass::gemm::device::Gemm<
     cutlass::int4b_t,
     cutlass::layout::RowMajor,
     cutlass::int4b_t,
     cutlass::layout::ColumnMajor,
     ElementOutput,
-    cutlass::layout::ColumnMajor,
+    cutlass::layout::RowMajor,
     ElementAccumulator,
     cutlass::arch::OpClassTensorOp,
     cutlass::arch::Sm75,
     cutlass::gemm::GemmShape<64, 64, 128>,
     cutlass::gemm::GemmShape<32, 32, 128>,
     cutlass::gemm::GemmShape<8, 8, 32>,
     cutlass::epilogue::thread::LinearCombinationClamp<
@@ -301,12 +300,13 @@
       ElementAccumulator,
       ElementCompute
     >,
     cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>,
     2
   >;
 
-  EXPECT_TRUE(test::gemm::device::TestAllGemm<Gemm>());
+  EXPECT_TRUE(test::gemm::device::TestAllGemmBasic<Gemm>());
 }
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
+
 #endif
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_s4t_s4n_s4n_tensor_op_s32_sm80.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_s4t_s4n_s4n_tensor_op_s32_sm80.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /**************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_s4t_s4n_s4t_tensor_op_s32_sm75.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_s4t_s4n_s4n_tensor_op_s32_sm75.cu`

 * *Files 2% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -48,27 +48,27 @@
 
 #include "testbed.h"
 
 #if defined(CUTLASS_ARCH_MMA_SM75_SUPPORTED)
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
-TEST(SM75_Device_Gemm_s4t_s4n_s4t_tensor_op_s32, 128x256x128_64x64x128) {
+TEST(SM75_Device_Gemm_s4t_s4n_s4n_tensor_op_s32, 128x256x128_64x64x128) {
 
   using ElementOutput = cutlass::int4b_t;
   using ElementAccumulator = int32_t;
   using ElementCompute = float;
 
   using Gemm = cutlass::gemm::device::Gemm<
     cutlass::int4b_t,
     cutlass::layout::RowMajor,
     cutlass::int4b_t,
     cutlass::layout::ColumnMajor,
     ElementOutput,
-    cutlass::layout::RowMajor,
+    cutlass::layout::ColumnMajor,
     ElementAccumulator,
     cutlass::arch::OpClassTensorOp,
     cutlass::arch::Sm75,
     cutlass::gemm::GemmShape<128, 256, 128>,
     cutlass::gemm::GemmShape<64, 64, 128>,
     cutlass::gemm::GemmShape<8, 8, 32>,
     cutlass::epilogue::thread::LinearCombinationClamp<
@@ -77,30 +77,30 @@
       ElementAccumulator,
       ElementCompute
     >,
     cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>,
     2
   >;
 
-  EXPECT_TRUE(test::gemm::device::TestAllGemm<Gemm>());
+  EXPECT_TRUE(test::gemm::device::TestAllGemmBasic<Gemm>());
 }
 
-TEST(SM75_Device_Gemm_s4t_s4n_s4t_tensor_op_s32, 256x128x128_64x64x128) {
+TEST(SM75_Device_Gemm_s4t_s4n_s4n_tensor_op_s32, 256x128x128_64x64x128) {
 
   using ElementOutput = cutlass::int4b_t;
   using ElementAccumulator = int32_t;
   using ElementCompute = float;
 
   using Gemm = cutlass::gemm::device::Gemm<
     cutlass::int4b_t,
     cutlass::layout::RowMajor,
     cutlass::int4b_t,
     cutlass::layout::ColumnMajor,
     ElementOutput,
-    cutlass::layout::RowMajor,
+    cutlass::layout::ColumnMajor,
     ElementAccumulator,
     cutlass::arch::OpClassTensorOp,
     cutlass::arch::Sm75,
     cutlass::gemm::GemmShape<256, 128, 128>,
     cutlass::gemm::GemmShape<64, 64, 128>,
     cutlass::gemm::GemmShape<8, 8, 32>,
     cutlass::epilogue::thread::LinearCombinationClamp<
@@ -109,30 +109,30 @@
       ElementAccumulator,
       ElementCompute
     >,
     cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>,
     2
   >;
 
-  EXPECT_TRUE(test::gemm::device::TestAllGemm<Gemm>());
+  EXPECT_TRUE(test::gemm::device::TestAllGemmBasic<Gemm>());
 }
 
-TEST(SM75_Device_Gemm_s4t_s4n_s4t_tensor_op_s32, 128x128x128_64x64x128) {
+TEST(SM75_Device_Gemm_s4t_s4n_s4n_tensor_op_s32, 128x128x128_64x64x128) {
 
   using ElementOutput = cutlass::int4b_t;
   using ElementAccumulator = int32_t;
   using ElementCompute = float;
 
   using Gemm = cutlass::gemm::device::Gemm<
     cutlass::int4b_t,
     cutlass::layout::RowMajor,
     cutlass::int4b_t,
     cutlass::layout::ColumnMajor,
     ElementOutput,
-    cutlass::layout::RowMajor,
+    cutlass::layout::ColumnMajor,
     ElementAccumulator,
     cutlass::arch::OpClassTensorOp,
     cutlass::arch::Sm75,
     cutlass::gemm::GemmShape<128, 128, 128>,
     cutlass::gemm::GemmShape<64, 64, 128>,
     cutlass::gemm::GemmShape<8, 8, 32>,
     cutlass::epilogue::thread::LinearCombinationClamp<
@@ -141,30 +141,30 @@
       ElementAccumulator,
       ElementCompute
     >,
     cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>,
     2
   >;
 
-  EXPECT_TRUE(test::gemm::device::TestAllGemm<Gemm>());
+  EXPECT_TRUE(test::gemm::device::TestAllGemmBasic<Gemm>());
 }
 
-TEST(SM75_Device_Gemm_s4t_s4n_s4t_tensor_op_s32, 64x256x128_64x64x128) {
+TEST(SM75_Device_Gemm_s4t_s4n_s4n_tensor_op_s32, 64x256x128_64x64x128) {
 
   using ElementOutput = cutlass::int4b_t;
   using ElementAccumulator = int32_t;
   using ElementCompute = float;
 
   using Gemm = cutlass::gemm::device::Gemm<
     cutlass::int4b_t,
     cutlass::layout::RowMajor,
     cutlass::int4b_t,
     cutlass::layout::ColumnMajor,
     ElementOutput,
-    cutlass::layout::RowMajor,
+    cutlass::layout::ColumnMajor,
     ElementAccumulator,
     cutlass::arch::OpClassTensorOp,
     cutlass::arch::Sm75,
     cutlass::gemm::GemmShape<64, 256, 128>,
     cutlass::gemm::GemmShape<64, 64, 128>,
     cutlass::gemm::GemmShape<8, 8, 32>,
     cutlass::epilogue::thread::LinearCombinationClamp<
@@ -173,30 +173,30 @@
       ElementAccumulator,
       ElementCompute
     >,
     cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>,
     2
   >;
 
-  EXPECT_TRUE(test::gemm::device::TestAllGemm<Gemm>());
+  EXPECT_TRUE(test::gemm::device::TestAllGemmBasic<Gemm>());
 }
 
-TEST(SM75_Device_Gemm_s4t_s4n_s4t_tensor_op_s32, 256x64x128_64x64x128) {
+TEST(SM75_Device_Gemm_s4t_s4n_s4n_tensor_op_s32, 256x64x128_64x64x128) {
 
   using ElementOutput = cutlass::int4b_t;
   using ElementAccumulator = int32_t;
   using ElementCompute = float;
 
   using Gemm = cutlass::gemm::device::Gemm<
     cutlass::int4b_t,
     cutlass::layout::RowMajor,
     cutlass::int4b_t,
     cutlass::layout::ColumnMajor,
     ElementOutput,
-    cutlass::layout::RowMajor,
+    cutlass::layout::ColumnMajor,
     ElementAccumulator,
     cutlass::arch::OpClassTensorOp,
     cutlass::arch::Sm75,
     cutlass::gemm::GemmShape<256, 64, 128>,
     cutlass::gemm::GemmShape<64, 64, 128>,
     cutlass::gemm::GemmShape<8, 8, 32>,
     cutlass::epilogue::thread::LinearCombinationClamp<
@@ -205,29 +205,30 @@
       ElementAccumulator,
       ElementCompute
     >,
     cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>,
     2
   >;
 
-  EXPECT_TRUE(test::gemm::device::TestAllGemm<Gemm>());
+  EXPECT_TRUE(test::gemm::device::TestAllGemmBasic<Gemm>());
 }
-TEST(SM75_Device_Gemm_s4t_s4n_s4t_tensor_op_s32, 64x128x128_32x64x128) {
+
+TEST(SM75_Device_Gemm_s4t_s4n_s4n_tensor_op_s32, 64x128x128_32x64x128) {
 
   using ElementOutput = cutlass::int4b_t;
   using ElementAccumulator = int32_t;
   using ElementCompute = float;
 
   using Gemm = cutlass::gemm::device::Gemm<
     cutlass::int4b_t,
     cutlass::layout::RowMajor,
     cutlass::int4b_t,
     cutlass::layout::ColumnMajor,
     ElementOutput,
-    cutlass::layout::RowMajor,
+    cutlass::layout::ColumnMajor,
     ElementAccumulator,
     cutlass::arch::OpClassTensorOp,
     cutlass::arch::Sm75,
     cutlass::gemm::GemmShape<64, 128, 128>,
     cutlass::gemm::GemmShape<32, 64, 128>,
     cutlass::gemm::GemmShape<8, 8, 32>,
     cutlass::epilogue::thread::LinearCombinationClamp<
@@ -236,30 +237,30 @@
       ElementAccumulator,
       ElementCompute
     >,
     cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>,
     2
   >;
 
-  EXPECT_TRUE(test::gemm::device::TestAllGemm<Gemm>());
+  EXPECT_TRUE(test::gemm::device::TestAllGemmBasic<Gemm>());
 }
 
-TEST(SM75_Device_Gemm_s4t_s4n_s4t_tensor_op_s32, 128x64x128_64x32x128) {
+TEST(SM75_Device_Gemm_s4t_s4n_s4n_tensor_op_s32, 128x64x128_64x32x128) {
 
   using ElementOutput = cutlass::int4b_t;
   using ElementAccumulator = int32_t;
   using ElementCompute = float;
 
   using Gemm = cutlass::gemm::device::Gemm<
     cutlass::int4b_t,
     cutlass::layout::RowMajor,
     cutlass::int4b_t,
     cutlass::layout::ColumnMajor,
     ElementOutput,
-    cutlass::layout::RowMajor,
+    cutlass::layout::ColumnMajor,
     ElementAccumulator,
     cutlass::arch::OpClassTensorOp,
     cutlass::arch::Sm75,
     cutlass::gemm::GemmShape<128, 64, 128>,
     cutlass::gemm::GemmShape<64, 32, 128>,
     cutlass::gemm::GemmShape<8, 8, 32>,
     cutlass::epilogue::thread::LinearCombinationClamp<
@@ -268,30 +269,30 @@
       ElementAccumulator,
       ElementCompute
     >,
     cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>,
     2
   >;
 
-  EXPECT_TRUE(test::gemm::device::TestAllGemm<Gemm>());
+  EXPECT_TRUE(test::gemm::device::TestAllGemmBasic<Gemm>());
 }
 
-TEST(SM75_Device_Gemm_s4t_s4n_s4t_tensor_op_s32, 64x64x128_32x32x128) {
+TEST(SM75_Device_Gemm_s4t_s4n_s4n_tensor_op_s32, 64x64x128_32x32x128) {
 
   using ElementOutput = cutlass::int4b_t;
   using ElementAccumulator = int32_t;
   using ElementCompute = float;
 
   using Gemm = cutlass::gemm::device::Gemm<
     cutlass::int4b_t,
     cutlass::layout::RowMajor,
     cutlass::int4b_t,
     cutlass::layout::ColumnMajor,
     ElementOutput,
-    cutlass::layout::RowMajor,
+    cutlass::layout::ColumnMajor,
     ElementAccumulator,
     cutlass::arch::OpClassTensorOp,
     cutlass::arch::Sm75,
     cutlass::gemm::GemmShape<64, 64, 128>,
     cutlass::gemm::GemmShape<32, 32, 128>,
     cutlass::gemm::GemmShape<8, 8, 32>,
     cutlass::epilogue::thread::LinearCombinationClamp<
@@ -300,13 +301,12 @@
       ElementAccumulator,
       ElementCompute
     >,
     cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>,
     2
   >;
 
-  EXPECT_TRUE(test::gemm::device::TestAllGemm<Gemm>());
+  EXPECT_TRUE(test::gemm::device::TestAllGemmBasic<Gemm>());
 }
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
-
 #endif
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_s4t_s4n_s4t_tensor_op_s32_sm80.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_s4t_s4n_s4t_tensor_op_s32_sm80.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /**************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_s8n_s8t_s8n_tensor_op_s32_sm75.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_s8n_s8t_s8n_tensor_op_s32_sm75.cu`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_s8n_s8t_s8n_tensor_op_s32_sm80.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_s8n_s8t_s8n_tensor_op_s32_sm80.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_s8t_s8n_s32n_tensor_op_s32_sm75.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_s8t_s8n_s32n_tensor_op_s32_sm75.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_s8t_s8n_s32n_tensor_op_s32_sm80.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_s8t_s8n_s32n_tensor_op_s32_sm80.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /**************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_s8t_s8n_s32n_wmma_tensor_op_s32_sm72.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_s8t_s8n_s32n_wmma_tensor_op_s32_sm72.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_s8t_s8n_s32t_tensor_op_s32_sm75.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_s8t_s8n_s32t_tensor_op_s32_sm75.cu`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_s8t_s8n_s32t_tensor_op_s32_sm80.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_s8t_s8n_s32t_tensor_op_s32_sm80.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /**************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_s8t_s8n_s32t_tensor_op_s32_sparse_sm80.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_s8t_s8n_s32t_tensor_op_s32_sparse_sm80.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /**************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_s8t_s8n_s32t_wmma_tensor_op_s32_sm72.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_s8t_s8n_s32t_wmma_tensor_op_s32_sm72.cu`

 * *Files 2% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_s8t_s8n_s8n_tensor_op_s32_sm75.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_s8t_s8n_s8n_tensor_op_s32_sm75.cu`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_s8t_s8n_s8n_tensor_op_s32_sm80.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_s8t_s8n_s8n_tensor_op_s32_sm80.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /**************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_s8t_s8n_s8n_wmma_tensor_op_s32_sm72.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_s8t_s8n_s8n_wmma_tensor_op_s32_sm72.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_s8t_s8n_s8t_tensor_op_s32_sm75.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_s8t_s8n_s8t_tensor_op_s32_sm75.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_s8t_s8n_s8t_tensor_op_s32_sm80.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_s8t_s8n_s8t_tensor_op_s32_sm80.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /**************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_s8t_s8n_s8t_wmma_tensor_op_s32_sm72.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_s8t_s8n_s8t_wmma_tensor_op_s32_sm72.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_splitk_serial_tensor_op_sm75.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_splitk_serial_tensor_op_sm75.cu`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_splitk_simt_sm50.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_splitk_simt_sm50.cu`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_splitk_tensor_op_sm70.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_splitk_tensor_op_sm70.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_splitk_tensor_op_sm75.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_splitk_tensor_op_sm75.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_tf32n_tf32n_f32t_tensor_op_f32_sm80.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_tf32n_tf32n_f32t_tensor_op_f32_sm80.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_tf32n_tf32t_f32t_tensor_op_f32_sm80.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_tf32n_tf32t_f32t_tensor_op_f32_sm80.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_tf32t_tf32n_f32t_tensor_op_f32_sm80.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_tf32t_tf32n_f32t_tensor_op_f32_sm80.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_tf32t_tf32t_f32t_tensor_op_f32_sm80.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_tf32t_tf32t_f32t_tensor_op_f32_sm80.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_u8t_u8n_s32t_wmma_tensor_op_s32_sm72.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_u8t_u8n_s32t_wmma_tensor_op_s32_sm72.cu`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_universal_cf32n_cf32n_cf32n_tensor_op_f32_sm80.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_universal_cf32n_cf32n_cf32n_tensor_op_f32_sm80.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_universal_cf64n_cf64t_cf64t_tensor_op_f64_gaussian_sm80.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_universal_cf64n_cf64t_cf64t_tensor_op_f64_gaussian_sm80.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_universal_cf64n_cf64t_cf64t_tensor_op_f64_sm80.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_universal_cf64n_cf64t_cf64t_tensor_op_f64_sm80.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_universal_f16n_f16t_f32n_tensor_op_f32_sm75.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_universal_f16n_f16t_f32n_tensor_op_f32_sm75.cu`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_universal_f16n_f16t_f32t_tensor_op_f32_sm75.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_universal_f16n_f16t_f32t_tensor_op_f32_sm75.cu`

 * *Files 2% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_with_broadcast_f16n_f16n_f16n_tensorop_f32_sm75.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_with_broadcast_f16n_f16n_f16n_tensorop_f32_sm75.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_with_reduction_f16n_f16n_f16n_tensorop_f32_sm75.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_with_reduction_f16n_f16n_f16n_tensorop_f32_sm75.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_with_reduction_f16t_f16n_f16n_tensorop_f32_sm80.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_with_reduction_f16t_f16n_f16n_tensorop_f32_sm80.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemv.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/gemv.cu`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/hemm_cf32h_cf32n_tensor_op_f32_ls_sm80.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/hemm_cf32h_cf32n_tensor_op_f32_ls_sm80.cu`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/hemm_cf32h_cf32n_tensor_op_f32_rs_sm80.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/hemm_cf32h_cf32n_tensor_op_f32_rs_sm80.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/hemm_cf32h_cf32n_tensor_op_fast_f32_ls_sm80.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/hemm_cf32h_cf32n_tensor_op_fast_f32_ls_sm80.cu`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/hemm_cf32h_cf32n_tensor_op_fast_f32_rs_sm80.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/hemm_cf32h_cf32n_tensor_op_fast_f32_rs_sm80.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/hemm_cf64_cf64_cf64_tensor_op_f64_sm90.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/hemm_cf64_cf64_cf64_tensor_op_f64_sm90.cu`

 * *Files 2% similar despite different names*

```diff
@@ -44,15 +44,15 @@
 #include "cutlass/util/reference/host/tensor_compare.h"
 #include "cutlass/util/reference/host/tensor_copy.h"
 #include "cutlass/util/reference/host/tensor_fill.h"
 #include "cutlass/util/tensor_view_io.h"
 
 #include "testbed_symm_universal.h"
 
-#if defined(CUTLASS_ARCH_MMA_SM90_F64_MMA_ENABLED)
+#if defined(CUTLASS_ARCH_MMA_SM90_SUPPORTED)
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
 TEST(SM90_Device_Hemm_cf64h_cf64n_ls_l_tensor_op_f64_gaussian, 32x32x16_16x16x16) {
 
   using ElementOutput = cutlass::complex<double>;
   using ElementAccumulator = cutlass::complex<double>;
@@ -128,8 +128,8 @@
   >;
 
   EXPECT_TRUE(test::gemm::device::TestAllSymmUniversal<Hemm>());
 }
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
-#endif // #if defined(CUTLASS_ARCH_MMA_SM90_F64_MMA_ENABLED)
+#endif // #if defined(CUTLASS_ARCH_MMA_SM90_SUPPORTED)
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/hemm_cf64h_cf64n_cf64n_tensor_op_ls_f64_gaussian_sm80.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/hemm_cf64h_cf64n_cf64n_tensor_op_ls_f64_gaussian_sm80.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/hemm_cf64h_cf64n_cf64n_tensor_op_ls_f64_sm80.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/hemm_cf64h_cf64n_cf64n_tensor_op_ls_f64_sm80.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/hemm_cf64h_cf64n_cf64n_tensor_op_rs_f64_sm80.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/hemm_cf64h_cf64n_cf64n_tensor_op_rs_f64_sm80.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/her2k_cf32h_cf32n_tensor_op_f32_sm80.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/her2k_cf32h_cf32n_tensor_op_f32_sm80.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/her2k_cf32h_cf32n_tensor_op_fast_f32_sm80.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/her2k_cf32h_cf32n_tensor_op_fast_f32_sm80.cu`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/her2k_cf64_cf64_tensor_op_f64_sm90.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/her2k_cf64n_cf64n_tensor_op_f64_sm80.cu`

 * *Files 3% similar despite different names*

```diff
@@ -42,19 +42,19 @@
 #include "cutlass/util/reference/host/tensor_compare.h"
 #include "cutlass/util/reference/host/tensor_copy.h"
 #include "cutlass/util/reference/host/tensor_fill.h"
 #include "cutlass/util/tensor_view_io.h"
 
 #include "testbed_rank2k_universal.h"
 
-#if defined(CUTLASS_ARCH_MMA_SM90_F64_MMA_ENABLED)
+#if defined(CUTLASS_ARCH_MMA_SM80_SUPPORTED)
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
-TEST(SM90_Device_Her2k_cf64n_cf64n_l_tensor_op_f64, 32x32x16_16x16x16) {
+TEST(SM80_Device_Her2k_cf64n_cf64n_l_tensor_op_f64, 32x32x16_16x16x16) {
 
   using ElementA = cutlass::complex<double>;
   using LayoutA = cutlass::layout::ColumnMajor;
 
   using ElementB = cutlass::complex<double>;
   using LayoutB = cutlass::layout::ColumnMajor;
 
@@ -68,18 +68,18 @@
     ElementB,
     LayoutB,
     ElementC,
     LayoutC,
     cutlass::FillMode::kLower,
     ElementAccumulator,
     cutlass::arch::OpClassTensorOp,
-    cutlass::arch::Sm90,
+    cutlass::arch::Sm80,
     cutlass::gemm::GemmShape<32, 32, 16>,
     cutlass::gemm::GemmShape<16, 16, 16>,
-    cutlass::gemm::GemmShape<16, 8, 4>,
+    cutlass::gemm::GemmShape<8, 8, 4>,
     cutlass::epilogue::thread::LinearCombination<
       ElementC,
       1,
       ElementAccumulator,
       ElementAccumulator
     >,
     cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>,
@@ -94,15 +94,15 @@
   >;
 
   EXPECT_TRUE(test::gemm::device::TestAllRank2KHermitianUniversal<Rank2K>());
 }
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
-TEST(SM90_Device_Her2k_cf64c_cf64n_u_tensor_op_f64, 32x32x16_16x16x16) {
+TEST(SM80_Device_Her2k_cf64h_cf64n_u_tensor_op_f64, 32x32x16_16x16x16) {
 
   using ElementA = cutlass::complex<double>;
   using LayoutA = cutlass::layout::RowMajor;
 
   using ElementB = cutlass::complex<double>;
   using LayoutB = cutlass::layout::RowMajor;
 
@@ -116,18 +116,18 @@
     ElementB,
     LayoutB,
     ElementC,
     LayoutC,
     cutlass::FillMode::kUpper,
     ElementAccumulator,
     cutlass::arch::OpClassTensorOp,
-    cutlass::arch::Sm90,
+    cutlass::arch::Sm80,
     cutlass::gemm::GemmShape<32, 32, 16>,
     cutlass::gemm::GemmShape<16, 16, 16>,
-    cutlass::gemm::GemmShape<16, 8, 4>,
+    cutlass::gemm::GemmShape<8, 8, 4>,
     cutlass::epilogue::thread::LinearCombination<
       ElementC,
       1,
       ElementAccumulator,
       ElementAccumulator
     >,
     cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>,
@@ -142,8 +142,8 @@
   >;
 
   EXPECT_TRUE(test::gemm::device::TestAllRank2KHermitianUniversal<Rank2K>());
 }
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
-#endif // #if defined(CUTLASS_ARCH_MMA_SM90_F64_MMA_ENABLED)
+#endif // #if defined(CUTLASS_ARCH_MMA_SM80_SUPPORTED)
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/her2k_cf64h_cf64n_tensor_op_f64_grouped_sm80.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/her2k_cf64h_cf64n_tensor_op_f64_grouped_sm80.cu`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/her2k_cf64n_cf64n_tensor_op_f64_grouped_sm80.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/her2k_cf64n_cf64n_tensor_op_f64_grouped_sm80.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/her2k_cf64n_cf64n_tensor_op_f64_sm80.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/her2k_cf64_cf64_tensor_op_f64_sm90.cu`

 * *Files 2% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -42,19 +42,19 @@
 #include "cutlass/util/reference/host/tensor_compare.h"
 #include "cutlass/util/reference/host/tensor_copy.h"
 #include "cutlass/util/reference/host/tensor_fill.h"
 #include "cutlass/util/tensor_view_io.h"
 
 #include "testbed_rank2k_universal.h"
 
-#if defined(CUTLASS_ARCH_MMA_SM80_SUPPORTED)
+#if defined(CUTLASS_ARCH_MMA_SM90_SUPPORTED)
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
-TEST(SM80_Device_Her2k_cf64n_cf64n_l_tensor_op_f64, 32x32x16_16x16x16) {
+TEST(SM90_Device_Her2k_cf64n_cf64n_l_tensor_op_f64, 32x32x16_16x16x16) {
 
   using ElementA = cutlass::complex<double>;
   using LayoutA = cutlass::layout::ColumnMajor;
 
   using ElementB = cutlass::complex<double>;
   using LayoutB = cutlass::layout::ColumnMajor;
 
@@ -68,18 +68,18 @@
     ElementB,
     LayoutB,
     ElementC,
     LayoutC,
     cutlass::FillMode::kLower,
     ElementAccumulator,
     cutlass::arch::OpClassTensorOp,
-    cutlass::arch::Sm80,
+    cutlass::arch::Sm90,
     cutlass::gemm::GemmShape<32, 32, 16>,
     cutlass::gemm::GemmShape<16, 16, 16>,
-    cutlass::gemm::GemmShape<8, 8, 4>,
+    cutlass::gemm::GemmShape<16, 8, 4>,
     cutlass::epilogue::thread::LinearCombination<
       ElementC,
       1,
       ElementAccumulator,
       ElementAccumulator
     >,
     cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>,
@@ -94,15 +94,15 @@
   >;
 
   EXPECT_TRUE(test::gemm::device::TestAllRank2KHermitianUniversal<Rank2K>());
 }
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
-TEST(SM80_Device_Her2k_cf64h_cf64n_u_tensor_op_f64, 32x32x16_16x16x16) {
+TEST(SM90_Device_Her2k_cf64c_cf64n_u_tensor_op_f64, 32x32x16_16x16x16) {
 
   using ElementA = cutlass::complex<double>;
   using LayoutA = cutlass::layout::RowMajor;
 
   using ElementB = cutlass::complex<double>;
   using LayoutB = cutlass::layout::RowMajor;
 
@@ -116,18 +116,18 @@
     ElementB,
     LayoutB,
     ElementC,
     LayoutC,
     cutlass::FillMode::kUpper,
     ElementAccumulator,
     cutlass::arch::OpClassTensorOp,
-    cutlass::arch::Sm80,
+    cutlass::arch::Sm90,
     cutlass::gemm::GemmShape<32, 32, 16>,
     cutlass::gemm::GemmShape<16, 16, 16>,
-    cutlass::gemm::GemmShape<8, 8, 4>,
+    cutlass::gemm::GemmShape<16, 8, 4>,
     cutlass::epilogue::thread::LinearCombination<
       ElementC,
       1,
       ElementAccumulator,
       ElementAccumulator
     >,
     cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>,
@@ -142,8 +142,8 @@
   >;
 
   EXPECT_TRUE(test::gemm::device::TestAllRank2KHermitianUniversal<Rank2K>());
 }
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
-#endif // #if defined(CUTLASS_ARCH_MMA_SM80_SUPPORTED)
+#endif // #if defined(CUTLASS_ARCH_MMA_SM90_SUPPORTED)
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/her2k_cf64n_cf64t_tensor_op_f64_sm80.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/her2k_cf64n_cf64t_tensor_op_f64_sm80.cu`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/herk_cf32h_cf32n_tensor_op_f32_sm80.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/herk_cf32h_cf32n_tensor_op_f32_sm80.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/herk_cf32h_cf32n_tensor_op_fast_f32_sm80.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/herk_cf32h_cf32n_tensor_op_fast_f32_sm80.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/herk_cf64_cf64_tensor_op_f64_sm90.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/herk_cf64_cf64_tensor_op_f64_sm90.cu`

 * *Files 3% similar despite different names*

```diff
@@ -42,15 +42,15 @@
 #include "cutlass/util/reference/host/tensor_compare.h"
 #include "cutlass/util/reference/host/tensor_copy.h"
 #include "cutlass/util/reference/host/tensor_fill.h"
 #include "cutlass/util/tensor_view_io.h"
 
 #include "testbed_rank_k_universal.h"
 
-#if defined(CUTLASS_ARCH_MMA_SM90_F64_MMA_ENABLED)
+#if defined(CUTLASS_ARCH_MMA_SM90_SUPPORTED)
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 // HERK operator on CUBLAS_OP_C (row-major + conj) input layouts
 TEST(SM90_Device_Herk_cf64h_cf64n_l_tensor_op_f64, 64x64x16_32x32x16) {
 
   using ElementA = cutlass::complex<double>;
   using LayoutA = cutlass::layout::RowMajor;
@@ -86,8 +86,8 @@
     cutlass::BlasMode::kHermitian
   >;
 
   EXPECT_TRUE(test::gemm::device::TestAllRankKUniversal<RankK>());
 }
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
-#endif // #if defined(CUTLASS_ARCH_MMA_SM90_F64_MMA_ENABLED)
+#endif // #if defined(CUTLASS_ARCH_MMA_SM90_SUPPORTED)
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/herk_cf64h_cf64n_tensor_op_f64_sm80.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/herk_cf64h_cf64n_tensor_op_f64_sm80.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/multistage_testbed.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/multistage_testbed.h`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -121,15 +121,15 @@
 
     result = cudaGetDeviceProperties(&properties, device_idx);
 
     if (result != cudaSuccess) {
       throw std::runtime_error("cudaGetDeviceProperties() failed");
     }
 
-    if (properties.sharedMemPerMultiprocessor < smem_size) {
+    if (properties.sharedMemPerBlockOptin < smem_size) {
       return false;
     }
 
     return true;
   }
 
   /// Executes one test
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/multistage_testbed_interleaved.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/multistage_testbed_interleaved.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/rank_2k_grouped_scheduler_sm80.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/rank_2k_grouped_scheduler_sm80.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/simt_cgemm_nn_sm50.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/simt_cgemm_nn_sm50.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/simt_cgemm_nt_sm50.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/simt_cgemm_nt_sm50.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/simt_cgemm_nt_sm80.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/simt_cgemm_nt_sm80.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/simt_cgemm_tn_sm50.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/simt_cgemm_tn_sm50.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/simt_cgemm_tn_sm80.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/simt_cgemm_tn_sm80.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/simt_cgemm_tt_sm50.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/simt_cgemm_tt_sm50.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/simt_dgemm_nn_sm50.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/simt_dgemm_nn_sm50.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/simt_dgemm_nt_sm50.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/simt_dgemm_nt_sm50.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/simt_dgemm_tn_sm50.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/simt_dgemm_tn_sm50.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/simt_dgemm_tt_sm50.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/simt_dgemm_tt_sm50.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/simt_f8gemm_tn_sm50.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/simt_f8gemm_tn_sm50.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/simt_hgemm_nn_sm50.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/simt_hgemm_nn_sm50.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/simt_hgemm_nt_sm50.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/simt_hgemm_nt_sm50.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/simt_hgemm_tn_sm50.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/simt_hgemm_tn_sm50.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/simt_hgemm_tt_sm50.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/simt_hgemm_tt_sm50.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/simt_igemm_nn_sm50.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/simt_igemm_nn_sm50.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/simt_igemm_nt_sm50.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/simt_igemm_nt_sm50.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/simt_igemm_tn_sm50.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/simt_igemm_tn_sm50.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/simt_igemm_tt_sm50.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/simt_igemm_tt_sm50.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/simt_int8_igemm_sm61.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/simt_int8_igemm_sm61.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/simt_int8_igemm_sm61_perf.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/simt_int8_igemm_sm61_perf.cu`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/simt_int8_igemm_sm61_sliced_k.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/simt_int8_igemm_sm61_sliced_k.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/simt_qgemm_nn_sm50.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/simt_qgemm_nn_sm50.cu`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -72,15 +72,15 @@
         cutlass::arch::OpClassSimt,
         cutlass::arch::Sm50,
         ThreadblockShape, WarpShape, InstructionShape,
         EpilogueOutputOp,
         cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>,
         2 // Stages
     >;
-    EXPECT_TRUE(test::gemm::device::TestAllGemm<Gemm>());
+    EXPECT_TRUE(test::gemm::device::TestAllGemmBasic<Gemm>());
 } )
 
 ////////////////////////////////////////////////////////////////////////////////
 // Elements / Thread:   4 x   4
 //    Threads / Warp:   4 x   8
 //     Warps / Block:   1 x   1
 //       Threadblock:  16 x  32 x  8
@@ -102,15 +102,15 @@
         cutlass::arch::OpClassSimt,
         cutlass::arch::Sm50,
         ThreadblockShape, WarpShape, InstructionShape,
         EpilogueOutputOp,
         cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>,
         2 // Stages
     >;
-    EXPECT_TRUE(test::gemm::device::TestAllGemm<Gemm>());
+    EXPECT_TRUE(test::gemm::device::TestAllGemmBasic<Gemm>());
 } )
 
 ////////////////////////////////////////////////////////////////////////////////
 // Elements / Thread:   2 x   2
 //    Threads / Warp:   4 x   8
 //     Warps / Block:   1 x   2
 //       Threadblock:   8 x  32 x  8
@@ -132,15 +132,15 @@
         cutlass::arch::OpClassSimt,
         cutlass::arch::Sm50,
         ThreadblockShape, WarpShape, InstructionShape,
         EpilogueOutputOp,
         cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>,
         2 // Stages
     >;
-    EXPECT_TRUE(test::gemm::device::TestAllGemm<Gemm>());
+    EXPECT_TRUE(test::gemm::device::TestAllGemmBasic<Gemm>());
 } )
 
 ////////////////////////////////////////////////////////////////////////////////
 // Elements / Thread:   2 x   4
 //    Threads / Warp:   4 x   8
 //     Warps / Block:   1 x   2
 //       Threadblock:   8 x  64 x  8
@@ -162,15 +162,15 @@
         cutlass::arch::OpClassSimt,
         cutlass::arch::Sm50,
         ThreadblockShape, WarpShape, InstructionShape,
         EpilogueOutputOp,
         cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>,
         2 // Stages
     >;
-    EXPECT_TRUE(test::gemm::device::TestAllGemm<Gemm>());
+    EXPECT_TRUE(test::gemm::device::TestAllGemmBasic<Gemm>());
 } )
 
 ////////////////////////////////////////////////////////////////////////////////
 // Elements / Thread:   4 x   2
 //    Threads / Warp:   4 x   8
 //     Warps / Block:   1 x   2
 //       Threadblock:  16 x  32 x  8
@@ -192,15 +192,15 @@
         cutlass::arch::OpClassSimt,
         cutlass::arch::Sm50,
         ThreadblockShape, WarpShape, InstructionShape,
         EpilogueOutputOp,
         cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>,
         2 // Stages
     >;
-    EXPECT_TRUE(test::gemm::device::TestAllGemm<Gemm>());
+    EXPECT_TRUE(test::gemm::device::TestAllGemmBasic<Gemm>());
 } )
 
 ////////////////////////////////////////////////////////////////////////////////
 // Elements / Thread:   4 x   4
 //    Threads / Warp:   4 x   8
 //     Warps / Block:   1 x   2
 //       Threadblock:  16 x  64 x  8
@@ -222,15 +222,15 @@
         cutlass::arch::OpClassSimt,
         cutlass::arch::Sm50,
         ThreadblockShape, WarpShape, InstructionShape,
         EpilogueOutputOp,
         cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>,
         2 // Stages
     >;
-    EXPECT_TRUE(test::gemm::device::TestAllGemm<Gemm>());
+    EXPECT_TRUE(test::gemm::device::TestAllGemmBasic<Gemm>());
 } )
 
 ////////////////////////////////////////////////////////////////////////////////
 // Elements / Thread:   4 x   4
 //    Threads / Warp:   8 x   4
 //     Warps / Block:   1 x   2
 //       Threadblock:  32 x  32 x  8
@@ -252,15 +252,15 @@
         cutlass::arch::OpClassSimt,
         cutlass::arch::Sm50,
         ThreadblockShape, WarpShape, InstructionShape,
         EpilogueOutputOp,
         cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>,
         2 // Stages
     >;
-    EXPECT_TRUE(test::gemm::device::TestAllGemm<Gemm>());
+    EXPECT_TRUE(test::gemm::device::TestAllGemmBasic<Gemm>());
 } )
 
 ////////////////////////////////////////////////////////////////////////////////
 // Elements / Thread:   4 x   4
 //    Threads / Warp:   4 x   8
 //     Warps / Block:   2 x   1
 //       Threadblock:  32 x  32 x  8
@@ -282,15 +282,15 @@
         cutlass::arch::OpClassSimt,
         cutlass::arch::Sm50,
         ThreadblockShape, WarpShape, InstructionShape,
         EpilogueOutputOp,
         cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>,
         2 // Stages
     >;
-    EXPECT_TRUE(test::gemm::device::TestAllGemm<Gemm>());
+    EXPECT_TRUE(test::gemm::device::TestAllGemmBasic<Gemm>());
 } )
 
 ////////////////////////////////////////////////////////////////////////////////
 // Elements / Thread:   2 x   2
 //    Threads / Warp:   4 x   8
 //     Warps / Block:   2 x   2
 //       Threadblock:  16 x  32 x  8
@@ -312,15 +312,15 @@
         cutlass::arch::OpClassSimt,
         cutlass::arch::Sm50,
         ThreadblockShape, WarpShape, InstructionShape,
         EpilogueOutputOp,
         cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>,
         2 // Stages
     >;
-    EXPECT_TRUE(test::gemm::device::TestAllGemm<Gemm>());
+    EXPECT_TRUE(test::gemm::device::TestAllGemmBasic<Gemm>());
 } )
 
 ////////////////////////////////////////////////////////////////////////////////
 // Elements / Thread:   2 x   4
 //    Threads / Warp:   4 x   8
 //     Warps / Block:   2 x   2
 //       Threadblock:  16 x  64 x  8
@@ -342,15 +342,15 @@
         cutlass::arch::OpClassSimt,
         cutlass::arch::Sm50,
         ThreadblockShape, WarpShape, InstructionShape,
         EpilogueOutputOp,
         cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>,
         2 // Stages
     >;
-    EXPECT_TRUE(test::gemm::device::TestAllGemm<Gemm>());
+    EXPECT_TRUE(test::gemm::device::TestAllGemmBasic<Gemm>());
 } )
 
 ////////////////////////////////////////////////////////////////////////////////
 // Elements / Thread:   4 x   2
 //    Threads / Warp:   4 x   8
 //     Warps / Block:   2 x   2
 //       Threadblock:  32 x  32 x  8
@@ -372,15 +372,15 @@
         cutlass::arch::OpClassSimt,
         cutlass::arch::Sm50,
         ThreadblockShape, WarpShape, InstructionShape,
         EpilogueOutputOp,
         cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>,
         2 // Stages
     >;
-    EXPECT_TRUE(test::gemm::device::TestAllGemm<Gemm>());
+    EXPECT_TRUE(test::gemm::device::TestAllGemmBasic<Gemm>());
 } )
 
 ////////////////////////////////////////////////////////////////////////////////
 // Elements / Thread:   4 x   4
 //    Threads / Warp:   4 x   8
 //     Warps / Block:   2 x   2
 //       Threadblock:  32 x  64 x  8
@@ -402,15 +402,15 @@
         cutlass::arch::OpClassSimt,
         cutlass::arch::Sm50,
         ThreadblockShape, WarpShape, InstructionShape,
         EpilogueOutputOp,
         cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>,
         2 // Stages
     >;
-    EXPECT_TRUE(test::gemm::device::TestAllGemm<Gemm>());
+    EXPECT_TRUE(test::gemm::device::TestAllGemmBasic<Gemm>());
 } )
 
 ////////////////////////////////////////////////////////////////////////////////
 // Elements / Thread:   4 x   4
 //    Threads / Warp:   8 x   4
 //     Warps / Block:   2 x   2
 //       Threadblock:  64 x  32 x  8
@@ -432,15 +432,15 @@
         cutlass::arch::OpClassSimt,
         cutlass::arch::Sm50,
         ThreadblockShape, WarpShape, InstructionShape,
         EpilogueOutputOp,
         cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>,
         2 // Stages
     >;
-    EXPECT_TRUE(test::gemm::device::TestAllGemm<Gemm>());
+    EXPECT_TRUE(test::gemm::device::TestAllGemmBasic<Gemm>());
 } )
 
 ////////////////////////////////////////////////////////////////////////////////
 // Elements / Thread:   2 x   2
 //    Threads / Warp:   4 x   8
 //     Warps / Block:   2 x   4
 //       Threadblock:  16 x  64 x 16
@@ -462,15 +462,15 @@
         cutlass::arch::OpClassSimt,
         cutlass::arch::Sm50,
         ThreadblockShape, WarpShape, InstructionShape,
         EpilogueOutputOp,
         cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>,
         2 // Stages
     >;
-    EXPECT_TRUE(test::gemm::device::TestAllGemm<Gemm>());
+    EXPECT_TRUE(test::gemm::device::TestAllGemmBasic<Gemm>());
 } )
 
 ////////////////////////////////////////////////////////////////////////////////
 // Elements / Thread:   2 x   2
 //    Threads / Warp:   8 x   4
 //     Warps / Block:   2 x   4
 //       Threadblock:  32 x  32 x  8
@@ -492,15 +492,15 @@
         cutlass::arch::OpClassSimt,
         cutlass::arch::Sm50,
         ThreadblockShape, WarpShape, InstructionShape,
         EpilogueOutputOp,
         cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>,
         2 // Stages
     >;
-    EXPECT_TRUE(test::gemm::device::TestAllGemm<Gemm>());
+    EXPECT_TRUE(test::gemm::device::TestAllGemmBasic<Gemm>());
 } )
 
 ////////////////////////////////////////////////////////////////////////////////
 // Elements / Thread:   4 x   2
 //    Threads / Warp:   4 x   8
 //     Warps / Block:   2 x   4
 //       Threadblock:  32 x  64 x  8
@@ -522,15 +522,15 @@
         cutlass::arch::OpClassSimt,
         cutlass::arch::Sm50,
         ThreadblockShape, WarpShape, InstructionShape,
         EpilogueOutputOp,
         cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>,
         2 // Stages
     >;
-    EXPECT_TRUE(test::gemm::device::TestAllGemm<Gemm>());
+    EXPECT_TRUE(test::gemm::device::TestAllGemmBasic<Gemm>());
 } )
 
 ////////////////////////////////////////////////////////////////////////////////
 // Elements / Thread:   4 x   4
 //    Threads / Warp:   4 x   8
 //     Warps / Block:   2 x   4
 //       Threadblock:  32 x 128 x  8
@@ -552,15 +552,15 @@
         cutlass::arch::OpClassSimt,
         cutlass::arch::Sm50,
         ThreadblockShape, WarpShape, InstructionShape,
         EpilogueOutputOp,
         cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>,
         2 // Stages
     >;
-    EXPECT_TRUE(test::gemm::device::TestAllGemm<Gemm>());
+    EXPECT_TRUE(test::gemm::device::TestAllGemmBasic<Gemm>());
 } )
 
 ////////////////////////////////////////////////////////////////////////////////
 // Elements / Thread:   4 x   4
 //    Threads / Warp:   8 x   4
 //     Warps / Block:   2 x   4
 //       Threadblock:  64 x  64 x  8
@@ -582,15 +582,15 @@
         cutlass::arch::OpClassSimt,
         cutlass::arch::Sm50,
         ThreadblockShape, WarpShape, InstructionShape,
         EpilogueOutputOp,
         cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>,
         2 // Stages
     >;
-    EXPECT_TRUE(test::gemm::device::TestAllGemm<Gemm>());
+    EXPECT_TRUE(test::gemm::device::TestAllGemmBasic<Gemm>());
 } )
 
 ////////////////////////////////////////////////////////////////////////////////
 // Elements / Thread:   2 x   2
 //    Threads / Warp:   4 x   8
 //     Warps / Block:   4 x   2
 //       Threadblock:  32 x  32 x  8
@@ -612,15 +612,15 @@
         cutlass::arch::OpClassSimt,
         cutlass::arch::Sm50,
         ThreadblockShape, WarpShape, InstructionShape,
         EpilogueOutputOp,
         cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>,
         2 // Stages
     >;
-    EXPECT_TRUE(test::gemm::device::TestAllGemm<Gemm>());
+    EXPECT_TRUE(test::gemm::device::TestAllGemmBasic<Gemm>());
 } )
 
 ////////////////////////////////////////////////////////////////////////////////
 // Elements / Thread:   4 x   2
 //    Threads / Warp:   4 x   8
 //     Warps / Block:   4 x   2
 //       Threadblock:  64 x  32 x  8
@@ -642,15 +642,15 @@
         cutlass::arch::OpClassSimt,
         cutlass::arch::Sm50,
         ThreadblockShape, WarpShape, InstructionShape,
         EpilogueOutputOp,
         cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>,
         2 // Stages
     >;
-    EXPECT_TRUE(test::gemm::device::TestAllGemm<Gemm>());
+    EXPECT_TRUE(test::gemm::device::TestAllGemmBasic<Gemm>());
 } )
 
 ////////////////////////////////////////////////////////////////////////////////
 // Elements / Thread:   4 x   4
 //    Threads / Warp:   4 x   8
 //     Warps / Block:   4 x   2
 //       Threadblock:  64 x  64 x  8
@@ -672,15 +672,15 @@
         cutlass::arch::OpClassSimt,
         cutlass::arch::Sm50,
         ThreadblockShape, WarpShape, InstructionShape,
         EpilogueOutputOp,
         cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>,
         2 // Stages
     >;
-    EXPECT_TRUE(test::gemm::device::TestAllGemm<Gemm>());
+    EXPECT_TRUE(test::gemm::device::TestAllGemmBasic<Gemm>());
 } )
 
 ////////////////////////////////////////////////////////////////////////////////
 // Elements / Thread:   4 x   4
 //    Threads / Warp:   8 x   4
 //     Warps / Block:   4 x   2
 //       Threadblock: 128 x  32 x  8
@@ -702,15 +702,15 @@
         cutlass::arch::OpClassSimt,
         cutlass::arch::Sm50,
         ThreadblockShape, WarpShape, InstructionShape,
         EpilogueOutputOp,
         cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>,
         2 // Stages
     >;
-    EXPECT_TRUE(test::gemm::device::TestAllGemm<Gemm>());
+    EXPECT_TRUE(test::gemm::device::TestAllGemmBasic<Gemm>());
 } )
 
 ////////////////////////////////////////////////////////////////////////////////
 // Elements / Thread:   2 x   2
 //    Threads / Warp:   4 x   8
 //     Warps / Block:   4 x   4
 //       Threadblock:  32 x  64 x 16
@@ -732,15 +732,15 @@
         cutlass::arch::OpClassSimt,
         cutlass::arch::Sm50,
         ThreadblockShape, WarpShape, InstructionShape,
         EpilogueOutputOp,
         cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>,
         2 // Stages
     >;
-    EXPECT_TRUE(test::gemm::device::TestAllGemm<Gemm>());
+    EXPECT_TRUE(test::gemm::device::TestAllGemmBasic<Gemm>());
 } )
 
 ////////////////////////////////////////////////////////////////////////////////
 // Elements / Thread:   2 x   2
 //    Threads / Warp:   8 x   4
 //     Warps / Block:   4 x   4
 //       Threadblock:  64 x  32 x 16
@@ -762,15 +762,15 @@
         cutlass::arch::OpClassSimt,
         cutlass::arch::Sm50,
         ThreadblockShape, WarpShape, InstructionShape,
         EpilogueOutputOp,
         cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>,
         2 // Stages
     >;
-    EXPECT_TRUE(test::gemm::device::TestAllGemm<Gemm>());
+    EXPECT_TRUE(test::gemm::device::TestAllGemmBasic<Gemm>());
 } )
 
 ////////////////////////////////////////////////////////////////////////////////
 // Elements / Thread:   4 x   2
 //    Threads / Warp:   4 x   8
 //     Warps / Block:   4 x   4
 //       Threadblock:  64 x  64 x  8
@@ -792,15 +792,15 @@
         cutlass::arch::OpClassSimt,
         cutlass::arch::Sm50,
         ThreadblockShape, WarpShape, InstructionShape,
         EpilogueOutputOp,
         cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>,
         2 // Stages
     >;
-    EXPECT_TRUE(test::gemm::device::TestAllGemm<Gemm>());
+    EXPECT_TRUE(test::gemm::device::TestAllGemmBasic<Gemm>());
 } )
 
 ////////////////////////////////////////////////////////////////////////////////
 // Elements / Thread:   4 x   4
 //    Threads / Warp:   4 x   8
 //     Warps / Block:   4 x   4
 //       Threadblock:  64 x 128 x  8
@@ -822,15 +822,15 @@
         cutlass::arch::OpClassSimt,
         cutlass::arch::Sm50,
         ThreadblockShape, WarpShape, InstructionShape,
         EpilogueOutputOp,
         cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>,
         2 // Stages
     >;
-    EXPECT_TRUE(test::gemm::device::TestAllGemm<Gemm>());
+    EXPECT_TRUE(test::gemm::device::TestAllGemmBasic<Gemm>());
 } )
 
 ////////////////////////////////////////////////////////////////////////////////
 // Elements / Thread:   4 x   4
 //    Threads / Warp:   8 x   4
 //     Warps / Block:   4 x   4
 //       Threadblock: 128 x  64 x  8
@@ -852,10 +852,10 @@
         cutlass::arch::OpClassSimt,
         cutlass::arch::Sm50,
         ThreadblockShape, WarpShape, InstructionShape,
         EpilogueOutputOp,
         cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>,
         2 // Stages
     >;
-    EXPECT_TRUE(test::gemm::device::TestAllGemm<Gemm>());
+    EXPECT_TRUE(test::gemm::device::TestAllGemmBasic<Gemm>());
 } )
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/simt_qgemm_nt_sm50.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/simt_qgemm_nt_sm50.cu`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -72,15 +72,15 @@
         cutlass::arch::OpClassSimt,
         cutlass::arch::Sm50,
         ThreadblockShape, WarpShape, InstructionShape,
         EpilogueOutputOp,
         cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>,
         2 // Stages
     >;
-    EXPECT_TRUE(test::gemm::device::TestAllGemm<Gemm>());
+    EXPECT_TRUE(test::gemm::device::TestAllGemmBasic<Gemm>());
 } )
 
 ////////////////////////////////////////////////////////////////////////////////
 // Elements / Thread:   4 x   4
 //    Threads / Warp:   4 x   8
 //     Warps / Block:   1 x   1
 //       Threadblock:  16 x  32 x  8
@@ -102,15 +102,15 @@
         cutlass::arch::OpClassSimt,
         cutlass::arch::Sm50,
         ThreadblockShape, WarpShape, InstructionShape,
         EpilogueOutputOp,
         cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>,
         2 // Stages
     >;
-    EXPECT_TRUE(test::gemm::device::TestAllGemm<Gemm>());
+    EXPECT_TRUE(test::gemm::device::TestAllGemmBasic<Gemm>());
 } )
 
 ////////////////////////////////////////////////////////////////////////////////
 // Elements / Thread:   2 x   2
 //    Threads / Warp:   4 x   8
 //     Warps / Block:   1 x   2
 //       Threadblock:   8 x  32 x  8
@@ -132,15 +132,15 @@
         cutlass::arch::OpClassSimt,
         cutlass::arch::Sm50,
         ThreadblockShape, WarpShape, InstructionShape,
         EpilogueOutputOp,
         cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>,
         2 // Stages
     >;
-    EXPECT_TRUE(test::gemm::device::TestAllGemm<Gemm>());
+    EXPECT_TRUE(test::gemm::device::TestAllGemmBasic<Gemm>());
 } )
 
 ////////////////////////////////////////////////////////////////////////////////
 // Elements / Thread:   2 x   4
 //    Threads / Warp:   4 x   8
 //     Warps / Block:   1 x   2
 //       Threadblock:   8 x  64 x  8
@@ -162,15 +162,15 @@
         cutlass::arch::OpClassSimt,
         cutlass::arch::Sm50,
         ThreadblockShape, WarpShape, InstructionShape,
         EpilogueOutputOp,
         cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>,
         2 // Stages
     >;
-    EXPECT_TRUE(test::gemm::device::TestAllGemm<Gemm>());
+    EXPECT_TRUE(test::gemm::device::TestAllGemmBasic<Gemm>());
 } )
 
 ////////////////////////////////////////////////////////////////////////////////
 // Elements / Thread:   4 x   2
 //    Threads / Warp:   4 x   8
 //     Warps / Block:   1 x   2
 //       Threadblock:  16 x  32 x  8
@@ -192,15 +192,15 @@
         cutlass::arch::OpClassSimt,
         cutlass::arch::Sm50,
         ThreadblockShape, WarpShape, InstructionShape,
         EpilogueOutputOp,
         cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>,
         2 // Stages
     >;
-    EXPECT_TRUE(test::gemm::device::TestAllGemm<Gemm>());
+    EXPECT_TRUE(test::gemm::device::TestAllGemmBasic<Gemm>());
 } )
 
 ////////////////////////////////////////////////////////////////////////////////
 // Elements / Thread:   4 x   4
 //    Threads / Warp:   4 x   8
 //     Warps / Block:   1 x   2
 //       Threadblock:  16 x  64 x  8
@@ -222,15 +222,15 @@
         cutlass::arch::OpClassSimt,
         cutlass::arch::Sm50,
         ThreadblockShape, WarpShape, InstructionShape,
         EpilogueOutputOp,
         cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>,
         2 // Stages
     >;
-    EXPECT_TRUE(test::gemm::device::TestAllGemm<Gemm>());
+    EXPECT_TRUE(test::gemm::device::TestAllGemmBasic<Gemm>());
 } )
 
 ////////////////////////////////////////////////////////////////////////////////
 // Elements / Thread:   4 x   4
 //    Threads / Warp:   8 x   4
 //     Warps / Block:   1 x   2
 //       Threadblock:  32 x  32 x  8
@@ -252,15 +252,15 @@
         cutlass::arch::OpClassSimt,
         cutlass::arch::Sm50,
         ThreadblockShape, WarpShape, InstructionShape,
         EpilogueOutputOp,
         cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>,
         2 // Stages
     >;
-    EXPECT_TRUE(test::gemm::device::TestAllGemm<Gemm>());
+    EXPECT_TRUE(test::gemm::device::TestAllGemmBasic<Gemm>());
 } )
 
 ////////////////////////////////////////////////////////////////////////////////
 // Elements / Thread:   4 x   4
 //    Threads / Warp:   4 x   8
 //     Warps / Block:   2 x   1
 //       Threadblock:  32 x  32 x  8
@@ -282,15 +282,15 @@
         cutlass::arch::OpClassSimt,
         cutlass::arch::Sm50,
         ThreadblockShape, WarpShape, InstructionShape,
         EpilogueOutputOp,
         cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>,
         2 // Stages
     >;
-    EXPECT_TRUE(test::gemm::device::TestAllGemm<Gemm>());
+    EXPECT_TRUE(test::gemm::device::TestAllGemmBasic<Gemm>());
 } )
 
 ////////////////////////////////////////////////////////////////////////////////
 // Elements / Thread:   2 x   2
 //    Threads / Warp:   4 x   8
 //     Warps / Block:   2 x   2
 //       Threadblock:  16 x  32 x  8
@@ -312,15 +312,15 @@
         cutlass::arch::OpClassSimt,
         cutlass::arch::Sm50,
         ThreadblockShape, WarpShape, InstructionShape,
         EpilogueOutputOp,
         cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>,
         2 // Stages
     >;
-    EXPECT_TRUE(test::gemm::device::TestAllGemm<Gemm>());
+    EXPECT_TRUE(test::gemm::device::TestAllGemmBasic<Gemm>());
 } )
 
 ////////////////////////////////////////////////////////////////////////////////
 // Elements / Thread:   2 x   4
 //    Threads / Warp:   4 x   8
 //     Warps / Block:   2 x   2
 //       Threadblock:  16 x  64 x  8
@@ -342,15 +342,15 @@
         cutlass::arch::OpClassSimt,
         cutlass::arch::Sm50,
         ThreadblockShape, WarpShape, InstructionShape,
         EpilogueOutputOp,
         cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>,
         2 // Stages
     >;
-    EXPECT_TRUE(test::gemm::device::TestAllGemm<Gemm>());
+    EXPECT_TRUE(test::gemm::device::TestAllGemmBasic<Gemm>());
 } )
 
 ////////////////////////////////////////////////////////////////////////////////
 // Elements / Thread:   4 x   2
 //    Threads / Warp:   4 x   8
 //     Warps / Block:   2 x   2
 //       Threadblock:  32 x  32 x  8
@@ -372,15 +372,15 @@
         cutlass::arch::OpClassSimt,
         cutlass::arch::Sm50,
         ThreadblockShape, WarpShape, InstructionShape,
         EpilogueOutputOp,
         cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>,
         2 // Stages
     >;
-    EXPECT_TRUE(test::gemm::device::TestAllGemm<Gemm>());
+    EXPECT_TRUE(test::gemm::device::TestAllGemmBasic<Gemm>());
 } )
 
 ////////////////////////////////////////////////////////////////////////////////
 // Elements / Thread:   4 x   4
 //    Threads / Warp:   4 x   8
 //     Warps / Block:   2 x   2
 //       Threadblock:  32 x  64 x  8
@@ -402,15 +402,15 @@
         cutlass::arch::OpClassSimt,
         cutlass::arch::Sm50,
         ThreadblockShape, WarpShape, InstructionShape,
         EpilogueOutputOp,
         cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>,
         2 // Stages
     >;
-    EXPECT_TRUE(test::gemm::device::TestAllGemm<Gemm>());
+    EXPECT_TRUE(test::gemm::device::TestAllGemmBasic<Gemm>());
 } )
 
 ////////////////////////////////////////////////////////////////////////////////
 // Elements / Thread:   4 x   4
 //    Threads / Warp:   8 x   4
 //     Warps / Block:   2 x   2
 //       Threadblock:  64 x  32 x  8
@@ -432,15 +432,15 @@
         cutlass::arch::OpClassSimt,
         cutlass::arch::Sm50,
         ThreadblockShape, WarpShape, InstructionShape,
         EpilogueOutputOp,
         cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>,
         2 // Stages
     >;
-    EXPECT_TRUE(test::gemm::device::TestAllGemm<Gemm>());
+    EXPECT_TRUE(test::gemm::device::TestAllGemmBasic<Gemm>());
 } )
 
 ////////////////////////////////////////////////////////////////////////////////
 // Elements / Thread:   2 x   2
 //    Threads / Warp:   4 x   8
 //     Warps / Block:   2 x   4
 //       Threadblock:  16 x  64 x 16
@@ -462,15 +462,15 @@
         cutlass::arch::OpClassSimt,
         cutlass::arch::Sm50,
         ThreadblockShape, WarpShape, InstructionShape,
         EpilogueOutputOp,
         cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>,
         2 // Stages
     >;
-    EXPECT_TRUE(test::gemm::device::TestAllGemm<Gemm>());
+    EXPECT_TRUE(test::gemm::device::TestAllGemmBasic<Gemm>());
 } )
 
 ////////////////////////////////////////////////////////////////////////////////
 // Elements / Thread:   2 x   2
 //    Threads / Warp:   8 x   4
 //     Warps / Block:   2 x   4
 //       Threadblock:  32 x  32 x  8
@@ -492,15 +492,15 @@
         cutlass::arch::OpClassSimt,
         cutlass::arch::Sm50,
         ThreadblockShape, WarpShape, InstructionShape,
         EpilogueOutputOp,
         cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>,
         2 // Stages
     >;
-    EXPECT_TRUE(test::gemm::device::TestAllGemm<Gemm>());
+    EXPECT_TRUE(test::gemm::device::TestAllGemmBasic<Gemm>());
 } )
 
 ////////////////////////////////////////////////////////////////////////////////
 // Elements / Thread:   4 x   2
 //    Threads / Warp:   4 x   8
 //     Warps / Block:   2 x   4
 //       Threadblock:  32 x  64 x  8
@@ -522,15 +522,15 @@
         cutlass::arch::OpClassSimt,
         cutlass::arch::Sm50,
         ThreadblockShape, WarpShape, InstructionShape,
         EpilogueOutputOp,
         cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>,
         2 // Stages
     >;
-    EXPECT_TRUE(test::gemm::device::TestAllGemm<Gemm>());
+    EXPECT_TRUE(test::gemm::device::TestAllGemmBasic<Gemm>());
 } )
 
 ////////////////////////////////////////////////////////////////////////////////
 // Elements / Thread:   4 x   4
 //    Threads / Warp:   4 x   8
 //     Warps / Block:   2 x   4
 //       Threadblock:  32 x 128 x  8
@@ -552,15 +552,15 @@
         cutlass::arch::OpClassSimt,
         cutlass::arch::Sm50,
         ThreadblockShape, WarpShape, InstructionShape,
         EpilogueOutputOp,
         cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>,
         2 // Stages
     >;
-    EXPECT_TRUE(test::gemm::device::TestAllGemm<Gemm>());
+    EXPECT_TRUE(test::gemm::device::TestAllGemmBasic<Gemm>());
 } )
 
 ////////////////////////////////////////////////////////////////////////////////
 // Elements / Thread:   4 x   4
 //    Threads / Warp:   8 x   4
 //     Warps / Block:   2 x   4
 //       Threadblock:  64 x  64 x  8
@@ -582,15 +582,15 @@
         cutlass::arch::OpClassSimt,
         cutlass::arch::Sm50,
         ThreadblockShape, WarpShape, InstructionShape,
         EpilogueOutputOp,
         cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>,
         2 // Stages
     >;
-    EXPECT_TRUE(test::gemm::device::TestAllGemm<Gemm>());
+    EXPECT_TRUE(test::gemm::device::TestAllGemmBasic<Gemm>());
 } )
 
 ////////////////////////////////////////////////////////////////////////////////
 // Elements / Thread:   2 x   2
 //    Threads / Warp:   4 x   8
 //     Warps / Block:   4 x   2
 //       Threadblock:  32 x  32 x  8
@@ -612,15 +612,15 @@
         cutlass::arch::OpClassSimt,
         cutlass::arch::Sm50,
         ThreadblockShape, WarpShape, InstructionShape,
         EpilogueOutputOp,
         cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>,
         2 // Stages
     >;
-    EXPECT_TRUE(test::gemm::device::TestAllGemm<Gemm>());
+    EXPECT_TRUE(test::gemm::device::TestAllGemmBasic<Gemm>());
 } )
 
 ////////////////////////////////////////////////////////////////////////////////
 // Elements / Thread:   4 x   2
 //    Threads / Warp:   4 x   8
 //     Warps / Block:   4 x   2
 //       Threadblock:  64 x  32 x  8
@@ -642,15 +642,15 @@
         cutlass::arch::OpClassSimt,
         cutlass::arch::Sm50,
         ThreadblockShape, WarpShape, InstructionShape,
         EpilogueOutputOp,
         cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>,
         2 // Stages
     >;
-    EXPECT_TRUE(test::gemm::device::TestAllGemm<Gemm>());
+    EXPECT_TRUE(test::gemm::device::TestAllGemmBasic<Gemm>());
 } )
 
 ////////////////////////////////////////////////////////////////////////////////
 // Elements / Thread:   4 x   4
 //    Threads / Warp:   4 x   8
 //     Warps / Block:   4 x   2
 //       Threadblock:  64 x  64 x  8
@@ -672,15 +672,15 @@
         cutlass::arch::OpClassSimt,
         cutlass::arch::Sm50,
         ThreadblockShape, WarpShape, InstructionShape,
         EpilogueOutputOp,
         cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>,
         2 // Stages
     >;
-    EXPECT_TRUE(test::gemm::device::TestAllGemm<Gemm>());
+    EXPECT_TRUE(test::gemm::device::TestAllGemmBasic<Gemm>());
 } )
 
 ////////////////////////////////////////////////////////////////////////////////
 // Elements / Thread:   4 x   4
 //    Threads / Warp:   8 x   4
 //     Warps / Block:   4 x   2
 //       Threadblock: 128 x  32 x  8
@@ -702,15 +702,15 @@
         cutlass::arch::OpClassSimt,
         cutlass::arch::Sm50,
         ThreadblockShape, WarpShape, InstructionShape,
         EpilogueOutputOp,
         cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>,
         2 // Stages
     >;
-    EXPECT_TRUE(test::gemm::device::TestAllGemm<Gemm>());
+    EXPECT_TRUE(test::gemm::device::TestAllGemmBasic<Gemm>());
 } )
 
 ////////////////////////////////////////////////////////////////////////////////
 // Elements / Thread:   2 x   2
 //    Threads / Warp:   4 x   8
 //     Warps / Block:   4 x   4
 //       Threadblock:  32 x  64 x 16
@@ -732,15 +732,15 @@
         cutlass::arch::OpClassSimt,
         cutlass::arch::Sm50,
         ThreadblockShape, WarpShape, InstructionShape,
         EpilogueOutputOp,
         cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>,
         2 // Stages
     >;
-    EXPECT_TRUE(test::gemm::device::TestAllGemm<Gemm>());
+    EXPECT_TRUE(test::gemm::device::TestAllGemmBasic<Gemm>());
 } )
 
 ////////////////////////////////////////////////////////////////////////////////
 // Elements / Thread:   2 x   2
 //    Threads / Warp:   8 x   4
 //     Warps / Block:   4 x   4
 //       Threadblock:  64 x  32 x 16
@@ -762,15 +762,15 @@
         cutlass::arch::OpClassSimt,
         cutlass::arch::Sm50,
         ThreadblockShape, WarpShape, InstructionShape,
         EpilogueOutputOp,
         cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>,
         2 // Stages
     >;
-    EXPECT_TRUE(test::gemm::device::TestAllGemm<Gemm>());
+    EXPECT_TRUE(test::gemm::device::TestAllGemmBasic<Gemm>());
 } )
 
 ////////////////////////////////////////////////////////////////////////////////
 // Elements / Thread:   4 x   2
 //    Threads / Warp:   4 x   8
 //     Warps / Block:   4 x   4
 //       Threadblock:  64 x  64 x  8
@@ -792,15 +792,15 @@
         cutlass::arch::OpClassSimt,
         cutlass::arch::Sm50,
         ThreadblockShape, WarpShape, InstructionShape,
         EpilogueOutputOp,
         cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>,
         2 // Stages
     >;
-    EXPECT_TRUE(test::gemm::device::TestAllGemm<Gemm>());
+    EXPECT_TRUE(test::gemm::device::TestAllGemmBasic<Gemm>());
 } )
 
 ////////////////////////////////////////////////////////////////////////////////
 // Elements / Thread:   4 x   4
 //    Threads / Warp:   4 x   8
 //     Warps / Block:   4 x   4
 //       Threadblock:  64 x 128 x  8
@@ -822,15 +822,15 @@
         cutlass::arch::OpClassSimt,
         cutlass::arch::Sm50,
         ThreadblockShape, WarpShape, InstructionShape,
         EpilogueOutputOp,
         cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>,
         2 // Stages
     >;
-    EXPECT_TRUE(test::gemm::device::TestAllGemm<Gemm>());
+    EXPECT_TRUE(test::gemm::device::TestAllGemmBasic<Gemm>());
 } )
 
 ////////////////////////////////////////////////////////////////////////////////
 // Elements / Thread:   4 x   4
 //    Threads / Warp:   8 x   4
 //     Warps / Block:   4 x   4
 //       Threadblock: 128 x  64 x  8
@@ -852,10 +852,10 @@
         cutlass::arch::OpClassSimt,
         cutlass::arch::Sm50,
         ThreadblockShape, WarpShape, InstructionShape,
         EpilogueOutputOp,
         cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>,
         2 // Stages
     >;
-    EXPECT_TRUE(test::gemm::device::TestAllGemm<Gemm>());
+    EXPECT_TRUE(test::gemm::device::TestAllGemmBasic<Gemm>());
 } )
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/simt_qgemm_tn_sm50.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/simt_qgemm_tn_sm50.cu`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -72,15 +72,15 @@
         cutlass::arch::OpClassSimt,
         cutlass::arch::Sm50,
         ThreadblockShape, WarpShape, InstructionShape,
         EpilogueOutputOp,
         cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>,
         2 // Stages
     >;
-    EXPECT_TRUE(test::gemm::device::TestAllGemm<Gemm>());
+    EXPECT_TRUE(test::gemm::device::TestAllGemmBasic<Gemm>());
 } )
 
 ////////////////////////////////////////////////////////////////////////////////
 // Elements / Thread:   4 x   4
 //    Threads / Warp:   4 x   8
 //     Warps / Block:   1 x   1
 //       Threadblock:  16 x  32 x  8
@@ -102,15 +102,15 @@
         cutlass::arch::OpClassSimt,
         cutlass::arch::Sm50,
         ThreadblockShape, WarpShape, InstructionShape,
         EpilogueOutputOp,
         cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>,
         2 // Stages
     >;
-    EXPECT_TRUE(test::gemm::device::TestAllGemm<Gemm>());
+    EXPECT_TRUE(test::gemm::device::TestAllGemmBasic<Gemm>());
 } )
 
 ////////////////////////////////////////////////////////////////////////////////
 // Elements / Thread:   2 x   2
 //    Threads / Warp:   4 x   8
 //     Warps / Block:   1 x   2
 //       Threadblock:   8 x  32 x  8
@@ -132,15 +132,15 @@
         cutlass::arch::OpClassSimt,
         cutlass::arch::Sm50,
         ThreadblockShape, WarpShape, InstructionShape,
         EpilogueOutputOp,
         cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>,
         2 // Stages
     >;
-    EXPECT_TRUE(test::gemm::device::TestAllGemm<Gemm>());
+    EXPECT_TRUE(test::gemm::device::TestAllGemmBasic<Gemm>());
 } )
 
 ////////////////////////////////////////////////////////////////////////////////
 // Elements / Thread:   2 x   4
 //    Threads / Warp:   4 x   8
 //     Warps / Block:   1 x   2
 //       Threadblock:   8 x  64 x  8
@@ -162,15 +162,15 @@
         cutlass::arch::OpClassSimt,
         cutlass::arch::Sm50,
         ThreadblockShape, WarpShape, InstructionShape,
         EpilogueOutputOp,
         cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>,
         2 // Stages
     >;
-    EXPECT_TRUE(test::gemm::device::TestAllGemm<Gemm>());
+    EXPECT_TRUE(test::gemm::device::TestAllGemmBasic<Gemm>());
 } )
 
 ////////////////////////////////////////////////////////////////////////////////
 // Elements / Thread:   4 x   2
 //    Threads / Warp:   4 x   8
 //     Warps / Block:   1 x   2
 //       Threadblock:  16 x  32 x  8
@@ -192,15 +192,15 @@
         cutlass::arch::OpClassSimt,
         cutlass::arch::Sm50,
         ThreadblockShape, WarpShape, InstructionShape,
         EpilogueOutputOp,
         cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>,
         2 // Stages
     >;
-    EXPECT_TRUE(test::gemm::device::TestAllGemm<Gemm>());
+    EXPECT_TRUE(test::gemm::device::TestAllGemmBasic<Gemm>());
 } )
 
 ////////////////////////////////////////////////////////////////////////////////
 // Elements / Thread:   4 x   4
 //    Threads / Warp:   4 x   8
 //     Warps / Block:   1 x   2
 //       Threadblock:  16 x  64 x  8
@@ -222,15 +222,15 @@
         cutlass::arch::OpClassSimt,
         cutlass::arch::Sm50,
         ThreadblockShape, WarpShape, InstructionShape,
         EpilogueOutputOp,
         cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>,
         2 // Stages
     >;
-    EXPECT_TRUE(test::gemm::device::TestAllGemm<Gemm>());
+    EXPECT_TRUE(test::gemm::device::TestAllGemmBasic<Gemm>());
 } )
 
 ////////////////////////////////////////////////////////////////////////////////
 // Elements / Thread:   4 x   4
 //    Threads / Warp:   8 x   4
 //     Warps / Block:   1 x   2
 //       Threadblock:  32 x  32 x  8
@@ -252,15 +252,15 @@
         cutlass::arch::OpClassSimt,
         cutlass::arch::Sm50,
         ThreadblockShape, WarpShape, InstructionShape,
         EpilogueOutputOp,
         cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>,
         2 // Stages
     >;
-    EXPECT_TRUE(test::gemm::device::TestAllGemm<Gemm>());
+    EXPECT_TRUE(test::gemm::device::TestAllGemmBasic<Gemm>());
 } )
 
 ////////////////////////////////////////////////////////////////////////////////
 // Elements / Thread:   4 x   4
 //    Threads / Warp:   4 x   8
 //     Warps / Block:   2 x   1
 //       Threadblock:  32 x  32 x  8
@@ -282,15 +282,15 @@
         cutlass::arch::OpClassSimt,
         cutlass::arch::Sm50,
         ThreadblockShape, WarpShape, InstructionShape,
         EpilogueOutputOp,
         cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>,
         2 // Stages
     >;
-    EXPECT_TRUE(test::gemm::device::TestAllGemm<Gemm>());
+    EXPECT_TRUE(test::gemm::device::TestAllGemmBasic<Gemm>());
 } )
 
 ////////////////////////////////////////////////////////////////////////////////
 // Elements / Thread:   2 x   2
 //    Threads / Warp:   4 x   8
 //     Warps / Block:   2 x   2
 //       Threadblock:  16 x  32 x  8
@@ -312,15 +312,15 @@
         cutlass::arch::OpClassSimt,
         cutlass::arch::Sm50,
         ThreadblockShape, WarpShape, InstructionShape,
         EpilogueOutputOp,
         cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>,
         2 // Stages
     >;
-    EXPECT_TRUE(test::gemm::device::TestAllGemm<Gemm>());
+    EXPECT_TRUE(test::gemm::device::TestAllGemmBasic<Gemm>());
 } )
 
 ////////////////////////////////////////////////////////////////////////////////
 // Elements / Thread:   2 x   4
 //    Threads / Warp:   4 x   8
 //     Warps / Block:   2 x   2
 //       Threadblock:  16 x  64 x  8
@@ -342,15 +342,15 @@
         cutlass::arch::OpClassSimt,
         cutlass::arch::Sm50,
         ThreadblockShape, WarpShape, InstructionShape,
         EpilogueOutputOp,
         cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>,
         2 // Stages
     >;
-    EXPECT_TRUE(test::gemm::device::TestAllGemm<Gemm>());
+    EXPECT_TRUE(test::gemm::device::TestAllGemmBasic<Gemm>());
 } )
 
 ////////////////////////////////////////////////////////////////////////////////
 // Elements / Thread:   4 x   2
 //    Threads / Warp:   4 x   8
 //     Warps / Block:   2 x   2
 //       Threadblock:  32 x  32 x  8
@@ -372,15 +372,15 @@
         cutlass::arch::OpClassSimt,
         cutlass::arch::Sm50,
         ThreadblockShape, WarpShape, InstructionShape,
         EpilogueOutputOp,
         cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>,
         2 // Stages
     >;
-    EXPECT_TRUE(test::gemm::device::TestAllGemm<Gemm>());
+    EXPECT_TRUE(test::gemm::device::TestAllGemmBasic<Gemm>());
 } )
 
 ////////////////////////////////////////////////////////////////////////////////
 // Elements / Thread:   4 x   4
 //    Threads / Warp:   4 x   8
 //     Warps / Block:   2 x   2
 //       Threadblock:  32 x  64 x  8
@@ -402,15 +402,15 @@
         cutlass::arch::OpClassSimt,
         cutlass::arch::Sm50,
         ThreadblockShape, WarpShape, InstructionShape,
         EpilogueOutputOp,
         cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>,
         2 // Stages
     >;
-    EXPECT_TRUE(test::gemm::device::TestAllGemm<Gemm>());
+    EXPECT_TRUE(test::gemm::device::TestAllGemmBasic<Gemm>());
 } )
 
 ////////////////////////////////////////////////////////////////////////////////
 // Elements / Thread:   4 x   4
 //    Threads / Warp:   8 x   4
 //     Warps / Block:   2 x   2
 //       Threadblock:  64 x  32 x  8
@@ -432,15 +432,15 @@
         cutlass::arch::OpClassSimt,
         cutlass::arch::Sm50,
         ThreadblockShape, WarpShape, InstructionShape,
         EpilogueOutputOp,
         cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>,
         2 // Stages
     >;
-    EXPECT_TRUE(test::gemm::device::TestAllGemm<Gemm>());
+    EXPECT_TRUE(test::gemm::device::TestAllGemmBasic<Gemm>());
 } )
 
 ////////////////////////////////////////////////////////////////////////////////
 // Elements / Thread:   2 x   2
 //    Threads / Warp:   4 x   8
 //     Warps / Block:   2 x   4
 //       Threadblock:  16 x  64 x 16
@@ -462,15 +462,15 @@
         cutlass::arch::OpClassSimt,
         cutlass::arch::Sm50,
         ThreadblockShape, WarpShape, InstructionShape,
         EpilogueOutputOp,
         cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>,
         2 // Stages
     >;
-    EXPECT_TRUE(test::gemm::device::TestAllGemm<Gemm>());
+    EXPECT_TRUE(test::gemm::device::TestAllGemmBasic<Gemm>());
 } )
 
 ////////////////////////////////////////////////////////////////////////////////
 // Elements / Thread:   2 x   2
 //    Threads / Warp:   8 x   4
 //     Warps / Block:   2 x   4
 //       Threadblock:  32 x  32 x  8
@@ -492,15 +492,15 @@
         cutlass::arch::OpClassSimt,
         cutlass::arch::Sm50,
         ThreadblockShape, WarpShape, InstructionShape,
         EpilogueOutputOp,
         cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>,
         2 // Stages
     >;
-    EXPECT_TRUE(test::gemm::device::TestAllGemm<Gemm>());
+    EXPECT_TRUE(test::gemm::device::TestAllGemmBasic<Gemm>());
 } )
 
 ////////////////////////////////////////////////////////////////////////////////
 // Elements / Thread:   4 x   2
 //    Threads / Warp:   4 x   8
 //     Warps / Block:   2 x   4
 //       Threadblock:  32 x  64 x  8
@@ -522,15 +522,15 @@
         cutlass::arch::OpClassSimt,
         cutlass::arch::Sm50,
         ThreadblockShape, WarpShape, InstructionShape,
         EpilogueOutputOp,
         cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>,
         2 // Stages
     >;
-    EXPECT_TRUE(test::gemm::device::TestAllGemm<Gemm>());
+    EXPECT_TRUE(test::gemm::device::TestAllGemmBasic<Gemm>());
 } )
 
 ////////////////////////////////////////////////////////////////////////////////
 // Elements / Thread:   4 x   4
 //    Threads / Warp:   4 x   8
 //     Warps / Block:   2 x   4
 //       Threadblock:  32 x 128 x  8
@@ -552,15 +552,15 @@
         cutlass::arch::OpClassSimt,
         cutlass::arch::Sm50,
         ThreadblockShape, WarpShape, InstructionShape,
         EpilogueOutputOp,
         cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>,
         2 // Stages
     >;
-    EXPECT_TRUE(test::gemm::device::TestAllGemm<Gemm>());
+    EXPECT_TRUE(test::gemm::device::TestAllGemmBasic<Gemm>());
 } )
 
 ////////////////////////////////////////////////////////////////////////////////
 // Elements / Thread:   4 x   4
 //    Threads / Warp:   8 x   4
 //     Warps / Block:   2 x   4
 //       Threadblock:  64 x  64 x  8
@@ -582,15 +582,15 @@
         cutlass::arch::OpClassSimt,
         cutlass::arch::Sm50,
         ThreadblockShape, WarpShape, InstructionShape,
         EpilogueOutputOp,
         cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>,
         2 // Stages
     >;
-    EXPECT_TRUE(test::gemm::device::TestAllGemm<Gemm>());
+    EXPECT_TRUE(test::gemm::device::TestAllGemmBasic<Gemm>());
 } )
 
 ////////////////////////////////////////////////////////////////////////////////
 // Elements / Thread:   2 x   2
 //    Threads / Warp:   4 x   8
 //     Warps / Block:   4 x   2
 //       Threadblock:  32 x  32 x  8
@@ -612,15 +612,15 @@
         cutlass::arch::OpClassSimt,
         cutlass::arch::Sm50,
         ThreadblockShape, WarpShape, InstructionShape,
         EpilogueOutputOp,
         cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>,
         2 // Stages
     >;
-    EXPECT_TRUE(test::gemm::device::TestAllGemm<Gemm>());
+    EXPECT_TRUE(test::gemm::device::TestAllGemmBasic<Gemm>());
 } )
 
 ////////////////////////////////////////////////////////////////////////////////
 // Elements / Thread:   4 x   2
 //    Threads / Warp:   4 x   8
 //     Warps / Block:   4 x   2
 //       Threadblock:  64 x  32 x  8
@@ -642,15 +642,15 @@
         cutlass::arch::OpClassSimt,
         cutlass::arch::Sm50,
         ThreadblockShape, WarpShape, InstructionShape,
         EpilogueOutputOp,
         cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>,
         2 // Stages
     >;
-    EXPECT_TRUE(test::gemm::device::TestAllGemm<Gemm>());
+    EXPECT_TRUE(test::gemm::device::TestAllGemmBasic<Gemm>());
 } )
 
 ////////////////////////////////////////////////////////////////////////////////
 // Elements / Thread:   4 x   4
 //    Threads / Warp:   4 x   8
 //     Warps / Block:   4 x   2
 //       Threadblock:  64 x  64 x  8
@@ -672,15 +672,15 @@
         cutlass::arch::OpClassSimt,
         cutlass::arch::Sm50,
         ThreadblockShape, WarpShape, InstructionShape,
         EpilogueOutputOp,
         cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>,
         2 // Stages
     >;
-    EXPECT_TRUE(test::gemm::device::TestAllGemm<Gemm>());
+    EXPECT_TRUE(test::gemm::device::TestAllGemmBasic<Gemm>());
 } )
 
 ////////////////////////////////////////////////////////////////////////////////
 // Elements / Thread:   4 x   4
 //    Threads / Warp:   8 x   4
 //     Warps / Block:   4 x   2
 //       Threadblock: 128 x  32 x  8
@@ -702,15 +702,15 @@
         cutlass::arch::OpClassSimt,
         cutlass::arch::Sm50,
         ThreadblockShape, WarpShape, InstructionShape,
         EpilogueOutputOp,
         cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>,
         2 // Stages
     >;
-    EXPECT_TRUE(test::gemm::device::TestAllGemm<Gemm>());
+    EXPECT_TRUE(test::gemm::device::TestAllGemmBasic<Gemm>());
 } )
 
 ////////////////////////////////////////////////////////////////////////////////
 // Elements / Thread:   2 x   2
 //    Threads / Warp:   4 x   8
 //     Warps / Block:   4 x   4
 //       Threadblock:  32 x  64 x 16
@@ -732,15 +732,15 @@
         cutlass::arch::OpClassSimt,
         cutlass::arch::Sm50,
         ThreadblockShape, WarpShape, InstructionShape,
         EpilogueOutputOp,
         cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>,
         2 // Stages
     >;
-    EXPECT_TRUE(test::gemm::device::TestAllGemm<Gemm>());
+    EXPECT_TRUE(test::gemm::device::TestAllGemmBasic<Gemm>());
 } )
 
 ////////////////////////////////////////////////////////////////////////////////
 // Elements / Thread:   2 x   2
 //    Threads / Warp:   8 x   4
 //     Warps / Block:   4 x   4
 //       Threadblock:  64 x  32 x 16
@@ -762,15 +762,15 @@
         cutlass::arch::OpClassSimt,
         cutlass::arch::Sm50,
         ThreadblockShape, WarpShape, InstructionShape,
         EpilogueOutputOp,
         cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>,
         2 // Stages
     >;
-    EXPECT_TRUE(test::gemm::device::TestAllGemm<Gemm>());
+    EXPECT_TRUE(test::gemm::device::TestAllGemmBasic<Gemm>());
 } )
 
 ////////////////////////////////////////////////////////////////////////////////
 // Elements / Thread:   4 x   2
 //    Threads / Warp:   4 x   8
 //     Warps / Block:   4 x   4
 //       Threadblock:  64 x  64 x  8
@@ -792,15 +792,15 @@
         cutlass::arch::OpClassSimt,
         cutlass::arch::Sm50,
         ThreadblockShape, WarpShape, InstructionShape,
         EpilogueOutputOp,
         cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>,
         2 // Stages
     >;
-    EXPECT_TRUE(test::gemm::device::TestAllGemm<Gemm>());
+    EXPECT_TRUE(test::gemm::device::TestAllGemmBasic<Gemm>());
 } )
 
 ////////////////////////////////////////////////////////////////////////////////
 // Elements / Thread:   4 x   4
 //    Threads / Warp:   4 x   8
 //     Warps / Block:   4 x   4
 //       Threadblock:  64 x 128 x  8
@@ -822,15 +822,15 @@
         cutlass::arch::OpClassSimt,
         cutlass::arch::Sm50,
         ThreadblockShape, WarpShape, InstructionShape,
         EpilogueOutputOp,
         cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>,
         2 // Stages
     >;
-    EXPECT_TRUE(test::gemm::device::TestAllGemm<Gemm>());
+    EXPECT_TRUE(test::gemm::device::TestAllGemmBasic<Gemm>());
 } )
 
 ////////////////////////////////////////////////////////////////////////////////
 // Elements / Thread:   4 x   4
 //    Threads / Warp:   8 x   4
 //     Warps / Block:   4 x   4
 //       Threadblock: 128 x  64 x  8
@@ -852,10 +852,10 @@
         cutlass::arch::OpClassSimt,
         cutlass::arch::Sm50,
         ThreadblockShape, WarpShape, InstructionShape,
         EpilogueOutputOp,
         cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>,
         2 // Stages
     >;
-    EXPECT_TRUE(test::gemm::device::TestAllGemm<Gemm>());
+    EXPECT_TRUE(test::gemm::device::TestAllGemmBasic<Gemm>());
 } )
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/simt_qgemm_tt_sm50.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/simt_qgemm_tt_sm50.cu`

 * *Files 3% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -72,15 +72,15 @@
         cutlass::arch::OpClassSimt,
         cutlass::arch::Sm50,
         ThreadblockShape, WarpShape, InstructionShape,
         EpilogueOutputOp,
         cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>,
         2 // Stages
     >;
-    EXPECT_TRUE(test::gemm::device::TestAllGemm<Gemm>());
+    EXPECT_TRUE(test::gemm::device::TestAllGemmBasic<Gemm>());
 } )
 
 ////////////////////////////////////////////////////////////////////////////////
 // Elements / Thread:   4 x   4
 //    Threads / Warp:   4 x   8
 //     Warps / Block:   1 x   1
 //       Threadblock:  16 x  32 x  8
@@ -102,15 +102,15 @@
         cutlass::arch::OpClassSimt,
         cutlass::arch::Sm50,
         ThreadblockShape, WarpShape, InstructionShape,
         EpilogueOutputOp,
         cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>,
         2 // Stages
     >;
-    EXPECT_TRUE(test::gemm::device::TestAllGemm<Gemm>());
+    EXPECT_TRUE(test::gemm::device::TestAllGemmBasic<Gemm>());
 } )
 
 ////////////////////////////////////////////////////////////////////////////////
 // Elements / Thread:   2 x   2
 //    Threads / Warp:   4 x   8
 //     Warps / Block:   1 x   2
 //       Threadblock:   8 x  32 x  8
@@ -132,15 +132,15 @@
         cutlass::arch::OpClassSimt,
         cutlass::arch::Sm50,
         ThreadblockShape, WarpShape, InstructionShape,
         EpilogueOutputOp,
         cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>,
         2 // Stages
     >;
-    EXPECT_TRUE(test::gemm::device::TestAllGemm<Gemm>());
+    EXPECT_TRUE(test::gemm::device::TestAllGemmBasic<Gemm>());
 } )
 
 ////////////////////////////////////////////////////////////////////////////////
 // Elements / Thread:   2 x   4
 //    Threads / Warp:   4 x   8
 //     Warps / Block:   1 x   2
 //       Threadblock:   8 x  64 x  8
@@ -162,15 +162,15 @@
         cutlass::arch::OpClassSimt,
         cutlass::arch::Sm50,
         ThreadblockShape, WarpShape, InstructionShape,
         EpilogueOutputOp,
         cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>,
         2 // Stages
     >;
-    EXPECT_TRUE(test::gemm::device::TestAllGemm<Gemm>());
+    EXPECT_TRUE(test::gemm::device::TestAllGemmBasic<Gemm>());
 } )
 
 ////////////////////////////////////////////////////////////////////////////////
 // Elements / Thread:   4 x   2
 //    Threads / Warp:   4 x   8
 //     Warps / Block:   1 x   2
 //       Threadblock:  16 x  32 x  8
@@ -192,15 +192,15 @@
         cutlass::arch::OpClassSimt,
         cutlass::arch::Sm50,
         ThreadblockShape, WarpShape, InstructionShape,
         EpilogueOutputOp,
         cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>,
         2 // Stages
     >;
-    EXPECT_TRUE(test::gemm::device::TestAllGemm<Gemm>());
+    EXPECT_TRUE(test::gemm::device::TestAllGemmBasic<Gemm>());
 } )
 
 ////////////////////////////////////////////////////////////////////////////////
 // Elements / Thread:   4 x   4
 //    Threads / Warp:   4 x   8
 //     Warps / Block:   1 x   2
 //       Threadblock:  16 x  64 x  8
@@ -222,15 +222,15 @@
         cutlass::arch::OpClassSimt,
         cutlass::arch::Sm50,
         ThreadblockShape, WarpShape, InstructionShape,
         EpilogueOutputOp,
         cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>,
         2 // Stages
     >;
-    EXPECT_TRUE(test::gemm::device::TestAllGemm<Gemm>());
+    EXPECT_TRUE(test::gemm::device::TestAllGemmBasic<Gemm>());
 } )
 
 ////////////////////////////////////////////////////////////////////////////////
 // Elements / Thread:   4 x   4
 //    Threads / Warp:   8 x   4
 //     Warps / Block:   1 x   2
 //       Threadblock:  32 x  32 x  8
@@ -252,15 +252,15 @@
         cutlass::arch::OpClassSimt,
         cutlass::arch::Sm50,
         ThreadblockShape, WarpShape, InstructionShape,
         EpilogueOutputOp,
         cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>,
         2 // Stages
     >;
-    EXPECT_TRUE(test::gemm::device::TestAllGemm<Gemm>());
+    EXPECT_TRUE(test::gemm::device::TestAllGemmBasic<Gemm>());
 } )
 
 ////////////////////////////////////////////////////////////////////////////////
 // Elements / Thread:   4 x   4
 //    Threads / Warp:   4 x   8
 //     Warps / Block:   2 x   1
 //       Threadblock:  32 x  32 x  8
@@ -282,15 +282,15 @@
         cutlass::arch::OpClassSimt,
         cutlass::arch::Sm50,
         ThreadblockShape, WarpShape, InstructionShape,
         EpilogueOutputOp,
         cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>,
         2 // Stages
     >;
-    EXPECT_TRUE(test::gemm::device::TestAllGemm<Gemm>());
+    EXPECT_TRUE(test::gemm::device::TestAllGemmBasic<Gemm>());
 } )
 
 ////////////////////////////////////////////////////////////////////////////////
 // Elements / Thread:   2 x   2
 //    Threads / Warp:   4 x   8
 //     Warps / Block:   2 x   2
 //       Threadblock:  16 x  32 x  8
@@ -312,15 +312,15 @@
         cutlass::arch::OpClassSimt,
         cutlass::arch::Sm50,
         ThreadblockShape, WarpShape, InstructionShape,
         EpilogueOutputOp,
         cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>,
         2 // Stages
     >;
-    EXPECT_TRUE(test::gemm::device::TestAllGemm<Gemm>());
+    EXPECT_TRUE(test::gemm::device::TestAllGemmBasic<Gemm>());
 } )
 
 ////////////////////////////////////////////////////////////////////////////////
 // Elements / Thread:   2 x   4
 //    Threads / Warp:   4 x   8
 //     Warps / Block:   2 x   2
 //       Threadblock:  16 x  64 x  8
@@ -342,15 +342,15 @@
         cutlass::arch::OpClassSimt,
         cutlass::arch::Sm50,
         ThreadblockShape, WarpShape, InstructionShape,
         EpilogueOutputOp,
         cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>,
         2 // Stages
     >;
-    EXPECT_TRUE(test::gemm::device::TestAllGemm<Gemm>());
+    EXPECT_TRUE(test::gemm::device::TestAllGemmBasic<Gemm>());
 } )
 
 ////////////////////////////////////////////////////////////////////////////////
 // Elements / Thread:   4 x   2
 //    Threads / Warp:   4 x   8
 //     Warps / Block:   2 x   2
 //       Threadblock:  32 x  32 x  8
@@ -372,15 +372,15 @@
         cutlass::arch::OpClassSimt,
         cutlass::arch::Sm50,
         ThreadblockShape, WarpShape, InstructionShape,
         EpilogueOutputOp,
         cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>,
         2 // Stages
     >;
-    EXPECT_TRUE(test::gemm::device::TestAllGemm<Gemm>());
+    EXPECT_TRUE(test::gemm::device::TestAllGemmBasic<Gemm>());
 } )
 
 ////////////////////////////////////////////////////////////////////////////////
 // Elements / Thread:   4 x   4
 //    Threads / Warp:   4 x   8
 //     Warps / Block:   2 x   2
 //       Threadblock:  32 x  64 x  8
@@ -402,15 +402,15 @@
         cutlass::arch::OpClassSimt,
         cutlass::arch::Sm50,
         ThreadblockShape, WarpShape, InstructionShape,
         EpilogueOutputOp,
         cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>,
         2 // Stages
     >;
-    EXPECT_TRUE(test::gemm::device::TestAllGemm<Gemm>());
+    EXPECT_TRUE(test::gemm::device::TestAllGemmBasic<Gemm>());
 } )
 
 ////////////////////////////////////////////////////////////////////////////////
 // Elements / Thread:   4 x   4
 //    Threads / Warp:   8 x   4
 //     Warps / Block:   2 x   2
 //       Threadblock:  64 x  32 x  8
@@ -432,15 +432,15 @@
         cutlass::arch::OpClassSimt,
         cutlass::arch::Sm50,
         ThreadblockShape, WarpShape, InstructionShape,
         EpilogueOutputOp,
         cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>,
         2 // Stages
     >;
-    EXPECT_TRUE(test::gemm::device::TestAllGemm<Gemm>());
+    EXPECT_TRUE(test::gemm::device::TestAllGemmBasic<Gemm>());
 } )
 
 ////////////////////////////////////////////////////////////////////////////////
 // Elements / Thread:   2 x   2
 //    Threads / Warp:   4 x   8
 //     Warps / Block:   2 x   4
 //       Threadblock:  16 x  64 x 16
@@ -462,15 +462,15 @@
         cutlass::arch::OpClassSimt,
         cutlass::arch::Sm50,
         ThreadblockShape, WarpShape, InstructionShape,
         EpilogueOutputOp,
         cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>,
         2 // Stages
     >;
-    EXPECT_TRUE(test::gemm::device::TestAllGemm<Gemm>());
+    EXPECT_TRUE(test::gemm::device::TestAllGemmBasic<Gemm>());
 } )
 
 ////////////////////////////////////////////////////////////////////////////////
 // Elements / Thread:   2 x   2
 //    Threads / Warp:   8 x   4
 //     Warps / Block:   2 x   4
 //       Threadblock:  32 x  32 x  8
@@ -492,15 +492,15 @@
         cutlass::arch::OpClassSimt,
         cutlass::arch::Sm50,
         ThreadblockShape, WarpShape, InstructionShape,
         EpilogueOutputOp,
         cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>,
         2 // Stages
     >;
-    EXPECT_TRUE(test::gemm::device::TestAllGemm<Gemm>());
+    EXPECT_TRUE(test::gemm::device::TestAllGemmBasic<Gemm>());
 } )
 
 ////////////////////////////////////////////////////////////////////////////////
 // Elements / Thread:   4 x   2
 //    Threads / Warp:   4 x   8
 //     Warps / Block:   2 x   4
 //       Threadblock:  32 x  64 x  8
@@ -522,15 +522,15 @@
         cutlass::arch::OpClassSimt,
         cutlass::arch::Sm50,
         ThreadblockShape, WarpShape, InstructionShape,
         EpilogueOutputOp,
         cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>,
         2 // Stages
     >;
-    EXPECT_TRUE(test::gemm::device::TestAllGemm<Gemm>());
+    EXPECT_TRUE(test::gemm::device::TestAllGemmBasic<Gemm>());
 } )
 
 ////////////////////////////////////////////////////////////////////////////////
 // Elements / Thread:   4 x   4
 //    Threads / Warp:   4 x   8
 //     Warps / Block:   2 x   4
 //       Threadblock:  32 x 128 x  8
@@ -552,15 +552,15 @@
         cutlass::arch::OpClassSimt,
         cutlass::arch::Sm50,
         ThreadblockShape, WarpShape, InstructionShape,
         EpilogueOutputOp,
         cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>,
         2 // Stages
     >;
-    EXPECT_TRUE(test::gemm::device::TestAllGemm<Gemm>());
+    EXPECT_TRUE(test::gemm::device::TestAllGemmBasic<Gemm>());
 } )
 
 ////////////////////////////////////////////////////////////////////////////////
 // Elements / Thread:   4 x   4
 //    Threads / Warp:   8 x   4
 //     Warps / Block:   2 x   4
 //       Threadblock:  64 x  64 x  8
@@ -582,15 +582,15 @@
         cutlass::arch::OpClassSimt,
         cutlass::arch::Sm50,
         ThreadblockShape, WarpShape, InstructionShape,
         EpilogueOutputOp,
         cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>,
         2 // Stages
     >;
-    EXPECT_TRUE(test::gemm::device::TestAllGemm<Gemm>());
+    EXPECT_TRUE(test::gemm::device::TestAllGemmBasic<Gemm>());
 } )
 
 ////////////////////////////////////////////////////////////////////////////////
 // Elements / Thread:   2 x   2
 //    Threads / Warp:   4 x   8
 //     Warps / Block:   4 x   2
 //       Threadblock:  32 x  32 x  8
@@ -612,15 +612,15 @@
         cutlass::arch::OpClassSimt,
         cutlass::arch::Sm50,
         ThreadblockShape, WarpShape, InstructionShape,
         EpilogueOutputOp,
         cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>,
         2 // Stages
     >;
-    EXPECT_TRUE(test::gemm::device::TestAllGemm<Gemm>());
+    EXPECT_TRUE(test::gemm::device::TestAllGemmBasic<Gemm>());
 } )
 
 ////////////////////////////////////////////////////////////////////////////////
 // Elements / Thread:   4 x   2
 //    Threads / Warp:   4 x   8
 //     Warps / Block:   4 x   2
 //       Threadblock:  64 x  32 x  8
@@ -642,15 +642,15 @@
         cutlass::arch::OpClassSimt,
         cutlass::arch::Sm50,
         ThreadblockShape, WarpShape, InstructionShape,
         EpilogueOutputOp,
         cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>,
         2 // Stages
     >;
-    EXPECT_TRUE(test::gemm::device::TestAllGemm<Gemm>());
+    EXPECT_TRUE(test::gemm::device::TestAllGemmBasic<Gemm>());
 } )
 
 ////////////////////////////////////////////////////////////////////////////////
 // Elements / Thread:   4 x   4
 //    Threads / Warp:   4 x   8
 //     Warps / Block:   4 x   2
 //       Threadblock:  64 x  64 x  8
@@ -672,15 +672,15 @@
         cutlass::arch::OpClassSimt,
         cutlass::arch::Sm50,
         ThreadblockShape, WarpShape, InstructionShape,
         EpilogueOutputOp,
         cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>,
         2 // Stages
     >;
-    EXPECT_TRUE(test::gemm::device::TestAllGemm<Gemm>());
+    EXPECT_TRUE(test::gemm::device::TestAllGemmBasic<Gemm>());
 } )
 
 ////////////////////////////////////////////////////////////////////////////////
 // Elements / Thread:   4 x   4
 //    Threads / Warp:   8 x   4
 //     Warps / Block:   4 x   2
 //       Threadblock: 128 x  32 x  8
@@ -702,15 +702,15 @@
         cutlass::arch::OpClassSimt,
         cutlass::arch::Sm50,
         ThreadblockShape, WarpShape, InstructionShape,
         EpilogueOutputOp,
         cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>,
         2 // Stages
     >;
-    EXPECT_TRUE(test::gemm::device::TestAllGemm<Gemm>());
+    EXPECT_TRUE(test::gemm::device::TestAllGemmBasic<Gemm>());
 } )
 
 ////////////////////////////////////////////////////////////////////////////////
 // Elements / Thread:   2 x   2
 //    Threads / Warp:   4 x   8
 //     Warps / Block:   4 x   4
 //       Threadblock:  32 x  64 x 16
@@ -732,15 +732,15 @@
         cutlass::arch::OpClassSimt,
         cutlass::arch::Sm50,
         ThreadblockShape, WarpShape, InstructionShape,
         EpilogueOutputOp,
         cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>,
         2 // Stages
     >;
-    EXPECT_TRUE(test::gemm::device::TestAllGemm<Gemm>());
+    EXPECT_TRUE(test::gemm::device::TestAllGemmBasic<Gemm>());
 } )
 
 ////////////////////////////////////////////////////////////////////////////////
 // Elements / Thread:   2 x   2
 //    Threads / Warp:   8 x   4
 //     Warps / Block:   4 x   4
 //       Threadblock:  64 x  32 x 16
@@ -762,15 +762,15 @@
         cutlass::arch::OpClassSimt,
         cutlass::arch::Sm50,
         ThreadblockShape, WarpShape, InstructionShape,
         EpilogueOutputOp,
         cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>,
         2 // Stages
     >;
-    EXPECT_TRUE(test::gemm::device::TestAllGemm<Gemm>());
+    EXPECT_TRUE(test::gemm::device::TestAllGemmBasic<Gemm>());
 } )
 
 ////////////////////////////////////////////////////////////////////////////////
 // Elements / Thread:   4 x   2
 //    Threads / Warp:   4 x   8
 //     Warps / Block:   4 x   4
 //       Threadblock:  64 x  64 x  8
@@ -792,15 +792,15 @@
         cutlass::arch::OpClassSimt,
         cutlass::arch::Sm50,
         ThreadblockShape, WarpShape, InstructionShape,
         EpilogueOutputOp,
         cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>,
         2 // Stages
     >;
-    EXPECT_TRUE(test::gemm::device::TestAllGemm<Gemm>());
+    EXPECT_TRUE(test::gemm::device::TestAllGemmBasic<Gemm>());
 } )
 
 ////////////////////////////////////////////////////////////////////////////////
 // Elements / Thread:   4 x   4
 //    Threads / Warp:   4 x   8
 //     Warps / Block:   4 x   4
 //       Threadblock:  64 x 128 x  8
@@ -822,15 +822,15 @@
         cutlass::arch::OpClassSimt,
         cutlass::arch::Sm50,
         ThreadblockShape, WarpShape, InstructionShape,
         EpilogueOutputOp,
         cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>,
         2 // Stages
     >;
-    EXPECT_TRUE(test::gemm::device::TestAllGemm<Gemm>());
+    EXPECT_TRUE(test::gemm::device::TestAllGemmBasic<Gemm>());
 } )
 
 ////////////////////////////////////////////////////////////////////////////////
 // Elements / Thread:   4 x   4
 //    Threads / Warp:   8 x   4
 //     Warps / Block:   4 x   4
 //       Threadblock: 128 x  64 x  8
@@ -852,10 +852,10 @@
         cutlass::arch::OpClassSimt,
         cutlass::arch::Sm50,
         ThreadblockShape, WarpShape, InstructionShape,
         EpilogueOutputOp,
         cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>,
         2 // Stages
     >;
-    EXPECT_TRUE(test::gemm::device::TestAllGemm<Gemm>());
+    EXPECT_TRUE(test::gemm::device::TestAllGemmBasic<Gemm>());
 } )
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/simt_sgemm_nn_sm50.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/simt_sgemm_nn_sm50.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/simt_sgemm_nt_sm50.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/simt_sgemm_nt_sm50.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/simt_sgemm_nt_sm80.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/simt_sgemm_nt_sm80.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/simt_sgemm_tn_sm50.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/simt_sgemm_tn_sm50.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/simt_sgemm_tn_sm80.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/simt_sgemm_tn_sm80.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/simt_sgemm_tt_sm50.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/simt_sgemm_tt_sm50.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/simt_zgemm_nn_sm50.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/simt_zgemm_nn_sm50.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/simt_zgemm_nt_sm50.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/simt_zgemm_nt_sm50.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/simt_zgemm_tn_sm50.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/simt_zgemm_tn_sm50.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/simt_zgemm_tt_sm50.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/simt_zgemm_tt_sm50.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/sm50_gemm_f32_f32_f32_simt.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/sm50_gemm_f32_f32_f32_simt.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/sm50_gemm_f64_f64_f64_simt.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/sm50_gemm_f64_f64_f64_simt.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/sm61_gemm_s8_s8_s32_simt.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/sm61_gemm_s8_s8_s32_simt.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/sm80_gemm_f16_f16_f32_tensor_op_f32.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/sm80_gemm_f16_f16_f32_tensor_op_f32.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/sm80_gemm_f32_f32_f32_simt.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/sm80_gemm_f32_f32_f32_simt.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/sm80_gemm_f64_f64_f64_simt.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/sm80_gemm_f64_f64_f64_simt.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/sm80_gemm_f64_f64_f64_tensor_op_f64.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/sm80_gemm_f64_f64_f64_tensor_op_f64.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/sm80_gemm_s8_s8_s32_tensor_op.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/sm80_gemm_s8_s8_s32_tensor_op.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/sm80_gemm_tf32_tf32_f32_tensor_op_f32.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/sm80_gemm_tf32_tf32_f32_tensor_op_f32.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/sm90_gemm_bf16_bf16_bf16_alignx_tensor_op_f32.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/sm90_gemm_bf16_bf16_bf16_alignx_tensor_op_f32.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/sm90_gemm_bf16_bf16_bf16_tensor_op_f32.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/sm90_gemm_bf16_bf16_bf16_tensor_op_f32.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/sm90_gemm_f16_f16_f16_alignx_tensor_op.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/sm90_gemm_f16_f16_f16_alignx_tensor_op.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/sm90_gemm_f16_f16_f16_tensor_op.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/sm90_gemm_f16_f16_f16_tensor_op.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/sm90_gemm_f16_f16_f16_tensor_op_f32_cluster_unspecialized.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/sm90_gemm_f16_f16_f16_tensor_op_f32_cluster_unspecialized.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/sm90_gemm_f16_f16_f16_tensor_op_f32_cluster_warpspecialized.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/sm90_gemm_f16_f16_f16_tensor_op_f32_cluster_warpspecialized.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/sm90_gemm_f16_f16_f16_tensor_op_f32_cluster_warpspecialized_persistent.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/sm90_gemm_f16_f16_f16_tensor_op_f32_cluster_warpspecialized_persistent.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/sm90_gemm_f32_f32_f32_tensor_op_f32.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/sm90_gemm_f32_f32_f32_tensor_op_f32.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/sm90_gemm_s8_s8_s8_alignx_tensor_op_s32.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/sm90_gemm_s8_s8_s8_alignx_tensor_op_s32.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/sm90_gemm_s8_s8_s8_tensor_op_s32.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/sm90_gemm_s8_s8_s8_tensor_op_s32.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/sm90_gemm_tf32_tf32_f32_alignx_tensor_op_f32.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/sm90_gemm_tf32_tf32_f32_alignx_tensor_op_f32.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/sm90_gemm_tf32_tf32_f32_tensor_op_f32.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/sm90_gemm_tf32_tf32_f32_tensor_op_f32.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/symm_cf32n_cf32n_tensor_op_f32_ls_sm80.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/symm_cf32n_cf32n_tensor_op_f32_ls_sm80.cu`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/symm_cf32n_cf32n_tensor_op_f32_rs_sm80.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/symm_cf32n_cf32n_tensor_op_f32_rs_sm80.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/symm_cf32n_cf32n_tensor_op_fast_f32_ls_sm80.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/symm_cf32n_cf32n_tensor_op_fast_f32_ls_sm80.cu`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/symm_cf32n_cf32n_tensor_op_fast_f32_rs_sm80.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/symm_cf32n_cf32n_tensor_op_fast_f32_rs_sm80.cu`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/symm_cf64_cf64_cf64_tensor_op_f64_sm90.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/symm_cf64_cf64_cf64_tensor_op_f64_sm90.cu`

 * *Files 2% similar despite different names*

```diff
@@ -44,15 +44,15 @@
 #include "cutlass/util/reference/host/tensor_compare.h"
 #include "cutlass/util/reference/host/tensor_copy.h"
 #include "cutlass/util/reference/host/tensor_fill.h"
 #include "cutlass/util/tensor_view_io.h"
 
 #include "testbed_symm_universal.h"
 
-#if defined(CUTLASS_ARCH_MMA_SM90_F64_MMA_ENABLED)
+#if defined(CUTLASS_ARCH_MMA_SM90_SUPPORTED)
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
 TEST(SM90_Device_Symm_cf64n_cf64n_ls_l_tensor_op_f64_gaussian, 32x32x16_16x16x16) {
 
   using ElementOutput = cutlass::complex<double>;
   using ElementAccumulator = cutlass::complex<double>;
@@ -126,8 +126,8 @@
   >;
 
   EXPECT_TRUE(test::gemm::device::TestAllSymmUniversal<Symm>());
 }
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
-#endif // #if defined(CUTLASS_ARCH_MMA_SM90_F64_MMA_ENABLED)
+#endif // #if defined(CUTLASS_ARCH_MMA_SM90_SUPPORTED)
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/symm_cf64n_cf64n_cf64n_tensor_op_ls_f64_gaussian_sm80.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/symm_cf64n_cf64n_cf64n_tensor_op_ls_f64_gaussian_sm80.cu`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/symm_cf64n_cf64n_cf64n_tensor_op_ls_f64_sm80.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/symm_cf64n_cf64n_cf64n_tensor_op_ls_f64_sm80.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/symm_cf64n_cf64n_cf64n_tensor_op_rs_f64_sm80.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/symm_cf64n_cf64n_cf64n_tensor_op_rs_f64_sm80.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/symm_f32n_f32n_tensor_op_fast_f32_ls_sm80.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/symm_f32n_f32n_tensor_op_fast_f32_ls_sm80.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/symm_f32n_f32n_tensor_op_fast_f32_rs_sm80.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/symm_f32n_f32n_tensor_op_fast_f32_rs_sm80.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/symm_f32t_f32t_tensor_op_fast_f32_ls_sm80.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/symm_f32t_f32t_tensor_op_fast_f32_ls_sm80.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/symm_f64_f64_tensor_op_f64_sm90.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/syr2k_cf32n_cf32t_tensor_op_fast_f32_sm80.cu`

 * *Files 8% similar despite different names*

```diff
@@ -25,111 +25,126 @@
  * SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER
  * CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,
  * OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
  * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
  *
  **************************************************************************************************/
 /*! \file
-    \brief Tests for device-wide SYMM interface
+    \brief Tests for device-wide SYRK interface
   
 */
 
 #include <iostream>
 
 #include "../../common/cutlass_unit_test.h"
 #include "cutlass/blas3.h"
-#include "cutlass/gemm/device/symm.h"
+#include "cutlass/gemm/device/rank_2k.h"
 #include "cutlass/util/host_tensor.h"
-#include "cutlass/util/reference/host/symm.h"
+#include "cutlass/util/reference/host/rank_2k.h"
 #include "cutlass/util/reference/host/tensor_compare.h"
 #include "cutlass/util/reference/host/tensor_copy.h"
 #include "cutlass/util/reference/host/tensor_fill.h"
 #include "cutlass/util/tensor_view_io.h"
 
-#include "testbed_symm_universal.h"
+#include "testbed_rank2k_universal.h"
 
-#if defined(CUTLASS_ARCH_MMA_SM90_F64_MMA_ENABLED)
+#if defined(CUTLASS_ARCH_MMA_SM80_SUPPORTED)
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
-TEST(SM90_Device_Symm_f64n_f64n_rs_l_tensor_op_f64, 32x32x16_16x16x16) {
+TEST(SM80_Device_Syr2k_cf32n_cf32t_l_tensor_op_fast_f32, 64x64x16_32x32x16) {
 
-  using ElementA = double;
+  using ElementA = cutlass::complex<float>;
   using LayoutA = cutlass::layout::ColumnMajor;
-  using ElementB = double;
+
+  using ElementB = cutlass::complex<float>;
   using LayoutB = cutlass::layout::ColumnMajor;
-  using ElementC = double;
-  using LayoutC = cutlass::layout::ColumnMajor;
-  using ElementAccumulator = double;
 
-  using Symm = cutlass::gemm::device::Symm<
+  using ElementC = cutlass::complex<float>;
+  using LayoutC = cutlass::layout::RowMajor;
+  using ElementAccumulator = cutlass::complex<float>;
+
+  using Rank2K = cutlass::gemm::device::Rank2K<
     ElementA,
     LayoutA,
-    cutlass::SideMode::kRight,
-    cutlass::FillMode::kLower,
     ElementB,
     LayoutB,
     ElementC,
     LayoutC,
+    cutlass::FillMode::kLower,
     ElementAccumulator,
     cutlass::arch::OpClassTensorOp,
-    cutlass::arch::Sm90,
+    cutlass::arch::Sm80,
+    cutlass::gemm::GemmShape<64, 64, 16>,
     cutlass::gemm::GemmShape<32, 32, 16>,
-    cutlass::gemm::GemmShape<16, 16, 16>,
-    cutlass::gemm::GemmShape<16, 8, 4>,
+    cutlass::gemm::GemmShape<16, 8, 8>,
     cutlass::epilogue::thread::LinearCombination<
       ElementC,
       1,
       ElementAccumulator,
       ElementAccumulator
     >,
     cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>,
-    4
+    4,     // kStages 
+    1,     // AlignmentA
+    1,     // AlignmentB
+    false, // SplitKSerial
+    cutlass::arch::OpMultiplyAddComplexFastF32,
+    cutlass::ComplexTransform::kNone,
+    cutlass::ComplexTransform::kNone,
+    cutlass::BlasMode::kSymmetric
   >;
 
-  EXPECT_TRUE(test::gemm::device::TestAllSymmUniversal<Symm>());
-
+  EXPECT_TRUE(test::gemm::device::TestAllRank2KUniversal<Rank2K>());
 }
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
-TEST(SM90_Device_Symm_f64t_f64t_ls_l_tensor_op_f64, 128x128x16_32x64x16) {
+TEST(SM80_Device_Syr2k_cf32n_cf32t_u_tensor_op_fast_f32, 64x64x16_32x32x16) {
+
+  using ElementA = cutlass::complex<float>;
+  using LayoutA = cutlass::layout::ColumnMajor;
+
+  using ElementB = cutlass::complex<float>;
+  using LayoutB = cutlass::layout::ColumnMajor;
 
-  using ElementA = double;
-  using LayoutA = cutlass::layout::RowMajor;
-  using ElementB = double;
-  using LayoutB = cutlass::layout::RowMajor;
-  using ElementC = double;
+  using ElementC = cutlass::complex<float>;
   using LayoutC = cutlass::layout::RowMajor;
-  using ElementAccumulator = double;
+  using ElementAccumulator = cutlass::complex<float>;
 
-  using Symm = cutlass::gemm::device::Symm<
+  using Rank2K = cutlass::gemm::device::Rank2K<
     ElementA,
     LayoutA,
-    cutlass::SideMode::kLeft,
-    cutlass::FillMode::kLower,
     ElementB,
     LayoutB,
     ElementC,
     LayoutC,
+    cutlass::FillMode::kUpper,
     ElementAccumulator,
     cutlass::arch::OpClassTensorOp,
-    cutlass::arch::Sm90,
-    cutlass::gemm::GemmShape<128, 128, 16>,
-    cutlass::gemm::GemmShape<32, 64, 16>,
-    cutlass::gemm::GemmShape<16, 8, 4>,
+    cutlass::arch::Sm80,
+    cutlass::gemm::GemmShape<64, 64, 16>,
+    cutlass::gemm::GemmShape<32, 32, 16>,
+    cutlass::gemm::GemmShape<16, 8, 8>,
     cutlass::epilogue::thread::LinearCombination<
       ElementC,
       1,
       ElementAccumulator,
       ElementAccumulator
     >,
     cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>,
-    3
+    4,     // kStages 
+    1,     // AlignmentA
+    1,     // AlignmentB
+    false, // SplitKSerial
+    cutlass::arch::OpMultiplyAddComplexFastF32,
+    cutlass::ComplexTransform::kNone,
+    cutlass::ComplexTransform::kNone,
+    cutlass::BlasMode::kSymmetric
   >;
 
-  EXPECT_TRUE(test::gemm::device::TestAllSymmUniversal<Symm>());
-
+  EXPECT_TRUE(test::gemm::device::TestAllRank2KUniversal<Rank2K>());
 }
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
-#endif // #if defined(CUTLASS_ARCH_MMA_SM90_F64_MMA_ENABLED)
+
+#endif // #if defined(CUTLASS_ARCH_MMA_SM80_SUPPORTED)
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/symm_f64n_f64n_tensor_op_f64_ls_sm80.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/symm_f64n_f64n_tensor_op_f64_ls_sm80.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/symm_f64n_f64n_tensor_op_f64_rs_sm80.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/symm_f64n_f64n_tensor_op_f64_rs_sm80.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/symm_f64n_f64t_tensor_op_f64_ls_sm80.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/symm_f64n_f64t_tensor_op_f64_ls_sm80.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/symm_f64n_f64t_tensor_op_f64_rs_sm80.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/symm_f64n_f64t_tensor_op_f64_rs_sm80.cu`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/symm_f64t_f64n_tensor_op_f64_ls_sm80.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/symm_f64t_f64n_tensor_op_f64_ls_sm80.cu`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/symm_f64t_f64n_tensor_op_f64_rs_sm80.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/symm_f64t_f64n_tensor_op_f64_rs_sm80.cu`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/symm_f64t_f64t_tensor_op_f64_ls_sm80.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/symm_f64t_f64t_tensor_op_f64_ls_sm80.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/symm_f64t_f64t_tensor_op_f64_rs_sm80.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/symm_f64t_f64t_tensor_op_f64_rs_sm80.cu`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/symm_tf32n_f32n_tensor_op_f32_ls_sm80.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/symm_tf32n_f32n_tensor_op_f32_ls_sm80.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/symm_tf32n_f32n_tensor_op_f32_rs_sm80.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/symm_tf32n_f32n_tensor_op_f32_rs_sm80.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/symm_tf32t_f32t_tensor_op_f32_ls_sm80.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/symm_tf32t_f32t_tensor_op_f32_ls_sm80.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/syr2k_cf32n_cf32n_tensor_op_f32_sm80.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/syr2k_cf32n_cf32n_tensor_op_f32_sm80.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/syr2k_cf32n_cf32n_tensor_op_fast_f32_sm80.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/syr2k_cf32n_cf32n_tensor_op_fast_f32_sm80.cu`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/syr2k_cf32n_cf32t_tensor_op_f32_sm80.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/syr2k_cf32n_cf32t_tensor_op_f32_sm80.cu`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/syr2k_cf32n_cf32t_tensor_op_fast_f32_sm80.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/syr2k_cf64n_cf64n_tensor_op_f64_sm80.cu`

 * *Files 6% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -47,100 +47,100 @@
 
 #include "testbed_rank2k_universal.h"
 
 #if defined(CUTLASS_ARCH_MMA_SM80_SUPPORTED)
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
-TEST(SM80_Device_Syr2k_cf32n_cf32t_l_tensor_op_fast_f32, 64x64x16_32x32x16) {
+TEST(SM80_Device_Syr2k_cf64n_cf64n_l_tensor_op_f64, 32x32x16_16x16x16) {
 
-  using ElementA = cutlass::complex<float>;
+  using ElementA = cutlass::complex<double>;
   using LayoutA = cutlass::layout::ColumnMajor;
 
-  using ElementB = cutlass::complex<float>;
+  using ElementB = cutlass::complex<double>;
   using LayoutB = cutlass::layout::ColumnMajor;
 
-  using ElementC = cutlass::complex<float>;
-  using LayoutC = cutlass::layout::RowMajor;
-  using ElementAccumulator = cutlass::complex<float>;
+  using ElementC = cutlass::complex<double>;
+  using LayoutC = cutlass::layout::ColumnMajor;
+  using ElementAccumulator = cutlass::complex<double>;
 
   using Rank2K = cutlass::gemm::device::Rank2K<
     ElementA,
     LayoutA,
     ElementB,
     LayoutB,
     ElementC,
     LayoutC,
     cutlass::FillMode::kLower,
     ElementAccumulator,
     cutlass::arch::OpClassTensorOp,
     cutlass::arch::Sm80,
-    cutlass::gemm::GemmShape<64, 64, 16>,
     cutlass::gemm::GemmShape<32, 32, 16>,
-    cutlass::gemm::GemmShape<16, 8, 8>,
+    cutlass::gemm::GemmShape<16, 16, 16>,
+    cutlass::gemm::GemmShape<8, 8, 4>,
     cutlass::epilogue::thread::LinearCombination<
       ElementC,
       1,
       ElementAccumulator,
       ElementAccumulator
     >,
     cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>,
     4,     // kStages 
     1,     // AlignmentA
     1,     // AlignmentB
     false, // SplitKSerial
-    cutlass::arch::OpMultiplyAddComplexFastF32,
+    cutlass::arch::OpMultiplyAddComplex,
     cutlass::ComplexTransform::kNone,
     cutlass::ComplexTransform::kNone,
     cutlass::BlasMode::kSymmetric
   >;
 
   EXPECT_TRUE(test::gemm::device::TestAllRank2KUniversal<Rank2K>());
 }
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
-TEST(SM80_Device_Syr2k_cf32n_cf32t_u_tensor_op_fast_f32, 64x64x16_32x32x16) {
+TEST(SM80_Device_Syr2k_cf64n_cf64n_u_tensor_op_f64, 32x32x16_16x16x16) {
 
-  using ElementA = cutlass::complex<float>;
+  using ElementA = cutlass::complex<double>;
   using LayoutA = cutlass::layout::ColumnMajor;
 
-  using ElementB = cutlass::complex<float>;
+  using ElementB = cutlass::complex<double>;
   using LayoutB = cutlass::layout::ColumnMajor;
 
-  using ElementC = cutlass::complex<float>;
-  using LayoutC = cutlass::layout::RowMajor;
-  using ElementAccumulator = cutlass::complex<float>;
+  using ElementC = cutlass::complex<double>;
+  using LayoutC = cutlass::layout::ColumnMajor;
+  using ElementAccumulator = cutlass::complex<double>;
 
   using Rank2K = cutlass::gemm::device::Rank2K<
     ElementA,
     LayoutA,
     ElementB,
     LayoutB,
     ElementC,
     LayoutC,
     cutlass::FillMode::kUpper,
     ElementAccumulator,
     cutlass::arch::OpClassTensorOp,
     cutlass::arch::Sm80,
-    cutlass::gemm::GemmShape<64, 64, 16>,
     cutlass::gemm::GemmShape<32, 32, 16>,
-    cutlass::gemm::GemmShape<16, 8, 8>,
+    cutlass::gemm::GemmShape<16, 16, 16>,
+    cutlass::gemm::GemmShape<8, 8, 4>,
     cutlass::epilogue::thread::LinearCombination<
       ElementC,
       1,
       ElementAccumulator,
       ElementAccumulator
     >,
     cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>,
     4,     // kStages 
     1,     // AlignmentA
     1,     // AlignmentB
     false, // SplitKSerial
-    cutlass::arch::OpMultiplyAddComplexFastF32,
+    cutlass::arch::OpMultiplyAddComplex,
     cutlass::ComplexTransform::kNone,
     cutlass::ComplexTransform::kNone,
     cutlass::BlasMode::kSymmetric
   >;
 
   EXPECT_TRUE(test::gemm::device::TestAllRank2KUniversal<Rank2K>());
 }
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/syr2k_cf64_cf64_tensor_op_f64_sm90.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/syr2k_cf64_cf64_tensor_op_f64_sm90.cu`

 * *Files 2% similar despite different names*

```diff
@@ -43,15 +43,15 @@
 #include "cutlass/util/reference/host/tensor_compare.h"
 #include "cutlass/util/reference/host/tensor_copy.h"
 #include "cutlass/util/reference/host/tensor_fill.h"
 #include "cutlass/util/tensor_view_io.h"
 
 #include "testbed_rank2k_universal.h"
 
-#if defined(CUTLASS_ARCH_MMA_SM90_F64_MMA_ENABLED)
+#if defined(CUTLASS_ARCH_MMA_SM90_SUPPORTED)
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
 TEST(SM90_Device_Syr2k_cf64n_cf64n_l_tensor_op_f64, 32x32x16_16x16x16) {
 
   using ElementA = cutlass::complex<double>;
   using LayoutA = cutlass::layout::ColumnMajor;
@@ -143,8 +143,8 @@
   >;
 
   EXPECT_TRUE(test::gemm::device::TestAllRank2KUniversal<Rank2K>());
 }
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
-#endif // #if defined(CUTLASS_ARCH_MMA_SM90_F64_MMA_ENABLED)
+#endif // #if defined(CUTLASS_ARCH_MMA_SM90_SUPPORTED)
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/syr2k_cf64n_cf64n_tensor_op_f64_grouped_sm80.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/syr2k_cf64n_cf64n_tensor_op_f64_grouped_sm80.cu`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/syr2k_cf64n_cf64n_tensor_op_f64_sm80.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/syr2k_cf64n_cf64t_tensor_op_f64_sm80.cu`

 * *Files 2% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -47,24 +47,24 @@
 
 #include "testbed_rank2k_universal.h"
 
 #if defined(CUTLASS_ARCH_MMA_SM80_SUPPORTED)
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
-TEST(SM80_Device_Syr2k_cf64n_cf64n_l_tensor_op_f64, 32x32x16_16x16x16) {
+TEST(SM80_Device_Syr2k_cf64n_cf64t_l_tensor_op_f64, 32x32x16_16x16x16) {
 
   using ElementA = cutlass::complex<double>;
   using LayoutA = cutlass::layout::ColumnMajor;
 
   using ElementB = cutlass::complex<double>;
   using LayoutB = cutlass::layout::ColumnMajor;
 
   using ElementC = cutlass::complex<double>;
-  using LayoutC = cutlass::layout::ColumnMajor;
+  using LayoutC = cutlass::layout::RowMajor;
   using ElementAccumulator = cutlass::complex<double>;
 
   using Rank2K = cutlass::gemm::device::Rank2K<
     ElementA,
     LayoutA,
     ElementB,
     LayoutB,
@@ -95,24 +95,24 @@
   >;
 
   EXPECT_TRUE(test::gemm::device::TestAllRank2KUniversal<Rank2K>());
 }
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
-TEST(SM80_Device_Syr2k_cf64n_cf64n_u_tensor_op_f64, 32x32x16_16x16x16) {
+TEST(SM80_Device_Syr2k_cf64n_cf64t_u_tensor_op_f64, 32x32x16_16x16x16) {
 
   using ElementA = cutlass::complex<double>;
   using LayoutA = cutlass::layout::ColumnMajor;
 
   using ElementB = cutlass::complex<double>;
   using LayoutB = cutlass::layout::ColumnMajor;
 
   using ElementC = cutlass::complex<double>;
-  using LayoutC = cutlass::layout::ColumnMajor;
+  using LayoutC = cutlass::layout::RowMajor;
   using ElementAccumulator = cutlass::complex<double>;
 
   using Rank2K = cutlass::gemm::device::Rank2K<
     ElementA,
     LayoutA,
     ElementB,
     LayoutB,
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/syr2k_cf64n_cf64t_tensor_op_f64_grouped_sm80.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/syr2k_cf64n_cf64t_tensor_op_f64_grouped_sm80.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/syr2k_cf64n_cf64t_tensor_op_f64_sm80.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/syrk_cf64n_cf64t_tensor_op_f64_sm80.cu`

 * *Files 6% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -33,45 +33,40 @@
   
 */
 
 #include <iostream>
 
 #include "../../common/cutlass_unit_test.h"
 #include "cutlass/blas3.h"
-#include "cutlass/gemm/device/rank_2k.h"
+#include "cutlass/gemm/device/rank_k.h"
 #include "cutlass/util/host_tensor.h"
-#include "cutlass/util/reference/host/rank_2k.h"
+#include "cutlass/util/reference/host/rank_k_complex.h"
 #include "cutlass/util/reference/host/tensor_compare.h"
 #include "cutlass/util/reference/host/tensor_copy.h"
 #include "cutlass/util/reference/host/tensor_fill.h"
 #include "cutlass/util/tensor_view_io.h"
 
-#include "testbed_rank2k_universal.h"
+#include "testbed_rank_k_universal.h"
 
 #if defined(CUTLASS_ARCH_MMA_SM80_SUPPORTED)
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
-TEST(SM80_Device_Syr2k_cf64n_cf64t_l_tensor_op_f64, 32x32x16_16x16x16) {
+TEST(SM80_Device_Syrk_cf64n_cf64t_l_tensor_op_f64, 32x32x16_16x16x16) {
 
   using ElementA = cutlass::complex<double>;
   using LayoutA = cutlass::layout::ColumnMajor;
 
-  using ElementB = cutlass::complex<double>;
-  using LayoutB = cutlass::layout::ColumnMajor;
-
   using ElementC = cutlass::complex<double>;
   using LayoutC = cutlass::layout::RowMajor;
   using ElementAccumulator = cutlass::complex<double>;
 
-  using Rank2K = cutlass::gemm::device::Rank2K<
+  using RankK = cutlass::gemm::device::RankK<
     ElementA,
     LayoutA,
-    ElementB,
-    LayoutB,
     ElementC,
     LayoutC,
     cutlass::FillMode::kLower,
     ElementAccumulator,
     cutlass::arch::OpClassTensorOp,
     cutlass::arch::Sm80,
     cutlass::gemm::GemmShape<32, 32, 16>,
@@ -82,44 +77,37 @@
       1,
       ElementAccumulator,
       ElementAccumulator
     >,
     cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>,
     4,     // kStages 
     1,     // AlignmentA
-    1,     // AlignmentB
     false, // SplitKSerial
     cutlass::arch::OpMultiplyAddComplex,
     cutlass::ComplexTransform::kNone,
-    cutlass::ComplexTransform::kNone,
     cutlass::BlasMode::kSymmetric
   >;
 
-  EXPECT_TRUE(test::gemm::device::TestAllRank2KUniversal<Rank2K>());
+  EXPECT_TRUE(test::gemm::device::TestAllRankKUniversal<RankK>());
 }
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
-TEST(SM80_Device_Syr2k_cf64n_cf64t_u_tensor_op_f64, 32x32x16_16x16x16) {
+TEST(SM80_Device_Syrk_cf64n_cf64t_u_tensor_op_f64, 32x32x16_16x16x16) {
 
   using ElementA = cutlass::complex<double>;
   using LayoutA = cutlass::layout::ColumnMajor;
 
-  using ElementB = cutlass::complex<double>;
-  using LayoutB = cutlass::layout::ColumnMajor;
-
   using ElementC = cutlass::complex<double>;
   using LayoutC = cutlass::layout::RowMajor;
   using ElementAccumulator = cutlass::complex<double>;
 
-  using Rank2K = cutlass::gemm::device::Rank2K<
+  using RankK = cutlass::gemm::device::RankK<
     ElementA,
     LayoutA,
-    ElementB,
-    LayoutB,
     ElementC,
     LayoutC,
     cutlass::FillMode::kUpper,
     ElementAccumulator,
     cutlass::arch::OpClassTensorOp,
     cutlass::arch::Sm80,
     cutlass::gemm::GemmShape<32, 32, 16>,
@@ -130,21 +118,19 @@
       1,
       ElementAccumulator,
       ElementAccumulator
     >,
     cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>,
     4,     // kStages 
     1,     // AlignmentA
-    1,     // AlignmentB
     false, // SplitKSerial
     cutlass::arch::OpMultiplyAddComplex,
     cutlass::ComplexTransform::kNone,
-    cutlass::ComplexTransform::kNone,
     cutlass::BlasMode::kSymmetric
   >;
 
-  EXPECT_TRUE(test::gemm::device::TestAllRank2KUniversal<Rank2K>());
+  EXPECT_TRUE(test::gemm::device::TestAllRankKUniversal<RankK>());
 }
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
 #endif // #if defined(CUTLASS_ARCH_MMA_SM80_SUPPORTED)
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/syr2k_cf64t_cf64n_tensor_op_f64_grouped_sm80.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/syr2k_cf64t_cf64n_tensor_op_f64_grouped_sm80.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/syr2k_cf64t_cf64t_tensor_op_f64_grouped_sm80.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/syr2k_cf64t_cf64t_tensor_op_f64_grouped_sm80.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/syr2k_f32n_f32n_tensor_op_fast_f32_sm80.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/syr2k_f32n_f32n_tensor_op_fast_f32_sm80.cu`

 * *Files 2% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/syr2k_f32t_f32n_tensor_op_fast_f32_sm80.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/syr2k_f32t_f32n_tensor_op_fast_f32_sm80.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/syr2k_f64_f64_tensor_op_f64_sm90.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/syr2k_f64_f64_tensor_op_f64_sm90.cu`

 * *Files 2% similar despite different names*

```diff
@@ -43,15 +43,15 @@
 #include "cutlass/util/reference/host/tensor_compare.h"
 #include "cutlass/util/reference/host/tensor_copy.h"
 #include "cutlass/util/reference/host/tensor_fill.h"
 #include "cutlass/util/tensor_view_io.h"
 
 #include "testbed_rank2k_universal.h"
 
-#if defined(CUTLASS_ARCH_MMA_SM90_F64_MMA_ENABLED)
+#if defined(CUTLASS_ARCH_MMA_SM90_SUPPORTED)
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
 TEST(SM90_Device_Syr2k_f64n_f64n_l_tensor_op_f64, 32x32x16_16x16x16) {
 
   using ElementA = double;
   using LayoutA = cutlass::layout::ColumnMajor;
@@ -127,8 +127,8 @@
 
   EXPECT_TRUE(test::gemm::device::TestAllRank2KUniversal<Rank2K>());
 
 }
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
-#endif // #if defined(CUTLASS_ARCH_MMA_SM90_F64_MMA_ENABLED)
+#endif // #if defined(CUTLASS_ARCH_MMA_SM90_SUPPORTED)
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/syr2k_f64n_f64n_tensor_op_f64_grouped_sm80.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/syr2k_f64n_f64n_tensor_op_f64_grouped_sm80.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/syr2k_f64n_f64n_tensor_op_f64_sm80.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/syr2k_f64n_f64n_tensor_op_f64_sm80.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/syr2k_f64n_f64t_tensor_op_f64_grouped_sm80.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/syr2k_f64n_f64t_tensor_op_f64_grouped_sm80.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/syr2k_f64n_f64t_tensor_op_f64_sm80.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/syr2k_f64n_f64t_tensor_op_f64_sm80.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/syr2k_f64t_f64n_tensor_op_f64_grouped_sm80.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/syr2k_f64t_f64n_tensor_op_f64_grouped_sm80.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/syr2k_f64t_f64n_tensor_op_f64_sm80.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/syr2k_f64t_f64n_tensor_op_f64_sm80.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/syr2k_f64t_f64t_tensor_op_f64_grouped_sm80.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/syr2k_f64t_f64t_tensor_op_f64_grouped_sm80.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/syr2k_tf32n_f32n_tensor_op_f32_sm80.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/syr2k_tf32n_f32n_tensor_op_f32_sm80.cu`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/syr2k_tf32t_f32n_tensor_op_f32_sm80.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/syr2k_tf32t_f32n_tensor_op_f32_sm80.cu`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/syrk_cf32n_cf32n_tensor_op_f32_sm80.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/syrk_cf32n_cf32n_tensor_op_f32_sm80.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/syrk_cf32n_cf32n_tensor_op_fast_f32_sm80.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/syrk_cf32n_cf32n_tensor_op_fast_f32_sm80.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/syrk_cf32n_cf32t_tensor_op_f32_sm80.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/syrk_cf32n_cf32t_tensor_op_f32_sm80.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/syrk_cf32n_cf32t_tensor_op_fast_f32_sm80.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/syrk_cf32n_cf32t_tensor_op_fast_f32_sm80.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/syrk_cf64_cf64_tensor_op_f64_sm90.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/syrk_cf64_cf64_tensor_op_f64_sm90.cu`

 * *Files 3% similar despite different names*

```diff
@@ -43,15 +43,15 @@
 #include "cutlass/util/reference/host/tensor_compare.h"
 #include "cutlass/util/reference/host/tensor_copy.h"
 #include "cutlass/util/reference/host/tensor_fill.h"
 #include "cutlass/util/tensor_view_io.h"
 
 #include "testbed_rank_k_universal.h"
 
-#if defined(CUTLASS_ARCH_MMA_SM90_F64_MMA_ENABLED)
+#if defined(CUTLASS_ARCH_MMA_SM90_SUPPORTED)
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
 TEST(SM90_Device_Syrk_cf64n_cf64n_l_tensor_op_f64, 32x32x16_16x16x16) {
 
   using ElementA = cutlass::complex<double>;
   using LayoutA = cutlass::layout::ColumnMajor;
@@ -129,8 +129,8 @@
   >;
 
   EXPECT_TRUE(test::gemm::device::TestAllRankKUniversal<RankK>());
 }
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
-#endif // #if defined(CUTLASS_ARCH_MMA_SM90_F64_MMA_ENABLED)
+#endif // #if defined(CUTLASS_ARCH_MMA_SM90_SUPPORTED)
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/syrk_cf64n_cf64n_tensor_op_f64_sm80.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/syrk_cf64n_cf64n_tensor_op_f64_sm80.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/syrk_cf64n_cf64t_tensor_op_f64_gaussian_sm80.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/syrk_cf64n_cf64t_tensor_op_f64_gaussian_sm80.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/syrk_cf64n_cf64t_tensor_op_f64_sm80.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/trmm_f64_f64_f64_tensor_op_f64_sm90.cu`

 * *Files 15% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -25,112 +25,103 @@
  * SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER
  * CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,
  * OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
  * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
  *
  **************************************************************************************************/
 /*! \file
-    \brief Tests for device-wide SYRK interface
+    \brief Tests for device-wide TRMM interface
+
   
 */
 
 #include <iostream>
 
 #include "../../common/cutlass_unit_test.h"
 #include "cutlass/blas3.h"
-#include "cutlass/gemm/device/rank_k.h"
+#include "cutlass/gemm/device/trmm.h"
 #include "cutlass/util/host_tensor.h"
-#include "cutlass/util/reference/host/rank_k_complex.h"
+#include "cutlass/util/reference/host/trmm.h"
 #include "cutlass/util/reference/host/tensor_compare.h"
 #include "cutlass/util/reference/host/tensor_copy.h"
 #include "cutlass/util/reference/host/tensor_fill.h"
 #include "cutlass/util/tensor_view_io.h"
 
-#include "testbed_rank_k_universal.h"
+#include "testbed_trmm_universal.h"
 
-#if defined(CUTLASS_ARCH_MMA_SM80_SUPPORTED)
+#if defined(CUTLASS_ARCH_MMA_SM90_SUPPORTED)
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
-TEST(SM80_Device_Syrk_cf64n_cf64t_l_tensor_op_f64, 32x32x16_16x16x16) {
+TEST(SM90_Device_Trmm_f64n_f64n_f64t_rs_l_nu_tensor_op_f64, 32x32x16_16x16x16) {
 
-  using ElementA = cutlass::complex<double>;
-  using LayoutA = cutlass::layout::ColumnMajor;
+  using ElementOutput = double;
+  using ElementAccumulator = double;
 
-  using ElementC = cutlass::complex<double>;
-  using LayoutC = cutlass::layout::RowMajor;
-  using ElementAccumulator = cutlass::complex<double>;
-
-  using RankK = cutlass::gemm::device::RankK<
-    ElementA,
-    LayoutA,
-    ElementC,
-    LayoutC,
+  using Trmm = cutlass::gemm::device::Trmm<
+    double,
+    cutlass::layout::ColumnMajor,
+    cutlass::SideMode::kRight,
     cutlass::FillMode::kLower,
+    cutlass::DiagType::kNonUnit,
+    double,
+    cutlass::layout::ColumnMajor,
+    ElementOutput,
+    cutlass::layout::RowMajor,
     ElementAccumulator,
     cutlass::arch::OpClassTensorOp,
-    cutlass::arch::Sm80,
+    cutlass::arch::Sm90,
     cutlass::gemm::GemmShape<32, 32, 16>,
     cutlass::gemm::GemmShape<16, 16, 16>,
-    cutlass::gemm::GemmShape<8, 8, 4>,
+    cutlass::gemm::GemmShape<16, 8, 4>,
     cutlass::epilogue::thread::LinearCombination<
-      ElementC,
+      ElementOutput,
       1,
       ElementAccumulator,
       ElementAccumulator
     >,
     cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>,
-    4,     // kStages 
-    1,     // AlignmentA
-    false, // SplitKSerial
-    cutlass::arch::OpMultiplyAddComplex,
-    cutlass::ComplexTransform::kNone,
-    cutlass::BlasMode::kSymmetric
+    4
   >;
 
-  EXPECT_TRUE(test::gemm::device::TestAllRankKUniversal<RankK>());
+  EXPECT_TRUE(test::gemm::device::TestAllTrmmUniversal<Trmm>());
 }
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
-TEST(SM80_Device_Syrk_cf64n_cf64t_u_tensor_op_f64, 32x32x16_16x16x16) {
+TEST(SM90_Device_Trmm_f64t_f64t_f64n_rs_l_nu_tensor_op_f64, 64x64x16_32x32x16) {
 
-  using ElementA = cutlass::complex<double>;
-  using LayoutA = cutlass::layout::ColumnMajor;
+  using ElementOutput = double;
+  using ElementAccumulator = double;
 
-  using ElementC = cutlass::complex<double>;
-  using LayoutC = cutlass::layout::RowMajor;
-  using ElementAccumulator = cutlass::complex<double>;
-
-  using RankK = cutlass::gemm::device::RankK<
-    ElementA,
-    LayoutA,
-    ElementC,
-    LayoutC,
-    cutlass::FillMode::kUpper,
+  using Trmm = cutlass::gemm::device::Trmm<
+    double,
+    cutlass::layout::RowMajor,
+    cutlass::SideMode::kRight,
+    cutlass::FillMode::kLower,
+    cutlass::DiagType::kNonUnit,
+    double,
+    cutlass::layout::RowMajor,
+    ElementOutput,
+    cutlass::layout::ColumnMajor,
     ElementAccumulator,
     cutlass::arch::OpClassTensorOp,
-    cutlass::arch::Sm80,
+    cutlass::arch::Sm90,
+    cutlass::gemm::GemmShape<64, 64, 16>,
     cutlass::gemm::GemmShape<32, 32, 16>,
-    cutlass::gemm::GemmShape<16, 16, 16>,
-    cutlass::gemm::GemmShape<8, 8, 4>,
+    cutlass::gemm::GemmShape<16, 8, 4>,
     cutlass::epilogue::thread::LinearCombination<
-      ElementC,
+      ElementOutput,
       1,
       ElementAccumulator,
       ElementAccumulator
     >,
     cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>,
-    4,     // kStages 
-    1,     // AlignmentA
-    false, // SplitKSerial
-    cutlass::arch::OpMultiplyAddComplex,
-    cutlass::ComplexTransform::kNone,
-    cutlass::BlasMode::kSymmetric
+    4
   >;
 
-  EXPECT_TRUE(test::gemm::device::TestAllRankKUniversal<RankK>());
+  EXPECT_TRUE(test::gemm::device::TestAllTrmmUniversal<Trmm>());
 }
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
-#endif // #if defined(CUTLASS_ARCH_MMA_SM80_SUPPORTED)
+#endif // #if defined(CUTLASS_ARCH_MMA_SM90_SUPPORTED)
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/syrk_f32n_f32t_tensor_op_fast_f32_sm80.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/syrk_f32n_f32t_tensor_op_fast_f32_sm80.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/syrk_f32t_f32t_tensor_op_fast_f32_sm80.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/syrk_f32t_f32t_tensor_op_fast_f32_sm80.cu`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/syrk_f64_f64_tensor_op_f64_sm90.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/syrk_f64_f64_tensor_op_f64_sm90.cu`

 * *Files 2% similar despite different names*

```diff
@@ -43,15 +43,15 @@
 #include "cutlass/util/reference/host/tensor_compare.h"
 #include "cutlass/util/reference/host/tensor_copy.h"
 #include "cutlass/util/reference/host/tensor_fill.h"
 #include "cutlass/util/tensor_view_io.h"
 
 #include "testbed_rank_k_universal.h"
 
-#if defined(CUTLASS_ARCH_MMA_SM90_F64_MMA_ENABLED)
+#if defined(CUTLASS_ARCH_MMA_SM90_SUPPORTED)
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
 TEST(SM90_Device_Syrk_f64n_f64t_l_tensor_op_f64, 128x64x16_64x32x16) {
 
   using ElementA = double;
   using LayoutA = cutlass::layout::ColumnMajor;
@@ -119,8 +119,8 @@
   >;
 
   EXPECT_TRUE(test::gemm::device::TestAllRankKUniversal<RankK>());
 }
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
-#endif // #if defined(CUTLASS_ARCH_MMA_SM90_F64_MMA_ENABLED)
+#endif // #if defined(CUTLASS_ARCH_MMA_SM90_SUPPORTED)
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/syrk_f64n_f64t_tensor_op_f64_sm80.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/syrk_f64n_f64t_tensor_op_f64_sm80.cu`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/syrk_f64t_f64n_tensor_op_f64_sm80.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/syrk_f64t_f64n_tensor_op_f64_sm80.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/syrk_tf32n_f32t_tensor_op_f32_sm80.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/syrk_tf32n_f32t_tensor_op_f32_sm80.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/syrk_tf32t_f32t_tensor_op_f32_sm80.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/syrk_tf32t_f32t_tensor_op_f32_sm80.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/testbed.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/testbed.h`

 * *Files 6% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -46,17 +46,19 @@
 #include "cutlass/util/reference/host/tensor_fill.h"
 #include "cutlass/util/reference/host/tensor_copy.h"
 #include "cutlass/util/reference/host/tensor_compare.h"
 #include "cutlass/util/reference/host/tensor_norm.h"
 #include "cutlass/util/reference/host/gemm.h"
 
 #include "testbed_utils.h"
+#include "testbed_universal.h"
 
 #include "cutlass/layout/matrix.h"
 #include "cutlass/matrix_coord.h"
+#include "cutlass/gemm/device/gemm_universal_adapter.h"
 
 namespace test {
 namespace gemm {
 namespace device {
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
@@ -305,28 +307,37 @@
 
     result = cudaGetDeviceProperties(&properties, device_idx);
 
     if (result != cudaSuccess) {
       throw std::runtime_error("cudaGetDeviceProperties() failed");
     }
 
-    if (properties.sharedMemPerMultiprocessor < smem_size) {
+    if (properties.sharedMemPerBlockOptin < smem_size) {
       return false;
     }
 
     return true;
   }
 
 
   /// Executes one test
   bool run(
-    cutlass::gemm::GemmCoord problem_size, 
+    cutlass::gemm::GemmCoord problem_size,
     int split_k_slices = 1,
-    ElementCompute alpha = ElementCompute(1), 
-    ElementCompute beta = ElementCompute(0)) {
+    ElementCompute alpha = ElementCompute(1),
+    ElementCompute beta = ElementCompute(0))
+  {
+/*
+    std::cout << "\n-----------------------\n";
+    std::cout << "problem size: " << problem_size << "\n";
+    std::cout << "split_k_slices: " << split_k_slices << "\n";
+    std::cout << "alpha: " << alpha << "\n";
+    std::cout << "beta: " << beta << "\n";
+    std::cout << "-----------------------\n\n";
+*/
 
     // Waive test if insufficient CUDA device
     if (!sufficient()) {
       if (CUTLASS_TEST_UNIT_ENABLE_WARNINGS) {
         std::cerr << "Test waived due to insufficient CUDA device." << std::endl;
       }
       return true;
@@ -383,15 +394,15 @@
     return passed;
   }
 };
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
 template <typename Gemm, bool Relu=false>
-bool TestAllGemm(
+bool TestAllGemmBasic(
     const typename Gemm::LayoutA::Stride& stride_factor_A = typename Gemm::LayoutA::Stride(),
     const typename Gemm::LayoutB::Stride& stride_factor_B = typename Gemm::LayoutB::Stride(),
     const typename Gemm::LayoutC::Stride& stride_factor_C = typename Gemm::LayoutC::Stride()) {
   bool passed = true;
 
   int const kMinimumOperandElementSize = 
     std::min(
@@ -474,14 +485,60 @@
     }
   }
 
   return passed;
 }
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
+
+template <typename Gemm, bool Relu=false>
+bool TestAllGemm(
+    const typename Gemm::LayoutA::Stride& stride_factor_A,
+    const typename Gemm::LayoutB::Stride& stride_factor_B = typename Gemm::LayoutB::Stride(),
+    const typename Gemm::LayoutC::Stride& stride_factor_C = typename Gemm::LayoutC::Stride())
+{
+  // Test basic GEMM with non-default stride factors
+  return TestAllGemmBasic<Gemm, Relu>(stride_factor_A, stride_factor_B, stride_factor_C);
+}
+
+template <typename Gemm, bool Relu=false>
+bool TestAllGemm()
+{
+#ifdef NDEBUG
+  // Non-debug builds also test basic GEMM with default stride factors
+  if (!TestAllGemmBasic<Gemm, Relu>()) {
+    return false;
+  }
+#endif // NDEBUG
+
+  // Test universal GEMM
+#if 0
+  // Define the universal kernel
+  using UniversalKernel = cutlass::gemm::kernel::GemmUniversal<
+    typename Gemm::GemmKernel::Mma,                                 // Mma
+    typename Gemm::GemmKernel::Epilogue,                            // Epilogue
+    cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>    // ThreadblockSwizzle
+  >;
+#else
+  // Define the streamk universal kernel
+  using UniversalKernel = cutlass::gemm::kernel::GemmUniversalStreamk<
+    typename Gemm::GemmKernel::Mma,                                 // Mma
+    typename Gemm::GemmKernel::Epilogue,                            // Epilogue
+    cutlass::gemm::threadblock::ThreadblockSwizzleStreamK           // ThreadblockSwizzle
+  >;
+#endif
+
+  // Define the universal adaptor
+  using UniversalGemm = cutlass::gemm::device::GemmUniversalAdapter<UniversalKernel>;
+
+  // Test universal GEMM
+  return TestAllGemmUniversal<UniversalGemm, Relu>();
+}
+
+/////////////////////////////////////////////////////////////////////////////////////////////////
 template <typename Gemm>
 bool TestGemmPerf(int iterations = 1) {
   bool passed = true;
 
   int problem_size_m[] = { 2048 };
 
   int problem_size_n[] = { 4352 };
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/testbed_complex.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/testbed_complex.h`

 * *Files 2% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -124,15 +124,15 @@
     
     result = cudaGetDeviceProperties(&properties, device_idx);
     
     if (result != cudaSuccess) {
     	throw std::runtime_error("cudaGetDeviceProperties() failed");
     }
     
-    if (properties.sharedMemPerMultiprocessor < smem_size) {
+    if (properties.sharedMemPerBlockOptin < smem_size) {
     	return false;
     }
     
     return true;
   }
 
   /// Executes one test
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/testbed_gemm_with_broadcast.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/testbed_gemm_with_broadcast.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -384,15 +384,15 @@
 
     result = cudaGetDeviceProperties(&properties, device_idx);
 
     if (result != cudaSuccess) {
       throw std::runtime_error("cudaGetDeviceProperties() failed");
     }
 
-    if (properties.sharedMemPerMultiprocessor < smem_size) {
+    if (properties.sharedMemPerBlockOptin < smem_size) {
       return false;
     }
 
     return true;
   }
 
   /// Executes one test
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/testbed_gemm_with_reduction.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/testbed_gemm_with_reduction.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -371,15 +371,15 @@
 
     result = cudaGetDeviceProperties(&properties, device_idx);
 
     if (result != cudaSuccess) {
       throw std::runtime_error("cudaGetDeviceProperties() failed");
     }
 
-    if (properties.sharedMemPerMultiprocessor < smem_size) {
+    if (properties.sharedMemPerBlockOptin < smem_size) {
       return false;
     }
 
     return true;
   }
 
   /// Executes one test
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/testbed_grouped.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/testbed_grouped.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/testbed_grouped_rank_2k.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/testbed_grouped_rank_2k.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/testbed_grouped_rank_2k_scheduler.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/testbed_grouped_rank_2k_scheduler.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/testbed_grouped_scheduler.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/testbed_grouped_scheduler.h`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -308,15 +308,16 @@
           int PrefetchTileCount,
           int ThreadCount,
           bool Transpose,
           cutlass::gemm::kernel::GroupScheduleMode GroupScheduleMode0,
           cutlass::gemm::kernel::GroupScheduleMode... Args>
 struct TestbedGroupedGemmScheduler {
 
-  using BaselinePV = BaselineProblemVisitor<cutlass::gemm::kernel::detail::GemmGroupedProblemSizeHelper<Transpose>,
+  using PSHelper = cutlass::gemm::kernel::detail::GemmGroupedProblemSizeHelper<ThreadblockShape, Transpose>;
+  using BaselinePV = BaselineProblemVisitor<PSHelper,
                                             ThreadblockShape,
                                             PrefetchTileCount,
                                             ThreadCount>;
 
   //
   // Data members
   //
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/testbed_interleaved.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/testbed_interleaved.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -126,15 +126,15 @@
 
     result = cudaGetDeviceProperties(&properties, device_idx);
 
     if (result != cudaSuccess) {
       throw std::runtime_error("cudaGetDeviceProperties() failed");
     }
 
-    if (properties.sharedMemPerMultiprocessor < smem_size) {
+    if (properties.sharedMemPerBlockOptin < smem_size) {
       return false;
     }
 
     return true;
   }
 
   /// Executes one test
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/testbed_planar_complex.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/testbed_planar_complex.h`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -136,15 +136,15 @@
 
     result = cudaGetDeviceProperties(&properties, device_idx);
 
     if (result != cudaSuccess) {
       throw std::runtime_error("cudaGetDeviceProperties() failed");
     }
 
-    if (properties.sharedMemPerMultiprocessor < smem_size) {
+    if (properties.sharedMemPerBlockOptin < smem_size) {
       return false;
     }
 
     return true;
   }
   
   bool run(
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/testbed_rank2k_universal.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/testbed_rank2k_universal.h`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -294,15 +294,15 @@
 
     result = cudaGetDeviceProperties(&properties, device_idx);
 
     if (result != cudaSuccess) {
       throw std::runtime_error("cudaGetDeviceProperties() failed");
     }
 
-    if (properties.sharedMemPerMultiprocessor < smem_size) {
+    if (properties.sharedMemPerBlockOptin < smem_size) {
       return false;
     }
 
     return true;
   }
 
   /// Executes one test
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/testbed_rank_k_universal.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/testbed_rank_k_universal.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -282,15 +282,15 @@
 
     result = cudaGetDeviceProperties(&properties, device_idx);
 
     if (result != cudaSuccess) {
       throw std::runtime_error("cudaGetDeviceProperties() failed");
     }
 
-    if (properties.sharedMemPerMultiprocessor < smem_size) {
+    if (properties.sharedMemPerBlockOptin < smem_size) {
       return false;
     }
 
     return true;
   }
 
   /// Executes one test
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/testbed_sanity.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/testbed_sanity.h`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/testbed_sparse.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/testbed_sparse.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -319,15 +319,15 @@
 
     result = cudaGetDeviceProperties(&properties, device_idx);
 
     if (result != cudaSuccess) {
       throw std::runtime_error("cudaGetDeviceProperties() failed");
     }
 
-    if (properties.sharedMemPerMultiprocessor < smem_size) {
+    if (properties.sharedMemPerBlockOptin < smem_size) {
       return false;
     }
 
     return true;
   }
 
   /// Executes one test
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/testbed_splitk.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/testbed_splitk.h`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -84,15 +84,15 @@
 
     result = cudaGetDeviceProperties(&properties, device_idx);
 
     if (result != cudaSuccess) {
       throw std::runtime_error("cudaGetDeviceProperties() failed");
     }
 
-    if (properties.sharedMemPerMultiprocessor < smem_size) {
+    if (properties.sharedMemPerBlockOptin < smem_size) {
       return false;
     }
 
     return true;
   }
   
   /// Executes one test
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/testbed_symm_universal.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/testbed_symm_universal.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -320,15 +320,15 @@
 
     result = cudaGetDeviceProperties(&properties, device_idx);
 
     if (result != cudaSuccess) {
       throw std::runtime_error("cudaGetDeviceProperties() failed");
     }
 
-    if (properties.sharedMemPerMultiprocessor < smem_size) {
+    if (properties.sharedMemPerBlockOptin < smem_size) {
       return false;
     }
 
     return true;
   }
 
   /// Executes one test
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/testbed_trmm_universal.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/testbed_trmm_universal.h`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -360,15 +360,15 @@
 
     result = cudaGetDeviceProperties(&properties, device_idx);
 
     if (result != cudaSuccess) {
       throw std::runtime_error("cudaGetDeviceProperties() failed");
     }
 
-    if (properties.sharedMemPerMultiprocessor < smem_size) {
+    if (properties.sharedMemPerBlockOptin < smem_size) {
       return false;
     }
 
     return true;
   }
 
   /// Executes one test
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/testbed_universal.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/testbed_universal.h`

 * *Files 4% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -54,15 +54,15 @@
 
 namespace test {
 namespace gemm {
 namespace device {
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
-template <typename Gemm>
+template <typename Gemm, bool Relu = false>
 struct TestbedUniversal {
 
   using ElementAccumulator = typename Gemm::ElementAccumulator;
   using ElementCompute = typename Gemm::GemmKernel::Epilogue::OutputOp::ElementCompute;
 
   /// Initialization
   cutlass::Distribution::Kind init_A;
@@ -87,15 +87,15 @@
     uint64_t seed_ = 2080
   ):
     init_A(init_A_), init_B(init_B_), init_C(init_C_), seed(seed_) { }
 
   /// Helper to initialize a tensor view
   template <typename Element, typename Layout>
   bool initialize_tensor(
-    cutlass::TensorView<Element, Layout> view, 
+    cutlass::TensorView<Element, Layout> view,
     cutlass::Distribution::Kind dist_kind,
     uint64_t seed) {
 
     if (dist_kind == cutlass::Distribution::Uniform) {
 
       double scope_max, scope_min;
       int bits_input = cutlass::sizeof_bits<Element>::value;
@@ -113,28 +113,28 @@
       } else {
         scope_max = 8;
         scope_min = -8;
       }
 
       cutlass::reference::host::TensorFillRandomUniform(
         view, seed, scope_max, scope_min, 0);
-    } 
+    }
     else if (dist_kind == cutlass::Distribution::Identity) {
 
       cutlass::reference::host::TensorFillIdentity(view);
-    } 
+    }
     else if (dist_kind == cutlass::Distribution::Gaussian) {
 
       cutlass::reference::host::TensorFillRandomGaussian(view, seed, 0, 0.5);
     }
     else if (dist_kind == cutlass::Distribution::Sequential) {
 
       cutlass::reference::host::BlockFillSequential(
         view.data(), view.capacity());
-    } 
+    }
     else {
       // TODO: Implement the rest
       EXPECT_TRUE(false) << "Not implemented";
       return false;
     }
 
     return true;
@@ -154,38 +154,39 @@
 
     EXPECT_TRUE(initialize_tensor(tensor_A.host_view(), init_A, seed + 2019));
     EXPECT_TRUE(initialize_tensor(tensor_B.host_view(), init_B, seed + 2018));
     EXPECT_TRUE(initialize_tensor(tensor_C.host_view(), init_C, seed + 2017));
 
     // It is possible to randomly initialize to all zeros, so override this with non-zeros
     // in the upper left corner of each operand.
-    tensor_A.host_view().at({0, 0}) = typename Gemm::ElementA(1);
-    tensor_B.host_view().at({0, 0}) = typename Gemm::ElementB(1);
-    tensor_C.host_view().at({0, 0}) = typename Gemm::ElementC(1);
+    cutlass::Coord<2> origin(0);
+    tensor_A.host_view().at(origin) = typename Gemm::ElementA(1);
+    tensor_B.host_view().at(origin) = typename Gemm::ElementB(1);
+    tensor_C.host_view().at(origin) = typename Gemm::ElementC(1);
 
     cutlass::reference::host::TensorCopy(reference_D.host_view(), tensor_C.host_view());
 
     tensor_A.sync_device();
     tensor_B.sync_device();
     tensor_C.sync_device();
     tensor_D.sync_device();
   }
 
   /// Compares computed reference with device reference and outputs to a file if incorrect
   bool compare_reference(
-    cutlass::gemm::GemmCoord problem_size, 
-    ElementCompute alpha, 
+    cutlass::gemm::GemmCoord problem_size,
+    ElementCompute alpha,
     ElementCompute beta) {
 
     tensor_D.sync_host();
 
     EXPECT_GT(cutlass::reference::host::TensorNorm(tensor_A.host_view()), 0);
     EXPECT_GT(cutlass::reference::host::TensorNorm(tensor_B.host_view()), 0);
     EXPECT_GT(cutlass::reference::host::TensorNorm(tensor_C.host_view()), 0);
-    
+
     EXPECT_GT(cutlass::reference::host::TensorNorm(tensor_D.host_view()), 0);
     EXPECT_GT(cutlass::reference::host::TensorNorm(reference_D.host_view()), 0);
 
     bool passed = cutlass::reference::host::TensorEquals(reference_D.host_view(), tensor_D.host_view());
 
     EXPECT_TRUE(passed) << " mismatched reference";
 
@@ -194,69 +195,80 @@
       /*
       std::stringstream fname;
 
       fname << "error_Gemm_device_"
         << problem_size.m() << "x"
         << problem_size.n() << "x"
         << problem_size.k() << "_"
-        << Gemm::ThreadblockShape::kM << "x"  
-        << Gemm::ThreadblockShape::kN << "x"  
+        << Gemm::ThreadblockShape::kM << "x"
+        << Gemm::ThreadblockShape::kN << "x"
         << Gemm::ThreadblockShape::kK << "_"
-        << Gemm::WarpShape::kM << "x"  
-        << Gemm::WarpShape::kN << "x"  
+        << Gemm::WarpShape::kM << "x"
+        << Gemm::WarpShape::kN << "x"
         << Gemm::WarpShape::kK << ".txt";
 
       std::ofstream file(fname.str());
       */
 
       std::ofstream file("testbed_universal_errors.txt");
 
       file
-        << "problem: " << problem_size 
+        << "problem: " << problem_size
         << ", alpha: " << alpha << ", beta: " << beta << "\n\n";
 
-      file 
+      file
         << "A =\n" << tensor_A.host_view()
         << "\nB =\n" << tensor_B.host_view()
         << "\nC =\n" << tensor_C.host_view()
         << "\n\nReference =\n" << reference_D.host_view()
         << "\nComputed =\n" << tensor_D.host_view();
     }
 
     return passed;
   }
 
   /// Verifies the result is a GEMM
   bool verify(
-    cutlass::gemm::GemmCoord problem_size, 
-    ElementCompute alpha, 
+    cutlass::gemm::GemmCoord problem_size,
+    ElementCompute alpha,
     ElementCompute beta) {
 
     //
     // Verify
     //
 
     cutlass::reference::host::GemmComplex<
         typename Gemm::ElementA, typename Gemm::LayoutA,
         typename Gemm::ElementB, typename Gemm::LayoutB,
-        typename Gemm::ElementC, typename Gemm::LayoutC, 
+        typename Gemm::ElementC, typename Gemm::LayoutC,
         ElementCompute, ElementAccumulator
     >(
       problem_size,
-      alpha, 
+      alpha,
       tensor_A.host_ref(),
       Gemm::kTransformA,
       tensor_B.host_ref(),
       Gemm::kTransformB,
-      beta, 
-      tensor_C.host_ref(), 
-      reference_D.host_ref(), 
+      beta,
+      tensor_C.host_ref(),
+      reference_D.host_ref(),
       ElementAccumulator(0)
     );
 
+    if (Relu) {
+      for (int i = 0; i < problem_size.m(); ++i) {
+        for (int j = 0; j < problem_size.n(); ++j) {
+           reference_D.at(cutlass::MatrixCoord(i, j)) =
+                  ((ElementCompute)reference_D.at(cutlass::MatrixCoord(i, j)) < (ElementCompute)0)
+                  ? (typename Gemm::ElementC)0
+                  : reference_D.at(cutlass::MatrixCoord(i, j));
+        }
+      }
+    }
+
     return compare_reference(problem_size, alpha, beta);
   }
 
   /// Returns true if the CUDA device is sufficient to execute the kernel.
   bool sufficient() const {
     //
     // Determine SMEM requirements and waive if not satisfied
@@ -274,28 +286,38 @@
 
     result = cudaGetDeviceProperties(&properties, device_idx);
 
     if (result != cudaSuccess) {
       throw std::runtime_error("cudaGetDeviceProperties() failed");
     }
 
-    if (properties.sharedMemPerMultiprocessor < smem_size) {
+    if (properties.sharedMemPerBlockOptin < smem_size) {
       return false;
     }
 
     return true;
   }
 
   /// Executes one test
   bool run(
     cutlass::gemm::GemmUniversalMode mode,
-    cutlass::gemm::GemmCoord problem_size, 
+    cutlass::gemm::GemmCoord problem_size,
     int batch_count = 1,
-    ElementCompute alpha = ElementCompute(1), 
-    ElementCompute beta = ElementCompute(0)) {
+    ElementCompute alpha = ElementCompute(1),
+    ElementCompute beta = ElementCompute(0))
+  {
+/*
+    std::cout << "\n-----------------------\n";
+    std::cout << "mode: " << (int) mode << "\n";
+    std::cout << "problem size: " << problem_size << "\n";
+    std::cout << "batch_count: " << batch_count << "\n";
+    std::cout << "alpha: " << alpha << "\n";
+    std::cout << "beta: " << beta << "\n";
+    std::cout << "-----------------------\n\n";
+*/
 
     // Waive test if insufficient CUDA device
     if (!sufficient()) {
       if (CUTLASS_TEST_UNIT_ENABLE_WARNINGS) {
         std::cerr << "Test waived due to insufficient CUDA device." << std::endl;
       }
       return true;
@@ -355,40 +377,40 @@
     }
 
     return passed;
   }
 };
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
-template <typename Gemm>
+template <typename Gemm, bool Relu = false>
 bool TestGemmUniversal(
   cutlass::gemm::GemmCoord const & problem_size,
   cutlass::gemm::GemmUniversalMode mode,
   int batch_count,
-  double alpha = 1.0, 
+  double alpha = 1.0,
   double beta = 2.0) {
 
   bool passed = true;
 
-  TestbedUniversal<Gemm> testbed;
-  
+  TestbedUniversal<Gemm, Relu> testbed;
+
   using ElementCompute = typename Gemm::EpilogueOutputOp::ElementCompute;
 
   passed = testbed.run(
     mode,
-    problem_size, 
+    problem_size,
     batch_count,
-    cutlass::from_real<ElementCompute>(alpha), 
+    cutlass::from_real<ElementCompute>(alpha),
     cutlass::from_real<ElementCompute>(beta)
   );
 
   return passed;
 }
 
-template <typename Gemm>
+template <typename Gemm, bool Relu = false>
 bool TestAllGemmUniversal() {
   bool passed = true;
 
 
   int const kMinimumOperandElementSize = 
     std::min(
       int(cutlass::sizeof_bits<typename Gemm::ElementA>::value), 
@@ -408,32 +430,32 @@
                           cutlass::platform::is_same<typename Gemm::LayoutB, cutlass::layout::RowMajor>::value ? 4 : kAlignment;
 
   int const kAlignmentK = cutlass::platform::is_same<typename Gemm::OperatorClass, cutlass::arch::OpClassSimt>::value &&
                           cutlass::platform::is_same<typename Gemm::ElementA, int8_t>::value &&
                           cutlass::platform::is_same<typename Gemm::ElementB, int8_t>::value &&
                           (cutlass::platform::is_same<typename Gemm::LayoutA, cutlass::layout::RowMajor>::value ||
                           cutlass::platform::is_same<typename Gemm::LayoutB, cutlass::layout::ColumnMajor>::value) ? 4 : kAlignment;
-  
-  
-  
+
+
+
   cutlass::gemm::GemmUniversalMode modes[] = {
     cutlass::gemm::GemmUniversalMode::kGemm,
   };
 
   int problem_size_m[] = {
     kAlignmentM, 512 - 3*kAlignmentM
   };
 
   int problem_size_n[] = {
     kAlignmentN, 512 - 2*kAlignmentN
   };
 
   int problem_size_k[] = {
-    kAlignmentK, 
-    Gemm::ThreadblockShape::kK * Gemm::kStages - kAlignmentK, 
+    kAlignmentK,
+    Gemm::ThreadblockShape::kK * Gemm::kStages - kAlignmentK,
     Gemm::ThreadblockShape::kK * Gemm::kStages * 3 - kAlignmentK
   };
 
   int batch_counts[] = {      // may be interpretted as batch count or split-K slices
     1, 2, 3, 5, 7
   };
 
@@ -464,21 +486,21 @@
                   if (k / batch_count < 2 * Gemm::ThreadblockShape::kK) {
                     continue;
                   }
                 }
 
                 cutlass::gemm::GemmCoord problem_size(m, n, k);
 
-                TestbedUniversal<Gemm> testbed;
+                TestbedUniversal<Gemm, Relu> testbed;
 
                 passed = testbed.run(
                   mode,
-                  problem_size, 
+                  problem_size,
                   batch_count,
-                  cutlass::from_real<ElementCompute>(alpha), 
+                  cutlass::from_real<ElementCompute>(alpha),
                   cutlass::from_real<ElementCompute>(beta)
                 );
 
                 if (!passed) {
                   return false;
                 }
               }
@@ -494,17 +516,17 @@
   for (int split_k_slices = 1; split_k_slices <= 3; ++split_k_slices) {
     TestbedUniversal<Gemm> testbed;
 
     cutlass::gemm::GemmCoord problem_size(72, 56, 8192);
 
     passed = testbed.run(
       cutlass::gemm::GemmUniversalMode::kGemm,
-      problem_size, 
+      problem_size,
       split_k_slices,
-      cutlass::from_real<ElementCompute>(1.0), 
+      cutlass::from_real<ElementCompute>(1.0),
       cutlass::from_real<ElementCompute>(2.0)
     );
 
     if (!passed) {
       break;
     }
   }
@@ -516,8 +538,7 @@
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
 } // namespace device
 } // namespace gemm
 } // namespace test
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
-
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/testbed_utils.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/testbed_utils.h`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/trmm_cf32n_cf32n_cf32t_tensor_op_f32_sm80.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/trmm_cf32n_cf32n_cf32t_tensor_op_f32_sm80.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/trmm_cf32n_cf32n_cf32t_tensor_op_fast_f32_sm80.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/trmm_cf32n_cf32n_cf32t_tensor_op_fast_f32_sm80.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/trmm_cf64_cf64_cf64_tensor_op_f64_sm90.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/trmm_cf64_cf64_cf64_tensor_op_f64_sm90.cu`

 * *Files 2% similar despite different names*

```diff
@@ -44,15 +44,15 @@
 #include "cutlass/util/reference/host/tensor_compare.h"
 #include "cutlass/util/reference/host/tensor_copy.h"
 #include "cutlass/util/reference/host/tensor_fill.h"
 #include "cutlass/util/tensor_view_io.h"
 
 #include "testbed_trmm_universal.h"
 
-#if defined(CUTLASS_ARCH_MMA_SM90_F64_MMA_ENABLED)
+#if defined(CUTLASS_ARCH_MMA_SM90_SUPPORTED)
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
 TEST(SM90_Device_Trmm_cf64n_cf64n_cf64t_ls_u_nu_tensor_op_f64_gaussian, 32x32x16_16x16x16) {
 
   using ElementOutput = cutlass::complex<double>;
   using ElementAccumulator = cutlass::complex<double>;
@@ -130,8 +130,8 @@
   >;
 
   EXPECT_TRUE(test::gemm::device::TestAllTrmmUniversal<Trmm>());
 }
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
-#endif // #if defined(CUTLASS_ARCH_MMA_SM90_F64_MMA_ENABLED)
+#endif // #if defined(CUTLASS_ARCH_MMA_SM90_SUPPORTED)
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/trmm_cf64n_cf64n_cf64t_tensor_op_f64_gaussian_sm80.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/trmm_cf64n_cf64n_cf64t_tensor_op_f64_gaussian_sm80.cu`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/trmm_cf64n_cf64n_cf64t_tensor_op_f64_sm80.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/trmm_cf64n_cf64n_cf64t_tensor_op_f64_sm80.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/trmm_f32n_f32t_f32t_tensor_op_fast_f32_ls_sm80.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/trmm_f32n_f32t_f32t_tensor_op_fast_f32_ls_sm80.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/trmm_f32n_f32t_f32t_tensor_op_fast_f32_rs_sm80.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/trmm_f32n_f32t_f32t_tensor_op_fast_f32_rs_sm80.cu`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/trmm_f32t_f32n_f32n_tensor_op_fast_f32_ls_sm80.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/trmm_f32t_f32n_f32n_tensor_op_fast_f32_ls_sm80.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/trmm_f32t_f32n_f32t_tensor_op_fast_f32_ls_sm80.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/trmm_f32t_f32n_f32t_tensor_op_fast_f32_ls_sm80.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/trmm_f64n_f64n_f64t_tensor_op_f64_ls_sm80.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/trmm_f64n_f64n_f64t_tensor_op_f64_ls_sm80.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/trmm_f64n_f64n_f64t_tensor_op_f64_rs_sm80.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/trmm_f64n_f64n_f64t_tensor_op_f64_rs_sm80.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/trmm_f64n_f64t_f64t_tensor_op_f64_rs_sm80.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/trmm_f64n_f64t_f64t_tensor_op_f64_rs_sm80.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/trmm_f64t_f64t_f64n_tensor_op_f64_ls_sm80.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/trmm_f64t_f64t_f64n_tensor_op_f64_ls_sm80.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/trmm_f64t_f64t_f64n_tensor_op_f64_rs_sm80.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/trmm_f64t_f64t_f64n_tensor_op_f64_rs_sm80.cu`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/trmm_tf32n_tf32t_f32t_tensor_op_f32_ls_sm80.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/trmm_tf32n_tf32t_f32t_tensor_op_f32_ls_sm80.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/trmm_tf32n_tf32t_f32t_tensor_op_f32_rs_sm80.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/trmm_tf32n_tf32t_f32t_tensor_op_f32_rs_sm80.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/trmm_tf32t_tf32n_f32n_tensor_op_f32_ls_sm80.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/trmm_tf32t_tf32n_f32n_tensor_op_f32_ls_sm80.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/trmm_tf32t_tf32n_f32t_tensor_op_f32_ls_sm80.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/device/trmm_tf32t_tf32n_f32t_tensor_op_f32_ls_sm80.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/kernel/batched_gemv.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/kernel/batched_gemv.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/kernel/testbed_gemv.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/kernel/testbed_gemv.h`

 * *Files 2% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -57,15 +57,15 @@
         typename ThreadShape_,
         typename ElementAB_,
         typename ElementAccumulator_,
         typename ElementCD_,
         typename LayoutA_,
         typename LayoutB_,
         typename LayoutCD_,
-        int LDG_B = 1, // batch tile size
+        int THREAD_B = 1, // batch tile size
         bool DEBUG=false>
 void batched_gemv_kernel_test(cutlass::gemm::BatchedGemmCoord problem_size,
                               ElementCD_ alpha = ElementCD_(1),
                               ElementCD_ beta = ElementCD_(0),
                               bool perf_test = false,
                               int perf_test_iter = 1)
 {
@@ -150,15 +150,15 @@
     matrix_C_computed.sync_device();
 
     ThreadBlockSwizzle swizzle;
 
     cutlass::gemm::BatchedGemmCoord tiled_size{ThreadBlockShape::kM,
                                                 ThreadBlockShape::kN,
                                                 problem_size.k(), // no split-k
-                                                DEBUG ? 1 : LDG_B };
+                                                DEBUG ? 1 : THREAD_B };
 
     cutlass::gemm::BatchedGemmCoord tiled_shape = swizzle.get_tiled_shape(problem_size, tiled_size);
 
     #if 0 
     printf("tiled_size = %d %d %d %d\n", tiled_size.m(), tiled_size.n(), tiled_size.k(), tiled_size.batch());
     printf("tiled_shape = %d %d %d %d\n", tiled_shape.m(), tiled_shape.n(), tiled_shape.k(), tiled_shape.batch());
     #endif
@@ -330,29 +330,29 @@
         typename ThreadShape_,
         typename ElementAB_,
         typename ElementAccumulator_,
         typename ElementCD_,
         typename LayoutA_,
         typename LayoutB_,
         typename LayoutCD_,
-        int LDG_B = 1, // batch tile size
+        int THREAD_B = 1, // batch tile size
         bool DEBUG=false>
 void batched_gemv_kernel_perf_test(cutlass::gemm::BatchedGemmCoord problem_size,
                                    ElementCD_ alpha = ElementCD_(1),
                                    ElementCD_ beta = ElementCD_(0),
                                    int iter = 50)
 {
     batched_gemv_kernel_test<ThreadBlockShape_,
                              ThreadShape_,
                              ElementAB_,
                              ElementAccumulator_,
                              ElementCD_,
                              LayoutA_,
                              LayoutB_,
                              LayoutCD_,
-                             LDG_B,
+                             THREAD_B,
                              DEBUG>(problem_size, alpha, beta, true, iter);
 }
     
 } // namespace threadblock
 } // namespace kernel
 } // namespace test
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/thread/gemm_sm50.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/thread/gemm_sm50.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/thread/gemm_sm60.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/thread/gemm_sm60.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/thread/gemm_sm61.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/thread/gemm_sm61.cu`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/thread/host/gemm_sm60_host.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/thread/host/gemm_sm60_host.cu`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/thread/host/testbed_host.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/thread/host/testbed_host.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/thread/testbed.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/thread/testbed.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/threadblock/batched_gemv.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/threadblock/batched_gemv.cu`

 * *Files 16% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -103,28 +103,28 @@
 
 template<typename Shape_,
          typename ElementAB_,
          typename ElementC_,
          typename LayoutA_,
          typename LayoutB_,
          typename LayoutC_,
-         int LDG_N,
-         int LDG_K,
+         int THREAD_N,
+         int THREAD_K,
          int MAX_THREADS_PER_BLOCK=512,
          bool DEBUG=false>
 void batched_gemv_threadblock_test(cutlass::gemm::GemmCoord problem_size, int num_batch)
 {
   using Shape = Shape_;
   using ElementA = ElementAB_;
   using LayoutA = LayoutA_;
   using ElementB = ElementAB_;
   using LayoutB = LayoutB_;
   using ElementC = ElementC_;
   using LayoutC = LayoutC_;
-  using ThreadShape = cutlass::gemm::GemmShape<1, LDG_N, LDG_K>;
+  using ThreadShape = cutlass::gemm::GemmShape<1, THREAD_N, THREAD_K>;
 
   using Core = typename cutlass::gemm::threadblock::DefaultGemvCore<
     Shape,
     ThreadShape,
     ElementA,
     LayoutA,
     ElementB,
@@ -188,22 +188,22 @@
   }
 
   matrix_A.sync_device();
   matrix_B.sync_device();
   matrix_C_computed.sync_device();
 
   dim3 grid(1, 1);      // only 1 CTA is used
-  dim3 block(Shape::kN / LDG_N, num_batch, 1);
+  dim3 block(Shape::kN / THREAD_N, num_batch, 1);
 
   #if 0
   printf("block dim = %d x %d\n", block.x, block.y);
   #endif
 
   // Some sanity checks
-  EXPECT_TRUE( problem_size.n() % LDG_N == 0 );
+  EXPECT_TRUE( problem_size.n() % THREAD_N == 0 );
   EXPECT_TRUE( block.x*block.y <= MAX_THREADS_PER_BLOCK );
 
   test::gemm::threadblock::batched_gemv_threadblock_test_kernel<Mma><<< grid, block >>>(
     problem_size,
     matrix_A.capacity(),
     matrix_B.capacity(),
     matrix_C_computed.capacity(),
@@ -257,390 +257,390 @@
 // C: ColumnMajor
 
 TEST(SM50_batched_gemv_threadblock, 4x1x64x64_crc_fp32_fp32_2N_2K) {
   
   using namespace test::gemm::threadblock;
   cutlass::gemm::GemmCoord problem_size(1, 64, 64);
   const int num_batch = 4;
-  const int LDG_N = 2;
-  const int LDG_K = 2;
+  const int THREAD_N = 2;
+  const int THREAD_K = 2;
  
-  using Shape = cutlass::gemm::GemmShape<1, 64, LDG_K>;
+  using Shape = cutlass::gemm::GemmShape<1, 64, THREAD_K>;
   batched_gemv_threadblock_test<Shape, float, float, 
                                 cutlass::layout::ColumnMajor,
                                 cutlass::layout::RowMajor,
                                 cutlass::layout::ColumnMajor,
-                                LDG_N, LDG_K>(problem_size, num_batch);
+                                THREAD_N, THREAD_K>(problem_size, num_batch);
 }
 
 TEST(SM50_batched_gemv_threadblock, 5x1x128x128_crc_fp32_fp32_4N_4K) {
   using namespace test::gemm::threadblock;
   cutlass::gemm::GemmCoord problem_size(1, 128, 128);
   const int num_batch = 5;
-  const int LDG_N = 4;
-  const int LDG_K = 4;
+  const int THREAD_N = 4;
+  const int THREAD_K = 4;
 
-  using Shape = cutlass::gemm::GemmShape<1, 128, LDG_K>;
+  using Shape = cutlass::gemm::GemmShape<1, 128, THREAD_K>;
   batched_gemv_threadblock_test<Shape, float, float, 
                                 cutlass::layout::ColumnMajor,
                                 cutlass::layout::RowMajor,
                                 cutlass::layout::ColumnMajor,
-                                LDG_N, LDG_K>(problem_size, num_batch);
+                                THREAD_N, THREAD_K>(problem_size, num_batch);
 }
 
 TEST(SM50_batched_gemv_threadblock, 16x1x17x64_crc_fp32_fp32_1N_4K) {
   using namespace test::gemm::threadblock;
   cutlass::gemm::GemmCoord problem_size(1, 17, 64);
   const int num_batch = 16;
-  const int LDG_N = 1;
-  const int LDG_K = 4;
+  const int THREAD_N = 1;
+  const int THREAD_K = 4;
 
-  using Shape = cutlass::gemm::GemmShape<1, 32, LDG_K>;
+  using Shape = cutlass::gemm::GemmShape<1, 32, THREAD_K>;
   batched_gemv_threadblock_test<Shape,
                                 float, float, 
                                 cutlass::layout::ColumnMajor,
                                 cutlass::layout::RowMajor,
                                 cutlass::layout::ColumnMajor,
-                                LDG_N, LDG_K>(problem_size, num_batch);
+                                THREAD_N, THREAD_K>(problem_size, num_batch);
 }
 
 TEST(SM50_batched_gemv_threadblock, 4x1x64x64_crc_fp16_fp32_2N_2K) {
   using namespace test::gemm::threadblock;
   cutlass::gemm::GemmCoord problem_size(1, 64, 64);
   const int num_batch = 4;
-  const int LDG_N = 2;
-  const int LDG_K = 2;
+  const int THREAD_N = 2;
+  const int THREAD_K = 2;
   
-  using Shape = cutlass::gemm::GemmShape<1, 64, LDG_K>;
+  using Shape = cutlass::gemm::GemmShape<1, 64, THREAD_K>;
   batched_gemv_threadblock_test<Shape,
                                 cutlass::half_t, float, 
                                 cutlass::layout::ColumnMajor,
                                 cutlass::layout::RowMajor,
                                 cutlass::layout::ColumnMajor,
-                                LDG_N, LDG_K>(problem_size, num_batch);
+                                THREAD_N, THREAD_K>(problem_size, num_batch);
 }
 
 TEST(SM50_batched_gemv_threadblock, 4x1x64x64_crc_fp16_fp32_2N_8K) {
   using namespace test::gemm::threadblock;
   cutlass::gemm::GemmCoord problem_size(1, 64, 64);
   const int num_batch = 4;
-  const int LDG_N = 2;
-  const int LDG_K = 8;
+  const int THREAD_N = 2;
+  const int THREAD_K = 8;
  
-  using Shape = cutlass::gemm::GemmShape<1, 64, LDG_K>;
+  using Shape = cutlass::gemm::GemmShape<1, 64, THREAD_K>;
   batched_gemv_threadblock_test<Shape,
                                 cutlass::half_t, float, 
                                 cutlass::layout::ColumnMajor,
                                 cutlass::layout::RowMajor,
                                 cutlass::layout::ColumnMajor,
-                                LDG_N, LDG_K>(problem_size, num_batch);
+                                THREAD_N, THREAD_K>(problem_size, num_batch);
 }
 
 TEST(SM50_batched_gemv_threadblock, 16x1x17x64_crc_fp16_fp32_1N_4K) {
   using namespace test::gemm::threadblock;
   cutlass::gemm::GemmCoord problem_size(1, 17, 64);
   const int num_batch = 16;
-  const int LDG_N = 1;
-  const int LDG_K = 4;
+  const int THREAD_N = 1;
+  const int THREAD_K = 4;
 
-  using Shape = cutlass::gemm::GemmShape<1, 32, LDG_K>;
+  using Shape = cutlass::gemm::GemmShape<1, 32, THREAD_K>;
   batched_gemv_threadblock_test<Shape,
                                 cutlass::half_t, float, 
                                 cutlass::layout::ColumnMajor,
                                 cutlass::layout::RowMajor,
                                 cutlass::layout::ColumnMajor,
-                                LDG_N, LDG_K>(problem_size, num_batch);
+                                THREAD_N, THREAD_K>(problem_size, num_batch);
 }
 
 TEST(SM50_batched_gemv_threadblock, 4x1x64x64_crc_i8_i32_2N_4K) {
   using namespace test::gemm::threadblock;
   cutlass::gemm::GemmCoord problem_size(1, 64, 64);
   const int num_batch = 4;
-  const int LDG_N = 2;
-  const int LDG_K = 4;
+  const int THREAD_N = 2;
+  const int THREAD_K = 4;
 
-  using Shape = cutlass::gemm::GemmShape<1, 128, LDG_K>;
+  using Shape = cutlass::gemm::GemmShape<1, 128, THREAD_K>;
   batched_gemv_threadblock_test<Shape,
                                 int8_t, int32_t, 
                                 cutlass::layout::ColumnMajor,
                                 cutlass::layout::RowMajor,
                                 cutlass::layout::ColumnMajor,
-                                LDG_N, LDG_K>(problem_size, num_batch);
+                                THREAD_N, THREAD_K>(problem_size, num_batch);
 }
 
 TEST(SM50_batched_gemv_threadblock, 16x1x17x64_crc_i8_i32_1N_4K) {
   using namespace test::gemm::threadblock;
   cutlass::gemm::GemmCoord problem_size(1, 17, 64);
   const int num_batch = 16;
-  const int LDG_N = 1;
-  const int LDG_K = 4;
+  const int THREAD_N = 1;
+  const int THREAD_K = 4;
 
-  using Shape = cutlass::gemm::GemmShape<1, 32, LDG_K>;
+  using Shape = cutlass::gemm::GemmShape<1, 32, THREAD_K>;
   batched_gemv_threadblock_test<Shape,
                                 int8_t, int32_t, 
                                 cutlass::layout::ColumnMajor,
                                 cutlass::layout::RowMajor,
                                 cutlass::layout::ColumnMajor,
-                                LDG_N, LDG_K>(problem_size, num_batch);
+                                THREAD_N, THREAD_K>(problem_size, num_batch);
 }
 
 // A: RowMajor
 // B: ColumnMajor
 // C: RowMajor
 
 TEST(SM50_batched_gemv_threadblock, 4x1x64x64_rcr_fp32_fp32_2N_2K) {
   
   using namespace test::gemm::threadblock;
   cutlass::gemm::GemmCoord problem_size(1, 64, 64);
   const int num_batch = 4;
-  const int LDG_N = 2;
-  const int LDG_K = 2;
+  const int THREAD_N = 2;
+  const int THREAD_K = 2;
  
-  using Shape = cutlass::gemm::GemmShape<1, 64, LDG_K>;
+  using Shape = cutlass::gemm::GemmShape<1, 64, THREAD_K>;
   batched_gemv_threadblock_test<Shape, float, float, 
                                 cutlass::layout::RowMajor,
                                 cutlass::layout::ColumnMajor,
                                 cutlass::layout::RowMajor,
-                                LDG_N, LDG_K>(problem_size, num_batch);
+                                THREAD_N, THREAD_K>(problem_size, num_batch);
 }
 
 TEST(SM50_batched_gemv_threadblock, 5x1x128x128_rcr_fp32_fp32_4N_4K) {
   using namespace test::gemm::threadblock;
   cutlass::gemm::GemmCoord problem_size(1, 128, 128);
   const int num_batch = 5;
-  const int LDG_N = 4;
-  const int LDG_K = 4;
+  const int THREAD_N = 4;
+  const int THREAD_K = 4;
 
-  using Shape = cutlass::gemm::GemmShape<1, 128, LDG_K>;
+  using Shape = cutlass::gemm::GemmShape<1, 128, THREAD_K>;
   batched_gemv_threadblock_test<Shape, float, float, 
                                 cutlass::layout::RowMajor,
                                 cutlass::layout::ColumnMajor,
                                 cutlass::layout::RowMajor,
-                                LDG_N, LDG_K>(problem_size, num_batch);
+                                THREAD_N, THREAD_K>(problem_size, num_batch);
 }
 
 TEST(SM50_batched_gemv_threadblock, 16x1x17x64_rcr_fp32_fp32_1N_4K) {
   using namespace test::gemm::threadblock;
   cutlass::gemm::GemmCoord problem_size(1, 17, 64);
   const int num_batch = 16;
-  const int LDG_N = 1;
-  const int LDG_K = 4;
+  const int THREAD_N = 1;
+  const int THREAD_K = 4;
 
-  using Shape = cutlass::gemm::GemmShape<1, 32, LDG_K>;
+  using Shape = cutlass::gemm::GemmShape<1, 32, THREAD_K>;
   batched_gemv_threadblock_test<Shape,
                                 float, float, 
                                 cutlass::layout::RowMajor,
                                 cutlass::layout::ColumnMajor,
                                 cutlass::layout::RowMajor,
-                                LDG_N, LDG_K>(problem_size, num_batch);
+                                THREAD_N, THREAD_K>(problem_size, num_batch);
 }
 
 TEST(SM50_batched_gemv_threadblock, 4x1x64x64_rcr_fp16_fp32_2N_2K) {
   using namespace test::gemm::threadblock;
   cutlass::gemm::GemmCoord problem_size(1, 64, 64);
   const int num_batch = 4;
-  const int LDG_N = 2;
-  const int LDG_K = 2;
+  const int THREAD_N = 2;
+  const int THREAD_K = 2;
   
-  using Shape = cutlass::gemm::GemmShape<1, 64, LDG_K>;
+  using Shape = cutlass::gemm::GemmShape<1, 64, THREAD_K>;
   batched_gemv_threadblock_test<Shape,
                                 cutlass::half_t, float, 
                                 cutlass::layout::RowMajor,
                                 cutlass::layout::ColumnMajor,
                                 cutlass::layout::RowMajor,
-                                LDG_N, LDG_K>(problem_size, num_batch);
+                                THREAD_N, THREAD_K>(problem_size, num_batch);
 }
 
 TEST(SM50_batched_gemv_threadblock, 4x1x64x64_rcr_fp16_fp32_2N_8K) {
   using namespace test::gemm::threadblock;
   cutlass::gemm::GemmCoord problem_size(1, 64, 64);
   const int num_batch = 4;
-  const int LDG_N = 2;
-  const int LDG_K = 8;
+  const int THREAD_N = 2;
+  const int THREAD_K = 8;
  
-  using Shape = cutlass::gemm::GemmShape<1, 64, LDG_K>;
+  using Shape = cutlass::gemm::GemmShape<1, 64, THREAD_K>;
   batched_gemv_threadblock_test<Shape,
                                 cutlass::half_t, float, 
                                 cutlass::layout::RowMajor,
                                 cutlass::layout::ColumnMajor,
                                 cutlass::layout::RowMajor,
-                                LDG_N, LDG_K>(problem_size, num_batch);
+                                THREAD_N, THREAD_K>(problem_size, num_batch);
 }
 
 TEST(SM50_batched_gemv_threadblock, 16x1x17x64_rcr_fp16_fp32_1N_4K) {
   using namespace test::gemm::threadblock;
   cutlass::gemm::GemmCoord problem_size(1, 17, 64);
   const int num_batch = 16;
-  const int LDG_N = 1;
-  const int LDG_K = 4;
+  const int THREAD_N = 1;
+  const int THREAD_K = 4;
 
-  using Shape = cutlass::gemm::GemmShape<1, 32, LDG_K>;
+  using Shape = cutlass::gemm::GemmShape<1, 32, THREAD_K>;
   batched_gemv_threadblock_test<Shape,
                                 cutlass::half_t, float, 
                                 cutlass::layout::RowMajor,
                                 cutlass::layout::ColumnMajor,
                                 cutlass::layout::RowMajor,
-                                LDG_N, LDG_K>(problem_size, num_batch);
+                                THREAD_N, THREAD_K>(problem_size, num_batch);
 }
 
 TEST(SM50_batched_gemv_threadblock, 4x1x64x64_rcr_i8_i32_2N_4K) {
   using namespace test::gemm::threadblock;
   cutlass::gemm::GemmCoord problem_size(1, 64, 64);
   const int num_batch = 4;
-  const int LDG_N = 2;
-  const int LDG_K = 4;
+  const int THREAD_N = 2;
+  const int THREAD_K = 4;
 
-  using Shape = cutlass::gemm::GemmShape<1, 128, LDG_K>;
+  using Shape = cutlass::gemm::GemmShape<1, 128, THREAD_K>;
   batched_gemv_threadblock_test<Shape,
                                 int8_t, int32_t, 
                                 cutlass::layout::RowMajor,
                                 cutlass::layout::ColumnMajor,
                                 cutlass::layout::RowMajor,
-                                LDG_N, LDG_K>(problem_size, num_batch);
+                                THREAD_N, THREAD_K>(problem_size, num_batch);
 }
 
 TEST(SM50_batched_gemv_threadblock, 16x1x17x64_rcr_i8_i32_1N_4K) {
   using namespace test::gemm::threadblock;
   cutlass::gemm::GemmCoord problem_size(1, 17, 64);
   const int num_batch = 16;
-  const int LDG_N = 1;
-  const int LDG_K = 4;
+  const int THREAD_N = 1;
+  const int THREAD_K = 4;
 
-  using Shape = cutlass::gemm::GemmShape<1, 32, LDG_K>;
+  using Shape = cutlass::gemm::GemmShape<1, 32, THREAD_K>;
   batched_gemv_threadblock_test<Shape,
                                 int8_t, int32_t, 
                                 cutlass::layout::RowMajor,
                                 cutlass::layout::ColumnMajor,
                                 cutlass::layout::RowMajor,
-                                LDG_N, LDG_K>(problem_size, num_batch);
+                                THREAD_N, THREAD_K>(problem_size, num_batch);
 }
 
 // A: RowMajor
 // B: ColumnMajor
 // C: ColumnMajor
 
 TEST(SM50_batched_gemv_threadblock, 4x1x64x64_rcc_fp32_fp32_2N_2K) {
   
   using namespace test::gemm::threadblock;
   cutlass::gemm::GemmCoord problem_size(1, 64, 64);
   const int num_batch = 4;
-  const int LDG_N = 2;
-  const int LDG_K = 2;
+  const int THREAD_N = 2;
+  const int THREAD_K = 2;
  
-  using Shape = cutlass::gemm::GemmShape<1, 64, LDG_K>;
+  using Shape = cutlass::gemm::GemmShape<1, 64, THREAD_K>;
   batched_gemv_threadblock_test<Shape, float, float, 
                                 cutlass::layout::RowMajor,
                                 cutlass::layout::ColumnMajor,
                                 cutlass::layout::ColumnMajor,
-                                LDG_N, LDG_K>(problem_size, num_batch);
+                                THREAD_N, THREAD_K>(problem_size, num_batch);
 }
 
 TEST(SM50_batched_gemv_threadblock, 5x1x128x128_rcc_fp32_fp32_4N_4K) {
   using namespace test::gemm::threadblock;
   cutlass::gemm::GemmCoord problem_size(1, 128, 128);
   const int num_batch = 5;
-  const int LDG_N = 4;
-  const int LDG_K = 4;
+  const int THREAD_N = 4;
+  const int THREAD_K = 4;
 
-  using Shape = cutlass::gemm::GemmShape<1, 128, LDG_K>;
+  using Shape = cutlass::gemm::GemmShape<1, 128, THREAD_K>;
   batched_gemv_threadblock_test<Shape, float, float, 
                                 cutlass::layout::RowMajor,
                                 cutlass::layout::ColumnMajor,
                                 cutlass::layout::ColumnMajor,
-                                LDG_N, LDG_K>(problem_size, num_batch);
+                                THREAD_N, THREAD_K>(problem_size, num_batch);
 }
 
 TEST(SM50_batched_gemv_threadblock, 16x1x17x64_rcc_fp32_fp32_1N_4K) {
   using namespace test::gemm::threadblock;
   cutlass::gemm::GemmCoord problem_size(1, 17, 64);
   const int num_batch = 16;
-  const int LDG_N = 1;
-  const int LDG_K = 4;
+  const int THREAD_N = 1;
+  const int THREAD_K = 4;
 
-  using Shape = cutlass::gemm::GemmShape<1, 32, LDG_K>;
+  using Shape = cutlass::gemm::GemmShape<1, 32, THREAD_K>;
   batched_gemv_threadblock_test<Shape,
                                 float, float, 
                                 cutlass::layout::RowMajor,
                                 cutlass::layout::ColumnMajor,
                                 cutlass::layout::ColumnMajor,
-                                LDG_N, LDG_K>(problem_size, num_batch);
+                                THREAD_N, THREAD_K>(problem_size, num_batch);
 }
 
 TEST(SM50_batched_gemv_threadblock, 4x1x64x64_rcc_fp16_fp32_2N_2K) {
   using namespace test::gemm::threadblock;
   cutlass::gemm::GemmCoord problem_size(1, 64, 64);
   const int num_batch = 4;
-  const int LDG_N = 2;
-  const int LDG_K = 2;
+  const int THREAD_N = 2;
+  const int THREAD_K = 2;
   
-  using Shape = cutlass::gemm::GemmShape<1, 64, LDG_K>;
+  using Shape = cutlass::gemm::GemmShape<1, 64, THREAD_K>;
   batched_gemv_threadblock_test<Shape,
                                 cutlass::half_t, float, 
                                 cutlass::layout::RowMajor,
                                 cutlass::layout::ColumnMajor,
                                 cutlass::layout::ColumnMajor,
-                                LDG_N, LDG_K>(problem_size, num_batch);
+                                THREAD_N, THREAD_K>(problem_size, num_batch);
 }
 
 TEST(SM50_batched_gemv_threadblock, 4x1x64x64_rcc_fp16_fp32_2N_8K) {
   using namespace test::gemm::threadblock;
   cutlass::gemm::GemmCoord problem_size(1, 64, 64);
   const int num_batch = 4;
-  const int LDG_N = 2;
-  const int LDG_K = 8;
+  const int THREAD_N = 2;
+  const int THREAD_K = 8;
  
-  using Shape = cutlass::gemm::GemmShape<1, 64, LDG_K>;
+  using Shape = cutlass::gemm::GemmShape<1, 64, THREAD_K>;
   batched_gemv_threadblock_test<Shape,
                                 cutlass::half_t, float, 
                                 cutlass::layout::RowMajor,
                                 cutlass::layout::ColumnMajor,
                                 cutlass::layout::ColumnMajor,
-                                LDG_N, LDG_K>(problem_size, num_batch);
+                                THREAD_N, THREAD_K>(problem_size, num_batch);
 }
 
 TEST(SM50_batched_gemv_threadblock, 16x1x17x64_rcc_fp16_fp32_1N_4K) {
   using namespace test::gemm::threadblock;
   cutlass::gemm::GemmCoord problem_size(1, 17, 64);
   const int num_batch = 16;
-  const int LDG_N = 1;
-  const int LDG_K = 4;
+  const int THREAD_N = 1;
+  const int THREAD_K = 4;
 
-  using Shape = cutlass::gemm::GemmShape<1, 32, LDG_K>;
+  using Shape = cutlass::gemm::GemmShape<1, 32, THREAD_K>;
   batched_gemv_threadblock_test<Shape,
                                 cutlass::half_t, float, 
                                 cutlass::layout::RowMajor,
                                 cutlass::layout::ColumnMajor,
                                 cutlass::layout::ColumnMajor,
-                                LDG_N, LDG_K>(problem_size, num_batch);
+                                THREAD_N, THREAD_K>(problem_size, num_batch);
 }
 
 TEST(SM50_batched_gemv_threadblock, 4x1x64x64_rcc_i8_i32_2N_4K) {
   using namespace test::gemm::threadblock;
   cutlass::gemm::GemmCoord problem_size(1, 64, 64);
   const int num_batch = 4;
-  const int LDG_N = 2;
-  const int LDG_K = 4;
+  const int THREAD_N = 2;
+  const int THREAD_K = 4;
 
-  using Shape = cutlass::gemm::GemmShape<1, 128, LDG_K>;
+  using Shape = cutlass::gemm::GemmShape<1, 128, THREAD_K>;
   batched_gemv_threadblock_test<Shape,
                                 int8_t, int32_t, 
                                 cutlass::layout::RowMajor,
                                 cutlass::layout::ColumnMajor,
                                 cutlass::layout::ColumnMajor,
-                                LDG_N, LDG_K>(problem_size, num_batch);
+                                THREAD_N, THREAD_K>(problem_size, num_batch);
 }
 
 TEST(SM50_batched_gemv_threadblock, 16x1x17x64_rcc_i8_i32_1N_4K) {
   using namespace test::gemm::threadblock;
   cutlass::gemm::GemmCoord problem_size(1, 17, 64);
   const int num_batch = 16;
-  const int LDG_N = 1;
-  const int LDG_K = 4;
+  const int THREAD_N = 1;
+  const int THREAD_K = 4;
 
-  using Shape = cutlass::gemm::GemmShape<1, 32, LDG_K>;
+  using Shape = cutlass::gemm::GemmShape<1, 32, THREAD_K>;
   batched_gemv_threadblock_test<Shape,
                                 int8_t, int32_t, 
                                 cutlass::layout::RowMajor,
                                 cutlass::layout::ColumnMajor,
                                 cutlass::layout::ColumnMajor,
-                                LDG_N, LDG_K>(problem_size, num_batch);
+                                THREAD_N, THREAD_K>(problem_size, num_batch);
 }
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/threadblock/epilogue_workspace.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/threadblock/epilogue_workspace.cu`

 * *Files 2% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/threadblock/mma_multistage.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/threadblock/mma_multistage.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/threadblock/mma_multistage_sparse.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/threadblock/mma_multistage_sparse.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/threadblock/mma_multistage_sparse_testbed.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/threadblock/mma_multistage_sparse_testbed.h`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -253,15 +253,15 @@
 
     result = cudaGetDeviceProperties(&properties, device_idx);
 
     if (result != cudaSuccess) {
       throw std::runtime_error("cudaGetDeviceProperties() failed");
     }
 
-    if (properties.sharedMemPerMultiprocessor < smem_size) {
+    if (properties.sharedMemPerBlockOptin < smem_size) {
       return false;
     }
 
     return true;
   }
 
   /// Runs the test
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/threadblock/mma_multistage_testbed.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/threadblock/mma_multistage_testbed.h`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/threadblock/mma_multistage_testbed_slicedk.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/threadblock/mma_multistage_testbed_slicedk.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/threadblock/mma_pipelined_simt.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/threadblock/mma_pipelined_simt.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/threadblock/mma_pipelined_slicedk.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/threadblock/mma_pipelined_slicedk.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/threadblock/mma_pipelined_sm70.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/threadblock/mma_pipelined_sm70.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/threadblock/mma_pipelined_sm75.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/threadblock/mma_pipelined_sm75.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/threadblock/mma_pipelined_sm80.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/threadblock/mma_pipelined_sm80.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/threadblock/mma_pipelined_testbed.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/threadblock/mma_pipelined_testbed.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/threadblock/mma_pipelined_testbed_slicedk.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/threadblock/mma_pipelined_testbed_slicedk.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/threadblock/mma_pipelined_wmma_sm70.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/threadblock/mma_pipelined_wmma_sm70.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/threadblock/mma_pipelined_wmma_sm75.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/threadblock/mma_pipelined_wmma_sm75.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/threadblock/mma_planar_complex_sm80.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/threadblock/mma_planar_complex_sm80.cu`

 * *Files 2% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/threadblock/mma_planar_complex_testbed.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/threadblock/mma_planar_complex_testbed.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/threadblock/mma_singlestage_wmma_sm70.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/threadblock/mma_singlestage_wmma_sm70.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/threadblock/mma_singlestage_wmma_sm75.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/threadblock/mma_singlestage_wmma_sm75.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/warp/gemm_complex_sm80.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/warp/gemm_complex_sm80.cu`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -52,15 +52,15 @@
 #include "testbed.h"
 
 #if defined(CUTLASS_ARCH_MMA_SM80_SUPPORTED)
 
 ////////////////////////////////////////////////////////////////////////////////////////////////////
 // complex<double> * complex<double> => complex<double>
 // Input data type: complex<double>
-// Math instruction: MMA.884.F64.F64
+// Math instruction: mma.sync.aligned.m8n8k4.f64.f64.f64.f64
 // Output data type: complex<double>
 ///////////////////////////////////////////////////////////////////////////////////////////////////
 TEST(SM80_warp_gemm_complex_tensor_op_f64, 8x8x4_8x8x4_nt) {
 
   using Shape = cutlass::gemm::GemmShape<8, 8, 4>;
   using InstructionShape = cutlass::gemm::GemmShape<8, 8, 4>;
   
@@ -289,15 +289,15 @@
 }
 
 ///////////////////////////////////////////////////////////////////////////////////////////////////
 
 ///////////////////////////////////////////////////////////////////////////////////////////////////
 // complex<float> * complex<float> => complex<float>
 // Input data type: complex<float>
-// Math instruction: MMA.1688.F32.TF32
+// Math instruction: mma.sync.aligned.m16n8k8.f32.tf32.tf32.f32
 // Output data type: complex<float>
 // Shared memory layout: Congrous
 ////////////////////////////////////////////////////////////////////////////////////////////////////
 
 TEST(SM80_warp_gemm_complex_tensor_op_f32, 16x16x8_16x8x8_nt) {
 
   using Shape = cutlass::gemm::GemmShape<16, 16, 8>;
@@ -491,15 +491,15 @@
       MmaTensorOp, cutlass::gemm::GemmShape<32, 32, 8> >()
       .run();
 }
 
 ///////////////////////////////////////////////////////////////////////////////////////////////////
 // complex<float> * complex<float> => complex<float>
 // Input data type: complex<float>
-// Math instruction: MMA.1688.F32.TF32
+// Math instruction: mma.sync.aligned.m16n8k8.f32.tf32.tf32.f32
 // Output data type: complex<float>
 // Shared memory layout: Crosswise
 ////////////////////////////////////////////////////////////////////////////////////////////////////
 TEST(SM80_warp_gemm_complex_tensor_op_f32, 16x16x8_16x8x8_tn) {
 
   using Shape = cutlass::gemm::GemmShape<16, 16, 8>;
   using InstructionShape = cutlass::gemm::GemmShape<16, 8, 8>;
@@ -522,15 +522,15 @@
   >::Type;
 
   test::gemm::warp::TransformedTestbedComplex<
       MmaTensorOp, cutlass::gemm::GemmShape<16, 16, 8> >()
       .run();
 }
 
-// TEST FAILS crosswise complex<float> TN MMA.1688.F32.TF32 test fails for k = 2*8 = 16
+// TEST FAILS crosswise complex<float> TN mma.sync.aligned.m16n8k8.f32.tf32.tf32.f32 test fails for k = 2*8 = 16
 TEST(SM80_warp_gemm_complex_tensor_op_f32, 16x16x16_16x8x8_tn) {
 
   using Shape = cutlass::gemm::GemmShape<16, 16, 16>;
   using InstructionShape = cutlass::gemm::GemmShape<16, 8, 8>;
   
   using Element = cutlass::complex<float>;
   using ElementC = cutlass::complex<float>;
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/warp/gemm_complex_sm90.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/warp/gemm_complex_sm90.cu`

 * *Files 0% similar despite different names*

```diff
@@ -46,15 +46,15 @@
 
 #include "cutlass/util/reference/host/tensor_fill.h"
 #include "cutlass/util/reference/host/tensor_compare.h"
 #include "cutlass/util/reference/host/gemm.h"
 
 #include "testbed.h"
 
-#if defined(CUTLASS_ARCH_MMA_SM90_F64_MMA_ENABLED)
+#if defined(CUTLASS_ARCH_MMA_SM90_SUPPORTED)
 
 TEST(SM90_warp_gemm_complex_tensor_op_f64, 16x8x4_16x8x4_nt) {
 
   using Shape = cutlass::gemm::GemmShape<16, 8, 4>;
   using InstructionShape = cutlass::gemm::GemmShape<16, 8, 4>;
   
   using Element = cutlass::complex<double>;
@@ -327,8 +327,8 @@
     ElementC,
     cutlass::layout::RowMajor
   >::Type;
 
   test::gemm::warp::TestbedComplex<MmaTensorOp, Shape>().run();
 }
 
-#endif // if defined(CUTLASS_ARCH_MMA_SM90_F64_MMA_ENABLED)
+#endif // if defined(CUTLASS_ARCH_MMA_SM90_SUPPORTED)
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/warp/gemm_gaussian_complex_sm80.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/warp/gemm_gaussian_complex_sm80.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/warp/gemm_sm50.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/warp/gemm_sm50.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/warp/gemm_sm60.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/warp/gemm_sm60.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/warp/gemm_sm61.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/warp/gemm_sm61.cu`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/warp/gemm_sm70.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/warp/gemm_sm70.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/warp/gemm_sm75.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/warp/gemm_sm75.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/warp/gemm_sm80.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/warp/gemm_sm80.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/warp/gemm_sm90.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/warp/gemm_sm90.cu`

 * *Files 2% similar despite different names*

```diff
@@ -46,15 +46,15 @@
 
 #include "cutlass/util/reference/host/tensor_fill.h"
 #include "cutlass/util/reference/host/tensor_compare.h"
 #include "cutlass/util/reference/host/gemm.h"
 
 #include "testbed.h"
 
-#if defined(CUTLASS_ARCH_MMA_SM90_F64_MMA_ENABLED)
+#if defined(CUTLASS_ARCH_MMA_SM90_SUPPORTED)
 
 TEST(SM90_warp_gemm_tensor_op_congruous_f64, 16x16x4_16x16x4_16x8x4) {
   using Shape = cutlass::gemm::GemmShape<16, 16, 4>;
   using InstructionShape = cutlass::gemm::GemmShape<16, 8, 4>;
   using Element = double;
   using ElementC = double;
   using LayoutA = cutlass::layout::ColumnMajorTensorOpMultiplicandCongruous64b;
@@ -199,8 +199,8 @@
 
   test::gemm::warp::Testbed<MmaTensorOp,
                             cutlass::gemm::GemmShape<32, 64, 16> >()
       .run();
 }
 ////////////////////////////////////////////////////////////////////////////////
 
-#endif // if defined(CUTLASS_ARCH_MMA_SM90_F64_MMA_ENABLED)
+#endif // if defined(CUTLASS_ARCH_MMA_SM90_SUPPORTED)
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/warp/gemm_sparse_sm80.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/warp/gemm_sparse_sm80.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/warp/testbed.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/warp/testbed.h`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/warp/wmma_sm70.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/warp/wmma_sm70.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/warp/wmma_sm72.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/warp/wmma_sm72.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/warp/wmma_sm75.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/gemm/warp/wmma_sm75.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/layout/matrix.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/layout/matrix.cu`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/layout/tensor.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/layout/tensor.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/layout/tensor_nhwc.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/layout/tensor_nhwc.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/nvrtc/cutlass/nvrtc/environment.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/nvrtc/cutlass/nvrtc/environment.h`

 * *Files 4% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/nvrtc/kernel/thread/testbed_kernel.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/nvrtc/kernel/thread/testbed_kernel.h`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/nvrtc/stdlib/stdint.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/nvrtc/stdlib/stdint.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/nvrtc/thread/gemm_nvrtc.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/nvrtc/thread/gemm_nvrtc.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/nvrtc/thread/testbed.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/nvrtc/thread/testbed.h`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/pipeline/pipeline_async.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/pipeline/pipeline_async.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/pipeline/pipeline_tma_async.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/pipeline/pipeline_tma_async.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/pipeline/pipeline_tma_async_warp_specialized.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/pipeline/pipeline_tma_async_warp_specialized.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/pipeline/pipeline_tma_async_warp_specialized_persistent.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/pipeline/pipeline_tma_async_warp_specialized_persistent.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/pipeline/sequence_barrier.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/pipeline/sequence_barrier.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/pipeline/testbed.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/pipeline/testbed.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/reduction/device/tensor_reduce_contiguous.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/reduction/device/tensor_reduce_contiguous.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/reduction/device/tensor_reduce_strided.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/reduction/device/tensor_reduce_strided.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/reduction/kernel/reduce_splitk.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/reduction/kernel/reduce_splitk.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/reduction/kernel/reduce_splitk_testbed.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/reduction/kernel/reduce_splitk_testbed.h`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/reduction/thread/reduction_thread.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/reduction/thread/reduction_thread.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/reduction/thread/testbed.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/reduction/thread/testbed.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/test_unit.cpp` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/test_unit.cpp`

 * *Files 2% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/transform/threadblock/predicated_tile_iterator.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/transform/threadblock/predicated_tile_iterator.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/transform/threadblock/regular_tile_iterator_tensor_op.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/transform/threadblock/regular_tile_iterator_tensor_op.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/util/cutlass_test_levels.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/util/cutlass_test_levels.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/util/tensor_reduce.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/test/unit/util/tensor_reduce.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/library/include/cutlass/library/arch_mappings.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/tools/library/include/cutlass/library/arch_mappings.h`

 * *Files 2% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -93,13 +93,18 @@
 };
 
 template <typename OperatorClass> struct ArchMap<arch::Sm86, OperatorClass> {
   static int const kMin = 86;
   static int const kMax = 1024;
 };
 
+template <typename OperatorClass> struct ArchMap<arch::Sm90, OperatorClass> {
+  static int const kMin = 90;
+  static int const kMax = 1024;
+};
+
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
 } // namespace library
 } // namespace cutlass
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/library/include/cutlass/library/handle.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/tools/library/include/cutlass/library/handle.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/library/include/cutlass/library/library.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/tools/library/include/cutlass/library/library.h`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -1441,14 +1441,17 @@
   /////////////////////////////////////////////////////////
   /// pointer to implicit gemm matrix A
   void const *A;
 
   /// pointer to implicit gemm matrix B
   void const *B;
 
+  /// pointer to reordered matrix B
+  void const *reordered_B;
+  
   /// pointer to implicit gemm matrix C
   void const *C;
 
   /// pointer to implicit gemm desitination matrix D
   void *D;
 
   /// Host or device pointer to alpha scalar
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/library/include/cutlass/library/manifest.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/tools/library/include/cutlass/library/manifest.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/library/include/cutlass/library/operation_table.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/tools/library/include/cutlass/library/operation_table.h`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/library/include/cutlass/library/singleton.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/tools/library/include/cutlass/library/singleton.h`

 * *Files 7% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/library/include/cutlass/library/util.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/tools/library/include/cutlass/library/util.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/compiler.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/python/cutlass/cpp/compiler.h`

 * *Files 2% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/cute.cpp` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/python/cutlass/cpp/test/conv/convolution.h`

 * *Files 13% similar despite different names*

```diff
@@ -25,30 +25,25 @@
  * SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER
  * CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,
  * OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
  * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
  *
  **************************************************************************************************/
 /* \file
-   \brief binding CuTe C++ APIs to Python
+   \brief Bind convolution related types to python
 */
-
+#pragma once
 #include <pybind11/pybind11.h>
 #include <pybind11/stl_bind.h>
 
-#include "cute/arch/mma_sm90_gmma.hpp"
+#include "conv_problems.h"
+#include "host.h"
 
 namespace py = pybind11;
 
+void bind_convolution_test(py::module &m) {
+    // Conv problem sizes
+    bind_conv_problem_size_test(m);
 
-PYBIND11_MODULE(cute, m) {
-
-    // module doc
-    m.doc() = "CuTe C++ bindings";
-
-    py::enum_<cute::GMMA::Major>(m, "GMMAMajor",
-        R"pbdoc(classification of CuTe GMMA tensor major specification)pbdoc")
-        .value("K", cute::GMMA::Major::K,
-            R"pbdoc(Tensor is contiguous in reduction dimension)pbdoc")
-        .value("MN", cute::GMMA::Major::MN,
-            R"pbdoc(Tensor is contiguous in non-reduction dimension)pbdoc");
+    py::module_ host_submodule = m.def_submodule("host");
+    bind_conv_host_references(host_submodule);
 }
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/cutlass.cpp` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/cutlass.cpp`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/include/arch.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/include/arch.h`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/include/conv/conv_problem_size.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/include/conv/conv_problem_size.h`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/include/conv/convolution.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/python/cutlass/cpp/include/conv/convolution.h`

 * *Files 2% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/include/conv/host.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/python/cutlass/cpp/include/conv/host.h`

 * *Files 4% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/include/epilogue/epilogue_visitor_generic.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/python/cutlass/cpp/include/epilogue/epilogue_visitor_generic.h`

 * *Files 4% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -26,22 +26,19 @@
  * CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,
  * OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
  * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
  *
  **************************************************************************************************/
 
 /*! \file
-  
-  \brief A file contains the epilogue visitor with CTA row-wise broadcast
 
-  The epilogue rearranges the result of a matrix product through shared memory to match canonical
-  tensor layouts in global memory. Epilogues support conversion and reduction operations.
-                          
+  \brief A generic wrapper around an epilogue visitor operation
 */
 
+
 #pragma once
 
 #include "cutlass/cutlass.h"
 #include "cutlass/arch/memory.h"
 #include "cutlass/arch/memory_sm75.h"
 #include "cutlass/gemm/kernel/gemm_transpose_operands.h"
 #include "cutlass/gemm/kernel/default_gemm.h"
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/include/epilogue/epilogue_visitor_op/binary_ops.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/python/cutlass/cpp/include/epilogue/epilogue_visitor_op/binary_ops.h`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/include/epilogue/epilogue_visitor_op/unary_ops.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/python/cutlass/cpp/include/epilogue/epilogue_visitor_op/unary_ops.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/include/epilogue/epilogue_visitor_op/visitor_op_accumulator.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/python/cutlass/cpp/include/epilogue/epilogue_visitor_op/visitor_op_accumulator.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/include/epilogue/epilogue_visitor_op/visitor_op_binary.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/python/cutlass/cpp/include/epilogue/epilogue_visitor_op/visitor_op_binary.h`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -80,19 +80,18 @@
 
     /// Fragment type returned by this visitor
     using VisitAccessType = Array<ElementCompute, kElementsPerAccess>; 
 
     /// Fragment type of accumulator
     using AccumulatorAccessType = Array<ElementAccumulator, kElementsPerAccess>;
 
-    /// Combination Op TODO: generalize this
     using BinaryOp = BinaryOp_<ElementCompute, kElementsPerAccess>;
 
     static_assert(kElementsPerAccess==VisitAccessTypeA::kElements, "kElementsPerAccess mismatches with Visitor A");
-    static_assert(kElementsPerAccess==VisitAccessTypeB::kElements, "kElementsPerAccess misnatches with Visitor B");
+    static_assert(kElementsPerAccess==VisitAccessTypeB::kElements, "kElementsPerAccess mismatches with Visitor B");
 
     /// SMEM buffer class required in the epilogue visitor
     struct SharedStorage {
         typename VisitorA::SharedStorage storage_a;
         typename VisitorB::SharedStorage storage_b;
 
         CUTLASS_HOST_DEVICE
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/include/epilogue/epilogue_visitor_op/visitor_op_column_broadcast.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/python/cutlass/cpp/include/epilogue/epilogue_visitor_op/visitor_op_column_broadcast.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/include/epilogue/epilogue_visitor_op/visitor_op_column_reduction.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/python/cutlass/cpp/include/epilogue/epilogue_visitor_op/visitor_op_column_reduction.h`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -51,28 +51,27 @@
 ///
 template <
     typename ThreadblockShape_,             /// Threadblock shape
     typename ElementAccumulator_,           ///< Data type of the Accumulator
     typename ElementReduction_,             ///< Data type of the output reduction in device memory
     typename ElementReductionAccumulator_ , ///< Data type to accumulate reduction in smem and register
     typename OutputTileIterator_,           ///< Tile Iterator type
-    typename Visitor_                       ///< preceeding visitor op
+    typename Visitor_                       ///< preceding visitor op
 >
 class VisitorOpColumnReduction {
 public:
     using ElementAccumulator = ElementAccumulator_;
     using ElementReductionAccumulator = ElementReductionAccumulator_;
     using ElementReduction = ElementReduction_;
     using OutputTileIterator = OutputTileIterator_;
     using ThreadblockShape = ThreadblockShape_;
     using Visitor = Visitor_;
 
     static int const kElementsPerAccess = OutputTileIterator::kElementsPerAccess;
 
-    // TODO: generalize the reduction op
     using ReductionOp = cutlass::plus<Array<ElementReductionAccumulator, kElementsPerAccess>>;
     using ReductionOpScalar = cutlass::plus<ElementReductionAccumulator>;
     using ElementOutput = typename OutputTileIterator::Element;
 
     
 
     /// Fragment type returned from Visitor
@@ -80,15 +79,15 @@
     using ElementVisitor = typename VisitAccessTypeVisitor::Element;
 
     using VisitAccessType = VisitAccessTypeVisitor;
 
     /// Fragment type of accumulator
     using AccumulatorAccessType = Array<ElementAccumulator, kElementsPerAccess>;
 
-    /// Fragment type of redcution
+    /// Fragment type of reduction
     using ReductionAccumulatorAccessType = Array<ElementReductionAccumulator, kElementsPerAccess>;
 
     /// Thread map used by output tile iterators
     using ThreadMap = typename OutputTileIterator::ThreadMap;
     /// Used for the reduction
     struct ReductionDetail {
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/include/epilogue/epilogue_visitor_op/visitor_op_linear_combination.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/include/epilogue/epilogue_visitor_op/visitor_op_linear_combination.h`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -26,16 +26,16 @@
  * CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,
  * OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
  * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
  *
  **************************************************************************************************/
 
 /*! \file
-  
-  \brief A file contains the epilogue visitor Op with Linear Combination
+
+  \brief Epilogue visitor operation that performs a linear combination of two visitor nodes
 */
 
 #pragma once
 #include "cutlass/cutlass.h"
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/include/epilogue/epilogue_visitor_op/visitor_op_row_broadcast.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/python/cutlass/cpp/include/epilogue/epilogue_visitor_op/visitor_op_row_broadcast.h`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/include/epilogue/epilogue_visitor_op/visitor_op_row_reduction.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/include/epilogue/epilogue_visitor_op/visitor_op_row_reduction.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -26,16 +26,16 @@
  * CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,
  * OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
  * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
  *
  **************************************************************************************************/
 
 /*! \file
-  
-  \brief A file contains the epilogue visitor Op with reduction over rows in CTA
+
+  \brief Epilogue visitor operation that performs a column-wise reduction within a threadblock
 */
 
 #pragma once
 #include "cutlass/cutlass.h"
 #include "stdio.h"
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/include/epilogue/epilogue_visitor_op/visitor_op_tensor_input.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/python/cutlass/cpp/include/epilogue/epilogue_visitor_op/visitor_op_tensor_input.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/include/epilogue/epilogue_visitor_op/visitor_op_tensor_output.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/python/cutlass/cpp/include/epilogue/epilogue_visitor_op/visitor_op_tensor_output.h`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/include/epilogue/epilogue_visitor_op/visitor_op_unary.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/include/epilogue/epilogue_visitor_op/visitor_op_unary.h`

 * *Files 2% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -26,16 +26,16 @@
  * CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,
  * OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
  * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
  *
  **************************************************************************************************/
 
 /*! \file
-  
-  \brief A file contains the epilogue visitor Op with Unary operation
+
+  \brief Epilogue visitor operator performing a unary operation atop a visitor node
 */
 
 #pragma once
 #include "cutlass/cutlass.h"
 #include "unary_ops.h"
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/include/epilogue/epilogue_visitor_with_layernorm.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/python/cutlass/cpp/include/epilogue/epilogue_visitor_with_layernorm.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -26,20 +26,19 @@
  * CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,
  * OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
  * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
  *
  **************************************************************************************************/
 
 /*! \file
-    \brief A file contains all functioning classes needed by GemmLayernorm.
+    \brief Epilogue visitor type used for partial computation of a layernorm operation
 
     GemmLayernorm example =  GEMM0 with partial reduction fused in epilogue (EpilogueVisitorLayerNorm)
                           +  lightweight full reduction kernel (ApplyFinalReduction)
-                          +  GEMM1 with elemenwise operations fused in mainloop (GemmLayernormMainloopFusion)
-                          
+                          +  GEMM1 with elementwise operations fused in mainloop (GemmLayernormMainloopFusion)
 */
 
 #pragma once
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
 #include "cutlass/cutlass.h"
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/include/gemm/gemm.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/include/gemm/gemm.h`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/include/gemm/gemm_universal_with_visitor.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/python/cutlass/cpp/include/gemm/gemm_universal_with_visitor.h`

 * *Files 5% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -26,22 +26,23 @@
  * CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,
  * OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
  * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
  *
  **************************************************************************************************/
 
 /*! \file
-    \brief 
+    \brief
 */
 
 #pragma once
 
 #include "cutlass/cutlass.h"
 #include "cutlass/fast_math.h"
 #include "cutlass/gemm/gemm.h"
+#include "cutlass/gemm/kernel/params_universal_base.h"
 #include "cutlass/matrix_coord.h"
 #include "cutlass/complex.h"
 #include "cutlass/semaphore.h"
 
 #include "cutlass/layout/matrix.h"
 
 #include "cutlass/trace.h"
@@ -100,35 +101,30 @@
   );
 
   //
   // Structures
   //
 
   /// Argument structure
-  struct Arguments {
+  struct Arguments : UniversalArgumentsBase {
 
     //
     // Data members
     //
 
-    GemmUniversalMode mode;
-    GemmCoord problem_size;
-    int batch_count;
-
     typename EpilogueVisitor::Arguments epilogue_visitor;
 
     void const * ptr_A;
     void const * ptr_B;
     void const * ptr_C;
     void * ptr_D;
 
     int64_t batch_stride_A;
     int64_t batch_stride_B;
     int64_t batch_stride_C;
-    int64_t batch_stride_D;
 
     typename LayoutA::Stride stride_a;
     typename LayoutB::Stride stride_b;
     typename LayoutC::Stride stride_c;
     typename LayoutC::Stride stride_d;
 
     typename LayoutA::Stride::LongIndex lda;
@@ -139,18 +135,16 @@
     int const * ptr_gather_A_indices;
     int const * ptr_gather_B_indices;
     int const * ptr_scatter_D_indices;
 
     //
     // Methods
     //
-    
-    Arguments(): 
-      mode(GemmUniversalMode::kGemm), 
-      batch_count(1), 
+
+    Arguments():
       ptr_A(nullptr), ptr_B(nullptr), ptr_C(nullptr), ptr_D(nullptr),
       ptr_gather_A_indices(nullptr),
       ptr_gather_B_indices(nullptr),
       ptr_scatter_D_indices(nullptr) {}
 
     /// constructs an arguments structure
     Arguments(
@@ -170,20 +164,18 @@
       typename LayoutB::Stride stride_b,
       typename LayoutC::Stride stride_c,
       typename LayoutC::Stride stride_d,
       int const *ptr_gather_A_indices = nullptr,
       int const *ptr_gather_B_indices = nullptr,
       int const *ptr_scatter_D_indices = nullptr
     ):
-      mode(mode), 
-      problem_size(problem_size), 
-      batch_count(batch_count),
-      epilogue_visitor(epilogue_visitor), 
-      ptr_A(ptr_A), ptr_B(ptr_B), ptr_C(ptr_C), ptr_D(ptr_D), 
-      batch_stride_A(batch_stride_A), batch_stride_B(batch_stride_B), batch_stride_C(batch_stride_C), batch_stride_D(batch_stride_D), 
+      UniversalArgumentsBase(mode, problem_size, batch_count, batch_stride_D),
+      epilogue_visitor(epilogue_visitor),
+      ptr_A(ptr_A), ptr_B(ptr_B), ptr_C(ptr_C), ptr_D(ptr_D),
+      batch_stride_A(batch_stride_A), batch_stride_B(batch_stride_B), batch_stride_C(batch_stride_C),
       stride_a(stride_a), stride_b(stride_b), stride_c(stride_c), stride_d(stride_d),
       ptr_gather_A_indices(ptr_gather_A_indices), ptr_gather_B_indices(ptr_gather_B_indices),
       ptr_scatter_D_indices(ptr_scatter_D_indices) {
       lda = 0;
       ldb = 0;
       ldc = 0;
       ldd = 0;
@@ -208,34 +200,32 @@
       typename LayoutB::Stride::LongIndex ldb,
       typename LayoutC::Stride::LongIndex ldc,
       typename LayoutC::Stride::LongIndex ldd,
       int const *ptr_gather_A_indices = nullptr,
       int const *ptr_gather_B_indices = nullptr,
       int const *ptr_scatter_D_indices = nullptr
     ):
-      mode(mode), 
-      problem_size(problem_size), 
-      batch_count(batch_count),
-      epilogue_visitor(epilogue_visitor), 
-      ptr_A(ptr_A), ptr_B(ptr_B), ptr_C(ptr_C), ptr_D(ptr_D), 
-      batch_stride_A(batch_stride_A), batch_stride_B(batch_stride_B), batch_stride_C(batch_stride_C), batch_stride_D(batch_stride_D),
+      UniversalArgumentsBase(mode, problem_size, batch_count, batch_stride_D),
+      epilogue_visitor(epilogue_visitor),
+      ptr_A(ptr_A), ptr_B(ptr_B), ptr_C(ptr_C), ptr_D(ptr_D),
+      batch_stride_A(batch_stride_A), batch_stride_B(batch_stride_B), batch_stride_C(batch_stride_C),
       lda(lda), ldb(ldb), ldc(ldc), ldd(ldd),
       ptr_gather_A_indices(ptr_gather_A_indices), ptr_gather_B_indices(ptr_gather_B_indices),
       ptr_scatter_D_indices(ptr_scatter_D_indices) {
       stride_a = make_Coord(lda);
       stride_b = make_Coord(ldb);
       stride_c = make_Coord(ldc);
       stride_d = make_Coord(ldd);
       CUTLASS_TRACE_HOST("GemmUniversal::Arguments::Arguments() - problem_size: " << problem_size);
       }
 
     /// Returns arguments for the transposed problem
     Arguments transposed_problem() const {
       Arguments args(*this);
-      
+
       std::swap(args.problem_size.m(), args.problem_size.n());
       std::swap(args.ptr_A, args.ptr_B);
       std::swap(args.lda, args.ldb);
       std::swap(args.stride_a, args.stride_b);
       std::swap(args.batch_stride_A, args.batch_stride_B);
       std::swap(args.ptr_gather_A_indices, args.ptr_gather_B_indices);
 
@@ -244,104 +234,79 @@
   };
 
   //
   // Structure for precomputing values in host memory and passing to kernels
   //
 
   /// Parameters structure
-  struct Params {
-
-    cutlass::gemm::GemmCoord problem_size;
-    cutlass::gemm::GemmCoord grid_tiled_shape;
-    int swizzle_log_tile;
+  struct Params : UniversalParamsBase<
+    ThreadblockSwizzle,
+    ThreadblockShape,
+    ElementA,
+    ElementB,
+    ElementC> {
+
+    using ParamsBase = UniversalParamsBase<
+      ThreadblockSwizzle,
+      ThreadblockShape,
+      ElementA,
+      ElementB,
+      ElementC>;
 
     typename Mma::IteratorA::Params params_A;
     typename Mma::IteratorB::Params params_B;
     typename EpilogueVisitor::OutputTileIterator::Params params_C;
     typename EpilogueVisitor::OutputTileIterator::Params params_D;
-    
-    typename EpilogueVisitor::Params epilogue_visitor;
 
-    GemmUniversalMode mode;
-    int batch_count;
-    int gemm_k_size;
+    typename EpilogueVisitor::Params epilogue_visitor;
 
     void * ptr_A;
     void * ptr_B;
     void * ptr_C;
     void * ptr_D;
 
     int64_t batch_stride_A;
     int64_t batch_stride_B;
     int64_t batch_stride_C;
-    int64_t batch_stride_D;
 
     int * ptr_gather_A_indices;
     int * ptr_gather_B_indices;
     int * ptr_scatter_D_indices;
 
     int *semaphore;
 
     //
     // Methods
     //
 
-    CUTLASS_HOST_DEVICE
-    Params():
-      swizzle_log_tile(0),
-      params_A(0),
-      params_B(0),
-      params_C(0),
-      params_D(0),
-      batch_count(0),
-      gemm_k_size(0),
-      mode(cutlass::gemm::GemmUniversalMode::kGemm),
-      ptr_A(nullptr),
-      ptr_B(nullptr),
-      ptr_C(nullptr),
-      ptr_D(nullptr),
-      batch_stride_A(0),
-      batch_stride_B(0),
-      batch_stride_C(0),
-      batch_stride_D(0),
-      ptr_gather_A_indices(nullptr),
-      ptr_gather_B_indices(nullptr),
-      ptr_scatter_D_indices(nullptr),
-      semaphore(nullptr) { }
+    /// Default constructor
+    Params() = default;
 
     CUTLASS_HOST_DEVICE
     Params(
       Arguments const &args,
-      cutlass::gemm::GemmCoord const & grid_tiled_shape,
-      int gemm_k_size,
-      void *workspace = nullptr
+      int device_sms,
+      int sm_occupancy
     ):
-      problem_size(args.problem_size),
-      grid_tiled_shape(grid_tiled_shape),
-      swizzle_log_tile(ThreadblockSwizzle().get_log_tile(grid_tiled_shape)),
+      ParamsBase(args, device_sms, sm_occupancy),
       params_A(args.lda ? make_Coord_with_padding<LayoutA::kStrideRank>(args.lda) : args.stride_a),
       params_B(args.ldb ? make_Coord_with_padding<LayoutB::kStrideRank>(args.ldb) : args.stride_b),
       params_C(args.ldc ? make_Coord_with_padding<LayoutC::kStrideRank>(args.ldc) : args.stride_c),
       params_D(args.ldd ? make_Coord_with_padding<LayoutC::kStrideRank>(args.ldd) : args.stride_d),
       epilogue_visitor(args.epilogue_visitor),
-      mode(args.mode),
-      batch_count(args.batch_count),
-      gemm_k_size(gemm_k_size),
       ptr_A(const_cast<void *>(args.ptr_A)),
       ptr_B(const_cast<void *>(args.ptr_B)),
       ptr_C(const_cast<void *>(args.ptr_C)),
       ptr_D(args.ptr_D),
       batch_stride_A(args.batch_stride_A),
       batch_stride_B(args.batch_stride_B),
       batch_stride_C(args.batch_stride_C),
-      batch_stride_D(args.batch_stride_D),
       ptr_gather_A_indices(const_cast<int *>(args.ptr_gather_A_indices)),
       ptr_gather_B_indices(const_cast<int *>(args.ptr_gather_B_indices)),
-      ptr_scatter_D_indices(const_cast<int *>(args.ptr_scatter_D_indices)),
-      semaphore(static_cast<int *>(workspace)) {
+      ptr_scatter_D_indices(const_cast<int *>(args.ptr_scatter_D_indices)) {
 
     }
 
     CUTLASS_HOST_DEVICE
     void update(
       Arguments const &args,
       void *workspace = nullptr) {
@@ -354,18 +319,17 @@
       ptr_gather_A_indices = const_cast<int *>(args.ptr_gather_A_indices);
       ptr_gather_B_indices = const_cast<int *>(args.ptr_gather_B_indices);
       ptr_scatter_D_indices = const_cast<int *>(args.ptr_scatter_D_indices);
 
       batch_stride_A = args.batch_stride_A;
       batch_stride_B = args.batch_stride_B;
       batch_stride_C = args.batch_stride_C;
-      batch_stride_D = args.batch_stride_D;
 
       epilogue_visitor = args.epilogue_visitor;
-      
+
       semaphore = static_cast<int *>(workspace);
       CUTLASS_TRACE_HOST("GemmUniversal::Params::update()");
     }
   };
 
   /// Shared memory storage structure
   union SharedStorage {
@@ -377,15 +341,15 @@
 public:
 
   //
   // Methods
   //
 
   CUTLASS_DEVICE
-  GemmUniversalwithEpilogueVisitor() { } 
+  GemmUniversalwithEpilogueVisitor() { }
 
   /// Determines whether kernel satisfies alignment
   static Status can_implement(
     cutlass::gemm::GemmCoord const & problem_size) {
 
     CUTLASS_TRACE_HOST("GemmUniversalwithEpilogueVisitor::can_implement()");
 
@@ -462,18 +426,22 @@
     return Status::kSuccess;
   }
 
   static Status can_implement(Arguments const &args) {
     return can_implement(args.problem_size);
   }
 
-  static size_t get_extra_workspace_size(Arguments const &args,
-                                         cutlass::gemm::GemmCoord const &grid_tiled_shape) {
-
-    return 0;
+  // Factory invocation
+  CUTLASS_DEVICE
+  static void invoke(
+    Params const &params,
+    SharedStorage &shared_storage)
+  {
+    GemmUniversalwithEpilogueVisitor op;
+    op(params, shared_storage);
   }
 
   /// Executes one GEMM
   CUTLASS_DEVICE
   void operator()(Params const &params, SharedStorage &shared_storage) {
 
     // Compute threadblock location
@@ -493,20 +461,20 @@
 
     ElementA *ptr_A = static_cast<ElementA *>(params.ptr_A);
     ElementB *ptr_B = static_cast<ElementB *>(params.ptr_B);
 
     //
     // Fetch pointers based on mode.
     //
-    if (params.mode == GemmUniversalMode::kGemm || 
+    if (params.mode == GemmUniversalMode::kGemm ||
       params.mode == GemmUniversalMode::kGemmSplitKParallel) {
 
       if (threadblock_tile_offset.k() + 1 < params.grid_tiled_shape.k()) {
 
-        problem_size_k = (threadblock_tile_offset.k() + 1) * params.gemm_k_size; 
+        problem_size_k = (threadblock_tile_offset.k() + 1) * params.gemm_k_size;
       }
 
       offset_k = threadblock_tile_offset.k() * params.gemm_k_size;
     }
     else if (params.mode == GemmUniversalMode::kBatched) {
       ptr_A += threadblock_tile_offset.k() * params.batch_stride_A;
       ptr_B += threadblock_tile_offset.k() * params.batch_stride_B;
@@ -567,18 +535,18 @@
     accumulators.clear();
 
     // Compute threadblock-scoped matrix multiply-add
     int gemm_k_iterations = (problem_size_k - offset_k + Mma::Shape::kK - 1) / Mma::Shape::kK;
 
     // Compute threadblock-scoped matrix multiply-add
     mma(
-      gemm_k_iterations, 
-      accumulators, 
-      iterator_A, 
-      iterator_B, 
+      gemm_k_iterations,
+      accumulators,
+      iterator_A,
+      iterator_B,
       accumulators);
 
     //
     // Epilogue
     //
 
     // EpilogueOutputOp output_op(params.output_op);
@@ -593,95 +561,73 @@
     MatrixCoord threadblock_offset(
       threadblock_tile_offset.m() * Mma::Shape::kM,
       threadblock_tile_offset.n() * Mma::Shape::kN
     );
 
     int block_idx = threadblock_tile_offset.m() + threadblock_tile_offset.n() * params.grid_tiled_shape.m();
 
-    ElementC *ptr_C = static_cast<ElementC *>(params.ptr_C); 
+    ElementC *ptr_C = static_cast<ElementC *>(params.ptr_C);
     ElementC *ptr_D = static_cast<ElementC *>(params.ptr_D);
 
     //
     // Fetch pointers based on mode.
     //
-    
+
     // Construct the semaphore.
     Semaphore semaphore(params.semaphore + block_idx, thread_idx);
 
-    // if (params.mode == GemmUniversalMode::kGemm) {
-
-    //   // TODO: fix this order
-    //   // If performing a reduction via split-K, fetch the initial synchronization
-    //   if (params.grid_tiled_shape.k() > 1) {
-        
-    //     // Fetch the synchronization lock initially but do not block.
-    //     semaphore.fetch();
-
-    //     // Indicate which position in a serial reduction the output operator is currently updating
-    //     output_op.set_k_partition(threadblock_tile_offset.k(), params.grid_tiled_shape.k());
-    //   }
-    // }
-    
     // Tile iterator loading from source tensor.
 
     EpilogueVisitor epilogue_visitor(
         params.epilogue_visitor,
         shared_storage.visitor,
         threadblock_offset,
         threadblock_tile_offset,
         thread_idx,
         params.problem_size.mn()
     );
 
-    // if (params.mode == GemmUniversalMode::kGemmSplitKParallel) {
-    //   ptr_D += threadblock_tile_offset.k() * params.batch_stride_D;
-    // }
     if (params.mode == GemmUniversalMode::kBatched || params.mode == GemmUniversalMode::kArray) {
       epilogue_visitor.set_batch_index(threadblock_tile_offset.k());
     }
 
     Epilogue epilogue(
       shared_storage.epilogue,
       thread_idx,
       warp_idx,
       lane_idx);
 
     // Wait on the semaphore - this latency may have been covered by iterator construction
     if (params.mode == GemmUniversalMode::kGemm && params.grid_tiled_shape.k() > 1) {
-        
-      // For subsequent threadblocks, the source matrix is held in the 'D' tensor.
-      // TODO: ???
-      // if (threadblock_tile_offset.k()) {
-      //   iterator_C = iterator_D;
-      // }
 
+      // For subsequent threadblocks, the source matrix is held in the 'D' tensor.
       semaphore.wait(threadblock_tile_offset.k());
     }
 
 
     // Execute the epilogue operator to update the destination tensor.
-    epilogue(epilogue_visitor, accumulators); 
-    
+    epilogue(epilogue_visitor, accumulators);
+
     //
     // Release the semaphore
     //
 
-    if (params.mode == GemmUniversalMode::kGemm && params.grid_tiled_shape.k() > 1) { 
+    if (params.mode == GemmUniversalMode::kGemm && params.grid_tiled_shape.k() > 1) {
 
       int lock = 0;
       if (params.grid_tiled_shape.k() == threadblock_tile_offset.k() + 1) {
 
         // The final threadblock resets the semaphore for subsequent grids.
         lock = 0;
       }
       else {
         // Otherwise, the semaphore is incremented
         lock = threadblock_tile_offset.k() + 1;
       }
-      
+
       semaphore.release(lock);
     }
   }
 };
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/include/gemm/host.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/python/cutlass/cpp/include/gemm/host.h`

 * *Files 4% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/include/layout/layout.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/python/cutlass/cpp/include/layout/layout.h`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/include/layout/matrix.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/python/cutlass/cpp/include/layout/matrix.h`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/include/layout/tensor.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/python/cutlass/cpp/include/layout/tensor.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/include/swizzling.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/include/swizzling.h`

 * *Files 3% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -34,19 +34,27 @@
 #pragma once
 #include <pybind11/pybind11.h>
 #include <pybind11/stl_bind.h>
 
 #include "cutlass/gemm/threadblock/threadblock_swizzle.h"
 #include "cutlass/conv/threadblock/threadblock_swizzle.h"
 
-#include <boost/core/demangle.hpp>
+#include <cxxabi.h>
 #include <cuda_runtime.h>
 
 namespace py = pybind11;
 
+std::string demangle(const char* mangled_name) {
+    std::size_t len = 0;
+    int status = 0;
+    std::unique_ptr<char> ptr(
+                __cxxabiv1::__cxa_demangle(mangled_name, nullptr, &len, &status));
+    return ptr.get();
+}
+
 template<typename T>
 void bind_identity_swizzle(py::module & m, std::string name) {
     py::class_<T>(m, name.c_str(),
         R"pbdoc(Threadblock swizzling function for GEMMs)pbdoc")
         .def(py::init<>())
         .def("get_tiled_shape",
             py::overload_cast<cutlass::gemm::GemmCoord, cutlass::gemm::GemmCoord, int>(
@@ -76,15 +84,15 @@
             :type problem_size: :class:`cutlass.gemm.GemmCoord`)
             )pbdoc")
         // TODO: the returned dim3 is not usable in python
         .def("get_grid_shape", &T::get_grid_shape,
             py::arg("tiled_shape"), 
             R"pbdoc(Computes CUDA grid dimensions given a size in units of logical tiles)pbdoc")
         .def("tag", [](const T & swizzle){
-            return boost::core::demangle(typeid(T).name());
+            return demangle(typeid(T).name());
         }, R"pbdoc(Returns the c++ name of the swizzling for code emittion)pbdoc");
 }
 
 template<typename T>
 void bind_swizzle(py::module & m, std::string name, std::string doc) {
     py::class_<T>(m, name.c_str(), doc.c_str())
         .def(py::init<>())
@@ -97,15 +105,15 @@
             :param problem_size: gemm(M, N, K)
             :type problem_size: :class:`cutlass.gemm.GemmCoord`
             )pbdoc")
         .def("get_grid_shape", &T::get_grid_shape,
             py::arg("tiled_shape"), 
             R"pbdoc(Computes CUDA grid dimensions given a size in units of logical tiles)pbdoc")
         .def("tag", [](const T & swizzle){
-            return boost::core::demangle(typeid(T).name());
+            return demangle(typeid(T).name());
         }, R"pbdoc(Returns the c++ name of the swizzling for code emittion)pbdoc");
 }
 
 template<typename T>
 void bind_dgrad_swizzle(py::module & m, std::string name) {
     py::class_<T>(m, name.c_str(),
         R"pbdoc(Threadblock swizzling function for strided dgrad convolution)pbdoc")
@@ -120,15 +128,15 @@
             :type problem_size: :class:`cutlass.gemm.GemmCoord`)
             )pbdoc")
         .def("get_grid_shape", [](const T & swizzle, cutlass::gemm::GemmCoord tiled_shape) {
             return dim3(tiled_shape.m(), tiled_shape.n(), tiled_shape.k());
         }, py::arg("tiled_shape"), 
             R"pbdoc(Computes CUDA grid dimensions given a size in units of logical tiles)pbdoc")
         .def("tag", [](const T & swizzle){
-            return boost::core::demangle(typeid(T).name());
+            return demangle(typeid(T).name());
         }, R"pbdoc(Returns the c++ name of the swizzling for code emittion)pbdoc");
 }
 
 void bind_threadblock_swizzle(py::module &m) {
 
     py::class_<dim3>(m, "dim3",
         R"pbdoc(A int3 type xyz contains three integers)pbdoc")
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/include/tensor_coord.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/python/cutlass/cpp/include/tensor_coord.h`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/include/tensor_ref_view.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/include/tensor_ref_view.h`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/include/types.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/python/cutlass/cpp/include/types.h`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/library.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/python/cutlass/cpp/library.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/test/conv/conv_problems.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/python/cutlass/cpp/test/conv/conv_problems.h`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/test/conv/convolution.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/test/conv/convolution.h`

 * *Files 4% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/test/conv/host.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/test/conv/host.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/test/gemm/gemm.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/python/cutlass/cpp/test/gemm/gemm.h`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/test/gemm/host.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/python/cutlass/cpp/test/gemm/host.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/library/src/conv2d_operation.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/tools/library/src/conv3d_operation.h`

 * *Files 4% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -31,17 +31,17 @@
 /* \file
   \brief Defines operations for all CONV operation kinds in CUTLASS Library.
 */
 
 #pragma once
 #include <iostream>
 #include "cutlass/cutlass.h"
-#include "cutlass/conv/kernel/default_conv2d_fprop.h"
-#include "cutlass/conv/kernel/default_conv2d_dgrad.h"
-#include "cutlass/conv/kernel/default_conv2d_wgrad.h"
+#include "cutlass/conv/kernel/default_conv3d_fprop.h"
+#include "cutlass/conv/kernel/default_conv3d_dgrad.h"
+#include "cutlass/conv/kernel/default_conv3d_wgrad.h"
 #include "cutlass/conv/device/implicit_gemm_convolution.h"
 
 #include "cutlass/library/library.h"
 #include "library_internal.h"
 #include "cutlass/util/host_tensor.h"
 
 #include "cutlass/util/reference/host/convolution.h"
@@ -51,15 +51,15 @@
 
 namespace cutlass {
 namespace library {
 
 ///////////////////////////////////////////////////////////////////////////////////////////////////
 
 template <typename Operator_>
-class Conv2dOperationBase : public Operation {
+class Conv3dOperationBase : public Operation {
 public:
 
   using Operator = Operator_;
 
   using ElementA = typename Operator::ElementA;
   using LayoutA = typename Operator::LayoutA;
   using ElementB = typename Operator::ElementB;
@@ -77,63 +77,57 @@
 
   /// 
   ConvDescription description_;
 
 public:
 
   /// Constructor
-  Conv2dOperationBase(char const *name = "unknown_conv2d") {
+  Conv3dOperationBase(char const *name = "unknown_conv3d") {
 
     description_.name = name;
     description_.provider = Provider::kCUTLASS;
-    description_.kind = OperationKind::kConv2d;
+    description_.kind = OperationKind::kConv3d;
     description_.conv_dim = Operator::kConvDim;
     
     description_.iterator_algorithm = IteratorAlgorithmMap<Operator::kIteratorAlgorithm>::kId;
 
     description_.tile_description.threadblock_shape = make_Coord(
       Operator::ThreadblockShape::kM,
       Operator::ThreadblockShape::kN,
       Operator::ThreadblockShape::kK);
 
     description_.tile_description.threadblock_stages = Operator::kStages;
 
     description_.tile_description.warp_count = make_Coord(
-      Operator::ImplicitGemmKernel::WarpCount::kM,
-      Operator::ImplicitGemmKernel::WarpCount::kN,
-      Operator::ImplicitGemmKernel::WarpCount::kK);
+      Operator::UnderlyingKernel::WarpCount::kM,
+      Operator::UnderlyingKernel::WarpCount::kN,
+      Operator::UnderlyingKernel::WarpCount::kK);
     
     description_.tile_description.math_instruction.instruction_shape = make_Coord(
       Operator::InstructionShape::kM,
       Operator::InstructionShape::kN,
       Operator::InstructionShape::kK);
 
     description_.tile_description.math_instruction.element_accumulator = 
       NumericTypeMap<ElementAccumulator>::kId;
 
     description_.tile_description.math_instruction.opcode_class = 
       OpcodeClassMap<typename Operator::OperatorClass>::kId;
 
-    description_.tile_description.math_instruction.math_operation =
-      MathOperationMap<typename Operator::MathOperator>::kId;
-
     description_.tile_description.minimum_compute_capability = 
       ArchMap<typename Operator::ArchTag, typename Operator::OperatorClass>::kMin;
 
     description_.tile_description.maximum_compute_capability = 
       ArchMap<typename Operator::ArchTag, typename Operator::OperatorClass>::kMax;
     
     description_.A = make_TensorDescription<ElementA, LayoutA>();
     description_.B = make_TensorDescription<ElementB, LayoutB>();
     description_.C = make_TensorDescription<ElementC, LayoutC>();
     description_.element_epilogue = NumericTypeMap<ElementCompute>::kId;
 
-    // TODO: Add split k mode Serial and parallel to convolutions
-    // description_.split_k_mode = Operator::kSplitK ? SplitKMode::kSerial : SplitKMode::kNone;
-
   }
 
   /// Returns the description of the GEMM operation
   virtual OperationDescription const & description() const {
     return description_;
   }
 };
@@ -141,15 +135,15 @@
 
 ///////////////////////////////////////////////////////////////////////////////////////////////////
 //
 // Conv2d library operation class for cutlass profiler
 //
 ///////////////////////////////////////////////////////////////////////////////////////////////////
 template <typename Operator_>
-class Conv2dOperation : public Conv2dOperationBase<Operator_> {
+class Conv3dOperation : public Conv3dOperationBase<Operator_> {
 public:
 
   using Operator = Operator_;
 
   using ElementA = typename Operator::ElementA;
   using LayoutA = typename Operator::LayoutA;
   using ElementB = typename Operator::ElementB;
@@ -160,27 +154,27 @@
   using ElementCompute = typename Operator::EpilogueOutputOp::ElementCompute;
   static cutlass::conv::Operator const kConvolutionalOperator = Operator::kConvolutionalOperator;
 
   using OperatorArguments = typename Operator::Arguments;
 
 public:
     /// Constructor
-  Conv2dOperation(char const *name = "unknown_conv2d_fprop") : Conv2dOperationBase<Operator_>(name) {
+  Conv3dOperation(char const *name = "unknown_conv3d_fprop") : Conv3dOperationBase<Operator_>(name) {
     this->description_.conv_kind = ConvKindMap<kConvolutionalOperator>::kId;
   }
 
 protected:
 
   /// Constructs the arguments structure given the configuration and arguments
   static Status construct_arguments_(
     OperatorArguments &operator_args,
-    Conv2dConfiguration const *configuration) {
+    Conv3dConfiguration const *configuration) {
 
 
-    operator_args.problem_size = configuration->problem_size;
+    operator_args.problem_size     = configuration->problem_size;
 
     operator_args.ref_A = 
     {
       nullptr, 
       LayoutA::packed(implicit_gemm_tensor_a_extent(kConvolutionalOperator, configuration->problem_size))
     };
     
@@ -198,15 +192,15 @@
     
     operator_args.ref_D = 
     {
       nullptr, 
       LayoutC::packed(implicit_gemm_tensor_c_extent(kConvolutionalOperator, configuration->problem_size))
     };
 
-    operator_args.split_k_mode = configuration->split_k_mode;
+    operator_args.split_k_mode     = configuration->split_k_mode;
 
     return Status::kSuccess;
   }
 
   /// Constructs the arguments structure given the configuration and arguments
   static Status update_arguments_(
     OperatorArguments &operator_args,
@@ -241,16 +235,16 @@
 public:
 
   /// Returns success if the operation can proceed
   virtual Status can_implement(
     void const *configuration_ptr, 
     void const *arguments_ptr) const {
 
-    Conv2dConfiguration const *configuration = 
-      static_cast<Conv2dConfiguration const *>(configuration_ptr);
+    Conv3dConfiguration const *configuration = 
+      static_cast<Conv3dConfiguration const *>(configuration_ptr);
 
     ConvArguments const *arguments = 
       static_cast<ConvArguments const *>(arguments_ptr);
 
     OperatorArguments args;
 
     Status status = construct_arguments_(args, configuration);
@@ -281,15 +275,15 @@
     void const *configuration_ptr,
     void const *arguments_ptr = nullptr) const {
 
     OperatorArguments args;
 
     Status status = construct_arguments_(
       args, 
-      static_cast<Conv2dConfiguration const *>(configuration_ptr));
+      static_cast<Conv3dConfiguration const *>(configuration_ptr));
 
     if (status != Status::kSuccess) {
       return 0;
     }
 
     return Operator::get_workspace_size(args);
   }
@@ -301,22 +295,22 @@
     void *device_workspace, 
     cudaStream_t stream = nullptr) const {
 
     OperatorArguments args;
 
     Status status = construct_arguments_(
       args, 
-      static_cast<Conv2dConfiguration const *>(configuration_ptr));
+      static_cast<Conv3dConfiguration const *>(configuration_ptr));
 
     if (status != Status::kSuccess) {
       return status;
     }
 
     Operator *op = new (host_workspace) Operator;
-    //std::cout << "initialize library::Conv2dOperation" << std::endl;
+    //std::cout << "initialize library::Conv3dOperation" << std::endl;
     //print_operator_args(args);
     return op->initialize(args, device_workspace, stream);
 
   }
 
   /// Runs the kernel
   virtual Status run(
@@ -338,50 +332,54 @@
     Operator *op = static_cast<Operator *>(host_workspace);
 
     status = op->update(args, device_workspace);
 
     if (status != Status::kSuccess) {
       return status;
     }
-    //std::cout << "run library::Conv2dOperation" << std::endl;
+    //std::cout << "run library::Conv3dOperation" << std::endl;
     //print_operator_args(args);
     return op->run(stream);
   }
 
-  /// Call print_operator_args  from the Conv2dOperation::initialize()
+  /// Call print_operator_args  from the Conv3dOperation::initialize()
   // to dump arguments passed on to cutlass operator for debugging
   void print_operator_args(OperatorArguments &operator_args) const {
-    std::cout << "Conv2dOperation::OperatorArguments" << std::endl
-              << "  problem_size:" << std::endl 
+    std::cout << "Conv3dOperation::OperatorArguments" << std::endl
+              << "  problem_size: " 
               << operator_args.problem_size << std::endl
               << "  split_k_mode: "
               << (operator_args.split_k_mode == cutlass::conv::SplitKMode::kSerial ? "serial" : "parallel") << std::endl
-              << "  epilouge (alpha, beta): "
+              << "  epilouge (alpha, beta): " 
               << operator_args.output_op.alpha << ", " 
               << operator_args.output_op.beta << std::endl
               << "  ref_A (ptr, {stride}): " 
               << operator_args.ref_A.data() << ", {"
               << operator_args.ref_A.stride(0) << ", " 
               << operator_args.ref_A.stride(1) << ", " 
-              << operator_args.ref_A.stride(2) << "}" << std::endl
+              << operator_args.ref_A.stride(2) << ", " 
+              << operator_args.ref_A.stride(3) << "}" << std::endl
               << "  ref_B (ptr, {stride}): " 
               << operator_args.ref_B.data() << ", {"
               << operator_args.ref_B.stride(0) << ", " 
               << operator_args.ref_B.stride(1) << ", " 
-              << operator_args.ref_B.stride(2) << "}" << std::endl
+              << operator_args.ref_B.stride(2) << ", " 
+              << operator_args.ref_B.stride(3) << "}" << std::endl
               << "  ref_C (ptr, {stride}): "
               << operator_args.ref_C.data() << ", {"
               << operator_args.ref_C.stride(0) << ", "
               << operator_args.ref_C.stride(1) << ", " 
-              << operator_args.ref_C.stride(2) << "}" << std::endl
+              << operator_args.ref_C.stride(2) << ", " 
+              << operator_args.ref_C.stride(3) << "}" << std::endl
               << "  ref_D (ptr, {stride}): "
               << operator_args.ref_D.data() << ", {"
               << operator_args.ref_D.stride(0) << ", "
               << operator_args.ref_D.stride(1) << ", " 
-              << operator_args.ref_D.stride(2) << "}" << std::endl;
+              << operator_args.ref_D.stride(2) << ", "
+              << operator_args.ref_D.stride(3) << "}" << std::endl;
   } 
 };
 
 } // namespace library
 } // namespace cutlass
 
 ///////////////////////////////////////////////////////////////////////////////////////////////////
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/library/src/conv3d_operation.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/tools/library/src/symm_operation.h`

 * *Files 20% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -25,229 +25,221 @@
  * SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER
  * CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,
  * OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
  * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
  *
  **************************************************************************************************/
 /* \file
-  \brief Defines operations for all CONV operation kinds in CUTLASS Library.
+   \brief Defines operations for all Symm operation kinds (Symm, Hemm) 
+    in CUTLASS Library.
+
+  
 */
 
 #pragma once
 #include <iostream>
 #include "cutlass/cutlass.h"
-#include "cutlass/conv/kernel/default_conv3d_fprop.h"
-#include "cutlass/conv/kernel/default_conv3d_dgrad.h"
-#include "cutlass/conv/kernel/default_conv3d_wgrad.h"
-#include "cutlass/conv/device/implicit_gemm_convolution.h"
+
+#include "cutlass/gemm/device/symm.h"
+#include "cutlass/gemm/kernel/default_symm_universal.h"
 
 #include "cutlass/library/library.h"
 #include "library_internal.h"
-#include "cutlass/util/host_tensor.h"
-
-#include "cutlass/util/reference/host/convolution.h"
-#include "cutlass/util/reference/host/tensor_compare.h"
 #include "cutlass/core_io.h"
 ///////////////////////////////////////////////////////////////////////////////////////////////////
 
 namespace cutlass {
 namespace library {
 
 ///////////////////////////////////////////////////////////////////////////////////////////////////
 
 template <typename Operator_>
-class Conv3dOperationBase : public Operation {
+class SymmOperationBase : public Operation {
 public:
-
   using Operator = Operator_;
-
   using ElementA = typename Operator::ElementA;
   using LayoutA = typename Operator::LayoutA;
   using ElementB = typename Operator::ElementB;
   using LayoutB = typename Operator::LayoutB;
   using ElementC = typename Operator::ElementC;
   using LayoutC = typename Operator::LayoutC;
   using ElementAccumulator = typename Operator::ElementAccumulator;
   using ElementCompute = typename Operator::EpilogueOutputOp::ElementCompute;
-  static cutlass::conv::IteratorAlgorithm const kIteratorAlgorithm = Operator::kIteratorAlgorithm;
-  static cutlass::conv::Operator const kConvolutionalOperator = Operator::kConvolutionalOperator;
+  static BlasMode const kBlasMode = Operator::kBlasMode;
+  static SideMode const kSideModeA = Operator::kSideModeA;
+  static FillMode const kFillModeA = Operator::kFillModeA;
 
   using OperatorArguments = typename Operator::Arguments;
 
 protected:
 
   /// 
-  ConvDescription description_;
+  SymmDescription description_;
 
 public:
 
   /// Constructor
-  Conv3dOperationBase(char const *name = "unknown_conv3d") {
+  SymmOperationBase(char const *name = "unknown_symm") {
 
     description_.name = name;
     description_.provider = Provider::kCUTLASS;
-    description_.kind = OperationKind::kConv3d;
-    description_.conv_dim = Operator::kConvDim;
-    
-    description_.iterator_algorithm = IteratorAlgorithmMap<Operator::kIteratorAlgorithm>::kId;
+    description_.symm_kind = SymmKind::kUniversal;
+    description_.side_mode = kSideModeA;    
+    description_.fill_mode = kFillModeA;    
+    description_.blas_mode = kBlasMode;
+
+    description_.kind = OperationKind::kSymm;
 
     description_.tile_description.threadblock_shape = make_Coord(
       Operator::ThreadblockShape::kM,
       Operator::ThreadblockShape::kN,
       Operator::ThreadblockShape::kK);
 
     description_.tile_description.threadblock_stages = Operator::kStages;
 
     description_.tile_description.warp_count = make_Coord(
-      Operator::ImplicitGemmKernel::WarpCount::kM,
-      Operator::ImplicitGemmKernel::WarpCount::kN,
-      Operator::ImplicitGemmKernel::WarpCount::kK);
+      Operator::SymmKernel::WarpCount::kM,
+      Operator::SymmKernel::WarpCount::kN,
+      Operator::SymmKernel::WarpCount::kK);
     
     description_.tile_description.math_instruction.instruction_shape = make_Coord(
       Operator::InstructionShape::kM,
       Operator::InstructionShape::kN,
       Operator::InstructionShape::kK);
 
     description_.tile_description.math_instruction.element_accumulator = 
       NumericTypeMap<ElementAccumulator>::kId;
 
     description_.tile_description.math_instruction.opcode_class = 
       OpcodeClassMap<typename Operator::OperatorClass>::kId;
 
+    description_.tile_description.math_instruction.math_operation =
+      MathOperationMap<typename Operator::Operator>::kId;
+
     description_.tile_description.minimum_compute_capability = 
       ArchMap<typename Operator::ArchTag, typename Operator::OperatorClass>::kMin;
 
     description_.tile_description.maximum_compute_capability = 
       ArchMap<typename Operator::ArchTag, typename Operator::OperatorClass>::kMax;
     
-    description_.A = make_TensorDescription<ElementA, LayoutA>();
-    description_.B = make_TensorDescription<ElementB, LayoutB>();
-    description_.C = make_TensorDescription<ElementC, LayoutC>();
+    description_.A = make_TensorDescription<ElementA, LayoutA>(Operator::kAlignmentA);
+    description_.B = make_TensorDescription<ElementB, LayoutB>(Operator::kAlignmentB);
+    description_.C = make_TensorDescription<ElementC, LayoutC>(Operator::kAlignmentC);
     description_.element_epilogue = NumericTypeMap<ElementCompute>::kId;
 
+    description_.split_k_mode = SplitKMode::kNone;
   }
-
-  /// Returns the description of the GEMM operation
+  
+  /// Returns the description of the SYMM operation
   virtual OperationDescription const & description() const {
     return description_;
   }
 };
 
-
-///////////////////////////////////////////////////////////////////////////////////////////////////
-//
-// Conv2d library operation class for cutlass profiler
-//
 ///////////////////////////////////////////////////////////////////////////////////////////////////
+
 template <typename Operator_>
-class Conv3dOperation : public Conv3dOperationBase<Operator_> {
+class SymmOperation : public SymmOperationBase<Operator_> {
 public:
 
   using Operator = Operator_;
-
   using ElementA = typename Operator::ElementA;
   using LayoutA = typename Operator::LayoutA;
   using ElementB = typename Operator::ElementB;
   using LayoutB = typename Operator::LayoutB;
   using ElementC = typename Operator::ElementC;
   using LayoutC = typename Operator::LayoutC;
+
   using ElementAccumulator = typename Operator::ElementAccumulator;
   using ElementCompute = typename Operator::EpilogueOutputOp::ElementCompute;
-  static cutlass::conv::Operator const kConvolutionalOperator = Operator::kConvolutionalOperator;
+
+  static BlasMode const kBlasMode = Operator::kBlasMode;
+  static SideMode const kSideModeA = Operator::kSideModeA;
+  static FillMode const kFillModeA = Operator::kFillModeA;
 
   using OperatorArguments = typename Operator::Arguments;
 
 public:
-    /// Constructor
-  Conv3dOperation(char const *name = "unknown_conv3d_fprop") : Conv3dOperationBase<Operator_>(name) {
-    this->description_.conv_kind = ConvKindMap<kConvolutionalOperator>::kId;
+
+  /// Constructor
+  SymmOperation(char const *name = "unknown_symm"): 
+    SymmOperationBase<Operator_>(name) {
+
+    this->description_.symm_kind = SymmKind::kUniversal;
   }
 
 protected:
 
   /// Constructs the arguments structure given the configuration and arguments
   static Status construct_arguments_(
     OperatorArguments &operator_args,
-    Conv3dConfiguration const *configuration) {
-
+    SymmConfiguration const *configuration) {
 
-    operator_args.problem_size     = configuration->problem_size;
+    //operator_args.mode = configuration->mode;
 
-    operator_args.ref_A = 
-    {
-      nullptr, 
-      LayoutA::packed(implicit_gemm_tensor_a_extent(kConvolutionalOperator, configuration->problem_size))
-    };
-    
-    operator_args.ref_B = 
-    {
-      nullptr, 
-      LayoutB::packed(implicit_gemm_tensor_b_extent(kConvolutionalOperator, configuration->problem_size))
-    };
-    
-    operator_args.ref_C = 
-    {
-      nullptr, 
-      LayoutC::packed(implicit_gemm_tensor_c_extent(kConvolutionalOperator, configuration->problem_size))
-    };
-    
-    operator_args.ref_D = 
-    {
-      nullptr, 
-      LayoutC::packed(implicit_gemm_tensor_c_extent(kConvolutionalOperator, configuration->problem_size))
-    };
-
-    operator_args.split_k_mode     = configuration->split_k_mode;
+    operator_args.problem_size = configuration->problem_size;
+    operator_args.batch_count = configuration->batch_count;
 
+    operator_args.lda = int(configuration->lda);
+    operator_args.ldb = int(configuration->ldb);
+    operator_args.ldc = int(configuration->ldc);
+    operator_args.ldd = int(configuration->ldd);
+    
     return Status::kSuccess;
   }
 
   /// Constructs the arguments structure given the configuration and arguments
   static Status update_arguments_(
     OperatorArguments &operator_args,
-    ConvArguments const *arguments) {
-
+    SymmArguments const *arguments) {
+    
     if (arguments->pointer_mode == ScalarPointerMode::kHost) {
       typename Operator::EpilogueOutputOp::Params params(
         *static_cast<ElementCompute const *>(arguments->alpha),
         *static_cast<ElementCompute const *>(arguments->beta)
       );
-      operator_args.output_op = params;
+      operator_args.epilogue = params;
     }
     else if (arguments->pointer_mode == ScalarPointerMode::kDevice){
       typename Operator::EpilogueOutputOp::Params params(
         static_cast<ElementCompute const *>(arguments->alpha),
         static_cast<ElementCompute const *>(arguments->beta)
       );
-      operator_args.output_op = params; 
+      operator_args.epilogue = params; 
     }
     else {
       return Status::kErrorInvalidProblem;
     }
 
-    operator_args.ref_A.reset(static_cast<ElementA *>(const_cast<void *>(arguments->A)));
-    operator_args.ref_B.reset(static_cast<ElementB *>(const_cast<void *>(arguments->B)));
-    operator_args.ref_C.reset(static_cast<ElementC *>(const_cast<void *>(arguments->C)));
-    operator_args.ref_D.reset(static_cast<ElementC *>(const_cast<void *>(arguments->D)));
-
+    // update arguments
+    operator_args.ptr_A = arguments->A;
+    operator_args.ptr_B = arguments->B;
+    operator_args.ptr_C = arguments->C;
+    operator_args.ptr_D = arguments->D;
+
+    operator_args.batch_stride_A = arguments->batch_stride_A;
+    operator_args.batch_stride_B = arguments->batch_stride_B;
+    operator_args.batch_stride_C = arguments->batch_stride_C;
+    operator_args.batch_stride_D = arguments->batch_stride_D;
+    
     return Status::kSuccess;
   }
 
 public:
 
   /// Returns success if the operation can proceed
   virtual Status can_implement(
     void const *configuration_ptr, 
     void const *arguments_ptr) const {
+    
+    SymmConfiguration const *configuration = 
+      static_cast<SymmConfiguration const *>(configuration_ptr);
 
-    Conv3dConfiguration const *configuration = 
-      static_cast<Conv3dConfiguration const *>(configuration_ptr);
-
-    ConvArguments const *arguments = 
-      static_cast<ConvArguments const *>(arguments_ptr);
+    SymmArguments const *arguments = 
+      static_cast<SymmArguments const *>(arguments_ptr);
 
     OperatorArguments args;
 
     Status status = construct_arguments_(args, configuration);
 
     if (status != Status::kSuccess) {
       return status;
@@ -256,15 +248,14 @@
     status = update_arguments_(args, arguments);
 
     if (status != Status::kSuccess) {
       return status;
     }
 
     return Operator::can_implement(args);
-
   }
   
   /// Gets the host-side workspace
   virtual uint64_t get_host_workspace_size(
     void const *configuration) const {
 
     return sizeof(Operator);
@@ -275,111 +266,114 @@
     void const *configuration_ptr,
     void const *arguments_ptr = nullptr) const {
 
     OperatorArguments args;
 
     Status status = construct_arguments_(
       args, 
-      static_cast<Conv3dConfiguration const *>(configuration_ptr));
+      static_cast<SymmConfiguration const *>(configuration_ptr));
 
     if (status != Status::kSuccess) {
       return 0;
     }
 
-    return Operator::get_workspace_size(args);
+    uint64_t size = Operator::get_workspace_size(args);
+
+    return size;
   }
   
   /// Initializes the workspace
   virtual Status initialize(
     void const *configuration_ptr, 
     void *host_workspace, 
     void *device_workspace, 
     cudaStream_t stream = nullptr) const {
 
     OperatorArguments args;
 
     Status status = construct_arguments_(
       args, 
-      static_cast<Conv3dConfiguration const *>(configuration_ptr));
+      static_cast<SymmConfiguration const *>(configuration_ptr));
 
     if (status != Status::kSuccess) {
       return status;
     }
 
     Operator *op = new (host_workspace) Operator;
-    //std::cout << "initialize library::Conv3dOperation" << std::endl;
+    
+    //std::cout << "initialize() library::SymmOperation" << std::endl;
     //print_operator_args(args);
-    return op->initialize(args, device_workspace, stream);
-
+    status = op->initialize(args, device_workspace, stream);
+    
+    return status;
   }
 
   /// Runs the kernel
   virtual Status run(
     void const *arguments_ptr,
     void *host_workspace, 
     void *device_workspace = nullptr, 
     cudaStream_t stream = nullptr) const {
 
     OperatorArguments args;
-
+    
     Status status = update_arguments_(
       args, 
-      static_cast<ConvArguments const *>(arguments_ptr));
+      static_cast<SymmArguments const *>(arguments_ptr));
 
     if (status != Status::kSuccess) {
       return status;
     }
-
+    
     Operator *op = static_cast<Operator *>(host_workspace);
 
-    status = op->update(args, device_workspace);
+    bool need_swapped_matrices = (kSideModeA == SideMode::kLeft && 
+                                    std::is_same<typename Operator::LayoutC, layout::ColumnMajor>::value) ||
+                                 (kSideModeA == SideMode::kRight &&
+                                    std::is_same<typename Operator::LayoutC, layout::RowMajor>::value);
+    if (need_swapped_matrices) {
+      status = op->update(args.swapped_matrices(), device_workspace);
+    } else {
+      status = op->update(args, device_workspace);
+    } 
 
     if (status != Status::kSuccess) {
       return status;
     }
-    //std::cout << "run library::Conv3dOperation" << std::endl;
+    
+    //std::cout << "run() library::SymmOperation" << std::endl;
     //print_operator_args(args);
-    return op->run(stream);
+    status = op->run(stream);
+    
+    return status;
   }
 
-  /// Call print_operator_args  from the Conv3dOperation::initialize()
+  /// Call print_operator_args  from the Conv2dOperation::initialize()
   // to dump arguments passed on to cutlass operator for debugging
   void print_operator_args(OperatorArguments &operator_args) const {
-    std::cout << "Conv3dOperation::OperatorArguments" << std::endl
-              << "  problem_size: " 
+    std::cout << "SymmOperation::OperatorArguments" << std::endl
+              << "  problem_size:" << std::endl 
               << operator_args.problem_size << std::endl
-              << "  split_k_mode: "
-              << (operator_args.split_k_mode == cutlass::conv::SplitKMode::kSerial ? "serial" : "parallel") << std::endl
-              << "  epilouge (alpha, beta): " 
-              << operator_args.output_op.alpha << ", " 
-              << operator_args.output_op.beta << std::endl
+              << "  epilouge (alpha, beta): "
+              << operator_args.epilogue.alpha << ", " 
+              << operator_args.epilogue.beta << std::endl
               << "  ref_A (ptr, {stride}): " 
-              << operator_args.ref_A.data() << ", {"
-              << operator_args.ref_A.stride(0) << ", " 
-              << operator_args.ref_A.stride(1) << ", " 
-              << operator_args.ref_A.stride(2) << ", " 
-              << operator_args.ref_A.stride(3) << "}" << std::endl
+              << operator_args.ptr_A << ", {"
+              << operator_args.lda << "}" << std::endl
               << "  ref_B (ptr, {stride}): " 
-              << operator_args.ref_B.data() << ", {"
-              << operator_args.ref_B.stride(0) << ", " 
-              << operator_args.ref_B.stride(1) << ", " 
-              << operator_args.ref_B.stride(2) << ", " 
-              << operator_args.ref_B.stride(3) << "}" << std::endl
+              << operator_args.ptr_B << ", {"
+              << operator_args.ldb << "}" << std::endl
               << "  ref_C (ptr, {stride}): "
-              << operator_args.ref_C.data() << ", {"
-              << operator_args.ref_C.stride(0) << ", "
-              << operator_args.ref_C.stride(1) << ", " 
-              << operator_args.ref_C.stride(2) << ", " 
-              << operator_args.ref_C.stride(3) << "}" << std::endl
+              << operator_args.ptr_C << ", {"
+              << operator_args.ldc << "}" << std::endl
               << "  ref_D (ptr, {stride}): "
-              << operator_args.ref_D.data() << ", {"
-              << operator_args.ref_D.stride(0) << ", "
-              << operator_args.ref_D.stride(1) << ", " 
-              << operator_args.ref_D.stride(2) << ", "
-              << operator_args.ref_D.stride(3) << "}" << std::endl;
+              << operator_args.ptr_D << ", {"
+              << operator_args.ldd << "}" << std::endl;
   } 
 };
 
+///////////////////////////////////////////////////////////////////////////////////////////////////
+
 } // namespace library
 } // namespace cutlass
 
 ///////////////////////////////////////////////////////////////////////////////////////////////////
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/library/src/gemm_operation.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/tools/library/src/gemm_operation.h`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -301,15 +301,15 @@
 
     if (status != Status::kSuccess) {
       return status;
     }
 
     Operator *op = static_cast<Operator *>(host_workspace);
 
-    status = op->update(args, device_workspace);
+    status = op->update(args);
 
     if (status != Status::kSuccess) {
       return status;
     }
 
     return op->run(stream);
   }
@@ -503,15 +503,15 @@
 
     if (status != Status::kSuccess) {
       return status;
     }
 
     Operator *op = static_cast<Operator *>(host_workspace);
 
-    status = op->update(args, device_workspace);
+    status = op->update(args);
 
     if (status != Status::kSuccess) {
       return status;
     }
 
     return op->run(stream);
   }
@@ -721,16 +721,16 @@
       static_cast<GemmUniversalArguments const *>(arguments_ptr));
 
     if (status != Status::kSuccess) {
       return status;
     }
     
     Operator *op = static_cast<Operator *>(host_workspace);
-    
-    status = op->update(args, device_workspace);
+
+    status = op->update(args);
 
     if (status != Status::kSuccess) {
       return status;
     }
     
     status = op->run(stream);
     
@@ -913,38 +913,38 @@
     
     return status;
   }
 
   /// Runs the kernel
   virtual Status run(
     void const *arguments_ptr,
-    void *host_workspace, 
-    void *device_workspace = nullptr, 
+    void *host_workspace,
+    void *device_workspace = nullptr,
     cudaStream_t stream = nullptr) const {
 
     OperatorArguments args;
-    
+
     Status status = update_arguments_(
-      args, 
+      args,
       static_cast<GemmPlanarComplexArguments const *>(arguments_ptr));
 
     if (status != Status::kSuccess) {
       return status;
     }
-    
+
     Operator *op = static_cast<Operator *>(host_workspace);
-    
-    status = op->update(args, device_workspace);
+
+    status = op->update(args);
 
     if (status != Status::kSuccess) {
       return status;
     }
-    
+
     status = op->run(stream);
-    
+
     return status;
   }
 };
 
 ///////////////////////////////////////////////////////////////////////////////////////////////////
 
 template <typename Operator_>
@@ -1130,15 +1130,15 @@
 
     if (status != Status::kSuccess) {
       return status;
     }
     
     Operator *op = static_cast<Operator *>(host_workspace);
     
-    status = op->update(args, device_workspace);
+    status = op->update(args);
 
     if (status != Status::kSuccess) {
       return status;
     }
     
     status = op->run(stream);
     
@@ -1332,15 +1332,15 @@
 
     if (status != Status::kSuccess) {
       return status;
     }
 
     Operator *op = static_cast<Operator *>(host_workspace);
 
-    status = op->update(args, device_workspace);
+    status = op->update(args);
 
     if (status != Status::kSuccess) {
       return status;
     }
 
     status = op->run(stream);
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/library/src/handle.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/tools/library/src/handle.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/library/src/library_internal.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/tools/library/src/library_internal.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/library/src/manifest.cpp` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/tools/library/src/manifest.cpp`

 * *Files 5% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/library/src/operation_table.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/tools/library/src/operation_table.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/library/src/rank_2k_operation.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/tools/library/src/rank_2k_operation.h`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/library/src/rank_k_operation.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/tools/library/src/rank_k_operation.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/library/src/reduction/init_reduction_operations.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/tools/library/src/reduction/init_reduction_operations.cu`

 * *Files 4% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/library/src/reduction/reduction_device.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/tools/library/src/reduction/reduction_device.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/library/src/reduction/reduction_operation.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/tools/library/src/reduction/reduction_operation.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/library/src/reference/conv2d.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/tools/library/src/reference/conv2d.cu`

 * *Files 2% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/library/src/reference/conv3d.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/tools/library/src/reference/conv3d.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/library/src/reference/conv_reference_operation.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/tools/library/src/reference/conv_reference_operation.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/library/src/reference/gemm.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/tools/library/src/reference/gemm.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/library/src/reference/gemm_reference_operation.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/tools/library/src/reference/gemm_reference_operation.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/library/src/reference/initialize_reference_operations.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/tools/library/src/reference/initialize_reference_operations.cu`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/library/src/singleton.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/tools/library/src/singleton.cu`

 * *Files 4% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -38,30 +38,24 @@
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
 namespace cutlass {
 namespace library {
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
-static std::unique_ptr<Singleton> instance;
-
-/////////////////////////////////////////////////////////////////////////////////////////////////
-
 Singleton::Singleton() {
 
   manifest.initialize();
 
   operation_table.append(manifest);
 }
 
 Singleton const & Singleton::get() {
-  if (!instance.get()) {
-    instance.reset(new Singleton);
-  }
-  return *instance.get();
+  static Singleton instance;
+  return instance;
 }
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
 } // namespace library
 } // namespace cutlass
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/library/src/symm_operation.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/tools/library/src/trmm_operation.h`

 * *Files 8% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -25,85 +25,84 @@
  * SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER
  * CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,
  * OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
  * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
  *
  **************************************************************************************************/
 /* \file
-   \brief Defines operations for all Symm operation kinds (Symm, Hemm) 
-    in CUTLASS Library.
+   \brief Defines operations for all TRMM operation kinds in CUTLASS Library.
 
   
 */
 
 #pragma once
-#include <iostream>
+
 #include "cutlass/cutlass.h"
 
-#include "cutlass/gemm/device/symm.h"
-#include "cutlass/gemm/kernel/default_symm_universal.h"
+#include "cutlass/gemm/device/trmm.h"
+#include "cutlass/gemm/kernel/default_trmm_universal.h"
+#include "cutlass/gemm/kernel/trmm_universal.h"
 
 #include "cutlass/library/library.h"
 #include "library_internal.h"
-#include "cutlass/core_io.h"
+
 ///////////////////////////////////////////////////////////////////////////////////////////////////
 
 namespace cutlass {
 namespace library {
 
 ///////////////////////////////////////////////////////////////////////////////////////////////////
 
 template <typename Operator_>
-class SymmOperationBase : public Operation {
+class TrmmOperationBase : public Operation {
 public:
   using Operator = Operator_;
   using ElementA = typename Operator::ElementA;
   using LayoutA = typename Operator::LayoutA;
+  static SideMode const kSideMode = Operator::kSideMode;
+  static FillMode const kFillMode = Operator::kFillMode;
+  static DiagType const kDiagType = Operator::kDiagType;
   using ElementB = typename Operator::ElementB;
   using LayoutB = typename Operator::LayoutB;
   using ElementC = typename Operator::ElementC;
   using LayoutC = typename Operator::LayoutC;
   using ElementAccumulator = typename Operator::ElementAccumulator;
   using ElementCompute = typename Operator::EpilogueOutputOp::ElementCompute;
-  static BlasMode const kBlasMode = Operator::kBlasMode;
-  static SideMode const kSideModeA = Operator::kSideModeA;
-  static FillMode const kFillModeA = Operator::kFillModeA;
 
   using OperatorArguments = typename Operator::Arguments;
 
 protected:
 
   /// 
-  SymmDescription description_;
+  TrmmDescription description_;
 
 public:
 
   /// Constructor
-  SymmOperationBase(char const *name = "unknown_symm") {
+  TrmmOperationBase(char const *name = "unknown_trmm") {
 
     description_.name = name;
     description_.provider = Provider::kCUTLASS;
-    description_.symm_kind = SymmKind::kUniversal;
-    description_.side_mode = kSideModeA;    
-    description_.fill_mode = kFillModeA;    
-    description_.blas_mode = kBlasMode;
-
-    description_.kind = OperationKind::kSymm;
+    description_.kind = OperationKind::kTrmm;
+    description_.trmm_kind = TrmmKind::kUniversal;
+    description_.side_mode = kSideMode;    
+    description_.fill_mode = kFillMode;    
+    description_.diag_type = kDiagType;    
 
     description_.tile_description.threadblock_shape = make_Coord(
       Operator::ThreadblockShape::kM,
       Operator::ThreadblockShape::kN,
       Operator::ThreadblockShape::kK);
 
     description_.tile_description.threadblock_stages = Operator::kStages;
 
     description_.tile_description.warp_count = make_Coord(
-      Operator::SymmKernel::WarpCount::kM,
-      Operator::SymmKernel::WarpCount::kN,
-      Operator::SymmKernel::WarpCount::kK);
+      Operator::TrmmKernel::WarpCount::kM,
+      Operator::TrmmKernel::WarpCount::kN,
+      Operator::TrmmKernel::WarpCount::kK);
     
     description_.tile_description.math_instruction.instruction_shape = make_Coord(
       Operator::InstructionShape::kM,
       Operator::InstructionShape::kN,
       Operator::InstructionShape::kK);
 
     description_.tile_description.math_instruction.element_accumulator = 
@@ -119,82 +118,80 @@
       ArchMap<typename Operator::ArchTag, typename Operator::OperatorClass>::kMin;
 
     description_.tile_description.maximum_compute_capability = 
       ArchMap<typename Operator::ArchTag, typename Operator::OperatorClass>::kMax;
     
     description_.A = make_TensorDescription<ElementA, LayoutA>(Operator::kAlignmentA);
     description_.B = make_TensorDescription<ElementB, LayoutB>(Operator::kAlignmentB);
-    description_.C = make_TensorDescription<ElementC, LayoutC>(Operator::kAlignmentC);
+    description_.D = make_TensorDescription<ElementC, LayoutC>(Operator::kAlignmentC);
     description_.element_epilogue = NumericTypeMap<ElementCompute>::kId;
 
     description_.split_k_mode = SplitKMode::kNone;
+    description_.transform_A = ComplexTransformMap<Operator::kTransformA>::kId;
   }
   
-  /// Returns the description of the SYMM operation
+  /// Returns the description of the TRMM operation
   virtual OperationDescription const & description() const {
     return description_;
   }
 };
 
 ///////////////////////////////////////////////////////////////////////////////////////////////////
 
 template <typename Operator_>
-class SymmOperation : public SymmOperationBase<Operator_> {
+class TrmmOperation : public TrmmOperationBase<Operator_> {
 public:
 
   using Operator = Operator_;
   using ElementA = typename Operator::ElementA;
   using LayoutA = typename Operator::LayoutA;
+  static SideMode const kSideMode = Operator::kSideMode;
+  static FillMode const kFillMode = Operator::kFillMode;
+  static DiagType const kDiagType = Operator::kDiagType;
   using ElementB = typename Operator::ElementB;
   using LayoutB = typename Operator::LayoutB;
   using ElementC = typename Operator::ElementC;
   using LayoutC = typename Operator::LayoutC;
-
   using ElementAccumulator = typename Operator::ElementAccumulator;
   using ElementCompute = typename Operator::EpilogueOutputOp::ElementCompute;
 
-  static BlasMode const kBlasMode = Operator::kBlasMode;
-  static SideMode const kSideModeA = Operator::kSideModeA;
-  static FillMode const kFillModeA = Operator::kFillModeA;
-
   using OperatorArguments = typename Operator::Arguments;
 
 public:
 
   /// Constructor
-  SymmOperation(char const *name = "unknown_symm"): 
-    SymmOperationBase<Operator_>(name) {
+  TrmmOperation(char const *name = "unknown_trmm"): 
+    TrmmOperationBase<Operator_>(name) {
 
-    this->description_.symm_kind = SymmKind::kUniversal;
+    this->description_.trmm_kind = TrmmKind::kUniversal;
   }
 
 protected:
 
   /// Constructs the arguments structure given the configuration and arguments
   static Status construct_arguments_(
     OperatorArguments &operator_args,
-    SymmConfiguration const *configuration) {
+    TrmmConfiguration const *configuration) {
 
     //operator_args.mode = configuration->mode;
 
     operator_args.problem_size = configuration->problem_size;
     operator_args.batch_count = configuration->batch_count;
 
     operator_args.lda = int(configuration->lda);
     operator_args.ldb = int(configuration->ldb);
-    operator_args.ldc = int(configuration->ldc);
     operator_args.ldd = int(configuration->ldd);
     
     return Status::kSuccess;
   }
 
   /// Constructs the arguments structure given the configuration and arguments
   static Status update_arguments_(
     OperatorArguments &operator_args,
-    SymmArguments const *arguments) {
+    TrmmArguments const *arguments) {
     
     if (arguments->pointer_mode == ScalarPointerMode::kHost) {
       typename Operator::EpilogueOutputOp::Params params(
         *static_cast<ElementCompute const *>(arguments->alpha),
         *static_cast<ElementCompute const *>(arguments->beta)
       );
       operator_args.epilogue = params;
@@ -209,37 +206,34 @@
     else {
       return Status::kErrorInvalidProblem;
     }
 
     // update arguments
     operator_args.ptr_A = arguments->A;
     operator_args.ptr_B = arguments->B;
-    operator_args.ptr_C = arguments->C;
-    operator_args.ptr_D = arguments->D;
-
     operator_args.batch_stride_A = arguments->batch_stride_A;
     operator_args.batch_stride_B = arguments->batch_stride_B;
-    operator_args.batch_stride_C = arguments->batch_stride_C;
+    operator_args.ptr_D = arguments->D;
     operator_args.batch_stride_D = arguments->batch_stride_D;
-    
+
     return Status::kSuccess;
   }
 
 public:
 
   /// Returns success if the operation can proceed
   virtual Status can_implement(
     void const *configuration_ptr, 
     void const *arguments_ptr) const {
     
-    SymmConfiguration const *configuration = 
-      static_cast<SymmConfiguration const *>(configuration_ptr);
+    TrmmConfiguration const *configuration = 
+      static_cast<TrmmConfiguration const *>(configuration_ptr);
 
-    SymmArguments const *arguments = 
-      static_cast<SymmArguments const *>(arguments_ptr);
+    TrmmArguments const *arguments = 
+      static_cast<TrmmArguments const *>(arguments_ptr);
 
     OperatorArguments args;
 
     Status status = construct_arguments_(args, configuration);
 
     if (status != Status::kSuccess) {
       return status;
@@ -266,15 +260,15 @@
     void const *configuration_ptr,
     void const *arguments_ptr = nullptr) const {
 
     OperatorArguments args;
 
     Status status = construct_arguments_(
       args, 
-      static_cast<SymmConfiguration const *>(configuration_ptr));
+      static_cast<TrmmConfiguration const *>(configuration_ptr));
 
     if (status != Status::kSuccess) {
       return 0;
     }
 
     uint64_t size = Operator::get_workspace_size(args);
 
@@ -288,24 +282,22 @@
     void *device_workspace, 
     cudaStream_t stream = nullptr) const {
 
     OperatorArguments args;
 
     Status status = construct_arguments_(
       args, 
-      static_cast<SymmConfiguration const *>(configuration_ptr));
+      static_cast<TrmmConfiguration const *>(configuration_ptr));
 
     if (status != Status::kSuccess) {
       return status;
     }
 
     Operator *op = new (host_workspace) Operator;
-    
-    //std::cout << "initialize() library::SymmOperation" << std::endl;
-    //print_operator_args(args);
+
     status = op->initialize(args, device_workspace, stream);
     
     return status;
   }
 
   /// Runs the kernel
   virtual Status run(
@@ -314,65 +306,40 @@
     void *device_workspace = nullptr, 
     cudaStream_t stream = nullptr) const {
 
     OperatorArguments args;
     
     Status status = update_arguments_(
       args, 
-      static_cast<SymmArguments const *>(arguments_ptr));
+      static_cast<TrmmArguments const *>(arguments_ptr));
 
     if (status != Status::kSuccess) {
       return status;
     }
     
     Operator *op = static_cast<Operator *>(host_workspace);
-
-    bool need_swapped_matrices = (kSideModeA == SideMode::kLeft && 
+   
+    bool need_swapped_matrices = (kSideMode == SideMode::kLeft && 
                                     std::is_same<typename Operator::LayoutC, layout::ColumnMajor>::value) ||
-                                 (kSideModeA == SideMode::kRight &&
+                                 (kSideMode == SideMode::kRight &&
                                     std::is_same<typename Operator::LayoutC, layout::RowMajor>::value);
     if (need_swapped_matrices) {
       status = op->update(args.swapped_matrices(), device_workspace);
     } else {
       status = op->update(args, device_workspace);
     } 
 
     if (status != Status::kSuccess) {
       return status;
     }
     
-    //std::cout << "run() library::SymmOperation" << std::endl;
-    //print_operator_args(args);
     status = op->run(stream);
     
     return status;
   }
-
-  /// Call print_operator_args  from the Conv2dOperation::initialize()
-  // to dump arguments passed on to cutlass operator for debugging
-  void print_operator_args(OperatorArguments &operator_args) const {
-    std::cout << "SymmOperation::OperatorArguments" << std::endl
-              << "  problem_size:" << std::endl 
-              << operator_args.problem_size << std::endl
-              << "  epilouge (alpha, beta): "
-              << operator_args.epilogue.alpha << ", " 
-              << operator_args.epilogue.beta << std::endl
-              << "  ref_A (ptr, {stride}): " 
-              << operator_args.ptr_A << ", {"
-              << operator_args.lda << "}" << std::endl
-              << "  ref_B (ptr, {stride}): " 
-              << operator_args.ptr_B << ", {"
-              << operator_args.ldb << "}" << std::endl
-              << "  ref_C (ptr, {stride}): "
-              << operator_args.ptr_C << ", {"
-              << operator_args.ldc << "}" << std::endl
-              << "  ref_D (ptr, {stride}): "
-              << operator_args.ptr_D << ", {"
-              << operator_args.ldd << "}" << std::endl;
-  } 
 };
 
 ///////////////////////////////////////////////////////////////////////////////////////////////////
 
 } // namespace library
 } // namespace cutlass
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/library/src/util.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/tools/library/src/util.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/profiler/src/conv2d_operation_profiler.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/tools/profiler/src/conv2d_operation_profiler.cu`

 * *Files 2% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -63,14 +63,15 @@
       {ArgumentTypeID::kInteger, {"w", "input_w"}, "Input W dimension of the Conv2d problem space"},
       {ArgumentTypeID::kInteger, {"c", "input_c"}, "Input C dimension of the Conv2d problem space"},
       {ArgumentTypeID::kInteger, {"k", "filter_k"}, "Filter K dimension of the Conv2d problem space"},
       {ArgumentTypeID::kInteger, {"r", "filter_r"}, "Filter R dimension of the Conv2d problem space"},
       {ArgumentTypeID::kInteger, {"s", "filter_s"}, "Filter S dimension of the Conv2d problem space"},
       {ArgumentTypeID::kInteger, {"p", "output_p"}, "Output P dimension of the Conv2d problem space"},
       {ArgumentTypeID::kInteger, {"q", "output_q"}, "Output Q dimension of the Conv2d problem space"},
+      {ArgumentTypeID::kInteger, {"g", "groups"}, "Number of convolution groups"},
       {ArgumentTypeID::kInteger, {"pad_h"}, "Padding in H direction"},
       {ArgumentTypeID::kInteger, {"pad_w"}, "Padding in W direction"},
       {ArgumentTypeID::kInteger, {"stride_h"}, "Stride in H direction"},
       {ArgumentTypeID::kInteger, {"stride_w"}, "Stride in W direction"},
       {ArgumentTypeID::kInteger, {"dilation_h"}, "Dilation in H direction"},
       {ArgumentTypeID::kInteger, {"dilation_w"}, "Dilation in W direction"},
       {ArgumentTypeID::kTensor, {"Activation"}, "Tensor storing the Activation operand"},
@@ -229,14 +230,19 @@
   }
   
   if (!arg_as_int(problem_.s, "s", problem_space, problem)) {
     // default value
     problem_.s = 3;
   }
 
+  if (!arg_as_int(problem_.groups, "g", problem_space, problem)) {
+    // default value
+    problem_.groups = 1;
+  }
+
   if (!arg_as_int(problem_.pad_h, "pad_h", problem_space, problem)) {
     // default value
     problem_.pad_h = 1;
   }
 
   if (!arg_as_int(problem_.pad_w, "pad_w", problem_space, problem)) {
     // default value
@@ -378,15 +384,15 @@
                                                 int(problem_.pad_w),
                                                 int(problem_.stride_h),
                                                 int(problem_.stride_w),
                                                 int(problem_.dilation_h),
                                                 int(problem_.dilation_w),
                                                 static_cast<conv::Mode>(static_cast<int>(problem_.conv_mode)),
                                                 int(problem_.split_k_slices),
-                                                1 // groups
+                                                int(problem_.groups)
                                               );
   
   conv_workspace_.configuration.split_k_mode = static_cast<conv::SplitKMode>(static_cast<int>(problem_.split_k_mode));
 
   conv_workspace_.set_stride_vector(
       problem_, operation_desc.conv_kind, operation_desc.A.layout,
       operation_desc.B.layout, operation_desc.C.layout);
@@ -450,14 +456,16 @@
   set_argument(result, "k", problem_space, problem_.k);
   set_argument(result, "r", problem_space, problem_.r);
   set_argument(result, "s", problem_space, problem_.s);
   
   set_argument(result, "p", problem_space, problem_.p);
   set_argument(result, "q", problem_space, problem_.q);
 
+  set_argument(result, "g", problem_space, problem_.groups);
+
   set_argument(result, "pad_h", problem_space, problem_.pad_h);
   set_argument(result, "pad_w", problem_space, problem_.pad_w);
 
   set_argument(result, "stride_h", problem_space, problem_.stride_h);
   set_argument(result, "stride_w", problem_space, problem_.stride_w);
 
   set_argument(result, "dilation_h", problem_space, problem_.dilation_h);
@@ -620,14 +628,27 @@
       operation_desc.B.element,
       operation_desc.B.layout,
       problem_.extent_b(operation_desc.conv_kind),
       conv_workspace_.configuration.stride_b,
       conv_workspace_.problem_count
     );
 
+    if(problem_.groups == problem_.c && problem_.groups == problem_.k){
+      // Depthwise direct conv kernel needs reorder the filter.
+      conv_workspace_.reordered_B = device_context.allocate_tensor(
+        options,
+        "B",
+        operation_desc.B.element,
+        operation_desc.B.layout,
+        problem_.extent_b(operation_desc.conv_kind),
+        conv_workspace_.configuration.stride_b,
+        conv_workspace_.problem_count
+      );
+    }
+
     conv_workspace_.C = device_context.allocate_tensor(
       options,
       "C",
       operation_desc.C.element,
       operation_desc.C.layout,
       problem_.extent_c(operation_desc.conv_kind),
       conv_workspace_.configuration.stride_c,
@@ -734,14 +755,20 @@
   conv_workspace_.arguments.B = conv_workspace_.B->data();
   conv_workspace_.arguments.C = conv_workspace_.C->data();
   conv_workspace_.arguments.D = conv_workspace_.Computed->data();
   conv_workspace_.arguments.alpha = problem_.alpha.data();
   conv_workspace_.arguments.beta = problem_.beta.data();
   conv_workspace_.arguments.pointer_mode = library::ScalarPointerMode::kHost;
 
+  if (conv_workspace_.reordered_B != nullptr){
+    conv_workspace_.arguments.reordered_B = conv_workspace_.reordered_B->data();
+  }else{
+    conv_workspace_.arguments.reordered_B = nullptr;
+  }
+
   conv_workspace_.Computed->copy_from_device(conv_workspace_.C->data());
   
   if (conv_workspace_.configuration.split_k_mode == conv::SplitKMode::kParallel) {
     // update library::ConvArguments for parallel split-k reduction
     conv_workspace_.arguments.D = conv_workspace_.device_workspace.data();
     conv_workspace_.arguments.alpha = problem_.alpha_one.data();
     conv_workspace_.arguments.beta = problem_.beta_zero.data();
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/profiler/src/conv2d_operation_profiler.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/tools/profiler/src/conv2d_operation_profiler.h`

 * *Files 2% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -71,14 +71,15 @@
 class Conv2dOperationProfiler : public OperationProfiler {
 public:
 
   /// Problem structure obtained from problem space
   struct Conv2dProblem {
 
     int64_t n, h, w, c, p, q, k, r, s;
+    int64_t groups;
     int64_t pad_h, pad_w;
     int64_t stride_h, stride_w;
     int64_t dilation_h, dilation_w;
 
     std::vector<uint8_t> alpha;
     std::vector<uint8_t> beta;
 
@@ -110,15 +111,15 @@
       q = ((w + pad_w - s * dilation_w) / stride_w) + 1;
     }
 
     // Returns equivalent gemm problem size for convolution
     cutlass::gemm::GemmCoord eq_gemm_size(library::ConvKind const &conv_kind) const {
 
       switch (conv_kind) {
-        case library::ConvKind::kFprop: return cutlass::gemm::GemmCoord(int(n * p * q), int(k), int(r * s * c));
+        case library::ConvKind::kFprop: return cutlass::gemm::GemmCoord(int(n * p * q), int(k), int(r * s * c / groups));
         case library::ConvKind::kDgrad: return cutlass::gemm::GemmCoord(int(n * h * w), int(c), int(k * r * s));
         case library::ConvKind::kWgrad: return cutlass::gemm::GemmCoord(int(k), int(r * s * c), int(n * p * q));
         default : throw std::runtime_error("Invalid Conv Operator (fprop, dgrad, wgrad)");
       }
     }
 
     // Returns extent for tensor A
@@ -132,15 +133,15 @@
       }
     }
 
     // Returns extent for tensor B
     std::vector<int> extent_b(library::ConvKind const &conv_kind) const {
 
       switch (conv_kind) {
-        case library::ConvKind::kFprop: return {int(k), int(r), int(s), int(c)};
+        case library::ConvKind::kFprop: return {int(k), int(r), int(s), int(c / groups)};
         case library::ConvKind::kDgrad: return {int(k), int(r), int(s), int(c)};
         case library::ConvKind::kWgrad: return {int(n), int(h), int(w), int(c)};
         default : throw std::runtime_error("Invalid Conv Operator (fprop, dgrad, wgrad)");
       }
     }
 
     // Returns extent for tensor C
@@ -224,14 +225,15 @@
 
   /// Workspace used 
   struct Conv2dWorkspace {
 
     /// Conv device allocations
     DeviceAllocation *A;
     DeviceAllocation *B;
+    DeviceAllocation *reordered_B;
     DeviceAllocation *C;
     DeviceAllocation *Computed;
     DeviceAllocation *Reference;
     
     /// Library configuration and arguments for convolution operator
     library::Conv2dConfiguration configuration;
     library::ConvArguments arguments;
@@ -266,14 +268,15 @@
     //
     // Methods
     //
 
     Conv2dWorkspace()
         : A(nullptr),
           B(nullptr),
+          reordered_B(nullptr),
           C(nullptr),
           Computed(nullptr),
           Reference(nullptr) {}
 
     // Set stride vector for tensor activations, filters, output
     void set_stride_vector(Conv2dProblem const &problem,
                            library::ConvKind const &conv_kind,
@@ -313,18 +316,18 @@
       } else {
         // Strides for the rest cases
         stride_activations.push_back(int(problem.c));
         stride_activations.push_back(int(problem.w) * int(problem.c));
         stride_activations.push_back(int(problem.h) * int(problem.w) *
                                      int(problem.c));
 
-        stride_filters.push_back(int(problem.c));
-        stride_filters.push_back(int(problem.s) * int(problem.c));
+        stride_filters.push_back(int(problem.c / problem.groups));
+        stride_filters.push_back(int(problem.s) * int(problem.c / problem.groups));
         stride_filters.push_back(int(problem.r) * int(problem.s) *
-                                 int(problem.c));
+                                 int(problem.c / problem.groups));
 
         stride_output.push_back(int(problem.k));
         stride_output.push_back(int(problem.q) * int(problem.k));
         stride_output.push_back(int(problem.q) * int(problem.p) *
                                 int(problem.k));
       }
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/profiler/src/conv3d_operation_profiler.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/tools/profiler/src/conv3d_operation_profiler.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/profiler/src/conv3d_operation_profiler.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/tools/profiler/src/conv3d_operation_profiler.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/profiler/src/cublas_helpers.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/tools/profiler/src/cublas_helpers.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/profiler/src/cublas_helpers.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/tools/profiler/src/cublas_helpers.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/profiler/src/cudnn_helpers.cpp` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/tools/profiler/src/cudnn_helpers.cpp`

 * *Files 2% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -191,15 +191,20 @@
       {
         cudnn_math_type = CUDNN_TENSOR_OP_MATH_ALLOW_CONVERSION;
       }
 
       return true;
     }
     case library::OpcodeClassID::kSimt:
-      return false;
+      #if (defined(CUDNN_VERSION) && CUDNN_VERSION <= 8000)
+        cudnn_math_type = CUDNN_DEFAULT_MATH;
+      #else
+        cudnn_math_type = CUDNN_FMA_MATH;
+      #endif
+      return true;
   }
 
   return false;
 }
 
 /// Cudnn compute type seems to be hardcoded to float (To handle a possible cudnn issue)
 float cast_cudnn_compute_type_to_float(library::NumericTypeID type, void const * src) {
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/profiler/src/cudnn_helpers.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/tools/profiler/src/cudnn_helpers.h`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -241,15 +241,15 @@
 
     status = get_cutlass_status(
       cudnnSetFilter4dDescriptor(
         filter_desc,
         data_type_filter,
         layout_filter,
         configuration.problem_size.K,
-        configuration.problem_size.C,
+        configuration.problem_size.C / configuration.problem_size.groups,
         configuration.problem_size.R,
         configuration.problem_size.S
     ));
 
     status = get_cutlass_status(
       cudnnSetTensor4dDescriptor(
         output_desc,
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/profiler/src/cutlass_profiler.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/tools/profiler/src/cutlass_profiler.cu`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/profiler/src/cutlass_profiler.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/tools/profiler/src/cutlass_profiler.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/profiler/src/debug.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/tools/profiler/src/debug.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/profiler/src/device_allocation.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/tools/profiler/src/device_allocation.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/profiler/src/device_allocation.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/tools/profiler/src/device_allocation.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/profiler/src/device_context.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/tools/profiler/src/device_context.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/profiler/src/device_context.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/tools/profiler/src/device_context.h`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/profiler/src/enumerated_types.cpp` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/tools/profiler/src/enumerated_types.cpp`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/profiler/src/enumerated_types.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/tools/profiler/src/enumerated_types.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/profiler/src/gemm_operation_profiler.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/tools/profiler/src/gemm_operation_profiler.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/profiler/src/gemm_operation_profiler.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/tools/profiler/src/gemm_operation_profiler.h`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/profiler/src/gpu_timer.cpp` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/tools/profiler/src/gpu_timer.cpp`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/profiler/src/gpu_timer.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/tools/profiler/src/gpu_timer.h`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/profiler/src/main.cpp` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/tools/profiler/src/main.cpp`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/profiler/src/operation_profiler.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/tools/profiler/src/operation_profiler.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/profiler/src/operation_profiler.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/tools/profiler/src/operation_profiler.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/profiler/src/options.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/tools/profiler/src/options.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/profiler/src/options.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/tools/profiler/src/options.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/profiler/src/performance_report.cpp` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/tools/profiler/src/performance_report.cpp`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/profiler/src/performance_report.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/tools/profiler/src/performance_report.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/profiler/src/performance_result.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/tools/profiler/src/performance_result.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/profiler/src/performance_result.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/tools/profiler/src/performance_result.h`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/profiler/src/problem_space.cpp` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/tools/profiler/src/problem_space.cpp`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/profiler/src/problem_space.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/tools/profiler/src/problem_space.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/profiler/src/rank_2k_operation_profiler.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/tools/profiler/src/rank_2k_operation_profiler.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/profiler/src/rank_2k_operation_profiler.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/tools/profiler/src/rank_2k_operation_profiler.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/profiler/src/rank_k_operation_profiler.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/tools/profiler/src/rank_k_operation_profiler.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/profiler/src/rank_k_operation_profiler.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/tools/profiler/src/rank_k_operation_profiler.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/profiler/src/reduction_operation_profiler.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/tools/profiler/src/reduction_operation_profiler.h`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/profiler/src/sparse_gemm_operation_profiler.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/tools/profiler/src/sparse_gemm_operation_profiler.cu`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/profiler/src/sparse_gemm_operation_profiler.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/tools/profiler/src/sparse_gemm_operation_profiler.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/profiler/src/symm_operation_profiler.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/tools/profiler/src/symm_operation_profiler.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/profiler/src/symm_operation_profiler.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/tools/profiler/src/symm_operation_profiler.h`

 * *Files 2% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/profiler/src/trmm_operation_profiler.cu` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/tools/profiler/src/trmm_operation_profiler.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/profiler/src/trmm_operation_profiler.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/tools/profiler/src/trmm_operation_profiler.h`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/command_line.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/command_line.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /******************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/debug.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/debug.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/device_dump.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/device_dump.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/device_groupnorm.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/device_groupnorm.h`

 * *Files 0% similar despite different names*

```diff
@@ -310,15 +310,15 @@
             }
             output_TVec_ptr[offset_in_group] = output_tmp_vec;
         }
     }
 }
 
 //ref_input & ref_output should be [N, H, W, C]
-//ref_gamma & ref_beta should be [1, 1, 1, C]
+//ref_gamma & ref_beta shoud be [1, 1, 1, C]
 template <typename T>
 void groupnorm(cutlass::Tensor4DCoord input_size,
                const int num_groups,
                const float eps,
                TensorRef<T, layout::TensorNHWC> ref_output,
                TensorRef<T, layout::TensorNHWC> ref_input,
                TensorRef<T, layout::TensorNHWC> ref_gamma,
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/device_layernorm.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/device_layernorm.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /******************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/device_memory.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/device_memory.h`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /******************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/device_nchw_to_nhwc.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/device_nchw_to_nhwc.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /******************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/device_nhwc_padding.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/device_nhwc_padding.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /******************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/device_nhwc_pooling.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/device_nhwc_pooling.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /******************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/device_nhwc_to_nchw.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/device_nhwc_to_nchw.h`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /******************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/device_utils.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/device_utils.h`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/distribution.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/distribution.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/exceptions.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/exceptions.h`

 * *Files 4% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /******************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/host_reorder.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/host_reorder.h`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/host_tensor.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/host_tensor.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/host_tensor_planar_complex.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/host_tensor_planar_complex.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/host_uncompress.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/host_uncompress.h`

 * *Files 21% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -38,14 +38,15 @@
 #include "cutlass/util/host_tensor.h"
 #include "cutlass/tensor_view.h"
 #include "cutlass/util/tensor_view_io.h"
 #include "cutlass/util/reference/host/gemm.h"
 
 namespace cutlass {
 
+// uncompress sparse tensor core A matrix
 template <typename ElementA, typename LayoutA, typename ElementE,
           typename LayoutE>
 void uncompress(TensorRef<ElementA, LayoutA> uncompressed_tensor_a,
                 TensorRef<ElementA, LayoutA> tensor_a,
                 TensorRef<ElementE, LayoutE> tensor_e, int row, int col) {
   // How many uncompressed data we can get with ElementE meta data
   int DecompressedElementsPerElementE =
@@ -115,9 +116,42 @@
                   ElementA(0);
           }
         }
       }
     }
   }
 }
+
+// uncompress ELL block sparse matrix
+template <typename ElementA, typename LayoutA,
+          typename ElementE, typename LayoutE>
+void uncompress_ell_block_sparse(
+                TensorRef<ElementA, LayoutA> uncompressed_tensor_a,
+                TensorRef<ElementA, LayoutA> tensor_a,
+                TensorRef<ElementE, LayoutE> ell_idx,
+                int rows, int cols,
+                int ell_num_cols, int ell_blocksize) {
+
+  for (int r = 0; r < rows / ell_blocksize; ++r) {
+    for (int c = 0; c < ell_num_cols / ell_blocksize; ++c) {
+
+      ElementE idx = ell_idx.at(MatrixCoord(r, c));
+
+      if (idx != -1) {
+        int row_begin = r * ell_blocksize;
+        int col_begin_real = idx * ell_blocksize;
+        int col_begin = c * ell_blocksize;
+  
+        for (int i = 0; i < ell_blocksize; ++i) {
+          for (int j = 0; j < ell_blocksize; ++j) {
+            uncompressed_tensor_a.at(MatrixCoord(row_begin + i, col_begin_real + j)) =
+                tensor_a.at(
+                    MatrixCoord(row_begin + i, col_begin +j));
+          }
+        }
+      }
+    }
+  }
+}
+
 } // namespace cutlass
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/index_sequence.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/index_sequence.h`

 * *Files 5% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/detail/inner_product.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/detail/inner_product.h`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/detail/linear_to_coordinate.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/detail/linear_to_coordinate.h`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/device/convolution.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/device/convolution.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/device/gemm.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/device/gemm.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/device/gemm_complex.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/device/gemm_complex.h`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/device/gemm_planar_complex.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/device/gemm_planar_complex.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/device/kernel/gemm.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/device/kernel/gemm.h`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/device/kernel/tensor_elementwise.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/device/kernel/tensor_elementwise.h`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/device/kernel/tensor_foreach.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/device/kernel/tensor_foreach.h`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/device/rank_2k_complex.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/device/rank_2k_complex.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/device/tensor_compare.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/device/tensor_compare.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/device/tensor_fill.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/device/tensor_fill.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/device/tensor_foreach.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/device/tensor_foreach.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/device/tensor_reduce.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/device/tensor_reduce.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/device/tensor_relu.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/device/tensor_relu.h`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/device/thread/gemm.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/device/thread/gemm.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/host/convolution.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/host/convolution.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/host/error_metrics.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/host/error_metrics.h`

 * *Files 2% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/host/gemm.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/host/gemm.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/host/gemm_complex.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/host/gemm_complex.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -35,16 +35,18 @@
 #pragma once
 
 #include "cutlass/coord.h"
 #include "cutlass/complex.h"
 #include "cutlass/numeric_types.h"
 #include "cutlass/functional.h"
 #include "cutlass/numeric_conversion.h"
+#include "cutlass/matrix_coord.h"
 
 #include "cutlass/tensor_view.h"
+
 #include "cutlass/gemm/gemm.h"
 
 namespace cutlass {
 namespace reference {
 namespace host {
 
 ////////////////////////////////////////////////////////////////////////////////////////////////////
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/host/gemm_planar_complex.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/host/gemm_planar_complex.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/host/rank_2k.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/host/rank_2k.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/host/rank_2k_complex.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/host/rank_2k_complex.h`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/host/rank_k_complex.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/host/rank_k_complex.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/host/symm.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/host/symm.h`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/host/symm_complex.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/host/trmm.h`

 * *Files 26% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -25,294 +25,190 @@
  * SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER
  * CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,
  * OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
  * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
  *
  **************************************************************************************************/
 /*! \file
-    \brief Reference implementation for complex-valued SYMM update in host-side code.
+    \brief Reference implementation for TRMM in host-side code.
 
-    
+  
 */
 
 #pragma once
 
 #include "cutlass/blas3.h"
-#include "cutlass/complex.h"
 #include "cutlass/numeric_conversion.h"
 #include "cutlass/tensor_view.h"
 #include "cutlass/gemm/gemm.h"
-#include <assert.h>
+#include "cutlass/arch/mma.h"
+#include "cutlass/util/host_tensor.h"
+
+#include "cutlass/util/reference/host/gemm.h"
 
 namespace cutlass {
 namespace reference {
 namespace host {
 
-////////////////////////////////////////////////////////////////////////////////////////////////////
-
-/// Computes a general matrix product among matrices (tensors of rank=2) pointed to by TensorRef
+/// Computes a Triangular Matrix Multiplication (tensors of rank=2) pointed to by TensorRef
 /// objects.
-///
-/// Explicitly naming types needed by this template can be cumbersome, particularly for the
-/// accumulator type, so a function argument 'initial_accum' is exposed. Passing
-/// AccumulatorType(0) as the last function argument can be easier than naming all template
-/// arguments explicitly.
 template <
   typename ElementA,
   typename LayoutA,
   SideMode SideModeA,
   FillMode FillModeA,
+  DiagType DiagTypeA,
   typename ElementB,
   typename LayoutB,
   typename ElementC,
   typename LayoutC,
   typename ScalarType,
   typename ComputeType,
-  BlasMode BlasMode_ = BlasMode::kSymmetric,
   typename InnerProductOp = multiply_add<ComputeType>,
   typename ConvertOp = NumericConverter<ElementC, ScalarType>
 >
-void compute_symm_complex(
+void compute_trmm(
   gemm::GemmCoord problem_size,
   ScalarType alpha,
   TensorRef<ElementA, LayoutA> tensor_a,
   TensorRef<ElementB, LayoutB> tensor_b,
-  ScalarType beta,
-  TensorRef<ElementC, LayoutC> tensor_c,
   TensorRef<ElementC, LayoutC> tensor_d,
-  ComputeType initial_accum,
-  int batch_count = 1,
-  int64_t batch_stride_A = 0,
-  int64_t batch_stride_B = 0,
-  int64_t batch_stride_C = 0,
-  int64_t batch_stride_D = 0) {
-  
-  static SideMode const kSideModeA = SideModeA;
-  static FillMode const kFillModeA = FillModeA;
-  static BlasMode const kBlasMode  = BlasMode_;
+  ComputeType initial_accum) {
 
   static_assert(
     LayoutA::kRank == 2 &&
-    LayoutB::kRank == 2 &&
     LayoutC::kRank == 2, "Tensors must be of rank 2");
 
-  static_assert(kSideModeA != SideMode::kInvalid
+  static_assert(SideModeA != SideMode::kInvalid
                 , "Side Mode can either be Left or Right.");
 
-  static_assert(
-    kFillModeA == FillMode::kLower || 
-    kFillModeA == FillMode::kUpper, 
-    "Fill Mode can either be Lower or Upper.");
+  static_assert(FillModeA == FillMode::kLower || FillModeA == FillMode::kUpper
+                , "Fill Mode can either be Lower or Upper.");
 
-  using CompareOp_w_diag =  typename TrMatrixCompareOp<kFillModeA, DiagType::kNonUnit>::Type;
-  using CompareOp_wo_diag = typename TrMatrixCompareOp<kFillModeA, DiagType::kZero>::Type;
+  using CompareOp = typename TrMatrixCompareOp<FillModeA, DiagTypeA>::Type;
 
   // Note: batch is ignored.
   int const M = problem_size.m();
   int const N = problem_size.n();
   // Assuming correct k-dimension value is passed
   int const K = problem_size.k();
-
+ 
   // Blocking necessary to speedup reference implementation
   int const Mblock = 16;
   int const Nblock = 16;
 
   ConvertOp convert_op;
   InnerProductOp inner_product_op;
-  CompareOp_w_diag compare_op_1;
-  CompareOp_wo_diag compare_op_2;
-
-  for (int batch_idx = 0; batch_idx < batch_count; ++batch_idx) {
-
-    // Compute matrix product using blocks
-    for (int row_block = 0; row_block < M; row_block += Mblock) {
-      for (int col_block = 0; col_block < N; col_block += Nblock) {
-
-        ComputeType accum[Mblock][Nblock];
-
-        for (int j = 0; j < Nblock; j++) {
-          for (int i = 0; i < Mblock; i++) {
-            accum[i][j] = initial_accum;
-          }
-        }
-
-        for (int k_block = 0; k_block < K; ++k_block) {
-          for (int j = 0; j < Nblock; j++) {
-            for (int i = 0; i < Mblock; i++) {
-              int row = row_block + i;
-              int col = col_block + j;
-
-              if (row < M && col < N) 
-              {
-                ElementA a_1 = ElementA();
-                ElementB b_1 = ElementB();
-                ElementA a_2 = ElementA();
-                ElementB b_2 = ElementB();
-                
-                // A x B or B x A (with diagonal)
-                if (kSideModeA == SideMode::kLeft) {
-                  a_1 = (compare_op_1(row, k_block)) ? 
-                        (tensor_a.at(MatrixCoord(row, k_block))) : ElementA();
-                  b_1 = tensor_b.at(MatrixCoord(k_block, col));
-                } else if (kSideModeA == SideMode::kRight) {
-                  a_1 = tensor_b.at(MatrixCoord(row, k_block));
-                  b_1 = (compare_op_1(k_block, col)) ? 
-                        tensor_a.at(MatrixCoord(k_block, col)) : ElementA();
-                }
-                ComputeType compute_a_1 = ComputeType(a_1);
-                ComputeType compute_b_1 = ComputeType(b_1);
-
-                // The imaginary parts of the diagonal elements of 
-                // a complex data type are assumed and set to zero
-                if (kBlasMode == BlasMode::kHermitian && kSideModeA == SideMode::kLeft && row == k_block) {
-                  compute_a_1 = real(compute_a_1);
-                } else if (kBlasMode == BlasMode::kHermitian && kSideModeA == SideMode::kRight && k_block == col) {
-                  compute_b_1 = real(compute_b_1);
-                }
-
-                accum[i][j] = inner_product_op(compute_a_1, compute_b_1,  accum[i][j]);
+  CompareOp compare_op;
 
-                // A^T x B or B x A^T (without diagonal)
-                if (kSideModeA == SideMode::kLeft) {
-                  a_2 = (compare_op_2(k_block, row)) ? 
-                        (tensor_a.at(MatrixCoord(k_block, row))) : ElementA();
-                  b_2 = tensor_b.at(MatrixCoord(k_block, col));
-                  if (kBlasMode == BlasMode::kHermitian)
-                    a_2 = conj(a_2);
-                } else if (kSideModeA == SideMode::kRight) {
-                  a_2 = tensor_b.at(MatrixCoord(row, k_block));
-                  b_2 = (compare_op_2(col, k_block)) ? 
-                        tensor_a.at(MatrixCoord(col, k_block)) : ElementA();
-                  if (kBlasMode == BlasMode::kHermitian)
-                    b_2 = conj(b_2);
-                }
+  for (int row_block = 0; row_block < M; row_block += Mblock) {
+    for (int col_block = 0; col_block < N; col_block += Nblock) {
 
-                ComputeType compute_a_2 = ComputeType(a_2);
-                ComputeType compute_b_2 = ComputeType(b_2);
+      ComputeType accum[Mblock][Nblock];
 
-                accum[i][j] = inner_product_op(compute_a_2, compute_b_2, accum[i][j]);
-              }
-            }
-          }
+      for (int j = 0; j < Nblock; j++) {
+        for (int i = 0; i < Mblock; i++) {
+          accum[i][j] = initial_accum;
         }
+      }
 
+      for (int k_block = 0; k_block < K; ++k_block) {
         for (int j = 0; j < Nblock; j++) {
           for (int i = 0; i < Mblock; i++) {
             int row = row_block + i;
             int col = col_block + j;
 
-            MatrixCoord coord = MatrixCoord(row, col);
-
             if (row < M && col < N) {
+              ElementA a = ElementA();
+              ElementB b = ElementB();
 
-              ScalarType c = tensor_c.at(coord);
+              if (SideModeA == SideMode::kLeft) {
+                a = (compare_op(row, k_block)) ? 
+                            (tensor_a.at(MatrixCoord(row, k_block))) : ElementA(0);
+                if (row == k_block && DiagTypeA == DiagType::kUnit) {
+                  a = ElementA(1);
+                }
+                b = tensor_b.at(MatrixCoord(k_block, col));
+              } else if (SideModeA == SideMode::kRight) {
+                a = tensor_b.at(MatrixCoord(row, k_block));
+                b = (compare_op(k_block, col)) ? 
+                      tensor_a.at(MatrixCoord(k_block, col)) : ElementA(0);
+                if (k_block == col && DiagTypeA == DiagType::kUnit) {
+                  b = ElementA(1);
+                }
+              }
+                            
+              ComputeType compute_a(cast_if_scalar<ComputeType>(a));
+              ComputeType compute_b(cast_if_scalar<ComputeType>(b));
 
-              tensor_d.at(coord) = convert_op(
-                alpha * ScalarType(accum[i][j]) + 
-                beta * c);
+              accum[i][j] = inner_product_op(compute_a, compute_b, accum[i][j]);
             }
           }
         }
+      }
 
-      } // for (col_block)
-    } // for (row_block)
-
-    tensor_a.add_pointer_offset(batch_stride_A);
-    tensor_b.add_pointer_offset(batch_stride_B);
-    tensor_c.add_pointer_offset(batch_stride_C);
-    tensor_d.add_pointer_offset(batch_stride_D);
-
-  } // for (batch_idx)
+      for (int j = 0; j < Nblock; j++) {
+        for (int i = 0; i < Mblock; i++) {
+          int row = row_block + i;
+          int col = col_block + j;
+
+          MatrixCoord coord = MatrixCoord(row, col);
+
+          if (row < M && col < N) {
+            tensor_d.at(coord) = convert_op(
+              alpha * ScalarType(accum[i][j]));
+          }
+        }
+      }
+    }
+  }
 }
 
 ////////////////////////////////////////////////////////////////////////////////////////////////////
 
 template <
   typename ElementA,
   typename LayoutA,
   SideMode SideModeA,
   FillMode FillModeA,
+  DiagType DiagTypeA,
   typename ElementB,
   typename LayoutB,
   typename ElementC,
   typename LayoutC,
   typename ScalarType,
   typename ComputeType,
-  BlasMode BlasMode_ = cutlass::BlasMode::kSymmetric,
-  typename InnerProductOp = cutlass::arch::OpMultiplyAddComplex
+  typename InnerProductOp = cutlass::arch::OpMultiplyAdd
 >
-struct SymmComplex;
+struct Trmm;
 
 ////////////////////////////////////////////////////////////////////////////////////////////////////
 
 /// Partial specialization for multiply-add
-template <typename ElementA, typename LayoutA,
-          SideMode SideModeA, FillMode FillModeA, 
-          typename ElementB, typename LayoutB,
-          typename ElementC, typename LayoutC,
-          typename ScalarType, typename ComputeType,
-          BlasMode BlasMode_>
-struct SymmComplex<ElementA, LayoutA, 
-                   SideModeA, FillModeA,
-                   ElementB, LayoutB,
-                   ElementC, LayoutC, ScalarType,
-                   ComputeType, BlasMode_,
-                   arch::OpMultiplyAddComplex> {
-
-  void operator()(gemm::GemmCoord problem_size, ScalarType alpha,
-                  TensorRef<ElementA, LayoutA> tensor_a,
-                  TensorRef<ElementB, LayoutB> tensor_b, ScalarType beta,
-                  TensorRef<ElementC, LayoutC> tensor_c,
-                  TensorRef<ElementC, LayoutC> tensor_d,
-                  ComputeType initial_accum = ComputeType(0)) {
-    static_assert(
-        LayoutA::kRank == 2 && LayoutC::kRank == 2,
-        "Tensors must be of rank 2");
-
-    compute_symm_complex<ElementA, LayoutA,
-                 SideModeA, FillModeA,
-                 ElementB, LayoutB,
-                 ElementC, LayoutC, 
-                 ScalarType, ComputeType, BlasMode_, multiply_add<ComputeType>>(
-                 problem_size, alpha, tensor_a, tensor_b, beta, tensor_c, tensor_d, initial_accum);
-  }
-};
-
-////////////////////////////////////////////////////////////////////////////////////////////////////
-
-/// Partial specialization for gaussian multiply-add 
-template <typename ElementA, typename LayoutA,
-          SideMode SideModeA, FillMode FillModeA,
-          typename ElementB, typename LayoutB,
-          typename ElementC, typename LayoutC,
-          typename ScalarType, typename ComputeType,
-          BlasMode BlasMode_>
-struct SymmComplex<ElementA, LayoutA, 
-                   SideModeA, FillModeA, 
-                   ElementB, LayoutB,
-                   ElementC, LayoutC, ScalarType,
-                   ComputeType, BlasMode_,
-                   arch::OpMultiplyAddGaussianComplex> {
+template <typename ElementA, typename LayoutA, SideMode SideModeA,
+           FillMode FillModeA, DiagType DiagTypeA, 
+           typename ElementB, typename LayoutB,
+           typename ElementC, typename LayoutC,
+          typename ScalarType, typename ComputeType>
+struct Trmm<ElementA, LayoutA, SideModeA, FillModeA, DiagTypeA, ElementB, LayoutB,
+            ElementC, LayoutC, ScalarType,
+            ComputeType, arch::OpMultiplyAdd> {
 
   void operator()(gemm::GemmCoord problem_size, ScalarType alpha,
                   TensorRef<ElementA, LayoutA> tensor_a,
-                  TensorRef<ElementB, LayoutB> tensor_b, ScalarType beta,
-                  TensorRef<ElementC, LayoutC> tensor_c,
+                  TensorRef<ElementB, LayoutB> tensor_b,
                   TensorRef<ElementC, LayoutC> tensor_d,
                   ComputeType initial_accum = ComputeType(0)) {
     static_assert(
         LayoutA::kRank == 2 && LayoutC::kRank == 2,
         "Tensors must be of rank 2");
 
-    compute_symm_complex<ElementA, LayoutA,
-                 SideModeA, FillModeA,
-                 ElementB, LayoutB,
-                 ElementC, LayoutC, 
-                 ScalarType, ComputeType, BlasMode_, multiply_add<ComputeType>>(
-                 problem_size, alpha, tensor_a, tensor_b, beta, tensor_c, tensor_d, initial_accum);
+    compute_trmm<ElementA, LayoutA, SideModeA, FillModeA, DiagTypeA, ElementB, LayoutB,
+                 ElementC, LayoutC, ScalarType, ComputeType, multiply_add<ComputeType>>(
+                 problem_size, alpha, tensor_a, tensor_b, tensor_d, initial_accum);
   }
 };
 
 ////////////////////////////////////////////////////////////////////////////////////////////////////
 
 } // namespace host
 } // namespace reference
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/host/tensor_compare.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/host/tensor_compare.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/host/tensor_copy.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/host/tensor_copy.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/host/tensor_elementwise.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/host/tensor_elementwise.h`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -64,30 +64,30 @@
 
   //
   // Data members
   //
 
   /// View of left-hand-side tensor
   TensorView<ElementD, LayoutD> view_d;
-  TensorRef<ElementA, LayoutA> ref_a;
-  TensorRef<ElementB, LayoutB> ref_b;
+  TensorRef<ElementA, LayoutA> view_a;
+  TensorRef<ElementB, LayoutB> view_b;
   BinaryFunc func;
 
   //
   // Methods
   //
 
   /// Constructor
   TensorFuncBinaryOp() { }
 
   /// Constructor
   TensorFuncBinaryOp(
     TensorView<ElementD, LayoutD> const & view_d_,
-    TensorRef<ElementA, LayoutA> const & ref_a_,
-    TensorRef<ElementB, LayoutB> const & ref_b_,
+    TensorRef<ElementA, LayoutA> const & view_a_,
+    TensorRef<ElementB, LayoutB> const & view_b_,
     BinaryFunc func = BinaryFunc()
   ):
     view_d(view_d_), view_a(view_a_), view_b(view_b_), func(func) { }
 
   /// Equality check
   void operator()(Coord<LayoutD::kRank> const &coord) const {
     view_d.at(coord) = func(
@@ -280,15 +280,15 @@
   typename ElementA,
   typename LayoutA
 >
 void TensorDiv(
   TensorView<ElementD, LayoutD> d,      ///< destination tensor view
   TensorRef<ElementA, LayoutA> a        ///< A tensor reference
 ) {
-  TensorMul(d, d, a);
+  TensorDiv(d, d, a);
 }
 
 
 ///////////////////////////////////////////////////////////////////////////////////////////////////
 
 /// Divides two tensors and stores in the destination tensor: d = a ./ b
 template <
@@ -308,15 +308,15 @@
   detail::TensorFuncBinaryOp<
     ElementD, 
     LayoutD,
     ElementA,
     LayoutA,
     ElementB,
     LayoutB,
-    cutlass::modulus<ElementD>
+    cutlass::divides<ElementD>
   > func(d, a, b);
 
   TensorForEach(
     d.extent(),
     func);
 }
 
@@ -327,15 +327,15 @@
   typename ElementA,
   typename LayoutA
 >
 void TensorModulus(
   TensorView<ElementD, LayoutD> d,      ///< destination tensor view
   TensorRef<ElementA, LayoutA> a        ///< A tensor reference
 ) {
-  TensorMul(d, d, a);
+  TensorDiv(d, d, a);
 }
 
 ///////////////////////////////////////////////////////////////////////////////////////////////////
 
 } // namespace host
 } // namespace reference
 } // namespace cutlass
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/host/tensor_fill.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/host/tensor_fill.h`

 * *Files 2% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -88,14 +88,33 @@
   ): view(view_), value(value_) { }
 
   void operator()(Coord<Layout::kRank> const & coord) const {
     view.at(coord) = value;
   }
 };
 
+/// Returns a pair of values of the Gaussian distribution generated by the Box Muller method 
+struct BoxMullerFunc {
+
+  BoxMullerFunc() {}
+
+  void operator()(
+    double* rnd,                     ///< Size-2 vector to be filled with random values
+    double  mean = 0,                ///< Mean of the Gaussian distribution
+    double  stddev = 1,              ///< Standard deviation of the Gaussian distribution
+    double  pi = std::acos(-1)) const {
+
+    double u1 = double(std::rand()) / double(RAND_MAX);
+    double u2 = double(std::rand()) / double(RAND_MAX);
+    rnd[0] = std::sqrt(-2 * std::log(u1)) * std::cos(2 * pi * u2);
+    rnd[1] = std::sqrt(-2 * std::log(u1)) * std::sin(2 * pi * u2);
+    rnd[0] = mean + stddev * rnd[0];
+    rnd[1] = mean + stddev * rnd[1];
+  }
+};
 } // namespace detail
 
 ///////////////////////////////////////////////////////////////////////////////////////////////////
 
 /// Fills a tensor with a uniform value
 template <
   typename Element,               ///< Element type
@@ -201,30 +220,26 @@
   }
 
   /// Compute random value and update RNG state
   complex<Element> operator()() const {
 
     Element reals[2];
 
-    for (int i = 0; i < 2; ++i) {
-      // Box-Muller transform to generate random numbers with Normal distribution
-      double u1 = double(std::rand()) / double(RAND_MAX);
-      double u2 = double(std::rand()) / double(RAND_MAX);
-
-      // Compute Gaussian random value
-      double rnd = std::sqrt(-2 * std::log(u1)) * std::cos(2 * pi * u2);
-      rnd = mean + stddev * rnd;
+    double rnd[2];
+    detail::BoxMullerFunc func;
+    func(rnd, mean, stddev, pi);
 
-      if (int_scale >= 0) {
-        rnd = double(int(rnd * double(1 << int_scale)));
-        reals[i] = from_real<Element>(rnd / double(1 << int_scale));
-      }
-      else {
-        reals[i] = from_real<Element>(rnd);
-      }
+    if (int_scale >= 0) {
+      rnd[0] = double(int(rnd[0] * double(1 << int_scale)));
+      rnd[1] = double(int(rnd[1] * double(1 << int_scale)));
+      reals[0] = from_real<Element>(rnd[0] / double(1 << int_scale));
+      reals[1] = from_real<Element>(rnd[1] / double(1 << int_scale));
+    } else {
+      reals[0] = from_real<Element>(rnd[0]);
+      reals[1] = from_real<Element>(rnd[1]);
     }
 
     return complex<Element>(reals[0], reals[1]);
   }
 };
 
 /// Partial specialization for initializing a complex value.
@@ -251,30 +266,35 @@
   }
 
   /// Compute random value and update RNG state
   Quaternion<Element> operator()() const {
 
     Element reals[4];
 
-    for (int i = 0; i < 4; ++i) {
-      // Box-Muller transform to generate random numbers with Normal distribution
-      double u1 = double(std::rand()) / double(RAND_MAX);
-      double u2 = double(std::rand()) / double(RAND_MAX);
-
-      // Compute Gaussian random value
-      double rnd = std::sqrt(-2 * std::log(u1)) * std::cos(2 * pi * u2);
-      rnd = mean + stddev * rnd;
+    double rnd1[2];
+    double rnd2[2];
+    detail::BoxMullerFunc func;
+    func(rnd1, mean, stddev, pi);
+    func(rnd2, mean, stddev, pi);
 
-      if (int_scale >= 0) {
-        rnd = double(int(rnd * double(1 << int_scale)));
-        reals[i] = from_real<Element>(rnd / double(1 << int_scale));
-      }
-      else {
-        reals[i] = from_real<Element>(rnd);
-      }
+    if (int_scale >= 0) {
+      rnd1[0] = double(int(rnd1[0] * double(1 << int_scale)));
+      rnd1[1] = double(int(rnd1[1] * double(1 << int_scale)));
+      rnd2[0] = double(int(rnd2[0] * double(1 << int_scale)));
+      rnd2[1] = double(int(rnd2[1] * double(1 << int_scale)));
+
+      reals[0] = from_real<Element>(rnd1[0] / double(1 << int_scale));
+      reals[1] = from_real<Element>(rnd1[1] / double(1 << int_scale));
+      reals[2] = from_real<Element>(rnd2[0] / double(1 << int_scale));
+      reals[3] = from_real<Element>(rnd2[1] / double(1 << int_scale));
+    } else {
+      reals[0] = from_real<Element>(rnd1[0]);
+      reals[1] = from_real<Element>(rnd1[1]);
+      reals[2] = from_real<Element>(rnd2[0]);
+      reals[3] = from_real<Element>(rnd2[1]);
     }
 
     return Quaternion<Element>(reals[0], reals[1], reals[2], reals[3]);
   }
 };
 
 /// Computes a random Gaussian distribution
@@ -307,15 +327,15 @@
 
   /// Compute random value and update RNG state
   void operator()(Coord<Layout::kRank> const &coord) const {
     view.at(coord) = func();
   }
 };
 
-/// Computes a random Gaussian distribution
+/// Computes a random Gaussian distribution for a rank-2 tensor
 template <
   typename Element,               ///< Element type
   typename Layout>                ///< Layout function
 struct TensorFillSymmetricGaussianFunc {
 
   using TensorView = TensorView<Element, Layout>;
 
@@ -400,15 +420,15 @@
                                                        ///  data.
   
   TensorFillRandomGaussian(dst.view_real(), seed, mean, stddev, bits);
   TensorFillRandomGaussian(dst.view_imag(), ~seed, mean, stddev, bits);
 }
 
 ///////////////////////////////////////////////////////////////////////////////////////////////////
-/// Fills a tensor with random values with a Gaussian distribution.
+/// Fills the upper or lower part of a symmetric rank-2 tensor with random values of a Gaussian distribution.
 template <
   typename Element,               ///< Element type
   typename Layout>                ///< Layout function
 void TensorFillSymmetricRandomGaussian(
   TensorView<Element, Layout> dst,        ///< destination tensor
   uint64_t seed,                          ///< seed for RNG
   cutlass::FillMode fill_mode,            ///< FillMode for symmetric matrices
@@ -430,15 +450,15 @@
     dst.extent(),
     func
   );
 }
 
 ///////////////////////////////////////////////////////////////////////////////////////////////////
 
-/// Fills a tensor with random values with a Gaussian distribution.
+/// Fills a tensor with random values of a Gaussian distribution.
 template <
   typename Element                        ///< Element type
 >
 void BlockFillRandomGaussian(
   Element *ptr,                           ///< destination buffer
   size_t capacity,                        ///< number of elements
   uint64_t seed,                          ///< seed for RNG
@@ -610,15 +630,15 @@
       }
     }
 
     return make_Quaternion(reals[0], reals[1], reals[2], reals[3]);
   }
 };
 
-/// Computes a random Gaussian distribution
+/// Computes a random uniform distribution
 template <
   typename Element,               ///< Element type
   typename Layout>                ///< Layout function
 struct TensorFillRandomUniformFunc {
 
   using TensorView = TensorView<Element, Layout>;
 
@@ -629,15 +649,15 @@
   TensorView view;
   RandomUniformFunc<Element> func;
 
   //
   // Methods
   //
 
-  /// Construction of Gaussian RNG functor.
+  /// Construction of uniform RNG functor.
   TensorFillRandomUniformFunc(
     TensorView view_ = TensorView(),
     RandomUniformFunc<Element> func_ = RandomUniformFunc<Element>()
   ):
     view(view_), func(func_) {
 
   }
@@ -645,15 +665,15 @@
   /// Compute random value and update RNG state
   void operator()(Coord<Layout::kRank> const &coord) const {
 
     view.at(coord) = func();
   }
 };
 
-/// Computes a random Gaussian distribution
+/// Fills the upper or lower part of a symmetric rank-2 tensor with random values of a uniform distribution.
 template <
   typename Element,               ///< Element type
   typename Layout>                ///< Layout function
 struct TensorFillSymmetricRandomUniformFunc {
 
   using TensorView = TensorView<Element, Layout>;
 
@@ -665,15 +685,15 @@
   RandomUniformFunc<Element> func;
   cutlass::FillMode fill_mode;
 
   //
   // Methods
   //
 
-  /// Construction of Gaussian RNG functor.
+  /// Construction of uniform RNG functor.
   TensorFillSymmetricRandomUniformFunc(
     TensorView view_ = TensorView(),
     RandomUniformFunc<Element> func_ = RandomUniformFunc<Element>(),
     cutlass::FillMode fill_mode_ = cutlass::FillMode::kInvalid
   ):
     view(view_), func(func_), fill_mode(fill_mode_) {
 
@@ -711,15 +731,15 @@
   cutlass::FillMode fill_mode;
   int alignment;
 
   //
   // Methods
   //
 
-  /// Construction of Gaussian RNG functor.
+  /// Construction of uniform RNG functor.
   TensorFillPadDiagonalRandomUniformFunc(
     TensorView view_ = TensorView(),
     RandomUniformFunc<Element> func_ = RandomUniformFunc<Element>(),
     cutlass::FillMode fill_mode_ = cutlass::FillMode::kInvalid,
     int alignment_ = 1
   ):
     view(view_), func(func_), fill_mode(fill_mode_), alignment(alignment_) {
@@ -743,15 +763,15 @@
   }
 };
 
 } // namespace detail
 
 ///////////////////////////////////////////////////////////////////////////////////////////////////
 
-/// Fills a tensor with random values with a uniform random distribution.
+/// Fills a tensor with random values of a uniform random distribution.
 template <
   typename Element,               ///< Element type
   typename Layout>                ///< Layout function
 void TensorFillRandomUniform(
   TensorView<Element, Layout> dst,        ///< destination tensor
   uint64_t seed,                          ///< seed for RNG
   double max = 1,                         ///< upper bound of distribution
@@ -768,15 +788,15 @@
 
   TensorForEach(
     dst.extent(),
     func
   );
 }
 
-/// Fills a tensor with random values with a uniform random distribution.
+/// Fills a tensor with random values of a uniform random distribution.
 template <
   typename Element,               ///< Element type
   typename Layout>                ///< Layout function
 void TensorFillRandomUniform(
   TensorViewPlanarComplex<Element, Layout> dst,        ///< destination tensor
   uint64_t seed,                                       ///< seed for RNG
   double max = 1,                                      ///< upper bound of distribution
@@ -1248,15 +1268,15 @@
 
 namespace detail {
 
 template <typename Element>
 struct RandomSparseMetaFunc {
   
   uint64_t seed;
-  double range;
+  int range;
   int MetaSizeInBits;
 
   //
   // Methods
   //
 
   RandomSparseMetaFunc(
@@ -1278,17 +1298,16 @@
     Element TwoToOneMeta[2] = {0x4, 0xe};
 
     Element * MetaArray = (MetaSizeInBits == 2) ? FourToTwoMeta : TwoToOneMeta;
 
     Element result = 0x0;
 
     for (int i = 0; i < cutlass::sizeof_bits<Element>::value / 4; ++i) {
-      double rnd = double(std::rand()) / double(RAND_MAX);
-      rnd = range * rnd;
-      Element meta = MetaArray[(int)rnd];
+      int rnd = std::rand() % range;
+      Element meta = MetaArray[rnd];
 
       result = (Element)(result | ((Element)(meta << (i * 4))));
     }
 
     return result;
   }
 };
@@ -1370,14 +1389,45 @@
 
   for (size_t i = 0; i < capacity; ++i) {
     ptr[i] = random_func();
   }
 }
 
 ///////////////////////////////////////////////////////////////////////////////////////////////////
+///////////////////////////////////////////////////////////////////////////////////////////////////
+
+/// Fills a ell block index matrix with random values with a uniform random distribution.
+template <
+  typename Element,                                ///< Element type
+  typename Layout>                                 ///< Layout function
+void TensorFillRandomEllIdx(
+  TensorView<Element, Layout> dst,                 ///< destination tensor
+  uint64_t seed,                                   ///< seed for RNG
+  int rows, int ell_cols, int cols) {              ///< dimension of the matrix 
+
+  std::srand((unsigned)seed);
+
+  for (int i = 0; i < rows; ++i) {
+    int col_idx = std::rand() % cols;
+   
+    for (int j = 0; j < ell_cols; ++j) {
+      dst.at({i, j}) = col_idx;
+
+      if (col_idx != -1) {
+        if (col_idx == (cols - 1)) {
+          col_idx = -1;
+        } else {
+          col_idx = std::rand() % (cols - col_idx - 1) + col_idx + 1;
+        }
+      }
+    }
+  }
+}
+
+///////////////////////////////////////////////////////////////////////////////////////////////////
 
 /// Copies a diagonal in from host memory without modifying off-diagonal elements.
 template <
   typename Element,               ///< Element type
   typename Layout>                ///< Layout function
 void TensorCopyDiagonalIn(
   TensorView<Element, Layout> dst,          ///< destination tensor
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/host/tensor_foreach.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/host/tensor_foreach.h`

 * *Files 2% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/host/tensor_norm.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/host/tensor_norm.h`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/host/tensor_reduce.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/host/tensor_reduce.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/host/trmm_complex.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/host/trmm_complex.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/tensor_view_io.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/tensor_view_io.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/type_traits.h` & `flash_attn-1.0.2/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/type_traits.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/flash_api.cpp` & `flash_attn-1.0.2/csrc/flash_attn/flash_api.cpp`

 * *Files 4% similar despite different names*

```diff
@@ -19,14 +19,15 @@
                       const size_t b,
                       const size_t seqlen_q,
                       const size_t seqlen_k,
                       const size_t seqlen_q_rounded,
                       const size_t seqlen_k_rounded,
                       const size_t h,
                       const size_t d,
+                      const size_t d_rounded,
                       // device pointers
                       const at::Tensor q,
                       const at::Tensor k,
                       const at::Tensor v,
                       at::Tensor out,
                       void *cu_seqlens_q_d,
                       void *cu_seqlens_k_d,
@@ -76,14 +77,15 @@
     params.b = b;
     params.h = h;
     params.seqlen_q = seqlen_q;
     params.seqlen_k = seqlen_k;
     params.seqlen_q_rounded = seqlen_q_rounded;
     params.seqlen_k_rounded = seqlen_k_rounded;
     params.d = d;
+    params.d_rounded = d_rounded;
 
     // Set the different scale values.
     params.scale_softmax = softmax_scale;
     params.scale_softmax_log2 = softmax_scale * M_LOG2E;
 
     // Set this to probability of keeping an element to simplify things.
     params.p_dropout = 1.f - p_dropout;
@@ -104,14 +106,15 @@
                       const size_t b,
                       const size_t seqlen_q,
                       const size_t seqlen_k,
                       const size_t seqlen_q_rounded,
                       const size_t seqlen_k_rounded,
                       const size_t h,
                       const size_t d,
+                      const size_t d_rounded,
                       // device pointers
                       const at::Tensor q,
                       const at::Tensor k,
                       const at::Tensor v,
                       const at::Tensor out,
                       const at::Tensor dout,
                       at::Tensor dq,
@@ -125,15 +128,15 @@
                       void *softmax_lse_d,
                       void *dsoftmax_sum_d,
                       float p_dropout,
                       float softmax_scale,
                       bool is_causal) {
 
     set_params_fprop(params,
-                     b, seqlen_q, seqlen_k, seqlen_q_rounded, seqlen_k_rounded, h, d,
+                     b, seqlen_q, seqlen_k, seqlen_q_rounded, seqlen_k_rounded, h, d, d_rounded,
                      q, k, v, out,
                      cu_seqlens_q_d,
                      cu_seqlens_k_d,
                      nullptr,
                      softmax_lse_d,
                      p_dropout,
                      softmax_scale,
@@ -186,15 +189,15 @@
     });
 }
 
 std::vector<at::Tensor>
 mha_fwd(const at::Tensor &q,         // batch_size x seqlen_q x num_heads x head_size
         const at::Tensor &k,         // batch_size x seqlen_k x num_heads x head_size
         const at::Tensor &v,         // batch_size x seqlen_k x num_heads x head_size
-        at::Tensor &out,             // batch_size x seqlen_q x num_heads x head_size
+        c10::optional<at::Tensor> &out_,             // batch_size x seqlen_q x num_heads x head_size
         const float p_dropout,
         const float softmax_scale,
         const bool is_causal,
         const bool return_softmax,
         c10::optional<at::Generator> gen_) {
 
     auto dprops = at::cuda::getCurrentDeviceProperties();
@@ -203,46 +206,65 @@
     bool is_sm90 = dprops->major == 9 && dprops->minor == 0;
     TORCH_CHECK(is_sm90 || is_sm8x || is_sm75);
 
     auto q_dtype = q.dtype();
     TORCH_CHECK(q_dtype == torch::kFloat16 || ((is_sm90 || is_sm8x) && q_dtype == torch::kBFloat16));
     TORCH_CHECK(k.dtype() == q_dtype);
     TORCH_CHECK(v.dtype() == q_dtype);
-    TORCH_CHECK(out.dtype() == q_dtype);
 
     TORCH_CHECK(q.is_cuda());
     TORCH_CHECK(k.is_cuda());
     TORCH_CHECK(v.is_cuda());
-    TORCH_CHECK(out.is_cuda());
 
     TORCH_CHECK(q.stride(-1) == 1);
     TORCH_CHECK(k.stride(-1) == 1);
     TORCH_CHECK(v.stride(-1) == 1);
-    TORCH_CHECK(out.stride(-1) == 1);
 
     const auto sizes = q.sizes();
 
     const int batch_size = sizes[0];
     const int seqlen_q = sizes[1];
     const int num_heads = sizes[2];
-    const int head_size = sizes[3];
+    const int head_size_og = sizes[3];
     const int seqlen_k = k.size(1);
-    // TORCH_CHECK((head_size % 8 == 0) && (head_size <= 192));
-    TORCH_CHECK((head_size % 32 == 0) && (head_size <= 192));
+    TORCH_CHECK(head_size_og <= 192);
+
+    CHECK_SHAPE(q, batch_size, seqlen_q, num_heads, head_size_og);
+    CHECK_SHAPE(k, batch_size, seqlen_k, num_heads, head_size_og);
+    CHECK_SHAPE(v, batch_size, seqlen_k, num_heads, head_size_og);
+
+    at::Tensor q_padded, k_padded, v_padded;
+    if (head_size_og % 8 != 0) {
+        q_padded = torch::nn::functional::pad(q, torch::nn::functional::PadFuncOptions({0, 8 - head_size_og % 8}));
+        k_padded = torch::nn::functional::pad(k, torch::nn::functional::PadFuncOptions({0, 8 - head_size_og % 8}));
+        v_padded = torch::nn::functional::pad(v, torch::nn::functional::PadFuncOptions({0, 8 - head_size_og % 8}));
+    } else {
+        q_padded = q;
+        k_padded = k;
+        v_padded = v;
+    }
+
+    at::Tensor out;
+    if (out_.has_value()) {
+        out = out_.value();
+        TORCH_CHECK(out.dtype() == q_dtype);
+        TORCH_CHECK(out.is_cuda());
+        TORCH_CHECK(out.stride(-1) == 1);
+        CHECK_SHAPE(out, batch_size, seqlen_q, num_heads, head_size_og);
+        if (head_size_og % 8 != 0) { out = torch::empty_like(q_padded); }
+    } else {
+        out = torch::empty_like(q_padded);
+    }
 
     auto round_multiple = [](int x, int m) { return (x + m - 1) / m * m; };
+    const int head_size = round_multiple(head_size_og, 8);
     const int head_size_rounded = round_multiple(head_size, 32);
     const int seqlen_q_rounded = round_multiple(seqlen_q, 128);
     const int seqlen_k_rounded = round_multiple(seqlen_k, 128);
 
-    CHECK_SHAPE(q, batch_size, seqlen_q, num_heads, head_size);
-    CHECK_SHAPE(k, batch_size, seqlen_k, num_heads, head_size);
-    CHECK_SHAPE(v, batch_size, seqlen_k, num_heads, head_size);
-    CHECK_SHAPE(out, batch_size, seqlen_q, num_heads, head_size);
-
     // Otherwise the kernel will be launched from cuda:0 device
     // Cast to char to avoid compiler warning about narrowing
     at::cuda::CUDAGuard device_guard{(char)q.get_device()};
 
     auto opts = q.options();
 
     auto softmax_lse = torch::empty({batch_size, num_heads, seqlen_q}, opts.dtype(at::kFloat));
@@ -255,16 +277,16 @@
 
     Flash_fwd_params params;
     set_params_fprop(params,
                      batch_size,
                      seqlen_q, seqlen_k,
                      seqlen_q_rounded, seqlen_k_rounded,
                      num_heads,
-                     head_size,
-                     q, k, v, out,
+                     head_size, head_size_rounded,
+                     q_padded, k_padded, v_padded, out,
                      /*cu_seqlens_q_d=*/nullptr,
                      /*cu_seqlens_k_d=*/nullptr,
                      return_softmax ? p.data_ptr() : nullptr,
                      softmax_lse.data_ptr(),
                      p_dropout,
                      softmax_scale,
                      is_causal);
@@ -280,17 +302,21 @@
         std::lock_guard<std::mutex> lock(gen->mutex_);
         params.philox_args = gen->philox_cuda_state(counter_offset);
     }
 
     auto stream = at::cuda::getCurrentCUDAStream().stream();
     run_mha_fwd(params, stream);
 
-    std::vector<at::Tensor> result = {softmax_lse};
-    if (return_softmax) { result.push_back(p); }
-    return result;
+    at::Tensor out_padded = out;
+    if (head_size_og % 8 != 0) {
+        out = out.index({"...", torch::indexing::Slice(torch::indexing::None, head_size_og)});
+        if (out_.has_value()) { out_.value().copy_(out); }
+    }
+
+    return {out, q_padded, k_padded, v_padded, out_padded, softmax_lse, p};
 }
 
 std::vector<at::Tensor>
 mha_varlen_fwd(const at::Tensor &q,  // total_q x num_heads x head_size, total_q := \sum_{i=0}^{b} s_i
                const at::Tensor &k,  // total_k x num_heads x head_size, total_k := \sum_{i=0}^{b} s_i
                const at::Tensor &v,  // total_k x num_heads x head_size, total_k := \sum_{i=0}^{b} s_i
                at::Tensor &out,      // total_q x num_heads x head_size, total_k := \sum_{i=0}^{b} s_i
@@ -372,15 +398,15 @@
 
     Flash_fwd_params params;
     set_params_fprop(params,
                      batch_size,
                      max_seqlen_q, max_seqlen_k,
                      max_seqlen_q, max_seqlen_k, // TODO: round these
                      num_heads,
-                     head_size,
+                     head_size, head_size_rounded,
                      q, k, v, out,
                      cu_seqlens_q.data_ptr(),
                      cu_seqlens_k.data_ptr(),
                      return_softmax ? s.data_ptr() : nullptr,
                      softmax_lse.data_ptr(),
                      p_dropout,
                      softmax_scale,
@@ -417,23 +443,23 @@
         } else if (params.d <= 128) {
             run_mha_bwd_<elem_type, 128>(params, stream, configure);
         }
     });
 }
 
 std::vector<at::Tensor>
-mha_bwd(const at::Tensor &dout,  // batch_size x seqlen_q x num_heads, x head_size
+mha_bwd(const at::Tensor &dout,  // batch_size x seqlen_q x num_heads, x head_size_og
         const at::Tensor &q,   // batch_size x seqlen_q x num_heads x head_size
         const at::Tensor &k,   // batch_size x seqlen_k x num_heads x head_size
         const at::Tensor &v,   // batch_size x seqlen_k x num_heads x head_size
         const at::Tensor &out,   // batch_size x seqlen_q x num_heads x head_size
-        const at::Tensor &softmax_lse_,     // b x h x s
-        at::Tensor &dq,   // batch_size x seqlen_q x num_heads x head_size
-        at::Tensor &dk,   // batch_size x seqlen_k x num_heads x head_size
-        at::Tensor &dv,   // batch_size x seqlen_k x num_heads x head_size
+        const at::Tensor &softmax_lse,     // b x h x seqlen_q
+        c10::optional<at::Tensor> &dq_,   // batch_size x seqlen_q x num_heads x head_size
+        c10::optional<at::Tensor> &dk_,   // batch_size x seqlen_k x num_heads x head_size
+        c10::optional<at::Tensor> &dv_,   // batch_size x seqlen_k x num_heads x head_size
         const float p_dropout,         // probability to drop
         const float softmax_scale,
         const bool is_causal,
         c10::optional<at::Generator> gen_) {
     auto dprops = at::cuda::getCurrentDeviceProperties();
     bool is_sm75 = dprops->major == 7 && dprops->minor == 5;
     bool is_sm8x = dprops->major == 8 && dprops->minor >= 0;
@@ -446,95 +472,118 @@
 
     auto q_dtype = q.dtype();
     TORCH_CHECK(q_dtype == torch::kFloat16 || ((is_sm90 || is_sm8x) && q_dtype == torch::kBFloat16));
     TORCH_CHECK(k.dtype() == q_dtype);
     TORCH_CHECK(v.dtype() == q_dtype);
     TORCH_CHECK(out.dtype() == q_dtype);
     TORCH_CHECK(dout.dtype() == q_dtype);
-    TORCH_CHECK(dq.dtype() == q_dtype);
-    TORCH_CHECK(dk.dtype() == q_dtype);
-    TORCH_CHECK(dv.dtype() == q_dtype);
 
     TORCH_CHECK(q.is_cuda());
     TORCH_CHECK(k.is_cuda());
     TORCH_CHECK(v.is_cuda());
     TORCH_CHECK(out.is_cuda());
     TORCH_CHECK(dout.is_cuda());
-    TORCH_CHECK(softmax_lse_.is_cuda());
+    TORCH_CHECK(softmax_lse.is_cuda());
 
     TORCH_CHECK(q.stride(-1) == 1);
     TORCH_CHECK(k.stride(-1) == 1);
     TORCH_CHECK(v.stride(-1) == 1);
-    TORCH_CHECK(out.is_contiguous());
-    TORCH_CHECK(dout.is_contiguous());
-    TORCH_CHECK(dq.stride(-1) == 1);
-    TORCH_CHECK(dk.stride(-1) == 1);
-    TORCH_CHECK(dv.stride(-1) == 1);
+    TORCH_CHECK(out.stride(-1) == 1);
+    TORCH_CHECK(dout.stride(-1) == 1);
 
     const auto sizes = q.sizes();
 
     const int batch_size = sizes[0];
     const int seqlen_q = sizes[1];
     const int num_heads = sizes[2];
+    const int head_size_og = dout.size(3);
     const int head_size = sizes[3];
     const int seqlen_k = k.size(1);
     TORCH_CHECK(batch_size > 0);
-    // TORCH_CHECK((head_size % 8 == 0) && (head_size <= 128));
-    TORCH_CHECK((head_size % 32 == 0) && (head_size <= 128));
+    TORCH_CHECK((head_size % 8 == 0) && (head_size <= 128));
+
+    auto round_multiple = [](int x, int m) { return (x + m - 1) / m * m; };
+    const int head_size_rounded = round_multiple(head_size, 32);
+    const int seqlen_q_rounded = round_multiple(seqlen_q, 128);
+    const int seqlen_k_rounded = round_multiple(seqlen_k, 128);
+
+    TORCH_CHECK(head_size == round_multiple(head_size_og, 8));
 
     CHECK_SHAPE(q, batch_size, seqlen_q, num_heads, head_size);
     CHECK_SHAPE(k, batch_size, seqlen_k, num_heads, head_size);
     CHECK_SHAPE(v, batch_size, seqlen_k, num_heads, head_size);
     CHECK_SHAPE(out, batch_size, seqlen_q, num_heads, head_size);
-    CHECK_SHAPE(dout, batch_size, seqlen_q, num_heads, head_size);
-    CHECK_SHAPE(dq, batch_size, seqlen_q, num_heads, head_size);
-    CHECK_SHAPE(dk, batch_size, seqlen_k, num_heads, head_size);
-    CHECK_SHAPE(dv, batch_size, seqlen_k, num_heads, head_size);
+    CHECK_SHAPE(dout, batch_size, seqlen_q, num_heads, head_size_og);
 
-    auto round_multiple = [](int x, int m) { return (x + m - 1) / m * m; };
-    const int head_size_rounded = round_multiple(head_size, 32);
-    const int seqlen_q_rounded = round_multiple(seqlen_q, 128);
-    const int seqlen_k_rounded = round_multiple(seqlen_k, 128);
+    at::Tensor dq, dk, dv;
+    if (dq_.has_value()) {
+        dq = dq_.value();
+        TORCH_CHECK(dq.dtype() == q_dtype);
+        TORCH_CHECK(dq.is_cuda());
+        TORCH_CHECK(dq.stride(-1) == 1);
+        CHECK_SHAPE(dq, batch_size, seqlen_q, num_heads, head_size);
+    } else {
+        dq = torch::empty_like(q);
+    }
+    if (dk_.has_value()) {
+        dk = dk_.value();
+        TORCH_CHECK(dk.dtype() == q_dtype);
+        TORCH_CHECK(dk.is_cuda());
+        TORCH_CHECK(dk.stride(-1) == 1);
+        CHECK_SHAPE(dk, batch_size, seqlen_k, num_heads, head_size);
+    } else {
+        dk = torch::empty_like(k);
+    }
+    if (dv_.has_value()) {
+        dv = dv_.value();
+        TORCH_CHECK(dv.dtype() == q_dtype);
+        TORCH_CHECK(dv.is_cuda());
+        TORCH_CHECK(dv.stride(-1) == 1);
+        CHECK_SHAPE(dv, batch_size, seqlen_k, num_heads, head_size);
+    } else {
+        dv = torch::empty_like(k);
+    }
+
+    at::Tensor dout_padded;
+    if (head_size_og % 8 != 0) {
+        dout_padded = torch::nn::functional::pad(dout, torch::nn::functional::PadFuncOptions({0, 8 - head_size_og % 8}));
+    } else {
+        dout_padded = dout;
+    }
 
     // TODO: will need to round depending on kBlockM
     // int seqlen_q = ((max_seqlen_q_ + 16 - 1) / 16) * 16;
     // bool loop = seqlen_k > blocksize_c;
     // TODO: change later, for now set to true for simplicity
     bool loop = true;
 
     // Otherwise the kernel will be launched from cuda:0 device
     // Cast to char to avoid compiler warning about narrowing
     at::cuda::CUDAGuard device_guard{(char)q.get_device()};
 
-    // It's possible the softmax_lse_ from the fwd has a different length since blocksize_c could be different.
-    // TODO: indexing into softmax_lse would be wrong if we have different lengths between fwd and bwd
-    // Since we assume that softmax_lse is contiguous.
-    auto softmax_lse = softmax_lse_.index({torch::indexing::Slice(), torch::indexing::Slice(), torch::indexing::Slice(torch::indexing::None, seqlen_q)}).contiguous();
-
     auto opts = q.options();
     auto softmax_d = torch::empty({batch_size, num_heads, seqlen_q_rounded}, opts.dtype(at::kFloat));
     at::Tensor dq_accum;
     at::Tensor dk_accum, dv_accum;
     if (loop) {
-        // TODO: this needs to be head_size_rounded
         dq_accum = torch::empty({batch_size, num_heads, seqlen_q_rounded, head_size_rounded}, opts.dtype(at::kFloat));
         dk_accum = torch::empty({batch_size, num_heads, seqlen_k_rounded, head_size_rounded}, opts.dtype(at::kFloat));
         dv_accum = torch::empty({batch_size, num_heads, seqlen_k_rounded, head_size_rounded}, opts.dtype(at::kFloat));
     }
 
     Flash_bwd_params params;
 
     set_params_dgrad(params,
                      batch_size,
                      seqlen_q, seqlen_k,
                      seqlen_q_rounded, seqlen_k_rounded,
                      num_heads,
-                     head_size,
+                     head_size, head_size_rounded,
                      q, k, v, out,
-                     dout, dq, dk, dv,
+                     dout_padded, dq, dk, dv,
                      nullptr,
                      nullptr,
                      loop ? dq_accum.data_ptr() : nullptr,
                      loop ? dk_accum.data_ptr() : nullptr,
                      loop ? dv_accum.data_ptr() : nullptr,
                      softmax_lse.data_ptr(),
                      softmax_d.data_ptr(),
@@ -565,15 +614,21 @@
         // See Note [Acquire lock when using random generators]
         std::lock_guard<std::mutex> lock(gen->mutex_);
         params.philox_args = gen->philox_cuda_state(counter_offset);
     }
 
     launch(params, stream, /*configure=*/false);
 
-    return { softmax_d };
+    if (head_size_og % 8 != 0) {
+        dq = dq.index({"...", torch::indexing::Slice(torch::indexing::None, head_size_og)});
+        dk = dk.index({"...", torch::indexing::Slice(torch::indexing::None, head_size_og)});
+        dv = dv.index({"...", torch::indexing::Slice(torch::indexing::None, head_size_og)});
+    }
+
+    return { dq, dk, dv, softmax_d };
 }
 
 std::vector<at::Tensor>
 mha_varlen_bwd(const at::Tensor &dout,  // total_q x num_heads, x head_size
                const at::Tensor &q,   // total_q x num_heads x head_size, total_q := \sum_{i=0}^{b} s_i
                const at::Tensor &k,   // total_k x num_heads x head_size, total_k := \sum_{i=0}^{b} s_i
                const at::Tensor &v,   // total_k x num_heads x head_size, total_k := \sum_{i=0}^{b} s_i
@@ -694,15 +749,15 @@
     Flash_bwd_params params;
 
     set_params_dgrad(params,
                      batch_size,
                      max_seqlen_q, max_seqlen_k,
                      max_seqlen_q, max_seqlen_k, // TODO: round these
                      num_heads,
-                     head_size,
+                     head_size, head_size,  // TODO: round this
                      q, k, v, out,
                      dout, dq, dk, dv,
                      cu_seqlens_q.data_ptr(),
                      cu_seqlens_k.data_ptr(),
                      loop ? dq_accum.data_ptr() : nullptr,
                      nullptr,
                      nullptr,
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/fmha_api.cpp` & `flash_attn-1.0.2/csrc/flash_attn/fmha_api.cpp`

 * *Files 1% similar despite different names*

```diff
@@ -306,24 +306,29 @@
                      is_causal,
                      num_splits);
 
     // number of times random will be generated per thread, to offset philox counter in thc random
     // state
     // We use a custom RNG that increases the offset by batch_size * nheads * 32.
     int64_t counter_offset = launch_params.params.b * launch_params.params.h * 32;
+    auto options = torch::TensorOptions().dtype(torch::kFloat32).device(torch::kCUDA);
+    auto rng_state = torch::empty({2}, options.dtype(torch::kInt64));
+    // Forward kernel will populate memory with the seed and offset.
+    launch_params.params.rng_state = reinterpret_cast<uint64_t*>(rng_state.data_ptr());
 
     if( is_dropout ) {
         // See Note [Acquire lock when using random generators]
         std::lock_guard<std::mutex> lock(gen->mutex_);
         launch_params.params.philox_args = gen->philox_cuda_state(counter_offset);
     }
 
     run_fmha_fwd(launch_params);
 
     std::vector<at::Tensor> result = {softmax_lse};
+    result.push_back(rng_state);
     if (return_softmax) {result.push_back(s);}
     return result;
 }
 
 void run_fmha_bwd(FMHA_dgrad_params &params, cudaStream_t stream, const bool configure) {
   if (params.d <= 32) {
       run_fmha_bwd_hdim32(params, stream, configure);
@@ -349,15 +354,16 @@
         const int max_seqlen_q_,
         const int max_seqlen_k_,          // max sequence length to choose the kernel
         const float p_dropout,         // probability to drop
         const float softmax_scale,
         const bool zero_tensors,
         const bool is_causal,
         const int num_splits,
-        c10::optional<at::Generator> gen_
+        c10::optional<at::Generator> gen_,
+        c10::optional<at::Tensor> &rng_state
 ) {
     auto dprops = at::cuda::getCurrentDeviceProperties();
     bool is_sm75 = dprops->major == 7 && dprops->minor == 5;
     bool is_sm80 = dprops->major == 8 && dprops->minor == 0;
     bool is_sm8x = dprops->major == 8 && dprops->minor >= 0;
     bool is_sm90 = dprops->major == 9 && dprops->minor == 0;
     TORCH_CHECK(is_sm90 || is_sm8x || is_sm75);
@@ -484,19 +490,23 @@
     }
 
     auto gen = at::get_generator_or_default<at::CUDAGeneratorImpl>(
         gen_, at::cuda::detail::getDefaultCUDAGenerator());
 
     // We use a custom RNG that increases the offset by batch_size * nheads * 32.
     int64_t counter_offset = params.b * params.h * 32;
-
-    if( is_dropout ) {
+    if ( rng_state.has_value() ) {
+        params.rng_state = reinterpret_cast<uint64_t*>(rng_state.value().data_ptr());
+    } else if( is_dropout ) {
         // See Note [Acquire lock when using random generators]
         std::lock_guard<std::mutex> lock(gen->mutex_);
         params.philox_args = gen->philox_cuda_state(counter_offset);
+        auto seeds = at::cuda::philox::unpack(params.philox_args);
+        params.rng_state[0] = std::get<0>(seeds);
+        params.rng_state[1] = std::get<1>(seeds);
     }
 
     launch(params, stream, /*configure=*/false);
 
     if (params.num_splits > 1) {
         dq.copy_(dq_tmp);
     }
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/src/block_info.h` & `flash_attn-1.0.2/csrc/flash_attn/src/block_info.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.1/csrc/flash_attn/src/flash.h` & `flash_attn-1.0.2/csrc/flash_attn/src/flash.h`

 * *Files 2% similar despite different names*

```diff
@@ -59,15 +59,15 @@
     // The pointer to the P matrix.
     void * __restrict__ p_ptr;
 
     // The pointer to the softmax sum.
     void * __restrict__ softmax_lse_ptr;
 
     // The dimensions.
-    int b, seqlen_q, seqlen_k, d, seqlen_q_rounded, seqlen_k_rounded;
+    int b, seqlen_q, seqlen_k, d, seqlen_q_rounded, seqlen_k_rounded, d_rounded;
 
     // The scaling factors for the kernel.
     float scale_softmax;
     float scale_softmax_log2;
 
     // array of length b+1 holding starting offset of each sequence.
     int * __restrict__ cu_seqlens_q;
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/src/flash_bwd_hdim128.cu` & `flash_attn-1.0.2/csrc/flash_attn/src/flash_bwd_hdim128.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.1/csrc/flash_attn/src/flash_bwd_hdim128_bf16_sm80.cu` & `flash_attn-1.0.2/csrc/flash_attn/src/flash_bwd_hdim128_bf16_sm80.cu`

 * *Files 0% similar despite different names*

```diff
@@ -3,10 +3,10 @@
 // Splitting the different head dimensions to different files to speed up compilation.
 
 #include "flash_bwd_launch_template.h"
 
 template<>
 void run_mha_bwd_<cutlass::bfloat16_t, 128>(Flash_bwd_params &params, cudaStream_t stream, const bool configure) {
     using elem_type = cutlass::bfloat16_t;
-    // run_flash_bwd_loop<Flash_bwd_kernel_traits<128, 64, 128, 8, 2, 4, 2, false, elem_type>>(params, stream, configure);
-    run_flash_bwd_loop<Flash_bwd_kernel_traits<128, 128, 64, 8, 4, 4, 4, false, elem_type>>(params, stream, configure);
+    run_flash_bwd_loop<Flash_bwd_kernel_traits<128, 64, 128, 8, 2, 4, 2, false, elem_type>>(params, stream, configure);
+    // run_flash_bwd_loop<Flash_bwd_kernel_traits<128, 128, 64, 8, 4, 4, 4, false, elem_type>>(params, stream, configure);
 }
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/src/flash_bwd_hdim128_fp16_sm80.cu` & `flash_attn-1.0.2/csrc/flash_attn/src/flash_bwd_hdim128_fp16_sm80.cu`

 * *Files 0% similar despite different names*

```diff
@@ -7,12 +7,12 @@
 template<>
 void run_mha_bwd_<cutlass::half_t, 128>(Flash_bwd_params &params, cudaStream_t stream, const bool configure) {
     using elem_type = cutlass::half_t;
     // run_flash_bwd_loop<Flash_bwd_kernel_traits<128, 32, 128, 8, 2, 2, 2, false, elem_type>>(params, stream, configure);
     // This is faster, in the case of sequence-parallel bwd (where we need fewer registers).
     // Out of these three, the 2nd one is slightly faster (2% faster than the first). Idk why.
     // run_flash_bwd_loop<Flash_bwd_kernel_traits<128, 64, 128, 8, 2, 2, 2, false, elem_type>>(params, stream, configure);
-    // run_flash_bwd_loop<Flash_bwd_kernel_traits<128, 64, 128, 8, 2, 4, 2, false, elem_type>>(params, stream, configure);
+    run_flash_bwd_loop<Flash_bwd_kernel_traits<128, 64, 128, 8, 2, 4, 2, false, elem_type>>(params, stream, configure);
     // run_flash_bwd_loop<Flash_bwd_kernel_traits<128, 64, 128, 8, 2, 4, 4, false, elem_type>>(params, stream, configure);
 
-    run_flash_bwd_loop<Flash_bwd_kernel_traits<128, 128, 64, 8, 4, 4, 4, false, elem_type>>(params, stream, configure);
+    // run_flash_bwd_loop<Flash_bwd_kernel_traits<128, 128, 64, 8, 4, 4, 4, false, elem_type>>(params, stream, configure);
 }
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/src/flash_bwd_hdim128_sm80_fp16.cu` & `flash_attn-1.0.2/csrc/flash_attn/src/flash_bwd_hdim128_sm80_fp16.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.1/csrc/flash_attn/src/flash_bwd_hdim64.cu` & `flash_attn-1.0.2/csrc/flash_attn/src/flash_bwd_hdim64.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.1/csrc/flash_attn/src/flash_bwd_hdim64_fp16_sm80.cu` & `flash_attn-1.0.2/csrc/flash_attn/src/flash_bwd_hdim64_fp16_sm80.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.1/csrc/flash_attn/src/flash_bwd_hdim64_sm80_fp16.cu` & `flash_attn-1.0.2/csrc/flash_attn/src/flash_bwd_hdim64_sm80_fp16.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.1/csrc/flash_attn/src/flash_bwd_hdim96.cu` & `flash_attn-1.0.2/csrc/flash_attn/src/flash_bwd_hdim96.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.1/csrc/flash_attn/src/flash_bwd_hdim96_fp16_sm80.cu` & `flash_attn-1.0.2/csrc/flash_attn/src/flash_bwd_hdim96_fp16_sm80.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.1/csrc/flash_attn/src/flash_bwd_hdim96_sm80_fp16.cu` & `flash_attn-1.0.2/csrc/flash_attn/src/flash_bwd_hdim96_sm80_fp16.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.1/csrc/flash_attn/src/flash_bwd_kernel.h` & `flash_attn-1.0.2/csrc/flash_attn/src/flash_bwd_kernel.h`

 * *Files 6% similar despite different names*

```diff
@@ -134,15 +134,15 @@
     const BlockInfo</*Varlen=*/false> binfo(params, bidb);
     if (m_block * kBlockM >= binfo.actual_seqlen_q) return;
 
     const index_t row_offset_do = binfo.q_offset(params.do_batch_stride, params.do_row_stride, bidb)
         + m_block * kBlockM * params.do_row_stride + bidh * params.do_head_stride;
     const index_t row_offset_o = binfo.q_offset(params.o_batch_stride, params.o_row_stride, bidb)
         + m_block * kBlockM * params.o_row_stride + bidh * params.o_head_stride;
-    const index_t row_offset_dq_accum = ((bidb * params.h + bidh) * params.seqlen_q_rounded + m_block * kBlockM) * params.d;
+    const index_t row_offset_dq_accum = ((bidb * params.h + bidh) * params.seqlen_q_rounded + m_block * kBlockM) * params.d_rounded;
     const index_t row_offset_dpsum = (bidb * params.h + bidh) * params.seqlen_q_rounded + m_block * kBlockM;
 
     Tensor gdO = make_tensor(make_gmem_ptr(reinterpret_cast<Element *>(params.do_ptr) + row_offset_do),
                              Shape<Int<kBlockM>, Int<kHeadDim>>{},
                              make_stride(params.do_row_stride, _1{}));
     Tensor gO = make_tensor(make_gmem_ptr(reinterpret_cast<Element *>(params.o_ptr) + row_offset_o),
                             Shape<Int<kBlockM>, Int<kHeadDim>>{},
@@ -160,26 +160,28 @@
     Tensor tdOgdO = gmem_thr_copy_dO.partition_S(gdO);
     Tensor tdOgO = gmem_thr_copy_dO.partition_S(gO);
     Tensor tdQgdQaccum = gmem_thr_copy_dQ_accum.partition_D(gdQaccum);
 
     Tensor cdO = make_identity_tensor(Shape<Int<kBlockM>, Int<kHeadDim>>{});    // (BLK_M,BLK_K) -> (blk_m,blk_k)
     Tensor tdOcdO = gmem_thr_copy_dO.partition_S(cdO);
 
+    // Allocate predicate tensors for k
+    Tensor tdOpdO = make_tensor<bool>(make_shape(size<2>(tdOgdO)));
+    // Set predicates for k bounds
+    #pragma unroll
+    for (int k = 0; k < size(tdOpdO); ++k) {tdOpdO(k) = get<1>(tdOcdO(0, 0, k)) < params.d;}
+
     Tensor tdOrdO = make_fragment_like(tdOgdO);
     Tensor tdOrO = make_fragment_like(tdOgO);
-    #pragma unroll
-    for (int m = 0; m < size<1>(tdOrdO); ++m) {
-        if (get<0>(tdOcdO(0, m, 0)) < binfo.actual_seqlen_q - m_block * kBlockM) {
-            copy(gmem_thr_copy_dO, tdOgdO(_, m, _), tdOrdO(_, m, _));
-            copy(gmem_thr_copy_dO, tdOgO(_, m, _), tdOrO(_, m, _));
-        } else {
-            clear(tdOrdO(_, m, _));
-            clear(tdOrO(_, m, _));
-        }
-    }
+    flash::copy</*Is_even_MN=*/false, /*Is_even_K=*/false, /*Clear_OOB_MN=*/true>(
+        gmem_thr_copy_dO, tdOgdO, tdOrdO, tdOcdO, tdOpdO, binfo.actual_seqlen_q - m_block * kBlockM
+    );
+    flash::copy</*Is_even_MN=*/false, /*Is_even_K=*/false, /*Clear_OOB_MN=*/true>(
+        gmem_thr_copy_dO, tdOgO, tdOrO, tdOcdO, tdOpdO, binfo.actual_seqlen_q - m_block * kBlockM
+    );
     // By right we need to scale dP up by 1/p_dropout, but instead we don't and only scale the final
     // results (dQ and dK) by 1/p_dropout. So we need to keep dP_sum scaled down by p_dropout here,
     // so that (dP - dP_sum) is on the same scale.
     dot_do_o<Kernel_traits::kGmemThreadsPerRow>(tdOrdO, tdOrO, dP_sum, dP_sum,
                                                 Kernel_traits::kNThreads / (Kernel_traits::kGmemThreadsPerRow), params.p_dropout);
     if (Clear_dQaccum) {
         Tensor zero = make_fragment_like(tdQgdQaccum);
@@ -206,15 +208,15 @@
     constexpr int kBlockN = Kernel_traits::kBlockN;
     constexpr int kHeadDim = Kernel_traits::kHeadDim;
 
     // TODO: change this when we need to deal with seqlen not a multiple of 128
     const BlockInfo</*Varlen=*/false> binfo(params, bidb);
     if (n_block * kBlockN >= binfo.actual_seqlen_k) return;
 
-    const index_t row_offset_dkv_accum = ((bidb * params.h + bidh) * params.seqlen_k_rounded + n_block * kBlockN) * params.d;
+    const index_t row_offset_dkv_accum = ((bidb * params.h + bidh) * params.seqlen_k_rounded + n_block * kBlockN) * params.d_rounded;
 
     Tensor gdKaccum = make_tensor(make_gmem_ptr(reinterpret_cast<ElementAccum *>(params.dk_accum_ptr) + row_offset_dkv_accum),
                                   Shape<Int<kBlockN>, Int<kHeadDim>>{}, Stride<Int<kHeadDim>, _1>{});
     Tensor gdVaccum = make_tensor(make_gmem_ptr(reinterpret_cast<ElementAccum *>(params.dv_accum_ptr) + row_offset_dkv_accum),
                                   Shape<Int<kBlockN>, Int<kHeadDim>>{}, Stride<Int<kHeadDim>, _1>{});
 
     auto gmem_thr_copy_dKV_accum = typename Kernel_traits::GmemTiledCopydQaccum{}.get_thread_slice(tidx);
@@ -253,15 +255,15 @@
     // TODO: change this when we need to deal with seqlen not a multiple of 128
     const BlockInfo</*Varlen=*/false> binfo(params, bidb);
     if (m_block * kBlockM >= binfo.actual_seqlen_q) return;
 
     const index_t row_offset_dq = binfo.q_offset(params.dq_batch_stride, params.dq_row_stride, bidb)
         + m_block * kBlockM * params.dq_row_stride + bidh * params.dq_head_stride;
     const index_t row_offset_dq_accum = ((bidb * params.h + bidh) * params.seqlen_q_rounded
-                                         + m_block * kBlockM) * params.d;
+                                         + m_block * kBlockM) * params.d_rounded;
 
     Tensor gdQ = make_tensor(make_gmem_ptr(reinterpret_cast<Element *>(params.dq_ptr) + row_offset_dq),
                              Shape<Int<kBlockM>, Int<kHeadDim>>{},
                              make_stride(params.dq_row_stride, _1{}));
     Tensor gdQaccum = make_tensor(make_gmem_ptr(reinterpret_cast<ElementAccum *>(params.dq_accum_ptr) + row_offset_dq_accum),
                                   Shape<Int<kBlockM>, Int<kHeadDim>>{},
                                   Stride<Int<kHeadDim>, _1>{});
@@ -295,20 +297,21 @@
     copy(smem_thr_copy_dQ, taccdQrdQ, taccdQsdQ);
     __syncthreads();
     Tensor tdQrdQ = make_tensor<Element>(shape(tdQgdQ));
     copy(gmem_thr_copy_dQ, tdQsdQ, tdQrdQ);
 
     Tensor cdQ = make_identity_tensor(Shape<Int<kBlockM>, Int<kHeadDim>>{});    // (BLK_M,BLK_K) -> (blk_m,blk_k)
     Tensor tdQcdQ = gmem_thr_copy_dQ.partition_D(cdQ);
+    Tensor tdQpdQ = make_tensor<bool>(make_shape(size<2>(tdQgdQ)));
     #pragma unroll
-    for (int m = 0; m < size<1>(tdQgdQ); ++m) {
-        if (get<0>(tdQcdQ(0, m, 0)) < binfo.actual_seqlen_q - m_block * kBlockM) {
-            copy(gmem_thr_copy_dQ, tdQrdQ(_, m, _), tdQgdQ(_, m, _));
-        }
-    }
+    for (int k = 0; k < size(tdQpdQ); ++k) { tdQpdQ(k) = get<1>(tdQcdQ(0, 0, k)) < params.d; }
+    // Clear_OOB_K must be false since we don't want to write zeros to gmem
+    flash::copy</*Is_even_MN=*/false, /*Is_even_K=*/false, /*Clear_OOB_MN=*/false, /*Clear_OOB_K=*/false>(
+        gmem_thr_copy_dQ, tdQrdQ, tdQgdQ, tdQcdQ, tdQpdQ, binfo.actual_seqlen_q - m_block * kBlockM
+    );
 }
 
 ////////////////////////////////////////////////////////////////////////////////////////////////////
 
 // Convert dK and dV from dKaccum and dVaccum (in float) to fp16/bf16.
 // This is used in the case where we want to parallelize the backward across seqlen_q.
 template<typename Kernel_traits, typename Params>
@@ -336,15 +339,15 @@
     if (n_block * kBlockN >= binfo.actual_seqlen_k) return;
 
     const index_t row_offset_dk = binfo.k_offset(params.dk_batch_stride, params.dk_row_stride, bidb)
         + n_block * kBlockN * params.dk_row_stride + bidh * params.dk_head_stride;
     const index_t row_offset_dv = binfo.k_offset(params.dv_batch_stride, params.dv_row_stride, bidb)
         + n_block * kBlockN * params.dv_row_stride + bidh * params.dv_head_stride;
     const index_t row_offset_dkv_accum = ((bidb * params.h + bidh) * params.seqlen_k_rounded
-                                          + n_block * kBlockN) * params.d;
+                                          + n_block * kBlockN) * params.d_rounded;
 
     Tensor gdK = make_tensor(make_gmem_ptr(reinterpret_cast<Element *>(params.dk_ptr) + row_offset_dk),
                              Shape<Int<kBlockN>, Int<kHeadDim>>{},
                              make_stride(params.dk_row_stride, _1{}));
     Tensor gdV = make_tensor(make_gmem_ptr(reinterpret_cast<Element *>(params.dv_ptr) + row_offset_dv),
                              Shape<Int<kBlockN>, Int<kHeadDim>>{},
                              make_stride(params.dv_row_stride, _1{}));
@@ -401,27 +404,30 @@
     __syncthreads();
     Tensor tdKrdK = make_tensor<Element>(shape(tdKgdK));
     Tensor tdVrdV = make_tensor<Element>(shape(tdVgdV));
     copy(gmem_thr_copy_dKV, tdKsdK, tdKrdK);
     copy(gmem_thr_copy_dKV, tdVsdV, tdVrdV);
 
     Tensor cdKV = make_identity_tensor(Shape<Int<kBlockN>, Int<kHeadDim>>{});    // (BLK_M,BLK_K) -> (blk_m,blk_k)
-    Tensor tdKcdK = gmem_thr_copy_dKV.partition_D(cdKV);
+    Tensor tdKVcdKV = gmem_thr_copy_dKV.partition_D(cdKV);
+    Tensor tdKVpdKV = make_tensor<bool>(make_shape(size<2>(tdKgdK)));
     #pragma unroll
-    for (int m = 0; m < size<1>(tdKgdK); ++m) {
-        if (get<0>(tdKcdK(0, m, 0)) < binfo.actual_seqlen_k - n_block * kBlockN) {
-            copy(gmem_thr_copy_dKV, tdKrdK(_, m, _), tdKgdK(_, m, _));
-            copy(gmem_thr_copy_dKV, tdVrdV(_, m, _), tdVgdV(_, m, _));
-        }
-    }
+    for (int k = 0; k < size(tdKVpdKV); ++k) { tdKVpdKV(k) = get<1>(tdKVcdKV(0, 0, k)) < params.d; }
+    // Clear_OOB_K must be false since we don't want to write zeros to gmem
+    flash::copy</*Is_even_MN=*/false, /*Is_even_K=*/false, /*Clear_OOB_MN=*/false, /*Clear_OOB_K=*/false>(
+        gmem_thr_copy_dKV, tdKrdK, tdKgdK, tdKVcdKV, tdKVpdKV, binfo.actual_seqlen_k - n_block * kBlockN
+    );
+    flash::copy</*Is_even_MN=*/false, /*Is_even_K=*/false, /*Clear_OOB_MN=*/false, /*Clear_OOB_K=*/false>(
+        gmem_thr_copy_dKV, tdVrdV, tdVgdV, tdKVcdKV, tdKVpdKV, binfo.actual_seqlen_k - n_block * kBlockN
+    );
 }
 
 ////////////////////////////////////////////////////////////////////////////////////////////////////
 
-template<typename Kernel_traits, bool Is_dropout, bool Is_causal, bool Is_even_M, bool Is_first, bool Is_last, bool Seq_parallel=false, typename Params>
+template<typename Kernel_traits, bool Is_dropout, bool Is_causal, bool Is_even_M, bool Is_even_K, bool Is_first, bool Is_last, bool Seq_parallel=false, typename Params>
 inline __device__ void compute_dq_dk_dv_1colblock(const Params &params, const int bidb, const int bidh, const int n_block) {
 
     using Element = typename Kernel_traits::Element;
     using ElementAccum = typename Kernel_traits::ElementAccum;
     using index_t = typename Kernel_traits::index_t;
 
     // Shared memory.
@@ -450,23 +456,21 @@
         + n_block * kBlockN * params.v_row_stride + bidh * params.v_head_stride;
     const index_t row_offset_do = binfo.q_offset(params.do_batch_stride, params.do_row_stride, bidb)
         + (m_block_max - 1) * kBlockM * params.do_row_stride + bidh * params.do_head_stride;
     const index_t row_offset_o = binfo.q_offset(params.o_batch_stride, params.o_row_stride, bidb)
         + (m_block_max - 1) * kBlockM * params.o_row_stride + bidh * params.o_head_stride;
     const index_t row_offset_dq = binfo.q_offset(params.dq_batch_stride, params.dq_row_stride, bidb)
         + (m_block_max - 1) * kBlockM * params.dq_row_stride + bidh * params.dq_head_stride;
-    // TODO: this needs to change when headdim is not a mutliple of 32.
     const index_t row_offset_dq_accum = ((bidb * params.h + bidh) * params.seqlen_q_rounded
-                                         + (m_block_max - 1) * kBlockM) * params.d;
+                                         + (m_block_max - 1) * kBlockM) * params.d_rounded;
     const index_t row_offset_lse = (bidb * params.h + bidh) * params.seqlen_q
         + (m_block_max - 1) * kBlockM;
     const index_t row_offset_dpsum = (bidb * params.h + bidh) * params.seqlen_q_rounded
         + (m_block_max - 1) * kBlockM;
 
-    // We assume that params.d == kHeadDim for now
     Tensor gQ = make_tensor(make_gmem_ptr(reinterpret_cast<Element *>(params.q_ptr) + row_offset_q),
                             Shape<Int<kBlockM>, Int<kHeadDim>>{},
                             make_stride(params.q_row_stride, _1{}));
     Tensor gK = make_tensor(make_gmem_ptr(reinterpret_cast<Element *>(params.k_ptr) + row_offset_k),
                             Shape<Int<kBlockN>, Int<kHeadDim>>{},
                             make_stride(params.k_row_stride, _1{}));
     Tensor gV = make_tensor(make_gmem_ptr(reinterpret_cast<Element *>(params.v_ptr) + row_offset_v),
@@ -510,39 +514,38 @@
     Tensor sPt = make_tensor(sP.data(), typename Kernel_traits::SmemLayoutPdStransposed{});
     Tensor sPtNoSwizzle = make_tensor(sP.data(), typename Kernel_traits::SmemLayoutPdStransposedNoSwizzle{});
     // sP and sdQ share the same memory so be careful
     Tensor sdQ = make_tensor(sP.data(), typename Kernel_traits::SmemLayoutdQ{});
     Tensor sdPsum = make_tensor(make_smem_ptr(reinterpret_cast<float2 *>((sP.data() + cute::max(size(sP), size(sdQ))).get())),
                                 Shape<Int<Kernel_traits::kSmemdPsumCount / 2>>{});
 
-    auto gmem_thr_copy_Q = typename Kernel_traits::GmemTiledCopyQ{}.get_thread_slice(tidx);
-    auto gmem_thr_copy_KV = typename Kernel_traits::GmemTiledCopyKV{}.get_thread_slice(tidx);
-    using GmemTiledCopydO = typename std::conditional<
+    auto gmem_thr_copy_QKV = typename Kernel_traits::GmemTiledCopyQKV{}.get_thread_slice(tidx);
+    using GmemTiledCopydO = std::conditional_t<
         Is_first,
         typename Kernel_traits::GmemTiledCopydO,
-        typename Kernel_traits::GmemTiledCopyQ
-    >::type;
+        typename Kernel_traits::GmemTiledCopyQKV
+    >;
     auto gmem_thr_copy_dO = GmemTiledCopydO{}.get_thread_slice(tidx);
     auto gmem_thr_copy_dQ = typename Kernel_traits::GmemTiledCopydQ{}.get_thread_slice(tidx);
-    using GmemLayoutAtomdQaccum = typename std::conditional<
+    using GmemLayoutAtomdQaccum = std::conditional_t<
         !Seq_parallel,
         typename Kernel_traits::GmemTiledCopydQaccum,
         typename Kernel_traits::GmemTiledCopydQaccumAtomicAdd
-    >::type;
+    >;
     auto gmem_thr_copy_dQ_accum = GmemLayoutAtomdQaccum{}.get_thread_slice(tidx);
 
-    Tensor tQgQ = gmem_thr_copy_Q.partition_S(gQ);
-    Tensor tQsQ = gmem_thr_copy_Q.partition_D(sQ);
+    Tensor tQgQ = gmem_thr_copy_QKV.partition_S(gQ);
+    Tensor tQsQ = gmem_thr_copy_QKV.partition_D(sQ);
     Tensor tdOgdO = gmem_thr_copy_dO.partition_S(gdO);
     Tensor tdOsdO = gmem_thr_copy_dO.partition_D(sdO);
     Tensor tdOgO = gmem_thr_copy_dO.partition_S(gO);
-    Tensor tKgK = gmem_thr_copy_KV.partition_S(gK);  // (KCPY, KCPY_N, KCPY_K)
-    Tensor tKsK = gmem_thr_copy_KV.partition_D(sK);
-    Tensor tVgV = gmem_thr_copy_KV.partition_S(gV);  // (VCPY, VCPY_N, VCPY_K)
-    Tensor tVsV = gmem_thr_copy_KV.partition_D(sV);
+    Tensor tKgK = gmem_thr_copy_QKV.partition_S(gK);  // (KCPY, KCPY_N, KCPY_K)
+    Tensor tKsK = gmem_thr_copy_QKV.partition_D(sK);
+    Tensor tVgV = gmem_thr_copy_QKV.partition_S(gV);  // (VCPY, VCPY_N, VCPY_K)
+    Tensor tVsV = gmem_thr_copy_QKV.partition_D(sV);
     Tensor tdQsdQ = gmem_thr_copy_dQ.partition_S(sdQ);    // ((Atom,AtomNum),ATOM_M,ATOM_N)
     Tensor tdQgdQ = gmem_thr_copy_dQ.partition_D(gdQ);
     Tensor tdQgdQaccum = gmem_thr_copy_dQ_accum.partition_D(gdQaccum);
     // if (cute::thread0()) { print(tdQgdQaccum.layout()); printf("\n"); }
     // __syncthreads();
     // if (blockIdx.x == 0 && blockIdx.y == 0 && blockIdx.z == 0 && tidx < 64) {
     //     printf("tidx = %d, tdQgdQaccum = 0x%p\n", tidx, tdQgdQaccum.data());
@@ -616,78 +619,71 @@
 
     //
     // PREDICATES
     //
 
     Tensor cQ = make_identity_tensor(make_shape(size<0>(sQ), size<1>(sQ)));    // (BLK_M,BLK_K) -> (blk_m,blk_k)
     Tensor cKV = make_identity_tensor(make_shape(size<0>(sK), size<1>(sK)));    // (BLK_N,BLK_K) -> (blk_n,blk_k)
-    Tensor tQcQ = gmem_thr_copy_Q.partition_D(cQ);
-    Tensor tKVcKV = gmem_thr_copy_KV.partition_D(cKV);
+    Tensor tQcQ = gmem_thr_copy_QKV.partition_D(cQ);
+    Tensor tKVcKV = gmem_thr_copy_QKV.partition_D(cKV);
+
+    // Allocate predicate tensors for k
+    Tensor tQpQ = make_tensor<bool>(make_shape(size<2>(tQsQ)));
+    Tensor tKVpKV = make_tensor<bool>(make_shape(size<2>(tKsK)));
+
+    // Set predicates for k bounds
+    if (!Is_even_K) {
+        #pragma unroll
+        for (int k = 0; k < size(tQpQ); ++k) { tQpQ(k) = get<1>(tQcQ(0, 0, k)) < params.d; }
+        #pragma unroll
+        for (int k = 0; k < size(tKVpKV); ++k) { tKVpKV(k) = get<1>(tKVcKV(0, 0, k)) < params.d; }
+    }
 
     // Prologue
 
     // We'll advance gdQ and gdQaccum before the 1st read/write.
     tdQgdQ.data() = tdQgdQ.data() + kBlockM * params.dq_row_stride;
-    tdQgdQaccum.data() = tdQgdQaccum.data() + kBlockM * params.d;
+    tdQgdQaccum.data() = tdQgdQaccum.data() + kBlockM * params.d_rounded;
 
     // TODO: if seqlen is not a multiple of kBlockM/N, we might need to exit early and write 0 to dK and dV
     int m_block = m_block_max - 1;
     if (m_block % 2 == 1) {
         tQsQ.data() = tQsQ.data() + size(sQ);
         tSsQ.data() = tSsQ.data() + size(sQ);
         tdKsQt.data() = tdKsQt.data() + size(sQ);
     }
 
     if (!Is_first && !Seq_parallel) { __syncthreads(); }
 
     if (Kernel_traits::Is_V_in_regs) {
-        // copy(gmem_thr_copy_KV, tVgV, tVsV);
-        #pragma unroll
-        for (int n = 0; n < size<1>(tVgV); ++n) {
-            if (get<0>(tKVcKV(0, n, 0)) < binfo.actual_seqlen_k - n_block * kBlockN) {
-                copy(gmem_thr_copy_KV, tVgV(_, n, _), tVsV(_, n, _));
-            } else {  // Clear the smem tiles to account for predicated off loads
-                clear(tVsV(_, n, _));
-            }
-        }
+        // Clear the smem tiles to account for predicated off loads
+        flash::copy</*Is_even_MN=*/false, Is_even_K, /*Clear_OOB_MN=*/true>(
+            gmem_thr_copy_QKV, tVgV, tVsV, tKVcKV, tKVpKV, binfo.actual_seqlen_k - n_block * kBlockN
+        );
         flash::cp_async_fence();
     }
 
     Tensor tdOrdO = make_fragment_like(tdOgdO);
     Tensor tdOrO = make_fragment_like(tdOgO);
-    #pragma unroll
-    for (int m = 0; m < size<1>(tdOrdO); ++m) {
-        if (Is_even_M || get<0>(tQcQ(0, m, 0)) < binfo.actual_seqlen_q - m_block * kBlockM) {
-            if (Is_first) {
-                copy(gmem_thr_copy_dO, tdOgdO(_, m, _), tdOrdO(_, m, _));
-                copy(gmem_thr_copy_dO, tdOgO(_, m, _), tdOrO(_, m, _));
-            } else {
-                copy(gmem_thr_copy_dO, tdOgdO(_, m, _), tdOsdO(_, m, _));
-            }
-        } else {  // Clear the smem tiles to account for predicated off loads
-            if (Is_first) {
-                clear(tdOrdO(_, m, _));
-                clear(tdOrO(_, m, _));
-            } else {
-                clear(tdOsdO(_, m, _));
-            }
-        }
-    }
-    if (Is_even_M) {
-        copy(gmem_thr_copy_Q, tQgQ, tQsQ);
+    if (!Is_first) {
+        // Clear the smem tiles to account for predicated off loads
+        flash::copy<Is_even_M, Is_even_K, /*Clear_OOB_MN=*/true>(
+            gmem_thr_copy_dO, tdOgdO, tdOsdO, tQcQ, tQpQ, binfo.actual_seqlen_q - m_block * kBlockM
+        );
     } else {
-        #pragma unroll
-        for (int m = 0; m < size<1>(tQsQ); ++m) {
-            if (get<0>(tQcQ(0, m, 0)) < binfo.actual_seqlen_q - m_block * kBlockM) {
-                copy(gmem_thr_copy_Q, tQgQ(_, m, _), tQsQ(_, m, _));
-            } else {  // Clear the smem tiles to account for predicated off loads
-                clear(tQsQ(_, m, _));
-            }
-        }
+        flash::copy<Is_even_M, Is_even_K, /*Clear_OOB_MN=*/true>(
+            gmem_thr_copy_dO, tdOgdO, tdOrdO, tQcQ, tQpQ, binfo.actual_seqlen_q - m_block * kBlockM
+        );
+        flash::copy<Is_even_M, Is_even_K, /*Clear_OOB_MN=*/true>(
+            gmem_thr_copy_dO, tdOgO, tdOrO, tQcQ, tQpQ, binfo.actual_seqlen_q - m_block * kBlockM
+        );
     }
+    flash::copy<Is_even_M, Is_even_K, /*Clear_OOB_MN=*/true>(
+        gmem_thr_copy_QKV, tQgQ, tQsQ, tQcQ, tQpQ, binfo.actual_seqlen_q - m_block * kBlockM
+    );
 
     Tensor caccS = make_identity_tensor(Shape<Int<kBlockM>, Int<kBlockN>>{});    // (BLK_M,BLK_N) -> (blk_m,blk_n)
     Tensor taccScS = thr_mma_sdp.partition_C(caccS);                           // (MMA,MMA_N,MMA_N)
     static_assert(decltype(size<0>(taccScS))::value == 4);
     // Convert to ((2, 2), MMA_N, MMA_N) then take only the row indices.
     Tensor taccScS_row = logical_divide(taccScS, Shape<_2>{})(make_coord(0, _), _, 0);
     Tensor lse = make_tensor<ElementAccum>(Shape<Int<decltype(size(taccScS_row))::value>>{});
@@ -695,35 +691,25 @@
     for (int mi = 0; mi < size(lse); ++mi) {
         // Using uint32_t row makes it 10us slower on d=128, not sure why.
         const int row = get<0>(taccScS_row(mi));
         lse(mi) = Is_even_M || row < binfo.actual_seqlen_q - m_block * kBlockM ? gLSE(row) : 0;
     }
 
     // Tensor tKrK = make_fragment_like(tKsK);
-    // // copy(gmem_thr_copy_KV, tKgK(_, _, _, 0), tKrK);
-    // copy(gmem_thr_copy_KV, tKgK, tKrK);
+    // // copy(gmem_thr_copy_QKV, tKgK(_, _, _, 0), tKrK);
+    // copy(gmem_thr_copy_QKV, tKgK, tKrK);
     // // if (cute::thread(1, 0)) { print(tKrK); }
 
-    // // Copy rmem to smem
-    // // copy(tQrQ, tQsQ);
-    // cute::cp_async_wait<0>();
-
-    // // Copy rmem to smem
-    // copy(tKrK, tKsK);
-    // copy(gmem_thr_copy_KV, tKgK, tKsK);
-    // if (!Kernel_traits::Is_V_in_regs) { copy(gmem_thr_copy_KV, tVgV, tVsV); }
-    #pragma unroll
-    for (int n = 0; n < size<1>(tKgK); ++n) {
-        if (get<0>(tKVcKV(0, n, 0)) < binfo.actual_seqlen_k - n_block * kBlockN) {
-            copy(gmem_thr_copy_KV, tKgK(_, n, _), tKsK(_, n, _));
-            if (!Kernel_traits::Is_V_in_regs) { copy(gmem_thr_copy_KV, tVgV(_, n, _), tVsV(_, n, _)); }
-        } else {  // Clear the smem tiles to account for predicated off loads
-            clear(tKsK(_, n, _));
-            if (!Kernel_traits::Is_V_in_regs) { clear(tVsV(_, n, _)); }
-        }
+    flash::copy</*Is_even_MN=*/false, Is_even_K, /*Clear_OOB_MN=*/true>(
+        gmem_thr_copy_QKV, tKgK, tKsK, tKVcKV, tKVpKV, binfo.actual_seqlen_k - n_block * kBlockN
+    );
+    if (!Kernel_traits::Is_V_in_regs) {
+        flash::copy</*Is_even_MN=*/false, Is_even_K, /*Clear_OOB_MN=*/true>(
+            gmem_thr_copy_QKV, tVgV, tVsV, tKVcKV, tKVpKV, binfo.actual_seqlen_k - n_block * kBlockN
+        );
     }
     flash::cp_async_fence();
 
     // if (cute::thread0()) { print(tdOgdO.layout()); printf("\n"); print(tdOrdO); print(tdOrO); }
     if (Is_first) {
         copy(tdOrdO, tdOsdO);
         dot_do_o<Kernel_traits::kGmemThreadsPerRow>(tdOrdO, tdOrO, gdPsum, sdPsum,
@@ -838,15 +824,15 @@
             for (int ni = 0; ni < size<1>(dS); ++ni) {
                 dS(mi, ni) = pointwise_mult(scores(mi, ni), dS(mi, ni), dP_sum(mi));
             }
         }
         // if (cute::thread0()) { print(dS); }
 
         Tensor acc_dq = partition_fragment_C(tiled_mma_dq, Shape<Int<kBlockM>, Int<kHeadDim>>{});  // MMA, MMA_N, MMA_K
-        tdQgdQaccum.data() = tdQgdQaccum.data() + (-int(kBlockM * params.d));
+        tdQgdQaccum.data() = tdQgdQaccum.data() + (-int(kBlockM * params.d_rounded));
         if (Is_first || Seq_parallel) {
             clear(acc_dq);
         } else {
             // Reshape acc_dq from (4, 1, 2) to (4, 2, 1) to write to gdQaccum
             Tensor acc_dq_reshaped = make_tensor(acc_dq.data(),
                                                  make_layout(get<0>(acc_dq.layout()),
                                                              get<2>(acc_dq.layout()),
@@ -857,15 +843,15 @@
         if (m_block > m_block_min) {
             // Double buffer for sQ
             const int sQ_offset = m_block % 2 == 0 ? size(sQ) : -size(sQ);
             tQsQ.data() = tQsQ.data() + sQ_offset;
             tSsQ.data() = tSsQ.data() + sQ_offset;
             // Advance gQ
             tQgQ.data() = tQgQ.data() + (-int(kBlockM * params.q_row_stride));
-            copy(gmem_thr_copy_Q, tQgQ, tQsQ);
+            flash::copy</*Is_even_MN=*/true, Is_even_K>(gmem_thr_copy_QKV, tQgQ, tQsQ, tQcQ, tQpQ);
             flash::cp_async_fence();
         }
 
         Tensor dS_reshaped = make_tensor(dS.data(), acc_dp.layout());
         // Convert dS from fp32 to fp16
         Tensor tdSrdS = flash::convert_type<Element>(dS_reshaped);
         // if (cute::thread0()) { print(tPrP); }
@@ -884,25 +870,19 @@
 
         __syncthreads(); // Need syncthreads since we're writing to the same sdO location
 
         if (m_block > m_block_min) {
             // Advance gdO
             tdOgdO.data() = tdOgdO.data() + (-int(kBlockM * params.do_row_stride));
             if (Is_first) {
-                // For some reason this calls global load with size=16 instead of 128.
-                // copy(gmem_thr_copy_dO, tdOgdO, tdOrdO);
-                // copy(gmem_thr_copy_dO, tdOgO, tdOrO);
                 tdOgO.data() = tdOgO.data() + (-int(kBlockM * params.o_row_stride));
-                #pragma unroll
-                for (int m = 0; m < size<1>(tdOrdO); ++m) {
-                    copy(gmem_thr_copy_dO, tdOgdO(_, m, _), tdOrdO(_, m, _));
-                    copy(gmem_thr_copy_dO, tdOgO(_, m, _), tdOrO(_, m, _));
-                }
+                flash::copy</*Is_even_MN=*/true, Is_even_K>(gmem_thr_copy_dO, tdOgdO, tdOrdO, tQcQ, tQpQ);
+                flash::copy</*Is_even_MN=*/true, Is_even_K>(gmem_thr_copy_dO, tdOgO, tdOrO, tQcQ, tQpQ);
             } else {
-                copy(gmem_thr_copy_dO, tdOgdO, tdOsdO);
+                flash::copy</*Is_even_MN=*/true, Is_even_K>(gmem_thr_copy_dO, tdOgdO, tdOsdO, tQcQ, tQpQ);
                 flash::cp_async_fence();
             }
         }
 
         flash::gemm(acc_dq, tdQrdS, tdQrKt, tdQsdS, tdQsKt, tiled_mma_dq, smem_thr_copy_dS, smem_thr_copy_Kt);
         // if (cute::thread0()) { print(acc_dq); }
 
@@ -1018,27 +998,30 @@
     __syncthreads();
     Tensor tdKrdK = make_tensor<Element>(shape(tdKgdK));
     copy(gmem_thr_copy_dKV, tdKsdK, tdKrdK);
     Tensor tdVrdV = make_tensor<Element>(shape(tdVgdV));
     copy(gmem_thr_copy_dKV, tdVsdV, tdVrdV);
     Tensor cdKV = make_identity_tensor(make_shape(size<0>(sdK), size<1>(sdK)));    // (BLK_N,BLK_K) -> (blk_n,blk_k)
     Tensor tdKVcdKV = gmem_thr_copy_dKV.partition_D(cdKV);
+    Tensor tdKVpdKV = make_tensor<bool>(make_shape(size<2>(tdKgdK)));
     #pragma unroll
-    for (int n = 0; n < size<1>(tdKgdK); ++n) {
-        if (get<0>(tdKVcdKV(0, n, 0)) < binfo.actual_seqlen_k - n_block * kBlockN) {
-            copy(gmem_thr_copy_dKV, tdKrdK(_, n, _), tdKgdK(_, n, _));
-            copy(gmem_thr_copy_dKV, tdVrdV(_, n, _), tdVgdV(_, n, _));
-        }
-    }
+    for (int k = 0; k < size(tdKVpdKV); ++k) { tdKVpdKV(k) = get<1>(tdKVcdKV(0, 0, k)) < params.d; }
+    // Clear_OOB_K must be false since we don't want to write zeros to gmem
+    flash::copy</*Is_even_MN=*/false, Is_even_K, /*Clear_OOB_MN=*/false, /*Clear_OOB_K=*/false>(
+        gmem_thr_copy_dKV, tdKrdK, tdKgdK, tdKVcdKV, tdKVpdKV, binfo.actual_seqlen_k - n_block * kBlockN
+    );
+    flash::copy</*Is_even_MN=*/false, Is_even_K, /*Clear_OOB_MN=*/false, /*Clear_OOB_K=*/false>(
+        gmem_thr_copy_dKV, tdVrdV, tdVgdV, tdKVcdKV, tdKVpdKV, binfo.actual_seqlen_k - n_block * kBlockN
+    );
 
 }
 
 ////////////////////////////////////////////////////////////////////////////////////////////////////
 
-template<typename Kernel_traits, bool Is_dropout, bool Is_causal, bool Is_even_N, typename Params>
+template<typename Kernel_traits, bool Is_dropout, bool Is_causal, bool Is_even_N, bool Is_even_K, typename Params>
 inline __device__ void compute_dq_dk_dv_1rowblock(const Params &params, const int bidb, const int bidh, const int m_block) {
 
     using Element = typename Kernel_traits::Element;
     using ElementAccum = typename Kernel_traits::ElementAccum;
     using index_t = typename Kernel_traits::index_t;
 
     // Shared memory.
@@ -1075,15 +1058,15 @@
         + (n_block_max - 1) * kBlockN * params.v_row_stride + bidh * params.v_head_stride;
     const index_t row_offset_do = binfo.q_offset(params.do_batch_stride, params.do_row_stride, bidb)
         + m_block * kBlockM * params.do_row_stride + bidh * params.do_head_stride;
     const index_t row_offset_o = binfo.q_offset(params.o_batch_stride, params.o_row_stride, bidb)
         + m_block * kBlockM * params.do_row_stride + bidh * params.o_head_stride;
     // We'll advance gdKaccum and gdVaccum before the first write.
     const index_t row_offset_dkv_accum = ((bidb * params.h + bidh) * params.seqlen_k_rounded
-                                          + n_block_max * kBlockN) * params.d;
+                                          + n_block_max * kBlockN) * params.d_rounded;
     const index_t row_offset_lse = (bidb * params.h + bidh) * params.seqlen_q + m_block * kBlockM;
 
     // We assume that params.d == kHeadDim for now
     Tensor gQ = make_tensor(make_gmem_ptr(reinterpret_cast<Element *>(params.q_ptr) + row_offset_q),
                             Shape<Int<kBlockM>, Int<kHeadDim>>{},
                             make_stride(params.q_row_stride, _1{}));
     Tensor gK = make_tensor(make_gmem_ptr(reinterpret_cast<Element *>(params.k_ptr) + row_offset_k),
@@ -1125,30 +1108,27 @@
     Tensor sdStNoSwizzle = make_tensor(sdS.data(), typename Kernel_traits::SmemLayoutPdStransposedNoSwizzle{});
     Tensor sP = make_tensor(sdS.data() + size(sdS), typename Kernel_traits::SmemLayoutPdS{});
     Tensor sPt = make_tensor(sP.data(), typename Kernel_traits::SmemLayoutPdStransposed{});
     Tensor sPtNoSwizzle = make_tensor(sP.data(), typename Kernel_traits::SmemLayoutPdStransposedNoSwizzle{});
     Tensor sdPsum = make_tensor(make_smem_ptr(reinterpret_cast<ElementAccum *>(sdS.data().get())),
                                 Shape<Int<kBlockM>>{});
 
-    // We want gQ to be CACHEGLOBAL and gK and gV to be CACHEALWAYS instead, so we swap the
-    // role of GmemTiledCopyQ a GmemTiledCopyKV
-    auto gmem_thr_copy_Q = typename Kernel_traits::GmemTiledCopyKV{}.get_thread_slice(tidx);
-    auto gmem_thr_copy_KV = typename Kernel_traits::GmemTiledCopyQ{}.get_thread_slice(tidx);
+    auto gmem_thr_copy_QKV = typename Kernel_traits::GmemTiledCopyQKV{}.get_thread_slice(tidx);
     auto gmem_thr_copy_dO = typename Kernel_traits::GmemTiledCopydO{}.get_thread_slice(tidx);
     auto gmem_thr_copy_dKV_accum = typename Kernel_traits::GmemTiledCopydQaccumAtomicAdd{}.get_thread_slice(tidx);
 
-    Tensor tQgQ = gmem_thr_copy_Q.partition_S(gQ);
-    Tensor tQsQ = gmem_thr_copy_Q.partition_D(sQ);
+    Tensor tQgQ = gmem_thr_copy_QKV.partition_S(gQ);
+    Tensor tQsQ = gmem_thr_copy_QKV.partition_D(sQ);
     Tensor tdOgdO = gmem_thr_copy_dO.partition_S(gdO);
     Tensor tdOsdO = gmem_thr_copy_dO.partition_D(sdO);
     Tensor tdOgO = gmem_thr_copy_dO.partition_S(gO);
-    Tensor tKgK = gmem_thr_copy_KV.partition_S(gK);  // (KCPY, KCPY_N, KCPY_K)
-    Tensor tKsK = gmem_thr_copy_KV.partition_D(sK);
-    Tensor tVgV = gmem_thr_copy_KV.partition_S(gV);  // (VCPY, VCPY_N, VCPY_K)
-    Tensor tVsV = gmem_thr_copy_KV.partition_D(sV);
+    Tensor tKgK = gmem_thr_copy_QKV.partition_S(gK);  // (KCPY, KCPY_N, KCPY_K)
+    Tensor tKsK = gmem_thr_copy_QKV.partition_D(sK);
+    Tensor tVgV = gmem_thr_copy_QKV.partition_S(gV);  // (VCPY, VCPY_N, VCPY_K)
+    Tensor tVsV = gmem_thr_copy_QKV.partition_D(sV);
     Tensor tdKgdKaccum = gmem_thr_copy_dKV_accum.partition_D(gdKaccum);
     Tensor tdVgdVaccum = gmem_thr_copy_dKV_accum.partition_D(gdVaccum);
 
     typename Kernel_traits::TiledMmaSdP tiled_mma_sdp;
     auto thr_mma_sdp = tiled_mma_sdp.get_thread_slice(tidx);
     Tensor tSrQ = thr_mma_sdp.partition_fragment_A(sQ);         // (MMA,MMA_N,MMA_K)
     Tensor tSrK = thr_mma_sdp.partition_fragment_B(sK);         // (MMA,MMA_N,MMA_K)
@@ -1205,65 +1185,59 @@
     // PREDICATES
     //
 
     // Construct identity layout for sQ and sK
     Tensor cQ = make_identity_tensor(make_shape(size<0>(sQ), size<1>(sQ)));    // (BLK_M,BLK_K) -> (blk_m,blk_k)
     Tensor cKV = make_identity_tensor(make_shape(size<0>(sK), size<1>(sK)));    // (BLK_N,BLK_K) -> (blk_n,blk_k)
     // Repeat the partitioning with identity layouts
-    Tensor tQcQ = gmem_thr_copy_Q.partition_S(cQ);       // (ACPY,ACPY_M,ACPY_K) -> (blk_m,blk_k)
-    Tensor tKVcKV = gmem_thr_copy_KV.partition_S(cKV);   // (BCPY,BCPY_N,BCPY_K) -> (blk_n,blk_k)
+    Tensor tQcQ = gmem_thr_copy_QKV.partition_S(cQ);       // (ACPY,ACPY_M,ACPY_K) -> (blk_m,blk_k)
+    Tensor tKVcKV = gmem_thr_copy_QKV.partition_S(cKV);   // (BCPY,BCPY_N,BCPY_K) -> (blk_n,blk_k)
+
+    // Allocate predicate tensors for k
+    Tensor tQpQ = make_tensor<bool>(make_shape(size<2>(tQsQ)));
+    Tensor tKVpKV = make_tensor<bool>(make_shape(size<2>(tKsK)));
+
+    // Set predicates for k bounds
+    if (!Is_even_K) {
+        #pragma unroll
+        for (int k = 0; k < size(tQpQ); ++k) { tQpQ(k) = get<1>(tQcQ(0, 0, k)) < params.d; }
+        #pragma unroll
+        for (int k = 0; k < size(tKVpKV); ++k) { tKVpKV(k) = get<1>(tKVcKV(0, 0, k)) < params.d; }
+    }
 
     // Prologue
 
     Tensor tdOrdO = make_fragment_like(tdOgdO);
     Tensor tdOrO = make_fragment_like(tdOgO);
 
-    #pragma unroll
-    for (int m = 0; m < size<1>(tdOrdO); ++m) {
-        if (get<0>(tQcQ(0, m, 0)) < binfo.actual_seqlen_q - m_block * kBlockM) {
-            copy(gmem_thr_copy_dO, tdOgdO(_, m, _), tdOrdO(_, m, _));
-            copy(gmem_thr_copy_dO, tdOgO(_, m, _), tdOrO(_, m, _));
-        } else {  // Clear the smem tiles to account for predicated off loads
-            clear(tdOrdO(_, m, _));
-            clear(tdOrO(_, m, _));
-        }
-    }
+    flash::copy</*Is_even_MN=*/false, Is_even_K, /*Clear_OOB_MN=*/true>(
+        gmem_thr_copy_dO, tdOgdO, tdOrdO, tQcQ, tQpQ, binfo.actual_seqlen_q - m_block * kBlockM
+    );
+    flash::copy</*Is_even_MN=*/false, Is_even_K, /*Clear_OOB_MN=*/true>(
+        gmem_thr_copy_dO, tdOgO, tdOrO, tQcQ, tQpQ, binfo.actual_seqlen_q - m_block * kBlockM
+    );
 
     Tensor tQrQ = make_fragment_like(tQgQ);
-    #pragma unroll
-    for (int m = 0; m < size<1>(tQgQ); ++m) {
-        if (get<0>(tQcQ(0, m, 0)) < binfo.actual_seqlen_q - m_block * kBlockM) {
-            copy(gmem_thr_copy_Q, tQgQ(_, m, _), tQsQ(_, m, _));
-        } else {
-            clear(tQsQ(_, m, _));
-        }
-    }
+    flash::copy</*Is_even_MN=*/false, Is_even_K, /*Clear_OOB_MN=*/true>(
+        gmem_thr_copy_QKV, tQgQ, tQsQ, tQcQ, tQpQ, binfo.actual_seqlen_q - m_block * kBlockM
+    );
 
     int n_block = n_block_max - 1;
     if (n_block % 2 == 1) {
         tKsK.data() = tKsK.data() + size(sK);
         tSsK.data() = tSsK.data() + size(sK);
         tdQsKt.data() = tdQsKt.data() + size(sK);
     }
 
-    if (Is_even_N) {
-        copy(gmem_thr_copy_KV, tKgK, tKsK);
-        copy(gmem_thr_copy_KV, tVgV, tVsV);
-    } else {
-        #pragma unroll
-        for (int n = 0; n < size<1>(tKgK); ++n) {
-            if (get<0>(tKVcKV(0, n, 0)) < binfo.actual_seqlen_k - n_block * kBlockN) {
-                copy(gmem_thr_copy_KV, tKgK(_, n, _), tKsK(_, n, _));
-                copy(gmem_thr_copy_KV, tVgV(_, n, _), tVsV(_, n, _));
-            } else {
-                clear(tKsK(_, n, _));
-                clear(tVsV(_, n, _));
-            }
-        }
-    }
+    flash::copy<Is_even_N, Is_even_K, /*Clear_OOB_MN=*/true>(
+        gmem_thr_copy_QKV, tKgK, tKsK, tKVcKV, tKVpKV, binfo.actual_seqlen_k - n_block * kBlockN
+    );
+    flash::copy<Is_even_N, Is_even_K, /*Clear_OOB_MN=*/true>(
+        gmem_thr_copy_QKV, tVgV, tVsV, tKVcKV, tKVpKV, binfo.actual_seqlen_k - n_block * kBlockN
+    );
 
     Tensor caccS = make_identity_tensor(Shape<Int<kBlockM>, Int<kBlockN>>{});    // (BLK_M,BLK_N) -> (blk_m,blk_n)
     Tensor taccScS = thr_mma_sdp.partition_C(caccS);                           // (MMA,MMA_N,MMA_N)
     static_assert(decltype(size<0>(taccScS))::value == 4);
     // Convert to ((2, 2), MMA_N, MMA_N) then take only the row indices.
     Tensor taccScS_row = logical_divide(taccScS, Shape<_2>{})(make_coord(0, _), _, 0);
     Tensor lse = make_tensor<ElementAccum>(Shape<Int<decltype(size(taccScS_row))::value>>{});
@@ -1367,34 +1341,34 @@
             // Double buffer for sK
             const int sK_offset = n_block % 2 == 0 ? size(sK) : -size(sK);
             tKsK.data() = tKsK.data() + sK_offset;
             tSsK.data() = tSsK.data() + sK_offset;
             // Advance gK, gV
             tKgK.data() = tKgK.data() + (-int(kBlockN * params.k_row_stride));
             tVgV.data() = tVgV.data() + (-int(kBlockN * params.v_row_stride));
-            copy(gmem_thr_copy_KV, tKgK, tKsK);
-            copy(gmem_thr_copy_KV, tVgV, tVsV);
+            flash::copy</*Is_even_MN=*/true, Is_even_K>(gmem_thr_copy_QKV, tKgK, tKsK, tKVcKV, tKVpKV);
+            flash::copy</*Is_even_MN=*/true, Is_even_K>(gmem_thr_copy_QKV, tVgV, tVsV, tKVcKV, tKVpKV);
             // This cp_async_fence needs to be in the if block, otherwise the synchronization
             // isn't right and we get race conditions.
             cute::cp_async_fence();
         }
 
         Tensor acc_dv = partition_fragment_C(tiled_mma_dkv, Shape<Int<kBlockN>, Int<kHeadDim>>{});  // MMA, MMA_N, MMA_K
         clear(acc_dv);
         flash::gemm(acc_dv, tdVrPt, tdVrdO, tdVsPt, tdVsdOt, tiled_mma_dkv, smem_thr_copy_PdSt, smem_thr_copy_QdOt);
         // if (threadIdx.x == 0 && blockIdx.y == 0 && blockIdx.z == 0) { print(acc_dv); }
-        tdVgdVaccum.data() = tdVgdVaccum.data() + (-int(kBlockN * params.d));
+        tdVgdVaccum.data() = tdVgdVaccum.data() + (-int(kBlockN * params.d_rounded));
         #pragma unroll
         for (int i = 0; i < size(acc_dv); ++i) { atomicAdd(&tdVgdVaccum(i), acc_dv(i)); }
 
         __syncthreads();
         Tensor acc_dk = partition_fragment_C(tiled_mma_dkv, Shape<Int<kBlockN>, Int<kHeadDim>>{});  // MMA, MMA_N, MMA_K
         clear(acc_dk);
         flash::gemm(acc_dk, tdKrdSt, tdKrQt, tdKsdSt, tdKsQt, tiled_mma_dkv, smem_thr_copy_PdSt, smem_thr_copy_QdOt);
-        tdKgdKaccum.data() = tdKgdKaccum.data() + (-int(kBlockN * params.d));
+        tdKgdKaccum.data() = tdKgdKaccum.data() + (-int(kBlockN * params.d_rounded));
         #pragma unroll
         for (int i = 0; i < size(acc_dk); ++i) { atomicAdd(&tdKgdKaccum(i), acc_dk(i)); }
 
         flash::gemm(acc_dq, tdQrdS, tdQrKt, tdQsdS, tdQsKt, tiled_mma_dq, smem_thr_copy_dS, smem_thr_copy_Kt);
         // Double buffer for sK
         tdQsKt.data() = tdQsKt.data() + (n_block % 2 == 0 ? size(sK) : -size(sK));
 
@@ -1430,79 +1404,81 @@
     __syncthreads();
 
     Tensor tdQrdQ = make_tensor<Element>(shape(tdQgdQ));
     copy(gmem_thr_copy_dQ, tdQsdQ, tdQrdQ);
 
     Tensor cdQ = make_identity_tensor(Shape<Int<kBlockM>, Int<kHeadDim>>{});    // (BLK_M,BLK_K) -> (blk_m,blk_k)
     Tensor tdQcdQ = gmem_thr_copy_dQ.partition_D(cdQ);
-
-    #pragma unroll
-    for (int m = 0; m < size<1>(tdQgdQ); ++m) {
-        if (get<0>(tdQcdQ(0, m, 0)) < binfo.actual_seqlen_q - m_block * kBlockM) {
-            copy(gmem_thr_copy_dQ, tdQrdQ(_, m, _), tdQgdQ(_, m, _));
-        }
+    Tensor tdQpdQ = make_tensor<bool>(make_shape(size<2>(tdQgdQ)));
+    if (!Is_even_K) {
+        #pragma unroll
+        for (int k = 0; k < size(tdQpdQ); ++k) { tdQpdQ(k) = get<1>(tdQcdQ(0, 0, k)) < params.d; }
     }
+    // Clear_OOB_K must be false since we don't want to write zeros to gmem
+    flash::copy</*Is_even_MN=*/false, Is_even_K, /*Clear_OOB_MN=*/false, /*Clear_OOB_K=*/false>(
+        gmem_thr_copy_dQ, tdQrdQ, tdQgdQ, tdQcdQ, tdQpdQ, binfo.actual_seqlen_q - m_block * kBlockM
+    );
 }
 
 ////////////////////////////////////////////////////////////////////////////////////////////////////
 
-template<typename Kernel_traits, bool Is_dropout, bool Is_causal, bool Is_even_M, typename Params>
+template<typename Kernel_traits, bool Is_dropout, bool Is_causal, bool Is_even_M, bool Is_even_K, typename Params>
 inline __device__ void compute_dq_dk_dv(const Params &params) {
 
     // The block index for the batch.
     const int bidb = blockIdx.x;
     // const int bidb = blockIdx.y;
     // The block index for the head.
     const int bidh = blockIdx.y;
     // const int bidh = blockIdx.z;
     // The thread index.
     const int tidx = threadIdx.x;
 
     const int n_block_max = (params.seqlen_k + Kernel_traits::kBlockN - 1) / Kernel_traits::kBlockN;
     if (n_block_max == 1) {
-        compute_dq_dk_dv_1colblock<Kernel_traits, Is_dropout, Is_causal, Is_even_M, true, true>(params, bidb, bidh, 0);
+        compute_dq_dk_dv_1colblock<Kernel_traits, Is_dropout, Is_causal, Is_even_M, Is_even_K, true, true>(params, bidb, bidh, 0);
     } else {
         // Iterating backward from n_block_max - 1 to 0 might save 1 register
-        compute_dq_dk_dv_1colblock<Kernel_traits, Is_dropout, Is_causal, Is_even_M, true, false>(params, bidb, bidh, n_block_max - 1);
+        compute_dq_dk_dv_1colblock<Kernel_traits, Is_dropout, Is_causal, Is_even_M, Is_even_K, true, false>(params, bidb, bidh, n_block_max - 1);
         for (int n_block = n_block_max - 2; n_block > 0; n_block--) {
-            compute_dq_dk_dv_1colblock<Kernel_traits, Is_dropout, Is_causal, Is_even_M, false, false>(params, bidb, bidh, n_block);
+            compute_dq_dk_dv_1colblock<Kernel_traits, Is_dropout, Is_causal, Is_even_M, Is_even_K, false, false>(params, bidb, bidh, n_block);
         }
-        compute_dq_dk_dv_1colblock<Kernel_traits, Is_dropout, Is_causal, Is_even_M, false, true>(params, bidb, bidh, 0);
+        compute_dq_dk_dv_1colblock<Kernel_traits, Is_dropout, Is_causal, Is_even_M, Is_even_K, false, true>(params, bidb, bidh, 0);
     }
 }
 
 ////////////////////////////////////////////////////////////////////////////////////////////////////
 
-template<typename Kernel_traits, bool Is_dropout, bool Is_causal, bool Is_even_M, typename Params>
+template<typename Kernel_traits, bool Is_dropout, bool Is_causal, bool Is_even_M, bool Is_even_K, typename Params>
 inline __device__ void compute_dq_dk_dv_seqq_parallel(const Params &params) {
 
     const int n_block = blockIdx.x;
     // The block index for the batch.
     const int bidb = blockIdx.y;
     // The block index for the head.
     const int bidh = blockIdx.z;
     // The thread index.
     const int tidx = threadIdx.x;
 
     const int tidx_global = (bidb * params.h + bidh) * blockDim.x * 2 + tidx;
 
-    compute_dq_dk_dv_1colblock<Kernel_traits, Is_dropout, Is_causal, Is_even_M, false, false, /*Seq_parallel=*/true>(params, bidb, bidh, n_block);
+    compute_dq_dk_dv_1colblock<Kernel_traits, Is_dropout, Is_causal, Is_even_M, Is_even_K, false, false, /*Seq_parallel=*/true>(params, bidb, bidh, n_block);
 }
 
 ////////////////////////////////////////////////////////////////////////////////////////////////////
 
-template<typename Kernel_traits, bool Is_dropout, bool Is_causal, bool Is_even_N, typename Params>
+template<typename Kernel_traits, bool Is_dropout, bool Is_causal, bool Is_even_N, bool Is_even_K, typename Params>
 inline __device__ void compute_dq_dk_dv_seqk_parallel(const Params &params) {
 
     const int m_block = blockIdx.x;
     // The block index for the batch.
     const int bidb = blockIdx.y;
     // The block index for the head.
     const int bidh = blockIdx.z;
     // The thread index.
     const int tidx = threadIdx.x;
 
-    compute_dq_dk_dv_1rowblock<Kernel_traits, Is_dropout, Is_causal, Is_even_N>(params, bidb, bidh, m_block);
+    compute_dq_dk_dv_1rowblock<Kernel_traits, Is_dropout, Is_causal, Is_even_N, Is_even_K>(params, bidb, bidh, m_block);
 }
 
 ////////////////////////////////////////////////////////////////////////////////////////////////////
 } // namespace flash
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/src/flash_bwd_kernel_bak.h` & `flash_attn-1.0.2/csrc/flash_attn/src/flash_bwd_kernel_bak.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.1/csrc/flash_attn/src/flash_bwd_kernel_new.h` & `flash_attn-1.0.2/csrc/flash_attn/src/flash_bwd_kernel_new.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.1/csrc/flash_attn/src/flash_bwd_kernel_reverse.h` & `flash_attn-1.0.2/csrc/flash_attn/src/flash_bwd_kernel_reverse.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.1/csrc/flash_attn/src/flash_bwd_launch_template.h` & `flash_attn-1.0.2/csrc/flash_attn/src/flash_bwd_launch_template.h`

 * *Files 4% similar despite different names*

```diff
@@ -12,43 +12,41 @@
 }
 
 template<typename Kernel_traits>
 __global__ void flash_bwd_clear_dkvaccum_kernel(Flash_bwd_params params) {
     flash::clear_dKVaccum<Kernel_traits>(params);
 }
 
-template<typename Kernel_traits, bool Is_dropout, bool Is_causal, bool Is_even_M>
+template<typename Kernel_traits, bool Is_dropout, bool Is_causal, bool Is_even_M, bool Is_even_K>
 __global__ void flash_bwd_dq_dk_dv_loop_kernel(Flash_bwd_params params) {
-    flash::compute_dq_dk_dv<Kernel_traits, Is_dropout, Is_causal, Is_even_M>(params);
+    flash::compute_dq_dk_dv<Kernel_traits, Is_dropout, Is_causal, Is_even_M, Is_even_K>(params);
 }
 
-template<typename Kernel_traits, bool Is_dropout, bool Is_causal, bool Is_even_M>
+template<typename Kernel_traits, bool Is_dropout, bool Is_causal, bool Is_even_M, bool Is_even_K>
 __global__ void flash_bwd_dq_dk_dv_loop_seqq_parallel_kernel(Flash_bwd_params params) {
-    flash::compute_dq_dk_dv_seqq_parallel<Kernel_traits, Is_dropout, Is_causal, Is_even_M>(params);
+    flash::compute_dq_dk_dv_seqq_parallel<Kernel_traits, Is_dropout, Is_causal, Is_even_M, Is_even_K>(params);
 }
 
-template<typename Kernel_traits, bool Is_dropout, bool Is_causal, bool Is_even_N>
+template<typename Kernel_traits, bool Is_dropout, bool Is_causal, bool Is_even_N, bool Is_even_K>
 __global__ void flash_bwd_dq_dk_dv_loop_seqk_parallel_kernel(Flash_bwd_params params) {
-    flash::compute_dq_dk_dv_seqk_parallel<Kernel_traits, Is_dropout, Is_causal, Is_even_N>(params);
+    flash::compute_dq_dk_dv_seqk_parallel<Kernel_traits, Is_dropout, Is_causal, Is_even_N, Is_even_K>(params);
 }
 
 template<typename Kernel_traits>
 __global__ void flash_bwd_convert_dq_kernel(Flash_bwd_params params) {
     flash::convert_dQ<Kernel_traits>(params);
 }
 
 template<typename Kernel_traits>
 __global__ void flash_bwd_convert_dkv_kernel(Flash_bwd_params params) {
     flash::convert_dKV<Kernel_traits>(params);
 }
 
 template<typename Kernel_traits>
 void run_flash_bwd_loop(Flash_bwd_params &params, cudaStream_t stream, const bool configure) {
-    // constexpr int smem_size_dq_dk_dv = Kernel_traits::kSmemSize;
-    constexpr int smem_size_dq_dk_dv = Kernel_traits::kSmemSize1rowblock;
     /* bool is_dropout = params.p_dropout < 1.f;  // params.p_dropout is the probability of "keeping" */
     /* // Work-around for gcc 7. It doesn't like nested BOOL_SWITCH. */
     /* BOOL_SWITCH(is_dropout, IsDropoutConst, [&] { */
     /*     auto kernel = params.is_causal */
     /*         ? &flash_bwd_dq_dk_dv_loop_kernel<Kernel_traits, IsDropoutConst, true> */
     /*         : &flash_bwd_dq_dk_dv_loop_kernel<Kernel_traits, IsDropoutConst, false>; */
     /*     if (params.seqlen_k == blocksize_c) { */
@@ -82,68 +80,71 @@
     /*     /\*     params.num_splits = num_splits_heuristic_bwd( *\/ */
     /*     /\*         params.b * params.h, dprops->multiProcessorCount, *\/ */
     /*     /\*         ctas_per_sm, params.seqlen_k, blocksize_c, params.is_causal *\/ */
     /*     /\*     ); *\/ */
     /*     /\* } *\/ */
     /*     if (configure) return; */
     /*     /\* if (params.num_splits == 1) { *\/ */
-    /*         dim3 grid(params.b, params.h, params.num_splits); */
-    /*         kernel<<<grid, Kernel_traits::THREADS, smem_size_dq_dk_dv, stream>>>(params); */
+    /*         dim3 grid_n(params.b, params.h, params.num_splits); */
+    /*         kernel<<<grid_n, Kernel_traits::THREADS, smem_size_dq_dk_dv, stream>>>(params); */
     /*     /\* } else { *\/ */
-    /*     /\*     dim3 grid_dot(params.b, params.h, (params.seqlen_q + 128 - 1) / 128); *\/ */
-    /*     /\*     flash_bwd_dot_do_o_kernel<Kernel_traits><<<grid_dot, Kernel_traits::THREADS, 0, stream>>>(params); *\/ */
+    /*     /\*     dim3 grid_m(params.b, params.h, (params.seqlen_q + 128 - 1) / 128); *\/ */
+    /*     /\*     flash_bwd_dot_do_o_kernel<Kernel_traits><<<grid_m, Kernel_traits::THREADS, 0, stream>>>(params); *\/ */
     /*     /\*     int num_splits = params.seqlen_k / blocksize_c;  // seqlen_k is divisible by blocksize_c *\/ */
-    /*     /\*     dim3 grid(params.b, params.h, num_splits); *\/ */
-    /*     /\*     kernel_seqparallel<<<grid, Kernel_traits::THREADS, smem_size_dq_dk_dv, stream>>>(params); *\/ */
+    /*     /\*     dim3 grid_n(params.b, params.h, num_splits); *\/ */
+    /*     /\*     kernel_seqparallel<<<grid_n, Kernel_traits::THREADS, smem_size_dq_dk_dv, stream>>>(params); *\/ */
     /*     /\* } *\/ */
     /*     C10_CUDA_KERNEL_LAUNCH_CHECK(); */
     /* }); */
     if (configure) return;
-    const int num_m_block = (params.seqlen_q + Kernel_traits::kBlockM - 1) / Kernel_traits::kBlockM;
-    dim3 grid_dot(num_m_block, params.b, params.h);
-    // flash_bwd_dot_do_o_kernel<true, Kernel_traits><<<grid_dot, Kernel_traits::kNThreads, 0, stream>>>(params);
-    // C10_CUDA_KERNEL_LAUNCH_CHECK();
-
     // dim3 grid(params.b, params.h);
+    const int num_m_block = (params.seqlen_q + Kernel_traits::kBlockM - 1) / Kernel_traits::kBlockM;
+    dim3 grid_m(num_m_block, params.b, params.h);
     const int num_n_block = (params.seqlen_k + Kernel_traits::kBlockN - 1) / Kernel_traits::kBlockN;
-    dim3 grid(num_n_block, params.b, params.h);
-    flash_bwd_clear_dkvaccum_kernel<Kernel_traits><<<grid, Kernel_traits::kNThreads, 0, stream>>>(params);
+    dim3 grid_n(num_n_block, params.b, params.h);
+
+    flash_bwd_dot_do_o_kernel<true, Kernel_traits><<<grid_m, Kernel_traits::kNThreads, 0, stream>>>(params);
+    // flash_bwd_clear_dkvaccum_kernel<Kernel_traits><<<grid_n, Kernel_traits::kNThreads, 0, stream>>>(params);
     C10_CUDA_KERNEL_LAUNCH_CHECK();
+
     // We also use is_even_M to set Unpadded in the BlockInfo constructor, so we need to check
     // for cu_seqlens_q as well.
     const bool is_even_M = params.cu_seqlens_q == nullptr && params.cu_seqlens_k == nullptr && params.seqlen_q % Kernel_traits::kBlockM == 0;
     const bool is_even_N = params.cu_seqlens_q == nullptr && params.cu_seqlens_k == nullptr && params.seqlen_k % Kernel_traits::kBlockN == 0;
+    const bool is_even_K = params.d == Kernel_traits::kHeadDim;
+    constexpr int smem_size_dq_dk_dv = Kernel_traits::kSmemSize;
+    // constexpr int smem_size_dq_dk_dv = Kernel_traits::kSmemSize1rowblock;
     BOOL_SWITCH(params.p_dropout < 1.f, IsDropoutConst, [&] {
         BOOL_SWITCH(params.is_causal, IsCausalConst, [&] {
-            // BOOL_SWITCH(is_even_M, IsEvenMConst, [&] {
-            BOOL_SWITCH(is_even_N, IsEvenNConst, [&] {
-                // auto kernel = &flash_bwd_dq_dk_dv_loop_seqq_parallel_kernel<Kernel_traits, IsDropoutConst, IsCausalConst, IsEvenMConst>;
-                // auto kernel = &flash_bwd_dq_dk_dv_loop_kernel<Kernel_traits, IsDropoutConst, IsCausalConst>;
-                auto kernel = &flash_bwd_dq_dk_dv_loop_seqk_parallel_kernel<Kernel_traits, IsDropoutConst, IsCausalConst, IsEvenNConst>;
-                // if (smem_size_dq_dk_dv >= 48 * 1024)  {
-                if (smem_size_dq_dk_dv >= 48 * 1024)  {
-                    C10_CUDA_CHECK(cudaFuncSetAttribute(
-                        kernel, cudaFuncAttributeMaxDynamicSharedMemorySize, smem_size_dq_dk_dv));
-                }
-                // kernel<<<grid, Kernel_traits::kNThreads, smem_size_dq_dk_dv, stream>>>(params);
-                kernel<<<grid_dot, Kernel_traits::kNThreads, smem_size_dq_dk_dv, stream>>>(params);
-                C10_CUDA_KERNEL_LAUNCH_CHECK();
+            BOOL_SWITCH(is_even_M, IsEvenMConst, [&] {
+            // BOOL_SWITCH(is_even_N, IsEvenNConst, [&] {
+                BOOL_SWITCH(is_even_K, IsEvenKConst, [&] {
+                    // auto kernel = &flash_bwd_dq_dk_dv_loop_kernel<Kernel_traits, IsDropoutConst, IsCausalConst>;
+                    auto kernel = &flash_bwd_dq_dk_dv_loop_seqq_parallel_kernel<Kernel_traits, IsDropoutConst, IsCausalConst, IsEvenMConst, IsEvenKConst>;
+                    // auto kernel = &flash_bwd_dq_dk_dv_loop_seqk_parallel_kernel<Kernel_traits, IsDropoutConst, IsCausalConst, IsEvenNConst, IsEvenKConst>;
+                    if (smem_size_dq_dk_dv >= 48 * 1024)  {
+                        C10_CUDA_CHECK(cudaFuncSetAttribute(
+                            kernel, cudaFuncAttributeMaxDynamicSharedMemorySize, smem_size_dq_dk_dv));
+                    }
+                    // kernel<<<grid_n, Kernel_traits::kNThreads, smem_size_dq_dk_dv, stream>>>(params);
+                    kernel<<<grid_m, Kernel_traits::kNThreads, smem_size_dq_dk_dv, stream>>>(params);
+                    C10_CUDA_KERNEL_LAUNCH_CHECK();
+                });
             });
         });
     });
 
-    // auto kernel_dq = &flash_bwd_convert_dq_kernel<Kernel_traits>;
-    // if (Kernel_traits::kSmemdQSize >= 48 * 1024)  {
-    //     C10_CUDA_CHECK(cudaFuncSetAttribute(
-    //         kernel_dq, cudaFuncAttributeMaxDynamicSharedMemorySize, Kernel_traits::kSmemdQSize));
-    // }
-    // kernel_dq<<<grid_dot, Kernel_traits::kNThreads, Kernel_traits::kSmemdQSize, stream>>>(params);
-    // C10_CUDA_KERNEL_LAUNCH_CHECK();
-    auto kernel_dkv = &flash_bwd_convert_dkv_kernel<Kernel_traits>;
-    if (Kernel_traits::kSmemKVSize >= 48 * 1024)  {
+    auto kernel_dq = &flash_bwd_convert_dq_kernel<Kernel_traits>;
+    if (Kernel_traits::kSmemdQSize >= 48 * 1024)  {
         C10_CUDA_CHECK(cudaFuncSetAttribute(
-            kernel_dkv, cudaFuncAttributeMaxDynamicSharedMemorySize, Kernel_traits::kSmemKVSize));
+            kernel_dq, cudaFuncAttributeMaxDynamicSharedMemorySize, Kernel_traits::kSmemdQSize));
     }
-    kernel_dkv<<<grid, Kernel_traits::kNThreads, Kernel_traits::kSmemKVSize, stream>>>(params);
+    kernel_dq<<<grid_m, Kernel_traits::kNThreads, Kernel_traits::kSmemdQSize, stream>>>(params);
+    // auto kernel_dkv = &flash_bwd_convert_dkv_kernel<Kernel_traits>;
+    // if (Kernel_traits::kSmemKVSize >= 48 * 1024)  {
+    //     C10_CUDA_CHECK(cudaFuncSetAttribute(
+    //         kernel_dkv, cudaFuncAttributeMaxDynamicSharedMemorySize, Kernel_traits::kSmemKVSize));
+    // }
+    // kernel_dkv<<<grid_n, Kernel_traits::kNThreads, Kernel_traits::kSmemKVSize, stream>>>(params);
     C10_CUDA_KERNEL_LAUNCH_CHECK();
 }
 //
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/src/flash_fwd_hdim128.cu` & `flash_attn-1.0.2/csrc/flash_attn/src/flash_fwd_hdim128.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.1/csrc/flash_attn/src/flash_fwd_hdim128_bf16_sm80.cu` & `flash_attn-1.0.2/csrc/flash_attn/src/flash_fwd_hdim128_bf16_sm80.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.1/csrc/flash_attn/src/flash_fwd_hdim128_fp16_sm80.cu` & `flash_attn-1.0.2/csrc/flash_attn/src/flash_fwd_hdim128_fp16_sm80.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.1/csrc/flash_attn/src/flash_fwd_hdim128_sm80_fp16.cu` & `flash_attn-1.0.2/csrc/flash_attn/src/flash_fwd_hdim128_sm80_fp16.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.1/csrc/flash_attn/src/flash_fwd_hdim160.cu` & `flash_attn-1.0.2/csrc/flash_attn/src/flash_fwd_hdim160.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.1/csrc/flash_attn/src/flash_fwd_hdim160_fp16_sm80.cu` & `flash_attn-1.0.2/csrc/flash_attn/src/flash_fwd_hdim160_fp16_sm80.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.1/csrc/flash_attn/src/flash_fwd_hdim160_sm80_fp16.cu` & `flash_attn-1.0.2/csrc/flash_attn/src/flash_fwd_hdim160_sm80_fp16.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.1/csrc/flash_attn/src/flash_fwd_hdim192.cu` & `flash_attn-1.0.2/csrc/flash_attn/src/flash_fwd_hdim192.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.1/csrc/flash_attn/src/flash_fwd_hdim192_fp16_sm80.cu` & `flash_attn-1.0.2/csrc/flash_attn/src/flash_fwd_hdim192_fp16_sm80.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.1/csrc/flash_attn/src/flash_fwd_hdim192_sm80_fp16.cu` & `flash_attn-1.0.2/csrc/flash_attn/src/flash_fwd_hdim192_sm80_fp16.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.1/csrc/flash_attn/src/flash_fwd_hdim32.cu` & `flash_attn-1.0.2/csrc/flash_attn/src/flash_fwd_hdim32.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.1/csrc/flash_attn/src/flash_fwd_hdim32_fp16_sm80.cu` & `flash_attn-1.0.2/csrc/flash_attn/src/flash_fwd_hdim32_fp16_sm80.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.1/csrc/flash_attn/src/flash_fwd_hdim32_sm80_fp16.cu` & `flash_attn-1.0.2/csrc/flash_attn/src/flash_fwd_hdim32_sm80_fp16.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.1/csrc/flash_attn/src/flash_fwd_hdim64.cu` & `flash_attn-1.0.2/csrc/flash_attn/src/flash_fwd_hdim64.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.1/csrc/flash_attn/src/flash_fwd_hdim64_bf16_sm80.cu` & `flash_attn-1.0.2/csrc/flash_attn/src/flash_fwd_hdim64_bf16_sm80.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.1/csrc/flash_attn/src/flash_fwd_hdim64_fp16_sm80.cu` & `flash_attn-1.0.2/csrc/flash_attn/src/flash_fwd_hdim64_fp16_sm80.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.1/csrc/flash_attn/src/flash_fwd_hdim64_sm80_fp16.cu` & `flash_attn-1.0.2/csrc/flash_attn/src/flash_fwd_hdim64_sm80_fp16.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.1/csrc/flash_attn/src/flash_fwd_hdim96.cu` & `flash_attn-1.0.2/csrc/flash_attn/src/flash_fwd_hdim96.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.1/csrc/flash_attn/src/flash_fwd_hdim96_fp16_sm80.cu` & `flash_attn-1.0.2/csrc/flash_attn/src/flash_fwd_hdim96_fp16_sm80.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.1/csrc/flash_attn/src/flash_fwd_hdim96_sm80_fp16.cu` & `flash_attn-1.0.2/csrc/flash_attn/src/flash_fwd_hdim96_sm80_fp16.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.1/csrc/flash_attn/src/flash_fwd_kernel.h` & `flash_attn-1.0.2/csrc/flash_attn/src/flash_fwd_kernel.h`

 * *Files 4% similar despite different names*

```diff
@@ -114,15 +114,15 @@
     for (int mi = 0; mi < size<1>(tPrP); ++mi) {
         copy(gmem_thr_copy_P, tPrP(_, mi), tPgP(_, mi, 0));
     }
 };
 
 ////////////////////////////////////////////////////////////////////////////////////////////////////
 
-template<typename Kernel_traits, bool Is_dropout, bool Is_causal, bool Is_even_N, bool Return_softmax, typename Params>
+template<typename Kernel_traits, bool Is_dropout, bool Is_causal, bool Is_even_N, bool Is_even_K, bool Return_softmax, typename Params>
 inline __device__ void compute_attn_1rowblock(const Params &params, const int bidb, const int bidh, const int m_block) {
 
     using Element = typename Kernel_traits::Element;
     using ElementAccum = typename Kernel_traits::ElementAccum;
     using index_t = typename Kernel_traits::index_t;
 
     // Shared memory.
@@ -180,24 +180,23 @@
     // Careful we're using the same smem for sQ and sK | sV if Share_Q_K_smem;
     Tensor sK = make_tensor(sQ.data() + (Kernel_traits::Share_Q_K_smem ? 0 : size(sQ)),
                             typename Kernel_traits::SmemLayoutKV{});
     Tensor sV = make_tensor(sK.data() + size(sK), typename Kernel_traits::SmemLayoutKV{});
     Tensor sVt = make_tensor(sV.data(), typename Kernel_traits::SmemLayoutVtransposed{});
     Tensor sVtNoSwizzle = make_tensor(sV.data(), typename Kernel_traits::SmemLayoutVtransposedNoSwizzle{});
 
-    auto gmem_thr_copy_Q = typename Kernel_traits::GmemTiledCopyQ{}.get_thread_slice(tidx);
-    auto gmem_thr_copy_KV = typename Kernel_traits::GmemTiledCopyKV{}.get_thread_slice(tidx);
+    auto gmem_thr_copy_QKV = typename Kernel_traits::GmemTiledCopyQKV{}.get_thread_slice(tidx);
     auto gmem_thr_copy_P = typename Kernel_traits::GmemTiledCopyP{}.get_thread_slice(tidx);
 
-    Tensor tQgQ = gmem_thr_copy_Q.partition_S(gQ);
-    Tensor tQsQ = gmem_thr_copy_Q.partition_D(sQ);
-    Tensor tKgK = gmem_thr_copy_KV.partition_S(gK);  // (KCPY, KCPY_N, KCPY_K)
-    Tensor tKsK = gmem_thr_copy_KV.partition_D(sK);
-    Tensor tVgV = gmem_thr_copy_KV.partition_S(gV);  // (VCPY, VCPY_N, VCPY_K)
-    Tensor tVsV = gmem_thr_copy_KV.partition_D(sV);
+    Tensor tQgQ = gmem_thr_copy_QKV.partition_S(gQ);
+    Tensor tQsQ = gmem_thr_copy_QKV.partition_D(sQ);
+    Tensor tKgK = gmem_thr_copy_QKV.partition_S(gK);  // (KCPY, KCPY_N, KCPY_K)
+    Tensor tKsK = gmem_thr_copy_QKV.partition_D(sK);
+    Tensor tVgV = gmem_thr_copy_QKV.partition_S(gV);  // (VCPY, VCPY_N, VCPY_K)
+    Tensor tVsV = gmem_thr_copy_QKV.partition_D(sV);
     Tensor tPgP = gmem_thr_copy_P.partition_D(gP);
 
     typename Kernel_traits::TiledMma tiled_mma;
     auto thr_mma = tiled_mma.get_thread_slice(tidx);
     Tensor tSrQ  = thr_mma.partition_fragment_A(sQ);                           // (MMA,MMA_M,MMA_K)
     Tensor tSrK  = thr_mma.partition_fragment_B(sK);                           // (MMA,MMA_N,MMA_K)
     Tensor tOrVt  = thr_mma.partition_fragment_B(sVtNoSwizzle);                // (MMA, MMA_K,MMA_N)
@@ -245,29 +244,35 @@
     //     for (int i = 0; i < size(tScQ); ++i) {
     //         printf("%d ", get<1>(tScQ(i)));
     //     }
     //     printf("\n");
     // }
 
     // Repeat the partitioning with identity layouts
-    Tensor tQcQ = gmem_thr_copy_Q.partition_S(cQ);       // (ACPY,ACPY_M,ACPY_K) -> (blk_m,blk_k)
-    Tensor tKVcKV = gmem_thr_copy_KV.partition_S(cKV);   // (BCPY,BCPY_N,BCPY_K) -> (blk_n,blk_k)
+    Tensor tQcQ = gmem_thr_copy_QKV.partition_S(cQ);       // (ACPY,ACPY_M,ACPY_K) -> (blk_m,blk_k)
+    Tensor tKVcKV = gmem_thr_copy_QKV.partition_S(cKV);   // (BCPY,BCPY_N,BCPY_K) -> (blk_n,blk_k)
+
+    // Allocate predicate tensors for k
+    Tensor tQpQ = make_tensor<bool>(make_shape(size<2>(tQsQ)));
+    Tensor tKVpKV = make_tensor<bool>(make_shape(size<2>(tKsK)));
+
+    // Set predicates for k bounds
+    if (!Is_even_K) {
+        #pragma unroll
+        for (int k = 0; k < size(tQpQ); ++k) { tQpQ(k) = get<1>(tQcQ(0, 0, k)) < params.d; }
+        #pragma unroll
+        for (int k = 0; k < size(tKVpKV); ++k) { tKVpKV(k) = get<1>(tKVcKV(0, 0, k)) < params.d; }
+    }
 
     // Prologue
 
     Tensor tQrQ = make_fragment_like(tQgQ);
-    // copy(gmem_thr_copy_Q, tQgQ, tQsQ);
     // We don't need to clear the sQ smem tiles since we'll only write out the valid outputs
-    // copy_if(gmem_thr_copy_Q, tQpQ, tQgQ, tQsQ);
-    #pragma unroll
-    for (int m = 0; m < size<1>(tQgQ); ++m) {
-        if (get<0>(tQcQ(0, m, 0)) < binfo.actual_seqlen_q - m_block * kBlockM) {
-            copy(gmem_thr_copy_Q, tQgQ(_, m, _), tQsQ(_, m, _));
-        }
-    }
+    flash::copy</*Is_even_MN=*/false, Is_even_K>(gmem_thr_copy_QKV, tQgQ, tQsQ, tQcQ, tQpQ,
+                                                 binfo.actual_seqlen_q - m_block * kBlockM);
     if (Kernel_traits::Is_Q_in_regs) { cute::cp_async_fence(); }
 
     // // Copy rmem to smem
     // // copy(tQrQ, tQsQ);
     // flash::cp_async_wait<0>();
     // __syncthreads();
     // // if (cute::thread(1, 0)) { print(tQsQ); }
@@ -280,27 +285,17 @@
         Tensor tSrQ_copy_view = smem_thr_copy_Q.retile_D(tSrQ);
         CUTE_STATIC_ASSERT_V(size<1>(tSsQ) == size<1>(tSrQ_copy_view));            // M
         copy(smem_thr_copy_Q, tSsQ, tSrQ_copy_view);
         __syncthreads();
     }
 
     int n_block = n_block_max - 1;
-    // if (blockIdx.y == 0 && blockIdx.z == 0 && threadIdx.x == 0) { printf("m_block = %d, tKgK.data = 0x%p, offset = %d\n", m_block, tKgK.data(),  * params.k_row_stride)); }
-    if (Is_even_N) {
-        copy(gmem_thr_copy_KV, tKgK, tKsK);
-    } else {
-        // We don't need to clear the sK smem tiles since we'll mask out the scores anyway.
-        // copy_if(gmem_thr_copy_KV, tKVpKV, tKgK, tKsK);
-        #pragma unroll
-        for (int n = 0; n < size<1>(tKgK); ++n) {
-            if (get<0>(tKVcKV(0, n, 0)) < binfo.actual_seqlen_k - n_block * kBlockN) {
-                copy(gmem_thr_copy_KV, tKgK(_, n, _), tKsK(_, n, _));
-            }
-        }
-    }
+    // We don't need to clear the sK smem tiles since we'll mask out the scores anyway.
+    flash::copy<Is_even_N, Is_even_K>(gmem_thr_copy_QKV, tKgK, tKsK, tKVcKV, tKVpKV,
+                                      binfo.actual_seqlen_k - n_block * kBlockN);
     cute::cp_async_fence();
 
     if (Kernel_traits::Is_Q_in_regs && !Kernel_traits::Share_Q_K_smem) {
         flash::cp_async_wait<1>();
         __syncthreads();
         Tensor tSrQ_copy_view = smem_thr_copy_Q.retile_D(tSrQ);
         CUTE_STATIC_ASSERT_V(size<1>(tSsQ) == size<1>(tSrQ_copy_view));            // M
@@ -326,27 +321,20 @@
         clear(acc_s);
         flash::cp_async_wait<0>();
         __syncthreads();
 
         // Advance gV
         if (masking_step > 0) {
             tVgV.data() = tVgV.data() + (-int(kBlockN * params.v_row_stride));
-        }
-        if (Is_even_N || masking_step > 0) {
-            copy(gmem_thr_copy_KV, tVgV, tVsV);
+            flash::copy</*Is_even_MN=*/true, Is_even_K>(gmem_thr_copy_QKV, tVgV, tVsV, tKVcKV, tKVpKV);
         } else {
             // Clear the smem tiles to account for predicated off loads
-            #pragma unroll
-            for (int n = 0; n < size<1>(tVgV); ++n) {
-                if (get<0>(tKVcKV(0, n, 0)) < binfo.actual_seqlen_k - n_block * kBlockN) {
-                    copy(gmem_thr_copy_KV, tVgV(_, n, _), tVsV(_, n, _));
-                } else {
-                    clear(tVsV(_, n, _));
-                }
-            }
+            flash::copy<Is_even_N, Is_even_K, /*Clear_OOB_MN=*/true>(
+                gmem_thr_copy_QKV, tVgV, tVsV, tKVcKV, tKVpKV, binfo.actual_seqlen_k - n_block * kBlockN
+            );
         }
         cute::cp_async_fence();
 
         flash::gemm</*A_in_regs=*/Kernel_traits::Is_Q_in_regs>(
             acc_s, tSrQ, tSrK, tSsQ, tSsK, tiled_mma, smem_thr_copy_Q, smem_thr_copy_K
         );
         // if (cute::thread0()) { print(acc_s); }
@@ -380,15 +368,15 @@
         }
 
         flash::cp_async_wait<0>();
         __syncthreads();
         if (n_block > 0) {
             // Advance gK
             tKgK.data() = tKgK.data() + (-int(kBlockN * params.k_row_stride));
-            copy(gmem_thr_copy_KV, tKgK, tKsK);
+            flash::copy</*Is_even_MN=*/true, Is_even_K>(gmem_thr_copy_QKV, tKgK, tKsK, tKVcKV, tKVpKV);
             // This cp_async_fence needs to be in the if block, otherwise the synchronization
             // isn't right and we get race conditions.
             cute::cp_async_fence();
         }
 
         // TODO: when we have key_padding_mask we'll need to Check_inf
         masking_step == 0
@@ -432,27 +420,27 @@
     for (; n_block >= 0; --n_block) {
         Tensor acc_s = partition_fragment_C(tiled_mma, Shape<Int<kBlockM>, Int<kBlockN>>{});  // (MMA=4, MMA_M, MMA_N)
         clear(acc_s);
         flash::cp_async_wait<0>();
         __syncthreads();
         // Advance gV
         tVgV.data() = tVgV.data() + (-int(kBlockN * params.v_row_stride));
-        copy(gmem_thr_copy_KV, tVgV, tVsV);
+        flash::copy</*Is_even_MN=*/true, Is_even_K>(gmem_thr_copy_QKV, tVgV, tVsV, tKVcKV, tKVpKV);
         cute::cp_async_fence();
 
         flash::gemm</*A_in_regs=*/Kernel_traits::Is_Q_in_regs>(
             acc_s, tSrQ, tSrK, tSsQ, tSsK, tiled_mma, smem_thr_copy_Q, smem_thr_copy_K
         );
 
         flash::cp_async_wait<0>();
         __syncthreads();
         if (n_block > 0) {
             // Advance gK
             tKgK.data() = tKgK.data() + (-int(kBlockN * params.k_row_stride));
-            copy(gmem_thr_copy_KV, tKgK, tKsK);
+            flash::copy</*Is_even_MN=*/true, Is_even_K>(gmem_thr_copy_QKV, tKgK, tKsK, tKVcKV, tKVpKV);
             // This cp_async_fence needs to be in the if block, otherwise the synchronization
             // isn't right and we get race conditions.
             cute::cp_async_fence();
         }
 
         // Reshape acc_s from (MMA=4, MMA_M, MMA_N) to (nrow=(2, MMA_M), ncol=(2, MMA_N))
         Tensor scores = make_tensor(acc_s.data(), flash::convert_layout_acc_rowcol(acc_s.layout()));
@@ -544,30 +532,29 @@
             if (row < binfo.actual_seqlen_q - m_block * kBlockM) { gLSE(row) = lse(mi); }
         }
     }
 
     // Construct identity layout for sO
     Tensor cO = make_identity_tensor(make_shape(size<0>(sO), size<1>(sO)));    // (BLK_M,BLK_K) -> (blk_m,blk_k)
     // Repeat the partitioning with identity layouts
-    Tensor tOcO = gmem_thr_copy_O.partition_D(cO);                             // (ACPY,ACPY_M,ACPY_K) -> (blk_m,blk_k)
-    // For some reason this calls global store with size=16 (instead of 128) so it's much slower.
-    // By calling copy on each slice indexed by m, it calls global store with size=128.
-    // copy(gmem_thr_copy_O, tOrO, tOgO);
-    #pragma unroll
-    for (int m = 0; m < size<1>(tOgO); ++m) {
-        // if (cute::thread0()) {printf("m = %d\n", get<0>(tQcQ(0, m, 0))); }
-        if (get<0>(tOcO(0, m, 0)) < binfo.actual_seqlen_q - m_block * kBlockM) {
-            copy(gmem_thr_copy_O, tOrO(_, m, _), tOgO(_, m, _));
-        }
+    Tensor tOcO = gmem_thr_copy_O.partition_D(cO);                           // (ACPY,ACPY_M,ACPY_K) -> (blk_m,blk_k)
+    Tensor tOpO = make_tensor<bool>(make_shape(size<2>(tOgO)));
+    if (!Is_even_K) {
+        #pragma unroll
+        for (int k = 0; k < size(tOpO); ++k) { tOpO(k) = get<1>(tOcO(0, 0, k)) < params.d; }
     }
+    // Clear_OOB_K must be false since we don't want to write zeros to gmem
+    flash::copy</*Is_even_MN=*/false, Is_even_K, /*Clear_OOB_MN=*/false, /*Clear_OOB_K=*/false>(
+        gmem_thr_copy_O, tOrO, tOgO, tOcO, tOpO, binfo.actual_seqlen_q - m_block * kBlockM
+    );
 }
 
 ////////////////////////////////////////////////////////////////////////////////////////////////////
 
-template<typename Kernel_traits, bool Is_dropout, bool Is_causal, bool Is_even_N, bool Return_softmax, typename Params>
+template<typename Kernel_traits, bool Is_dropout, bool Is_causal, bool Is_even_N, bool Is_even_K, bool Return_softmax, typename Params>
 inline __device__ void compute_attn(const Params &params) {
     const int m_block = blockIdx.x;
     // The block index for the batch.
     const int bidb = blockIdx.y;
     // The block index for the head.
     const int bidh = blockIdx.z;
 
@@ -575,13 +562,13 @@
     // them to have the same number of threads or have to traverse the attention matrix
     // in the same order.
     // In the Philox RNG, we use the offset to store the batch, head, and the lane id
     // (within a warp). We use the subsequence to store the location of the 16 x 16 blocks within
     // the attention matrix. This way, as long as we have the batch, head, and the location of
     // the 16 x 16 block within the attention matrix, we can generate the exact same dropout pattern.
 
-    flash::compute_attn_1rowblock<Kernel_traits, Is_dropout, Is_causal, Is_even_N, Return_softmax>(params, bidb, bidh, m_block);
+    flash::compute_attn_1rowblock<Kernel_traits, Is_dropout, Is_causal, Is_even_N, Is_even_K, Return_softmax>(params, bidb, bidh, m_block);
 }
 
 ////////////////////////////////////////////////////////////////////////////////////////////////////
 
-} // namespace flash
+} // namespace flash
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/src/flash_fwd_kernel_old.h` & `flash_attn-1.0.2/csrc/flash_attn/src/flash_fwd_kernel_old.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.1/csrc/flash_attn/src/flash_fwd_launch_template.h` & `flash_attn-1.0.2/csrc/flash_attn/src/flash_fwd_launch_template.h`

 * *Files 27% similar despite different names*

```diff
@@ -4,17 +4,17 @@
 
 #pragma once
 
 #include "static_switch.h"
 #include "flash.h"
 #include "flash_fwd_kernel.h"
 
-template<typename Kernel_traits, bool Is_dropout, bool Is_causal, bool Is_even_N, bool Return_softmax>
+template<typename Kernel_traits, bool Is_dropout, bool Is_causal, bool Is_even_N, bool Is_even_K, bool Return_softmax>
 __global__ void flash_fwd_loop_kernel(Flash_fwd_params params) {
-    flash::compute_attn<Kernel_traits, Is_dropout, Is_causal, Is_even_N, Return_softmax>(params);
+    flash::compute_attn<Kernel_traits, Is_dropout, Is_causal, Is_even_N, Is_even_K, Return_softmax>(params);
 }
 
 template<typename Kernel_traits, bool Is_dropout>
 void run_flash_loop_(Flash_fwd_params &params, cudaStream_t stream) {
     constexpr size_t smem_size = Kernel_traits::kSmemSize;
     // printf("smem_size = %d\n", smem_size);
 
@@ -23,24 +23,27 @@
     // https://github.com/HazyResearch/flash-attention/issues/21
 
     const int num_m_block = (params.seqlen_q + Kernel_traits::kBlockM - 1) / Kernel_traits::kBlockM;
     dim3 grid(num_m_block, params.b, params.h);
     // We also use is_even_N to set Unpadded in the BlockInfo constructor, so we need to check
     // for cu_seqlens_q as well.
     const bool is_even_N = params.cu_seqlens_q == nullptr && params.cu_seqlens_k == nullptr && params.seqlen_k % Kernel_traits::kBlockN == 0;
+    const bool is_even_K = params.d == Kernel_traits::kHeadDim;
     const bool return_softmax = params.p_ptr != nullptr;
     BOOL_SWITCH(params.is_causal, IsCausalConst, [&] {
         BOOL_SWITCH(is_even_N, IsEvenNConst, [&] {
-            BOOL_SWITCH(return_softmax, ReturnSoftmaxConst, [&] {
-                // Will only return softmax if dropout, to reduce compilation time.
-                auto kernel = &flash_fwd_loop_kernel<Kernel_traits, Is_dropout, IsCausalConst, IsEvenNConst, ReturnSoftmaxConst && Is_dropout>;
-                // auto kernel = &flash_fwd_loop_kernel<Kernel_traits, false, IsCausalConst, IsEvenNConst, false>;
-                if (smem_size >= 48 * 1024) {
-                    C10_CUDA_CHECK(cudaFuncSetAttribute(
-                        kernel, cudaFuncAttributeMaxDynamicSharedMemorySize, smem_size));
-                }
-                kernel<<<grid, Kernel_traits::kNThreads, smem_size, stream>>>(params);
-                C10_CUDA_KERNEL_LAUNCH_CHECK();
+            BOOL_SWITCH(is_even_K, IsEvenKConst, [&] {
+                BOOL_SWITCH(return_softmax, ReturnSoftmaxConst, [&] {
+                    // Will only return softmax if dropout, to reduce compilation time.
+                    auto kernel = &flash_fwd_loop_kernel<Kernel_traits, Is_dropout, IsCausalConst, IsEvenNConst, IsEvenKConst, ReturnSoftmaxConst && Is_dropout>;
+                    // auto kernel = &flash_fwd_loop_kernel<Kernel_traits, false, false, true, IsEvenKConst, false>;
+                    if (smem_size >= 48 * 1024) {
+                        C10_CUDA_CHECK(cudaFuncSetAttribute(
+                            kernel, cudaFuncAttributeMaxDynamicSharedMemorySize, smem_size));
+                    }
+                    kernel<<<grid, Kernel_traits::kNThreads, smem_size, stream>>>(params);
+                    C10_CUDA_KERNEL_LAUNCH_CHECK();
+                });
             });
         });
     });
 }
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/src/fmha/gemm.h` & `flash_attn-1.0.2/csrc/flash_attn/src/fmha/gemm.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.1/csrc/flash_attn/src/fmha/gmem_tile.h` & `flash_attn-1.0.2/csrc/flash_attn/src/fmha/gmem_tile.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.1/csrc/flash_attn/src/fmha/kernel_traits.h` & `flash_attn-1.0.2/csrc/flash_attn/src/fmha/kernel_traits.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.1/csrc/flash_attn/src/fmha/mask.h` & `flash_attn-1.0.2/csrc/flash_attn/src/fmha/mask.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.1/csrc/flash_attn/src/fmha/smem_tile.h` & `flash_attn-1.0.2/csrc/flash_attn/src/fmha/smem_tile.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.1/csrc/flash_attn/src/fmha/softmax.h` & `flash_attn-1.0.2/csrc/flash_attn/src/fmha/softmax.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.1/csrc/flash_attn/src/fmha/utils.h` & `flash_attn-1.0.2/csrc/flash_attn/src/fmha/utils.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.1/csrc/flash_attn/src/fmha.h` & `flash_attn-1.0.2/csrc/flash_attn/src/fmha.h`

 * *Files 2% similar despite different names*

```diff
@@ -121,14 +121,16 @@
     float scale_bmm1_rp_dropout;
 
     // Scale factor of 1 / (1 - p_dropout), in half2.
     uint32_t scale_dropout;
 
     // Random state.
     at::PhiloxCudaState philox_args;
+    // Pointer to the RNG seed (idx 0) and offset (idx 1).
+    uint64_t * rng_state;
 
     bool is_bf16;
     bool is_causal;
 
     int num_splits; // How many SMs per attention matrix.
 };
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/src/fmha_block_dgrad_fp16_kernel_loop.sm80.cu` & `flash_attn-1.0.2/csrc/flash_attn/src/fmha_block_dgrad_fp16_kernel_loop.sm80.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.1/csrc/flash_attn/src/fmha_block_dgrad_kernel_1xN_loop.h` & `flash_attn-1.0.2/csrc/flash_attn/src/fmha_block_dgrad_kernel_1xN_loop.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.1/csrc/flash_attn/src/fmha_block_fprop_fp16_kernel.sm80.cu` & `flash_attn-1.0.2/csrc/flash_attn/src/fmha_block_fprop_fp16_kernel.sm80.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.1/csrc/flash_attn/src/fmha_block_fprop_kernel_1xN.h` & `flash_attn-1.0.2/csrc/flash_attn/src/fmha_block_fprop_kernel_1xN.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.1/csrc/flash_attn/src/fmha_blockmask.h` & `flash_attn-1.0.2/csrc/flash_attn/src/fmha_blockmask.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.1/csrc/flash_attn/src/fmha_bwd_hdim32.cu` & `flash_attn-1.0.2/csrc/flash_attn/src/fmha_bwd_hdim32.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.1/csrc/flash_attn/src/fmha_bwd_hdim64.cu` & `flash_attn-1.0.2/csrc/flash_attn/src/fmha_bwd_hdim64.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.1/csrc/flash_attn/src/fmha_bwd_launch_template.h` & `flash_attn-1.0.2/csrc/flash_attn/src/fmha_bwd_launch_template.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.1/csrc/flash_attn/src/fmha_dgrad_kernel_1xN_loop.h` & `flash_attn-1.0.2/csrc/flash_attn/src/fmha_dgrad_kernel_1xN_loop.h`

 * *Files 1% similar despite different names*

```diff
@@ -790,16 +790,17 @@
     // The block index for the batch.
     const int bidb = blockIdx.x;
     // The block index for the head.
     const int bidh = blockIdx.y;
     // The thread index.
     const int tidx = threadIdx.x;
 
-    auto seeds = at::cuda::philox::unpack(params.philox_args);
-    Philox ph(std::get<0>(seeds), 0, std::get<1>(seeds) + (bidb * params.h + bidh) * 32 + tidx % 32);
+    auto seed = params.rng_state[0];
+    auto offset = params.rng_state[1];
+    Philox ph(seed, 0, offset + (bidb * params.h + bidh) * 32 + tidx % 32);
 
     if (loop_steps == 1) {
         compute_dq_dk_dv_1xN_one_iter<Kernel_traits, Is_dropout, Is_causal, true, true>(params, ph, 0);
     } else if (loop_steps == 2) {
         compute_dq_dk_dv_1xN_one_iter<Kernel_traits, Is_dropout, Is_causal, true, false>(params, ph, 0);
         compute_dq_dk_dv_1xN_one_iter<Kernel_traits, Is_dropout, Is_causal, false, true>(params, ph, 1);
     } else {
@@ -823,16 +824,17 @@
     // The block index for the batch.
     const int bidb = blockIdx.x;
     // The block index for the head.
     const int bidh = blockIdx.y;
     // The thread index.
     const int tidx = threadIdx.x;
 
-    auto seeds = at::cuda::philox::unpack(params.philox_args);
-    Philox ph(std::get<0>(seeds), 0, std::get<1>(seeds) + (bidb * params.h + bidh) * 32 + tidx % 32);
+    auto seed = params.rng_state[0];
+    auto offset = params.rng_state[1];
+    Philox ph(seed, 0, offset + (bidb * params.h + bidh) * 32 + tidx % 32);
 
     int loop_step_idx = blockIdx.z;
     compute_dq_dk_dv_1xN_one_iter<Kernel_traits, Is_dropout, Is_causal, false, false, /*Seq_parallel=*/true>(params, ph, loop_step_idx);
 }
 
 ////////////////////////////////////////////////////////////////////////////////////////////////////
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/src/fmha_fprop_kernel_1xN.h` & `flash_attn-1.0.2/csrc/flash_attn/src/fmha_fprop_kernel_1xN.h`

 * *Files 1% similar despite different names*

```diff
@@ -663,25 +663,31 @@
 template<typename Kernel_traits, bool Is_dropout, bool Is_causal, bool Return_softmax, typename Params>
 inline __device__ void device_1xN_loop(const Params &params) {
 
     // The block index for the batch.
     const int bidb = blockIdx.x;
     // The block index for the head.
     const int bidh = blockIdx.y;
+    // The block index.
+    const int bidx = gridDim.x * bidh + bidb;
     // The thread index.
     const int tidx = threadIdx.x;
 
     // We want the fwd and bwd to generate the same dropout pattern (RNG), without restricting
     // them to have the same number of threads or have to traverse the attention matrix
     // in the same order.
     // In the Philox RNG, we use the offset to store the batch, head, and the lane id
     // (within a warp). We use the subsequence to store the location of the 16 x 16 blocks within
     // the attention matrix. This way, as long as we have the batch, head, and the location of
     // the 16 x 16 block within the attention matrix, we can generate the exact same dropout pattern.
     auto seeds = at::cuda::philox::unpack(params.philox_args);
+    if (bidx == 0 && tidx == 0) {
+        params.rng_state[0] = std::get<0>(seeds);
+        params.rng_state[1] = std::get<1>(seeds);
+    }
     Philox ph(std::get<0>(seeds), 0, std::get<1>(seeds) + (bidb * params.h + bidh) * 32 + tidx % 32);
     constexpr int M = Kernel_traits::Cta_tile_p::M;
     const int STEPS = (params.seqlen_q + M - 1) / M;
 
     constexpr int blocksize_c = Kernel_traits::Cta_tile_p::N;
     if (params.seqlen_k == blocksize_c) {
         fmha::device_1xN_<Kernel_traits, Is_dropout, Is_causal, Return_softmax, true, true>(params, bidb, bidh, STEPS, ph, 0);
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/src/fmha_fwd_hdim32.cu` & `flash_attn-1.0.2/csrc/flash_attn/src/fmha_fwd_hdim32.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.1/csrc/flash_attn/src/fmha_fwd_hdim64.cu` & `flash_attn-1.0.2/csrc/flash_attn/src/fmha_fwd_hdim64.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.1/csrc/flash_attn/src/fmha_fwd_launch_template.h` & `flash_attn-1.0.2/csrc/flash_attn/src/fmha_fwd_launch_template.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.1/csrc/flash_attn/src/fmha_kernel.h` & `flash_attn-1.0.2/csrc/flash_attn/src/fmha_kernel.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.1/csrc/flash_attn/src/fmha_utils.h` & `flash_attn-1.0.2/csrc/flash_attn/src/fmha_utils.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.1/csrc/flash_attn/src/kernel_traits.h` & `flash_attn-1.0.2/csrc/flash_attn/src/kernel_traits.h`

 * *Files 5% similar despite different names*

```diff
@@ -23,19 +23,19 @@
     static constexpr bool Has_cp_async = false;
 #endif
 
     using ElementAccum = float;
     using index_t = uint32_t;
 
 #if defined(__CUDA_ARCH__) &&  __CUDA_ARCH__ >= 800
-    using MMA_Atom_Arch = typename std::conditional<
+    using MMA_Atom_Arch = std::conditional_t<
         std::is_same_v<elem_type, cutlass::half_t>,
         MMA_Atom<SM80_16x8x16_F32F16F16F32_TN>,
         MMA_Atom<SM80_16x8x16_F32BF16BF16F32_TN>
-    >::type;
+    >;
     using ValLayoutMNK = Layout<Shape<_1, _2, _1>>;
 #else
     using MMA_Atom_Arch = MMA_Atom<SM75_16x8x8_F32F16F16F32_TN>;
     using ValLayoutMNK = Layout<Shape<_1, _2, _2>>;
 #endif
 
 #if defined(__CUDA_ARCH__) &&  __CUDA_ARCH__ >= 750
@@ -125,32 +125,23 @@
     // thread 0 - 7 will write to the first page and thread 8 - 15 will write to the second page,
     // to the same banks.
     static constexpr int kGmemThreadsPerRow = kBlockKSmem / kGmemElemsPerLoad;
     static_assert(kNThreads % kGmemThreadsPerRow == 0, "kNThreads must be a multiple of kGmemThreadsPerRow");
     using GmemLayoutAtom = Layout<Shape <Int<kNThreads / kGmemThreadsPerRow>, Int<kGmemThreadsPerRow>>,
                                   Stride<Int<kGmemThreadsPerRow>, _1>>;
 
-    // CACHEGLOBAL makes more sense than CACHEALWAYS for Q, but in practice it doesn't seem to
-    // make a difference in speed.
-    using Q_gmem_copy_struct = typename std::conditional<
+    // We use CACHEGLOBAL instead of CACHEALWAYS for both Q and K/V, since we won't be reading
+    // from the same address by the same threadblock. This is slightly faster.
+    using Gmem_copy_struct = std::conditional_t<
         Has_cp_async,
         SM80_CP_ASYNC_CACHEGLOBAL<cute::uint128_t>,
         DefaultCopy
-    >::type;
-    using KV_gmem_copy_struct = typename std::conditional<
-        Has_cp_async,
-        SM80_CP_ASYNC_CACHEALWAYS<cute::uint128_t>,
-        DefaultCopy
-    >::type;
-    using GmemTiledCopyQ = decltype(
-        make_tiled_copy(Copy_Atom<Q_gmem_copy_struct, elem_type>{},
-                        GmemLayoutAtom{},
-                        Layout<Shape<_1, _8>>{}));  // Val layout, 8 vals per read
-    using GmemTiledCopyKV = decltype(
-        make_tiled_copy(Copy_Atom<KV_gmem_copy_struct, elem_type>{},
+    >;
+    using GmemTiledCopyQKV = decltype(
+        make_tiled_copy(Copy_Atom<Gmem_copy_struct, elem_type>{},
                         GmemLayoutAtom{},
                         Layout<Shape<_1, _8>>{}));  // Val layout, 8 vals per read
     using GmemTiledCopyO = decltype(
         make_tiled_copy(Copy_Atom<DefaultCopy, elem_type>{},
                         GmemLayoutAtom{},
                         Layout<Shape<_1, _8>>{}));  // Val layout, 8 vals per store
     static constexpr int kGmemThreadsPerRowP = kBlockN / kGmemElemsPerLoad;
@@ -317,44 +308,44 @@
     // Using kBlockKSmem instead of kHeadDim here to avoid bank conflicts, but doesn't seem
     // to affect speed in practice.
     static constexpr int kGmemThreadsPerRow = kBlockKSmem / kGmemElemsPerLoad;
     static_assert(kNThreads % kGmemThreadsPerRow == 0, "kNThreads must be a multiple of kGmemThreadsPerRow");
     using GmemLayoutAtom = Layout<Shape <Int<kNThreads / kGmemThreadsPerRow>, Int<kGmemThreadsPerRow>>,
                                   Stride<Int<kGmemThreadsPerRow>, _1>>;
 
-    // CACHEGLOBAL makes more sense than CACHEALWAYS for K and V, but in practice it doesn't seem to
-    // make a difference in speed.
-    using GmemTiledCopyQ = decltype(
-        make_tiled_copy(Copy_Atom<SM80_CP_ASYNC_CACHEALWAYS<cute::uint128_t>, elem_type>{},
-                        GmemLayoutAtom{},
-                        Layout<Shape < _1, _8>>{}));  // Val layout, 8 vals per read
-    using GmemTiledCopyKV = decltype(
-        make_tiled_copy(Copy_Atom<SM80_CP_ASYNC_CACHEGLOBAL<cute::uint128_t>, elem_type>{},
-        // make_tiled_copy(Copy_Atom<DefaultCopy, elem_type>{},
+    // We use CACHEGLOBAL instead of CACHEALWAYS for both Q and K/V, since we won't be reading
+    // from the same address by the same threadblock. This is slightly faster.
+    using Gmem_copy_struct = std::conditional_t<
+        Has_cp_async,
+        SM80_CP_ASYNC_CACHEGLOBAL<cute::uint128_t>,
+        DefaultCopy
+    >;
+    using GmemTiledCopyQKV = decltype(
+        make_tiled_copy(Copy_Atom<Gmem_copy_struct, elem_type>{},
                         GmemLayoutAtom{},
-                        Layout<Shape < _1, _8>>{}));  // Val layout, 8 vals per read
+                        Layout<Shape<_1, _8>>{}));  // Val layout, 8 vals per read
     using GmemTiledCopydO = decltype(
         make_tiled_copy(Copy_Atom<DefaultCopy, elem_type>{},
                         GmemLayoutAtom{},
                         Layout<Shape < _1, _8>>{}));  // Val layout, 8 vals per store
     using GmemTiledCopydKV = decltype(
         make_tiled_copy(Copy_Atom<DefaultCopy, elem_type>{},
                         GmemLayoutAtom{},
                         Layout<Shape < _1, _8>>{}));  // Val layout, 8 vals per store
     using GmemTiledCopydQ = decltype(
         make_tiled_copy(Copy_Atom<DefaultCopy, elem_type>{},
                         GmemLayoutAtom{},
                         Layout<Shape < _1, _8>>{}));  // Val layout, 8 vals per store
-    using GmemLayoutAtomdQaccum = typename std::conditional<
+    using GmemLayoutAtomdQaccum = std::conditional_t<
         kBlockKSmem == 32,
         Layout<Shape <_32, _8>,  // Thread layout, 8 threads per row
                Stride< _8, _1>>,
         Layout<Shape <_16, _16>,  // Thread layout, 16 threads per row
                Stride< _16, _1>>
-    >::type;
+    >;
     using GmemTiledCopydQaccum = decltype(
         make_tiled_copy(Copy_Atom<DefaultCopy, ElementAccum>{},
                         GmemLayoutAtomdQaccum{},
                         Layout<Shape < _1, _4>>{}));  // Val layout, 4 vals per store
 
     using GmemTiledCopydQaccumAtomicAdd = decltype(
         make_tiled_copy(Copy_Atom<DefaultCopy, ElementAccum>{},
```

### Comparing `flash_attn-1.0.1/csrc/flash_attn/src/mask.h` & `flash_attn-1.0.2/csrc/flash_attn/src/mask.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.1/csrc/flash_attn/src/philox.cuh` & `flash_attn-1.0.2/csrc/flash_attn/src/philox.cuh`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.1/csrc/flash_attn/src/softmax.h` & `flash_attn-1.0.2/csrc/flash_attn/src/softmax.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.1/csrc/flash_attn/src/static_switch.h` & `flash_attn-1.0.2/csrc/flash_attn/src/static_switch.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.1/csrc/flash_attn/src/utils.h` & `flash_attn-1.0.2/csrc/flash_attn/src/utils.h`

 * *Files 18% similar despite different names*

```diff
@@ -312,8 +312,77 @@
 #if defined(CUTE_ARCH_CP_ASYNC_SM80_ENABLED)
     asm volatile("cp.async.wait_group %0;\n" :: "n"(N));
 #endif
 }
 
 ////////////////////////////////////////////////////////////////////////////////////////////////////
 
+template <bool Is_even_MN=true, bool Is_even_K=true, bool Clear_OOB_MN=false, bool Clear_OOB_K=true,
+          typename TiledCopy, typename Engine0, typename Layout0, typename Engine1, typename Layout1,
+          typename Engine2, typename Layout2, typename Engine3, typename Layout3>
+inline __device__ void copy(TiledCopy thr_copy, Tensor<Engine0, Layout0> const &S,
+                            Tensor<Engine1, Layout1> &D, Tensor<Engine2, Layout2> const &identity_MN,
+                            Tensor<Engine3, Layout3> const &predicate_K, int max_MN=0) {
+    CUTE_STATIC_ASSERT_V(rank(S) == Int<3>{});
+    CUTE_STATIC_ASSERT_V(rank(D) == Int<3>{});
+    CUTE_STATIC_ASSERT_V(size<0>(S) == size<0>(D));                     // MMA
+    CUTE_STATIC_ASSERT_V(size<1>(S) == size<1>(D));                     // MMA_M
+    CUTE_STATIC_ASSERT_V(size<2>(S) == size<2>(D));                     // MMA_K
+    // There's no case where !Clear_OOB_K && Clear_OOB_MN
+    static_assert(!(Clear_OOB_MN && !Clear_OOB_K));
+    #pragma unroll
+    for (int m = 0; m < size<1>(S); ++m) {
+        if (Is_even_MN || get<0>(identity_MN(0, m, 0)) < max_MN) {
+            #pragma unroll
+            for (int k = 0; k < size<2>(S); ++k) {
+                if (Is_even_K || predicate_K(k)) {
+                    copy(thr_copy, S(_, m, k), D(_, m, k));
+                } else if (Clear_OOB_K) {
+                    clear(D(_, m, k));
+                }
+            }
+        } else if (Clear_OOB_MN) {
+            clear(D(_, m, _));
+        }
+    }
+    // TD [2023-04-13]: Strange that the code below can cause race condition.
+    // I think it's because the copies are under an if statement.
+    // if (Is_even_K) {
+    //     #pragma unroll
+    //     for (int m = 0; m < size<1>(S); ++m) {
+    //         if (Is_even_MN || get<0>(identity_MN(0, m, 0)) < max_MN) {
+    //             copy(thr_copy, S(_, m, _), D(_, m, _));
+    //         } else if (Clear_OOB_MN) {
+    //             clear(D(_, m, _));
+    //         }
+    //     }
+    // } else {  // It's slightly faster in this case if iterate over K first
+    //     #pragma unroll
+    //     for (int k = 0; k < size<2>(S); ++k) {
+    //         if (predicate_K(k)) {
+    //             #pragma unroll
+    //             for (int m = 0; m < size<1>(S); ++m) {
+    //                 if (Is_even_MN || get<0>(identity_MN(0, m, 0)) < max_MN) {
+    //                     copy(thr_copy, S(_, m, k), D(_, m, k));
+    //                 } else if (Clear_OOB_MN) {
+    //                     clear(D(_, m, k));
+    //                 }
+    //             }
+    //         } else if (Clear_OOB_K) {  // There's no case where !Clear_OOB_K && Clear_OOB_MN
+    //             if (Clear_OOB_MN || Is_even_MN) {
+    //                 clear(D(_, _, k));
+    //             } else {
+    //                 #pragma unroll
+    //                 for (int m = 0; m < size<1>(S); ++m) {
+    //                     if (!(Is_even_MN || get<0>(identity_MN(0, m, 0)) < max_MN)) {
+    //                         clear(D(_, m, k));
+    //                     }
+    //                 }
+    //             }
+    //         }
+    //     }
+    // }
+}
+
+////////////////////////////////////////////////////////////////////////////////////////////////////
+
 }  // namespace flash
```

### Comparing `flash_attn-1.0.1/csrc/flash_gen/decoder_masked_multihead_attention.cu` & `flash_attn-1.0.2/csrc/flash_gen/decoder_masked_multihead_attention.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.1/csrc/ft_attention/cuda_bf16_fallbacks.cuh` & `flash_attn-1.0.2/csrc/ft_attention/cuda_bf16_fallbacks.cuh`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.1/csrc/ft_attention/cuda_bf16_wrapper.h` & `flash_attn-1.0.2/csrc/ft_attention/cuda_bf16_wrapper.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.1/csrc/ft_attention/decoder_masked_multihead_attention.cu` & `flash_attn-1.0.2/csrc/ft_attention/decoder_masked_multihead_attention.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.1/csrc/ft_attention/decoder_masked_multihead_attention.h` & `flash_attn-1.0.2/csrc/ft_attention/decoder_masked_multihead_attention.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.1/csrc/ft_attention/decoder_masked_multihead_attention_utils.h` & `flash_attn-1.0.2/csrc/ft_attention/decoder_masked_multihead_attention_utils.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.1/csrc/ft_attention/ft_attention.cpp` & `flash_attn-1.0.2/csrc/ft_attention/ft_attention.cpp`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.1/csrc/fused_dense_lib/fused_dense.cpp` & `flash_attn-1.0.2/csrc/fused_dense_lib/fused_dense.cpp`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.1/csrc/fused_dense_lib/fused_dense_cuda.cu` & `flash_attn-1.0.2/csrc/fused_dense_lib/fused_dense_cuda.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.1/csrc/fused_softmax/fused_softmax.cpp` & `flash_attn-1.0.2/csrc/fused_softmax/fused_softmax.cpp`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.1/csrc/fused_softmax/scaled_masked_softmax.h` & `flash_attn-1.0.2/csrc/fused_softmax/scaled_masked_softmax.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.1/csrc/fused_softmax/scaled_masked_softmax_cuda.cu` & `flash_attn-1.0.2/csrc/fused_softmax/scaled_masked_softmax_cuda.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.1/csrc/fused_softmax/scaled_upper_triang_masked_softmax.h` & `flash_attn-1.0.2/csrc/fused_softmax/scaled_upper_triang_masked_softmax.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.1/csrc/fused_softmax/scaled_upper_triang_masked_softmax_cuda.cu` & `flash_attn-1.0.2/csrc/fused_softmax/scaled_upper_triang_masked_softmax_cuda.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.1/csrc/fused_softmax/type_shim.h` & `flash_attn-1.0.2/csrc/fused_softmax/type_shim.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.1/csrc/layer_norm/ln.h` & `flash_attn-1.0.2/csrc/layer_norm/ln.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.1/csrc/layer_norm/ln_api.cpp` & `flash_attn-1.0.2/csrc/layer_norm/ln_api.cpp`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.1/csrc/layer_norm/ln_bwd_1024.cu` & `flash_attn-1.0.2/csrc/layer_norm/ln_bwd_1024.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.1/csrc/layer_norm/ln_bwd_1280.cu` & `flash_attn-1.0.2/csrc/layer_norm/ln_bwd_1280.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.1/csrc/layer_norm/ln_bwd_1536.cu` & `flash_attn-1.0.2/csrc/layer_norm/ln_bwd_1536.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.1/csrc/layer_norm/ln_bwd_2048.cu` & `flash_attn-1.0.2/csrc/layer_norm/ln_bwd_2048.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.1/csrc/layer_norm/ln_bwd_256.cu` & `flash_attn-1.0.2/csrc/layer_norm/ln_bwd_256.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.1/csrc/layer_norm/ln_bwd_2560.cu` & `flash_attn-1.0.2/csrc/layer_norm/ln_bwd_2560.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.1/csrc/layer_norm/ln_bwd_3072.cu` & `flash_attn-1.0.2/csrc/layer_norm/ln_bwd_3072.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.1/csrc/layer_norm/ln_bwd_4096.cu` & `flash_attn-1.0.2/csrc/layer_norm/ln_bwd_4096.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.1/csrc/layer_norm/ln_bwd_512.cu` & `flash_attn-1.0.2/csrc/layer_norm/ln_bwd_512.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.1/csrc/layer_norm/ln_bwd_5120.cu` & `flash_attn-1.0.2/csrc/layer_norm/ln_bwd_5120.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.1/csrc/layer_norm/ln_bwd_6144.cu` & `flash_attn-1.0.2/csrc/layer_norm/ln_bwd_6144.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.1/csrc/layer_norm/ln_bwd_7168.cu` & `flash_attn-1.0.2/csrc/layer_norm/ln_bwd_7168.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.1/csrc/layer_norm/ln_bwd_768.cu` & `flash_attn-1.0.2/csrc/layer_norm/ln_bwd_768.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.1/csrc/layer_norm/ln_bwd_8192.cu` & `flash_attn-1.0.2/csrc/layer_norm/ln_bwd_8192.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.1/csrc/layer_norm/ln_bwd_kernels.cuh` & `flash_attn-1.0.2/csrc/layer_norm/ln_bwd_kernels.cuh`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.1/csrc/layer_norm/ln_bwd_semi_cuda_kernel_old.cu` & `flash_attn-1.0.2/csrc/layer_norm/ln_bwd_semi_cuda_kernel_old.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.1/csrc/layer_norm/ln_fwd_1024.cu` & `flash_attn-1.0.2/csrc/layer_norm/ln_fwd_1024.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.1/csrc/layer_norm/ln_fwd_10240.cu` & `flash_attn-1.0.2/csrc/layer_norm/ln_fwd_10240.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.1/csrc/layer_norm/ln_fwd_12288.cu` & `flash_attn-1.0.2/csrc/layer_norm/ln_fwd_12288.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.1/csrc/layer_norm/ln_fwd_128.cu` & `flash_attn-1.0.2/csrc/layer_norm/ln_fwd_128.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.1/csrc/layer_norm/ln_fwd_1280.cu` & `flash_attn-1.0.2/csrc/layer_norm/ln_fwd_1280.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.1/csrc/layer_norm/ln_fwd_1536.cu` & `flash_attn-1.0.2/csrc/layer_norm/ln_fwd_1536.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.1/csrc/layer_norm/ln_fwd_2048.cu` & `flash_attn-1.0.2/csrc/layer_norm/ln_fwd_2048.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.1/csrc/layer_norm/ln_fwd_256.cu` & `flash_attn-1.0.2/csrc/layer_norm/ln_fwd_256.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.1/csrc/layer_norm/ln_fwd_2560.cu` & `flash_attn-1.0.2/csrc/layer_norm/ln_fwd_2560.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.1/csrc/layer_norm/ln_fwd_3072.cu` & `flash_attn-1.0.2/csrc/layer_norm/ln_fwd_3072.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.1/csrc/layer_norm/ln_fwd_384.cu` & `flash_attn-1.0.2/csrc/layer_norm/ln_fwd_384.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.1/csrc/layer_norm/ln_fwd_4096.cu` & `flash_attn-1.0.2/csrc/layer_norm/ln_fwd_4096.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.1/csrc/layer_norm/ln_fwd_512.cu` & `flash_attn-1.0.2/csrc/layer_norm/ln_fwd_512.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.1/csrc/layer_norm/ln_fwd_5120.cu` & `flash_attn-1.0.2/csrc/layer_norm/ln_fwd_5120.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.1/csrc/layer_norm/ln_fwd_6144.cu` & `flash_attn-1.0.2/csrc/layer_norm/ln_fwd_6144.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.1/csrc/layer_norm/ln_fwd_7168.cu` & `flash_attn-1.0.2/csrc/layer_norm/ln_fwd_7168.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.1/csrc/layer_norm/ln_fwd_768.cu` & `flash_attn-1.0.2/csrc/layer_norm/ln_fwd_768.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.1/csrc/layer_norm/ln_fwd_8192.cu` & `flash_attn-1.0.2/csrc/layer_norm/ln_fwd_8192.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.1/csrc/layer_norm/ln_fwd_9216.cu` & `flash_attn-1.0.2/csrc/layer_norm/ln_fwd_9216.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.1/csrc/layer_norm/ln_fwd_cuda_kernel_old.cu` & `flash_attn-1.0.2/csrc/layer_norm/ln_fwd_cuda_kernel_old.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.1/csrc/layer_norm/ln_fwd_kernels.cuh` & `flash_attn-1.0.2/csrc/layer_norm/ln_fwd_kernels.cuh`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.1/csrc/layer_norm/ln_kernel_traits.h` & `flash_attn-1.0.2/csrc/layer_norm/ln_kernel_traits.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.1/csrc/layer_norm/ln_parallel_bwd_1024.cu` & `flash_attn-1.0.2/csrc/layer_norm/ln_parallel_bwd_1024.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.1/csrc/layer_norm/ln_parallel_bwd_1280.cu` & `flash_attn-1.0.2/csrc/layer_norm/ln_parallel_bwd_1280.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.1/csrc/layer_norm/ln_parallel_bwd_1536.cu` & `flash_attn-1.0.2/csrc/layer_norm/ln_parallel_bwd_1536.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.1/csrc/layer_norm/ln_parallel_bwd_2048.cu` & `flash_attn-1.0.2/csrc/layer_norm/ln_parallel_bwd_2048.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.1/csrc/layer_norm/ln_parallel_bwd_256.cu` & `flash_attn-1.0.2/csrc/layer_norm/ln_parallel_bwd_256.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.1/csrc/layer_norm/ln_parallel_bwd_2560.cu` & `flash_attn-1.0.2/csrc/layer_norm/ln_parallel_bwd_2560.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.1/csrc/layer_norm/ln_parallel_bwd_3072.cu` & `flash_attn-1.0.2/csrc/layer_norm/ln_parallel_bwd_3072.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.1/csrc/layer_norm/ln_parallel_bwd_4096.cu` & `flash_attn-1.0.2/csrc/layer_norm/ln_parallel_bwd_4096.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.1/csrc/layer_norm/ln_parallel_bwd_512.cu` & `flash_attn-1.0.2/csrc/layer_norm/ln_parallel_bwd_512.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.1/csrc/layer_norm/ln_parallel_bwd_5120.cu` & `flash_attn-1.0.2/csrc/layer_norm/ln_parallel_bwd_5120.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.1/csrc/layer_norm/ln_parallel_bwd_6144.cu` & `flash_attn-1.0.2/csrc/layer_norm/ln_parallel_bwd_6144.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.1/csrc/layer_norm/ln_parallel_bwd_7168.cu` & `flash_attn-1.0.2/csrc/layer_norm/ln_parallel_bwd_7168.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.1/csrc/layer_norm/ln_parallel_bwd_768.cu` & `flash_attn-1.0.2/csrc/layer_norm/ln_parallel_bwd_768.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.1/csrc/layer_norm/ln_parallel_bwd_8192.cu` & `flash_attn-1.0.2/csrc/layer_norm/ln_parallel_bwd_8192.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.1/csrc/layer_norm/ln_parallel_fwd_1024.cu` & `flash_attn-1.0.2/csrc/layer_norm/ln_parallel_fwd_1024.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.1/csrc/layer_norm/ln_parallel_fwd_128.cu` & `flash_attn-1.0.2/csrc/layer_norm/ln_parallel_fwd_128.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.1/csrc/layer_norm/ln_parallel_fwd_1280.cu` & `flash_attn-1.0.2/csrc/layer_norm/ln_parallel_fwd_1280.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.1/csrc/layer_norm/ln_parallel_fwd_1536.cu` & `flash_attn-1.0.2/csrc/layer_norm/ln_parallel_fwd_1536.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.1/csrc/layer_norm/ln_parallel_fwd_2048.cu` & `flash_attn-1.0.2/csrc/layer_norm/ln_parallel_fwd_2048.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.1/csrc/layer_norm/ln_parallel_fwd_256.cu` & `flash_attn-1.0.2/csrc/layer_norm/ln_parallel_fwd_256.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.1/csrc/layer_norm/ln_parallel_fwd_2560.cu` & `flash_attn-1.0.2/csrc/layer_norm/ln_parallel_fwd_2560.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.1/csrc/layer_norm/ln_parallel_fwd_3072.cu` & `flash_attn-1.0.2/csrc/layer_norm/ln_parallel_fwd_3072.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.1/csrc/layer_norm/ln_parallel_fwd_4096.cu` & `flash_attn-1.0.2/csrc/layer_norm/ln_parallel_fwd_4096.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.1/csrc/layer_norm/ln_parallel_fwd_512.cu` & `flash_attn-1.0.2/csrc/layer_norm/ln_parallel_fwd_512.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.1/csrc/layer_norm/ln_parallel_fwd_5120.cu` & `flash_attn-1.0.2/csrc/layer_norm/ln_parallel_fwd_5120.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.1/csrc/layer_norm/ln_parallel_fwd_6144.cu` & `flash_attn-1.0.2/csrc/layer_norm/ln_parallel_fwd_6144.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.1/csrc/layer_norm/ln_parallel_fwd_7168.cu` & `flash_attn-1.0.2/csrc/layer_norm/ln_parallel_fwd_7168.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.1/csrc/layer_norm/ln_parallel_fwd_768.cu` & `flash_attn-1.0.2/csrc/layer_norm/ln_parallel_fwd_768.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.1/csrc/layer_norm/ln_parallel_fwd_8192.cu` & `flash_attn-1.0.2/csrc/layer_norm/ln_parallel_fwd_8192.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.1/csrc/layer_norm/ln_parallel_res_fwd_kernel.cuh` & `flash_attn-1.0.2/csrc/layer_norm/ln_parallel_res_fwd_kernel.cuh`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.1/csrc/layer_norm/ln_parallel_residual_bwd_512.cu` & `flash_attn-1.0.2/csrc/layer_norm/ln_parallel_residual_bwd_512.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.1/csrc/layer_norm/ln_parallel_residual_bwd_kernels.cuh` & `flash_attn-1.0.2/csrc/layer_norm/ln_parallel_residual_bwd_kernels.cuh`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.1/csrc/layer_norm/ln_parallel_residual_fwd_kernel.cuh` & `flash_attn-1.0.2/csrc/layer_norm/ln_parallel_residual_fwd_kernel.cuh`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.1/csrc/layer_norm/ln_parallel_residual_fwd_kernels.cuh` & `flash_attn-1.0.2/csrc/layer_norm/ln_parallel_residual_fwd_kernels.cuh`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.1/csrc/layer_norm/ln_utils.cuh` & `flash_attn-1.0.2/csrc/layer_norm/ln_utils.cuh`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.1/csrc/layer_norm/static_switch.h` & `flash_attn-1.0.2/csrc/layer_norm/static_switch.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.1/csrc/rotary/rotary.cpp` & `flash_attn-1.0.2/csrc/rotary/rotary.cpp`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.1/csrc/rotary/rotary_cuda.cu` & `flash_attn-1.0.2/csrc/rotary/rotary_cuda.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.1/csrc/xentropy/interface.cpp` & `flash_attn-1.0.2/csrc/xentropy/interface.cpp`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.1/csrc/xentropy/xentropy_kernel.cu` & `flash_attn-1.0.2/csrc/xentropy/xentropy_kernel.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.1/flash_attn/attention_kernl.py` & `flash_attn-1.0.2/flash_attn/attention_kernl.py`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.1/flash_attn/bert_padding.py` & `flash_attn-1.0.2/flash_attn/bert_padding.py`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.1/flash_attn/flash_attention.py` & `flash_attn-1.0.2/flash_attn/flash_attention.py`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.1/flash_attn/flash_attn_interface.py` & `flash_attn-1.0.2/flash_attn/flash_attn_interface.py`

 * *Files 11% similar despite different names*

```diff
@@ -14,53 +14,52 @@
                         dropout_p, softmax_scale, causal, return_softmax, num_splits=0,
                         generator=None):
     """
     num_splits: how much to parallelize over the seqlen_q dimension. num_splits=0 means
     it will be set by an internal heuristic. We're exposing num_splits mostly for benchmarking.
     Don't change it unless you know what you're doing.
     """
-    softmax_lse, *rest = flash_attn_cuda.fwd(
+    softmax_lse, rng_state, *rest = flash_attn_cuda.fwd(
         q, k, v, out, cu_seqlens_q, cu_seqlens_k, max_seqlen_q, max_seqlen_k, dropout_p,
         softmax_scale, False, causal, return_softmax, num_splits, generator
     )
     # if out.isnan().any() or softmax_lse.isnan().any():
     #     breakpoint()
     S_dmask = rest[0] if return_softmax else None
-    return out, softmax_lse, S_dmask
+    return out, softmax_lse, rng_state, S_dmask
 
 
 def _flash_attn_backward(dout, q, k, v, out, softmax_lse, dq, dk, dv, cu_seqlens_q, cu_seqlens_k,
-                         max_seqlen_q, max_seqlen_k, dropout_p, softmax_scale, causal, num_splits=0,
-                         generator=None):
+                         max_seqlen_q, max_seqlen_k, dropout_p, softmax_scale, causal,
+                         rng_state=None, num_splits=0, generator=None):
     """
     num_splits: whether to parallelize over the seqlen_k dimension (num_splits > 1) or
     not (num_splits = 1). num_splits=0 means it will be set by an internal heuristic.
     Any value above 1 will call the same kernel (i.e. num_splits=2 would call the same kernel
     as num_splits=3), so effectively the choices are 0, 1, and 2.
     This hyperparameter can be tuned for performance, but default value (heuristic) should work fine.
     """
     dout = dout.contiguous()  # CUDA code assumes that dout is contiguous
     _, _, _, softmax_d = flash_attn_cuda.bwd(
         dout, q, k, v, out, softmax_lse, dq, dk, dv, cu_seqlens_q, cu_seqlens_k,
-        max_seqlen_q, max_seqlen_k, dropout_p, softmax_scale, False, causal, num_splits, generator)
+        max_seqlen_q, max_seqlen_k, dropout_p, softmax_scale, False, causal,
+        num_splits, generator, rng_state)
     # if dk.isnan().any() or dk.isnan().any() or dv.isnan().any() or softmax_d.isnan().any():
     #     breakpoint()
     return dq, dk, dv, softmax_d
 
 
 class FlashAttnQKVPackedFunc(torch.autograd.Function):
 
     @staticmethod
     def forward(ctx, qkv, cu_seqlens, max_seqlen, dropout_p, softmax_scale, causal,
                 return_softmax, deterministic):
-        # Save rng_state because the backward pass will regenerate the dropout mask
-        rng_state = torch.cuda.get_rng_state() if dropout_p > 0 else None
         if softmax_scale is None:
             softmax_scale = qkv.shape[-1] ** (-0.5)
-        out, softmax_lse, S_dmask = _flash_attn_forward(
+        out, softmax_lse, rng_state, S_dmask = _flash_attn_forward(
             qkv[:, 0], qkv[:, 1], qkv[:, 2], torch.empty_like(qkv[:, 0]), cu_seqlens, cu_seqlens,
             max_seqlen, max_seqlen, dropout_p, softmax_scale, causal=causal,
             return_softmax=return_softmax
         )
         ctx.save_for_backward(qkv, out, softmax_lse, cu_seqlens, rng_state)
         ctx.dropout_p = dropout_p
         ctx.max_seqlen = max_seqlen
@@ -68,39 +67,32 @@
         ctx.causal = causal
         ctx.deterministic = deterministic
         return out if not return_softmax else (out, softmax_lse, S_dmask)
 
     @staticmethod
     def backward(ctx, dout, *args):
         qkv, out, softmax_lse, cu_seqlens, rng_state = ctx.saved_tensors
-        if rng_state is not None:
-            cur_rng_state = torch.cuda.get_rng_state()
-            torch.cuda.set_rng_state(rng_state)
         dqkv = torch.empty_like(qkv)
         _flash_attn_backward(
             dout, qkv[:, 0], qkv[:, 1], qkv[:, 2], out, softmax_lse,
             dqkv[:, 0], dqkv[:, 1], dqkv[:, 2], cu_seqlens, cu_seqlens,
             ctx.max_seqlen, ctx.max_seqlen, ctx.dropout_p, ctx.softmax_scale, ctx.causal,
-            num_splits=1 if ctx.deterministic else 0,
+            rng_state=rng_state, num_splits=1 if ctx.deterministic else 0,
         )
-        if rng_state is not None:
-            torch.cuda.set_rng_state(cur_rng_state)
         return dqkv, None, None, None, None, None, None, None
 
 
 class FlashAttnKVPackedFunc(torch.autograd.Function):
 
     @staticmethod
     def forward(ctx, q, kv, cu_seqlens_q, cu_seqlens_k, max_seqlen_q, max_seqlen_k, dropout_p,
                 softmax_scale, causal, return_softmax, deterministic):
-        # Save rng_state because the backward pass will regenerate the dropout mask
-        rng_state = torch.cuda.get_rng_state() if dropout_p > 0 else None
         if softmax_scale is None:
             softmax_scale = q.shape[-1] ** (-0.5)
-        out, softmax_lse, S_dmask = _flash_attn_forward(
+        out, softmax_lse, rng_state, S_dmask = _flash_attn_forward(
             q, kv[:, 0], kv[:, 1], torch.empty_like(q), cu_seqlens_q, cu_seqlens_k, max_seqlen_q,
             max_seqlen_k, dropout_p, softmax_scale, causal=causal, return_softmax=return_softmax
         )
         ctx.save_for_backward(q, kv, out, softmax_lse, cu_seqlens_q, cu_seqlens_k, rng_state)
         ctx.dropout_p = dropout_p
         ctx.max_seqlen_q = max_seqlen_q
         ctx.max_seqlen_k = max_seqlen_k
@@ -108,40 +100,33 @@
         ctx.causal = causal
         ctx.deterministic = deterministic
         return out if not return_softmax else (out, softmax_lse, S_dmask)
 
     @staticmethod
     def backward(ctx, dout, *args):
         q, kv, out, softmax_lse, cu_seqlens_q, cu_seqlens_k, rng_state = ctx.saved_tensors
-        if rng_state is not None:
-            cur_rng_state = torch.cuda.get_rng_state()
-            torch.cuda.set_rng_state(rng_state)
         dq = torch.empty_like(q)
         dkv = torch.empty_like(kv)
         _flash_attn_backward(
             dout, q, kv[:, 0], kv[:, 1], out, softmax_lse,
             dq, dkv[:, 0], dkv[:, 1], cu_seqlens_q, cu_seqlens_k,
             ctx.max_seqlen_q, ctx.max_seqlen_k, ctx.dropout_p, ctx.softmax_scale, ctx.causal,
-            num_splits=1 if ctx.deterministic else 0,
+            rng_state=rng_state, num_splits=1 if ctx.deterministic else 0,
         )
-        if rng_state is not None:
-            torch.cuda.set_rng_state(cur_rng_state)
         return dq, dkv, None, None, None, None, None, None, None, None, None
 
 
 class FlashAttnFunc(torch.autograd.Function):
 
     @staticmethod
     def forward(ctx, q, k, v, cu_seqlens_q, cu_seqlens_k, max_seqlen_q, max_seqlen_k, dropout_p,
                 softmax_scale, causal, return_softmax, deterministic):
-        # Save rng_state because the backward pass will regenerate the dropout mask
-        rng_state = torch.cuda.get_rng_state() if dropout_p > 0 else None
         if softmax_scale is None:
             softmax_scale = q.shape[-1] ** (-0.5)
-        out, softmax_lse, S_dmask = _flash_attn_forward(
+        out, softmax_lse, rng_state, S_dmask = _flash_attn_forward(
             q, k, v, torch.empty_like(q), cu_seqlens_q, cu_seqlens_k, max_seqlen_q, max_seqlen_k,
             dropout_p, softmax_scale, causal=causal, return_softmax=return_softmax
         )
         ctx.save_for_backward(q, k, v, out, softmax_lse, cu_seqlens_q, cu_seqlens_k, rng_state)
         ctx.dropout_p = dropout_p
         ctx.max_seqlen_q = max_seqlen_q
         ctx.max_seqlen_k = max_seqlen_k
@@ -149,25 +134,20 @@
         ctx.causal = causal
         ctx.deterministic = deterministic
         return out if not return_softmax else (out, softmax_lse, S_dmask)
 
     @staticmethod
     def backward(ctx, dout, *args):
         q, k, v, out, softmax_lse, cu_seqlens_q, cu_seqlens_k, rng_state = ctx.saved_tensors
-        if rng_state is not None:
-            cur_rng_state = torch.cuda.get_rng_state()
-            torch.cuda.set_rng_state(rng_state)
         dq, dk, dv = torch.empty_like(q), torch.empty_like(k), torch.empty_like(v)
         _flash_attn_backward(
             dout, q, k, v, out, softmax_lse, dq, dk, dv, cu_seqlens_q, cu_seqlens_k,
             ctx.max_seqlen_q, ctx.max_seqlen_k, ctx.dropout_p, ctx.softmax_scale, ctx.causal,
-            num_splits=1 if ctx.deterministic else 0,
+            rng_state=rng_state, num_splits=1 if ctx.deterministic else 0,
         )
-        if rng_state is not None:
-            torch.cuda.set_rng_state(cur_rng_state)
         return dq, dk, dv, None, None, None, None, None, None, None, None, None
 
 
 class FlashAttnQKVPackedSplitFunc(torch.autograd.Function):
 
     @staticmethod
     def forward(ctx, qkv, cu_seqlens, max_seqlen0, max_seqlen1, batch_size0, dropout_p,
```

### Comparing `flash_attn-1.0.1/flash_attn/flash_attn_triton.py` & `flash_attn-1.0.2/flash_attn/flash_attn_triton.py`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.1/flash_attn/flash_attn_triton_og.py` & `flash_attn-1.0.2/flash_attn/flash_attn_triton_og.py`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.1/flash_attn/flash_attn_triton_single_query.py` & `flash_attn-1.0.2/flash_attn/flash_attn_triton_single_query.py`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.1/flash_attn/flash_attn_triton_tmp.py` & `flash_attn-1.0.2/flash_attn/flash_attn_triton_tmp.py`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.1/flash_attn/flash_attn_triton_tmp_og.py` & `flash_attn-1.0.2/flash_attn/flash_attn_triton_tmp_og.py`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.1/flash_attn/flash_attn_triton_varlen.py` & `flash_attn-1.0.2/flash_attn/flash_attn_triton_varlen.py`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.1/flash_attn/flash_blocksparse_attention.py` & `flash_attn-1.0.2/flash_attn/flash_blocksparse_attention.py`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.1/flash_attn/flash_blocksparse_attn_interface.py` & `flash_attn-1.0.2/flash_attn/flash_blocksparse_attn_interface.py`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.1/flash_attn/fused_softmax.py` & `flash_attn-1.0.2/flash_attn/fused_softmax.py`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.1/flash_attn/layers/patch_embed.py` & `flash_attn-1.0.2/flash_attn/layers/patch_embed.py`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.1/flash_attn/layers/rotary.py` & `flash_attn-1.0.2/flash_attn/layers/rotary.py`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.1/flash_attn/losses/cross_entropy.py` & `flash_attn-1.0.2/flash_attn/losses/cross_entropy.py`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.1/flash_attn/losses/cross_entropy_apex.py` & `flash_attn-1.0.2/flash_attn/losses/cross_entropy_apex.py`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.1/flash_attn/losses/cross_entropy_parallel.py` & `flash_attn-1.0.2/flash_attn/losses/cross_entropy_parallel.py`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.1/flash_attn/models/bert.py` & `flash_attn-1.0.2/flash_attn/models/bert.py`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.1/flash_attn/models/gpt.py` & `flash_attn-1.0.2/flash_attn/models/gpt.py`

 * *Files 0% similar despite different names*

```diff
@@ -89,15 +89,15 @@
 
 
 def create_mlp_cls(config, layer_idx=None, process_group=None, device=None, dtype=None):
     factory_kwargs = {'device': device, 'dtype': dtype}
     inner_dim = config.n_inner if config.n_inner is not None else 4 * config.hidden_size
     fused_mlp = getattr(config, 'fused_mlp', False)
     if fused_mlp:
-        assert config.activation_function in ['gelu_new', 'gelu_fast', 'gelu_approx', 'relu']
+        assert config.activation_function in ['gelu_new', 'gelu_fast', 'gelu_approx', 'relu', 'sqrelu']
     fused_dense_sqrelu_dense = getattr(config, 'fused_dense_sqrelu_dense', False)
     if fused_dense_sqrelu_dense:
         assert config.activation_function == 'sqrelu', ('fused_dense_sqrelu_dense only '
                                                'supports approximate activation_function sqrelu')
     assert not (fused_dense_sqrelu_dense and fused_mlp)
     if process_group is not None:
         assert fused_mlp, 'Tensor Parallel is only implemented for FusedMLP'
@@ -119,15 +119,15 @@
         if isinstance(mlp_checkpoint_lvl, Sequence):
             assert layer_idx is not None
             mlp_checkpoint_lvl = mlp_checkpoint_lvl[layer_idx]
         if fused_mlp:
             if FusedMLP is None:
                 raise ImportError('fused_dense is not installed')
             activation = ('gelu_approx' if config.activation_function
-                          in ['gelu_new', 'gelu_fast', 'gelu_approx'] else 'relu')
+                          in ['gelu_new', 'gelu_fast', 'gelu_approx'] else config.activation_function)
             mlp_cls = FusedMLP if process_group is None else ParallelFusedMLP
             parallel_kwargs = ({'process_group': process_group,
                                 'sequence_parallel': getattr(config, 'sequence_parallel', True)}
                                if process_group is not None else {})
             mlp_cls = partial(mlp_cls, hidden_features=inner_dim, activation=activation,
                               checkpoint_lvl=mlp_checkpoint_lvl,
                               **parallel_kwargs, **factory_kwargs)
```

### Comparing `flash_attn-1.0.1/flash_attn/models/gpt_j.py` & `flash_attn-1.0.2/flash_attn/models/gpt_j.py`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.1/flash_attn/models/gpt_neox.py` & `flash_attn-1.0.2/flash_attn/models/gpt_neox.py`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.1/flash_attn/models/gptj.py` & `flash_attn-1.0.2/flash_attn/models/gptj.py`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.1/flash_attn/models/opt.py` & `flash_attn-1.0.2/flash_attn/models/opt.py`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.1/flash_attn/models/vit.py` & `flash_attn-1.0.2/flash_attn/models/vit.py`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.1/flash_attn/modules/block.py` & `flash_attn-1.0.2/flash_attn/modules/block.py`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.1/flash_attn/modules/embedding.py` & `flash_attn-1.0.2/flash_attn/modules/embedding.py`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.1/flash_attn/modules/mha.py` & `flash_attn-1.0.2/flash_attn/modules/mha.py`

 * *Files 0% similar despite different names*

```diff
@@ -491,15 +491,16 @@
                     assert inference_params.fused_ft_kernel
                     assert ft_attention is not None
                     context = ft_attention.single_query_attention(
                         *rearrange(qkv, 'b 1 three h d -> b three h d').unbind(dim=1),
                         *inference_params.key_value_memory_dict[self.layer_idx],
                         inference_params.lengths_per_sample, inference_params.sequence_len_offset,
                         self.rotary_emb_dim,
-                        not self.rotary_emb.interleaved  # neox_rotary_style
+                        # neox_rotary_style
+                        (not self.rotary_emb.interleaved) if self.rotary_emb_dim > 0 else True
                     )
                     context = rearrange(context, 'b h d -> b 1 h d')
         else:
             if not self.return_residual:
                 q = self.Wq(x if mixer_subset is None else x[:, mixer_subset])
                 kv = self.Wkv(x_kv if x_kv is not None else x)
             else:
@@ -605,15 +606,16 @@
                 assert inference_params.fused_ft_kernel
                 assert ft_attention is not None
                 context = ft_attention.single_query_attention(
                     *rearrange(qkv, 'b 1 three h d -> b three h d').unbind(dim=1),
                     *inference_params.key_value_memory_dict[self.layer_idx],
                     inference_params.lengths_per_sample, inference_params.sequence_len_offset,
                     self.rotary_emb_dim,
-                    not self.rotary_emb.interleaved  # neox_rotary_style
+                    # neox_rotary_style
+                    (not self.rotary_emb.interleaved) if self.rotary_emb_dim > 0 else True
                 )
                 context = rearrange(context, 'b h d -> b 1 h d')
         if seqlen is None:
             context = rearrange(context, 'b s h d -> b s (h d)')
         else:
             context = rearrange(context, 'b s h d -> (b s) (h d)')
         out = self.out_proj(context)
```

### Comparing `flash_attn-1.0.1/flash_attn/modules/mlp.py` & `flash_attn-1.0.2/flash_attn/modules/mlp.py`

 * *Files 20% similar despite different names*

```diff
@@ -13,15 +13,15 @@
 class Mlp(nn.Module):
 
     def __init__(self, in_features, hidden_features=None, out_features=None, activation=F.gelu,
                  return_residual=False, device=None, dtype=None):
         factory_kwargs = {'device': device, 'dtype': dtype}
         super().__init__()
         out_features = out_features or in_features
-        hidden_features = hidden_features or in_features
+        hidden_features = hidden_features or in_features * 4
         self.return_residual = return_residual
         self.fc1 = nn.Linear(in_features, hidden_features, **factory_kwargs)
         self.activation = activation
         self.fc2 = nn.Linear(hidden_features, out_features, **factory_kwargs)
 
     def forward(self, x):
         y = self.fc1(x)
```

### Comparing `flash_attn-1.0.1/flash_attn/ops/fused_dense.py` & `flash_attn-1.0.2/flash_attn/ops/fused_dense.py`

 * *Files 2% similar despite different names*

```diff
@@ -11,24 +11,19 @@
 from torch import Tensor
 from torch.distributed import ProcessGroup
 from torch.cuda.amp import custom_bwd, custom_fwd
 
 # import fused_dense_cuda  # from apex
 import fused_dense_lib as fused_dense_cuda
 
-from flash_attn.ops.gelu_activation import gelu_bwd
+from flash_attn.ops.activations import gelu_bwd, relu_bwd, sqrelu_fwd, sqrelu_bwd
 from flash_attn.utils.distributed import all_gather_raw, reduce_scatter_raw, all_reduce_raw
 from flash_attn.utils.distributed import reduce_scatter, all_reduce
 
 
-@torch.jit.script
-def relu_bwd(g, x):
-    return torch.where(x >= 0, g, 0.0).to(dtype=x.dtype)
-
-
 class FusedDenseFunc(torch.autograd.Function):
 
     @staticmethod
     @custom_fwd
     def forward(ctx, x, weight, bias, return_residual=False, process_group=None,
                 sequence_parallel=True):
         """
@@ -205,15 +200,17 @@
 
         checkpoint_lvl:
         0: no recomputation in the bwd
         1: recompute gelu_out / relu_out in the bwd
         2: recompute pre_act and gelu_out / relu_out in the bwd
         """
         assert -1 <= heuristic <= 4
-        assert activation in ['gelu_approx', 'relu']
+        assert activation in ['gelu_approx', 'relu', 'sqrelu']
+        if activation == 'sqrelu':
+            assert heuristic == -1
         if not save_pre_act:
             checkpoint_lvl = 2
         assert checkpoint_lvl in [0, 1, 2]
         ctx.return_residual = return_residual
         ctx.process_group = process_group
         ctx.sequence_parallel = sequence_parallel
         ctx.checkpoint_lvl = checkpoint_lvl
@@ -244,16 +241,17 @@
         batch_dim = batch_shape.numel()
         # https://github.com/pytorch/pytorch/blob/5b51849b48a7dbccd297286cc0110def4706f9e7/aten/src/ATen/native/cuda/Blas.cpp#L174
         if min(batch_dim, n, *weight1.shape, *weight2.shape) > 65535 * 32:
             raise RuntimeError('fused_dense only supports matrix dims <= 2M')
         if heuristic == -1:
             pre_act = F.linear(total_x, weight1, bias1)
             activation_fn = (partial(F.gelu, approximate='tanh') if activation == 'gelu_approx'
-                             else F.relu)
-            output1 = activation_fn(pre_act)
+                             else (sqrelu_fwd if activation == 'sqrelu' else F.relu))
+            with torch.jit.fuser('fuser2'):
+                output1 = activation_fn(pre_act)
             # This is before adding bias1
             # pre_act = F.linear(total_x.reshape(batch_dim, n), weight1)
             # with torch.jit.fuser('fuser2'):
             #     output1 = bias_gelu(pre_act, bias1)
         else:
             is_gelu = activation == 'gelu_approx'
             output1, *rest = fused_dense_cuda.linear_act_forward(
@@ -275,15 +273,15 @@
     @staticmethod
     @custom_bwd
     def backward(ctx, grad_output, *args):
         grad_output = grad_output.contiguous()
         checkpoint_lvl = ctx.checkpoint_lvl
         activation = ctx.activation
         activation_fn = (partial(F.gelu, approximate='tanh') if activation == 'gelu_approx'
-                            else F.relu)
+                         else (sqrelu_fwd if activation == 'sqrelu' else F.relu))
         if ctx.return_residual:
             grad_input, = args
             grad_input = grad_input.contiguous()
         process_group = ctx.process_group
         sequence_parallel = ctx.sequence_parallel
         x, weight1, weight2, *rest = ctx.saved_tensors
         if process_group is None or not sequence_parallel:
@@ -293,22 +291,24 @@
         if checkpoint_lvl in [0, 1]:
             if process_group is not None and sequence_parallel:
                 total_x, handle_x = all_gather_raw(x, process_group, async_op=True)
             if checkpoint_lvl == 0 or (checkpoint_lvl == 1 and activation == 'relu'):
                 pre_act, output1 = rest
             elif checkpoint_lvl == 1:
                 pre_act, = rest
-                output1 = activation_fn(pre_act)
+                with torch.jit.fuser('fuser2'):
+                    output1 = activation_fn(pre_act)
         elif checkpoint_lvl == 2:
             bias1, = rest
             if process_group is not None and sequence_parallel:
                 total_x, _ = all_gather_raw(x, process_group)
             if ctx.heuristic == -1:
                 pre_act = F.linear(total_x, weight1, bias1)
-                output1 = activation_fn(pre_act)
+                with torch.jit.fuser('fuser2'):
+                    output1 = activation_fn(pre_act)
             else:
                 output1, pre_act = fused_dense_cuda.linear_act_forward(
                     total_x.reshape(batch_dim, total_x.shape[-1]), weight1, bias1,
                     activation == 'gelu_approx', True, ctx.heuristic
                 )
 
         grad_output = grad_output.reshape(batch_dim, grad_output.shape[-1])
@@ -320,16 +320,17 @@
             )
         else:
             grad_weight2 = None
             grad_bias2 = grad_output if ctx.needs_input_grad[4] else None
         if ctx.heuristic == -1:
             # grad_pre_act = matmul_dgelu(grad_output, weight2, pre_act)
             grad_output1 = F.linear(grad_output, weight2.t())
+            activation_grad_fn = (gelu_bwd if activation == 'gelu_approx'
+                                  else (sqrelu_bwd if activation == 'sqrelu' else relu_bwd))
             with torch.jit.fuser('fuser2'):
-                activation_grad_fn = gelu_bwd if activation == 'gelu_approx' else relu_bwd
                 grad_pre_act = activation_grad_fn(grad_output1, pre_act)
         else:
             # The cublasLt epilogue has to compute both gelu/relu grad and bias grad, we can't
             # just compute gelu/relu grad
             grad_pre_act, grad_bias1 = fused_dense_cuda.bias_act_linear_dgrad_bgrad(
                 weight2, grad_output, pre_act, activation == 'gelu_approx', ctx.heuristic
             )
@@ -376,15 +377,15 @@
     x: Tensor, weight1: Tensor, weight2: Tensor, bias1: Optional[Tensor] = None,
     bias2: Optional[Tensor] = None, activation: str = 'gelu_approx',
     save_pre_act: bool = True, return_residual: bool = False,
     checkpoint_lvl: int = 0, heuristic: int = 0,
     process_group: Optional[ProcessGroup] = None,
     sequence_parallel: bool = True
 ):
-    assert activation in ['gelu_approx', 'relu']
+    assert activation in ['gelu_approx', 'relu', 'sqrelu']
     dtype_eligible = (x.dtype in [torch.float16, torch.bfloat16]
                       or (x.dtype == torch.float32 and torch.is_autocast_enabled()))
     # If we save pre-activation, dimension must be divisible by 128 (relu) or 8 (gelu)
     dim_eligible = not save_pre_act or (x.shape[-1] % (128 if activation == 'relu' else 8) == 0)
     if (x.is_cuda and weight1.is_cuda and weight2.is_cuda and (bias1 is None or bias1.is_cuda)
         and (bias2 is None or bias2.is_cuda) and dtype_eligible and dim_eligible):
         return FusedMLPFunc.apply(
@@ -424,23 +425,23 @@
                 For H100, we set heuristic=-1 for both fp16 and bf16 as the fused cuBlasLt implementation
                 is slower than the unfused version.
         return_residual: whether to return the input x along with the output. This is for
             performance reason: for post-norm architecture, returning the input allows us
             to fuse the backward of nn.Linear with the residual connection.
         """
         assert checkpoint_lvl in [0, 1, 2]
-        assert activation in ['gelu_approx', 'relu']
+        assert activation in ['gelu_approx', 'relu', 'sqrelu']
         factory_kwargs = {'device': device, 'dtype': dtype}
         super().__init__()
         if out_features is None:
             out_features = in_features
         self.activation = activation
         self.return_residual = return_residual
         self.checkpoint_lvl = checkpoint_lvl
-        self.heuristic = heuristic
+        self.heuristic = heuristic if activation != 'sqrelu' else -1
         self.fc1 = nn.Linear(in_features, hidden_features, bias=bias1, **factory_kwargs)
         self.fc2 = nn.Linear(hidden_features, out_features, bias=bias2, **factory_kwargs)
 
     def forward(self, x, process_group=None):
         dtype = x.dtype if not torch.is_autocast_enabled() else torch.get_autocast_gpu_dtype()
         if self.heuristic == 'auto':
             if self.activation == 'gelu_approx':
@@ -485,25 +486,25 @@
             -1: don't fuse gemm + gelu (separate kernel)
             0..4: use this heuristic for the algo section in the fused gemm + gelu
             'auto': heuristic will be picked automatically:
                 For CUDA >= 11.8, we set heuristic=0 for both fp16 and bf16 for best perf.
                 For CUDA <= 11.7, we set heuristic=1 for fp16 and heuristic=-1 for bf16.
         """
         assert checkpoint_lvl in [0, 1, 2]
-        assert activation in ['gelu_approx', 'relu']
+        assert activation in ['gelu_approx', 'relu', 'sqrelu']
         assert process_group is not None
         factory_kwargs = {'device': device, 'dtype': dtype}
         super().__init__()
         if out_features is None:
             out_features = in_features
         self.activation = activation
         self.process_group = process_group
         self.sequence_parallel = sequence_parallel
         self.checkpoint_lvl = checkpoint_lvl
-        self.heuristic = heuristic
+        self.heuristic = heuristic if activation != 'sqrelu' else -1
         self.fc1 = ColumnParallelLinear(in_features, hidden_features, process_group,
                                         bias=bias1, **factory_kwargs)
         self.fc2 = RowParallelLinear(hidden_features, out_features, process_group,
                                      bias=bias2, **factory_kwargs)
 
     def forward(self, x):
         dtype = x.dtype if not torch.is_autocast_enabled() else torch.get_autocast_gpu_dtype()
```

### Comparing `flash_attn-1.0.1/flash_attn/ops/gelu_activation.py` & `flash_attn-1.0.2/flash_attn/ops/gelu_activation.py`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.1/flash_attn/ops/layer_norm.py` & `flash_attn-1.0.2/flash_attn/ops/layer_norm.py`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.1/flash_attn/ops/rms_norm.py` & `flash_attn-1.0.2/flash_attn/ops/rms_norm.py`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.1/flash_attn/rotary.py` & `flash_attn-1.0.2/flash_attn/rotary.py`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.1/flash_attn/triton/fused_attention.py` & `flash_attn-1.0.2/flash_attn/triton/fused_attention.py`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.1/flash_attn/utils/benchmark.py` & `flash_attn-1.0.2/flash_attn/utils/benchmark.py`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.1/flash_attn/utils/distributed.py` & `flash_attn-1.0.2/flash_attn/utils/distributed.py`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.1/flash_attn/utils/generation.py` & `flash_attn-1.0.2/flash_attn/utils/generation.py`

 * *Files 3% similar despite different names*

```diff
@@ -103,18 +103,20 @@
         inference_params.max_batch_size = batch_size
         inference_params.sequence_len_offset = 0
     else:
         inference_params = InferenceParams(max_sequence_len=max_length, max_batch_size=batch_size,
                                            fused_ft_kernel=fused_ft_kernel)
     scores = []
     with torch.inference_mode():
-        logits = model(input_ids, inference_params=inference_params).logits[:, -1]
         if timing:
+            if tensor_parallel > 1:
+                torch.distributed.barrier()
             torch.cuda.synchronize()
             start = time.time()
+        logits = model(input_ids, inference_params=inference_params).logits[:, -1]
         if vocab_size is not None:
             logits = logits[..., :vocab_size]
         scores.append(logits if not cg else logits.clone())
         if teacher_outputs is None or teacher_output_len <= seqlen_og:
             next_token = sample(logits, top_k=top_k, top_p=top_p, temperature=temperature)
         else:
             next_token = teacher_outputs[:, seqlen_og]
@@ -139,16 +141,18 @@
             sequences.append(next_token)
             inference_params.sequence_len_offset += 1
             if eos_token_id is not None and (next_token == eos_token_id).all():
                 break
             if inference_params.sequence_len_offset >= max_length - 1:
                 break
         if timing:
+            if tensor_parallel > 1:
+                torch.distributed.barrier()
             torch.cuda.synchronize()
-            print(f'Decoding time: {(time.time() - start) * 1000:.0f}ms')
+            print(f'Prompt processing + decoding time: {(time.time() - start) * 1000:.0f}ms')
     output_cls = GreedySearchDecoderOnlyOutput if top_k == 1 else SampleDecoderOnlyOutput
     return output_cls(
         sequences=torch.cat([input_ids, torch.stack(sequences, dim=1)], dim=1),
         scores=tuple(scores)
     )
 
 
@@ -182,17 +186,17 @@
     This is used to determine which cuda graph to use.
     Arguments:
         seqlen: int
     """
     return 0 if seqlen < 32 else (1 if seqlen < 2048 else 2)
 
 
-def seqlen_type_to_seqlen(seqlen_type: int) -> int:
+def seqlen_type_to_max_seqlen(seqlen_type: int) -> int:
     assert seqlen_type in [0, 1, 2]
-    return 1 if seqlen_type == 0 else (32 if seqlen_type == 1 else 2048)
+    return 32 if seqlen_type == 0 else (2048 if seqlen_type == 1 else 2**32)
 
 
 @dataclass
 class DecodingCGCache:
     max_batch_size: int = 0
     max_seqlen: int = 0
     device = None
@@ -231,35 +235,37 @@
             max_sequence_len=max_seqlen, max_batch_size=batch_size,
             sequence_len_offset=seqlen_og, key_value_memory_dict=kv_cache, fused_ft_kernel=True,
             lengths_per_sample=lengths_per_sample
         )
         cache.mempool = torch.cuda.graphs.graph_pool_handle()
     for s_type in range(seqlen_to_seqlen_type(seqlen_og), seqlen_to_seqlen_type(max_seqlen) + 1):
         if s_type not in cache.callables:
-            seqlen = min(max(seqlen_og, seqlen_type_to_seqlen(s_type)), max_seqlen)
+            max_seqlen_ = min(max(seqlen_og, seqlen_type_to_max_seqlen(s_type)), max_seqlen)
             cache.callables[s_type] = capture_graph(
-                model, cache.inference_params, batch_size, seqlen_og, seqlen, mempool=cache.mempool,
+                model, cache.inference_params, batch_size, max_seqlen_, mempool=cache.mempool,
                 n_warmups=n_warmups
             )
 
     def dispatch(input_ids, position_ids, seqlen):
         return cache.callables[seqlen_to_seqlen_type(seqlen)](input_ids, position_ids, seqlen)
 
     cache.run = dispatch
-    cache.inference_params.sequence_length_offset = 0  # Reset so it's not confusing
+    cache.inference_params.sequence_len_offset = 0  # Reset so it's not confusing
     return cache
 
 
-def capture_graph(model, inference_params, batch_size, seqlen_og, max_seqlen, mempool=None,
-                  n_warmups=2):
-    assert max_seqlen >= seqlen_og
+def capture_graph(model, inference_params, batch_size, max_seqlen, mempool=None, n_warmups=2):
     device = next(iter(model.parameters())).device
     input_ids = torch.full((batch_size, 1), 0, dtype=torch.long, device=device)
     position_ids = torch.full((batch_size, 1), 0, dtype=torch.long, device=device)
-    inference_params.lengths_per_sample[:] = seqlen_og
+    sequence_len_offset_og = inference_params.sequence_len_offset
+    # TD [2023-04-14]: important for correctness of the FT's attention kernel, as seqlen_cpu is
+    # used to determine the size of smem. Hence seqlen_cpu must be >= lengths_per_sample.
+    inference_params.sequence_len_offset = max_seqlen - 1
+    inference_params.lengths_per_sample[:] = max_seqlen - 1
 
     # Warmup before capture
     s = torch.cuda.Stream()
     s.wait_stream(torch.cuda.current_stream())
     with torch.cuda.stream(s):
         for _ in range(n_warmups):
             logits = model(input_ids, position_ids=position_ids,
@@ -281,8 +287,9 @@
     def run(new_input_ids, new_position_ids, seqlen):
         inference_params.lengths_per_sample[:] = seqlen
         input_ids.copy_(new_input_ids)
         position_ids.copy_(new_position_ids)
         graph.replay()
         return logits
 
+    inference_params.sequence_len_offset = sequence_len_offset_og
     return run
```

### Comparing `flash_attn-1.0.1/flash_attn/utils/pretrained.py` & `flash_attn-1.0.2/flash_attn/utils/pretrained.py`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.1/flash_attn.egg-info/PKG-INFO` & `flash_attn-1.0.2/flash_attn.egg-info/PKG-INFO`

 * *Files 1% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 Metadata-Version: 2.1
 Name: flash-attn
-Version: 1.0.1
+Version: 1.0.2
 Summary: Flash Attention: Fast and Memory-Efficient Exact Attention
 Home-page: https://github.com/HazyResearch/flash-attention
 Author: Tri Dao
 Author-email: trid@stanford.edu
 License: UNKNOWN
 Platform: UNKNOWN
 Classifier: Programming Language :: Python :: 3
```

### Comparing `flash_attn-1.0.1/flash_attn.egg-info/SOURCES.txt` & `flash_attn-1.0.2/flash_attn.egg-info/SOURCES.txt`

 * *Files 1% similar despite different names*

```diff
@@ -80,14 +80,15 @@
 csrc/flash_attn/cutlass/examples/23_ampere_gemm_operand_reduction_fusion/ampere_gemm_operand_reduction_fusion.cu
 csrc/flash_attn/cutlass/examples/24_gemm_grouped/gemm_grouped.cu
 csrc/flash_attn/cutlass/examples/25_ampere_fprop_mainloop_fusion/ampere_3d_fprop_mainloop_fusion.cu
 csrc/flash_attn/cutlass/examples/25_ampere_fprop_mainloop_fusion/ampere_fprop_mainloop_fusion.cu
 csrc/flash_attn/cutlass/examples/26_ampere_wgrad_mainloop_fusion/ampere_wgrad_mainloop_fusion.cu
 csrc/flash_attn/cutlass/examples/27_ampere_3xtf32_fast_accurate_tensorop_gemm/27_ampere_3xtf32_fast_accurate_tensorop_gemm.cu
 csrc/flash_attn/cutlass/examples/28_ampere_3xtf32_fast_accurate_tensorop_fprop/ampere_3xtf32_fast_accurate_tensorop_fprop.cu
+csrc/flash_attn/cutlass/examples/29_ampere_3xtf32_fast_accurate_tensorop_complex_gemm/29_3xtf32_complex_gemm.cu
 csrc/flash_attn/cutlass/examples/29_ampere_3xtf32_fast_accurate_tensorop_complex_gemm/29_ampere_3xtf32_fast_accurate_tensorop_complex_gemm.cu
 csrc/flash_attn/cutlass/examples/30_wgrad_split_k/30_wgrad_split_k.cu
 csrc/flash_attn/cutlass/examples/31_basic_syrk/basic_syrk.cu
 csrc/flash_attn/cutlass/examples/32_basic_trmm/basic_trmm.cu
 csrc/flash_attn/cutlass/examples/33_ampere_3xtf32_tensorop_symm/ampere_3xtf32_tensorop_symm.cu
 csrc/flash_attn/cutlass/examples/34_transposed_conv2d/34_transposed_conv2d.cu
 csrc/flash_attn/cutlass/examples/35_gemm_softmax/gemm_softmax.cu
@@ -95,26 +96,30 @@
 csrc/flash_attn/cutlass/examples/35_gemm_softmax/gemm_with_softmax.h
 csrc/flash_attn/cutlass/examples/36_gather_scatter_fusion/gather_scatter_fusion.cu
 csrc/flash_attn/cutlass/examples/37_gemm_layernorm_gemm_fusion/gemm_layernorm.cu
 csrc/flash_attn/cutlass/examples/37_gemm_layernorm_gemm_fusion/gemm_with_epilogue_visitor.h
 csrc/flash_attn/cutlass/examples/37_gemm_layernorm_gemm_fusion/gemm_with_layernorm.h
 csrc/flash_attn/cutlass/examples/38_syr2k_grouped/syr2k_grouped.cu
 csrc/flash_attn/cutlass/examples/39_gemm_permute/gemm_permute.cu
+csrc/flash_attn/cutlass/examples/39_gemm_permute/layouts.h
+csrc/flash_attn/cutlass/examples/39_gemm_permute/permute_info.h
 csrc/flash_attn/cutlass/examples/41_fused_multi_head_attention/attention_scaling_coefs_updater.h
 csrc/flash_attn/cutlass/examples/41_fused_multi_head_attention/debug_utils.h
 csrc/flash_attn/cutlass/examples/41_fused_multi_head_attention/default_fmha_grouped.h
 csrc/flash_attn/cutlass/examples/41_fused_multi_head_attention/epilogue_pipelined.h
 csrc/flash_attn/cutlass/examples/41_fused_multi_head_attention/epilogue_rescale_output.h
 csrc/flash_attn/cutlass/examples/41_fused_multi_head_attention/epilogue_thread_apply_logsumexp.h
 csrc/flash_attn/cutlass/examples/41_fused_multi_head_attention/find_default_mma.h
 csrc/flash_attn/cutlass/examples/41_fused_multi_head_attention/fmha_grouped.h
 csrc/flash_attn/cutlass/examples/41_fused_multi_head_attention/fmha_grouped_problem_visitor.h
+csrc/flash_attn/cutlass/examples/41_fused_multi_head_attention/fused_multi_head_attention_backward.cu
 csrc/flash_attn/cutlass/examples/41_fused_multi_head_attention/fused_multihead_attention_fixed_seqlen.cu
 csrc/flash_attn/cutlass/examples/41_fused_multi_head_attention/fused_multihead_attention_variable_seqlen.cu
 csrc/flash_attn/cutlass/examples/41_fused_multi_head_attention/gemm_kernel_utils.h
+csrc/flash_attn/cutlass/examples/41_fused_multi_head_attention/kernel_backward.h
 csrc/flash_attn/cutlass/examples/41_fused_multi_head_attention/kernel_forward.h
 csrc/flash_attn/cutlass/examples/41_fused_multi_head_attention/mma_from_smem.h
 csrc/flash_attn/cutlass/examples/41_fused_multi_head_attention/epilogue/epilogue_pipelined.h
 csrc/flash_attn/cutlass/examples/41_fused_multi_head_attention/epilogue/epilogue_rescale_output.h
 csrc/flash_attn/cutlass/examples/41_fused_multi_head_attention/epilogue/epilogue_thread_apply_logsumexp.h
 csrc/flash_attn/cutlass/examples/41_fused_multi_head_attention/gemm/custom_mma.h
 csrc/flash_attn/cutlass/examples/41_fused_multi_head_attention/gemm/custom_mma_base.h
@@ -153,15 +158,18 @@
 csrc/flash_attn/cutlass/examples/45_dual_gemm/threadblock/dual_epilogue.h
 csrc/flash_attn/cutlass/examples/45_dual_gemm/threadblock/dual_mma_base.h
 csrc/flash_attn/cutlass/examples/45_dual_gemm/threadblock/dual_mma_multistage.h
 csrc/flash_attn/cutlass/examples/46_depthwise_simt_conv2dfprop/depthwise_simt_conv2dfprop.cu
 csrc/flash_attn/cutlass/examples/47_ampere_gemm_universal_streamk/ampere_gemm_universal_streamk.cu
 csrc/flash_attn/cutlass/examples/48_hopper_warp_specialized_gemm/48_hopper_warp_specialized_gemm.cu
 csrc/flash_attn/cutlass/examples/49_hopper_gemm_schedules_with_collective_builder/49_hopper_gemm_schedules_with_collective_builder.cu
+csrc/flash_attn/cutlass/examples/49_hopper_gemm_with_collective_builder/49_collective_builder.cu
 csrc/flash_attn/cutlass/examples/50_hopper_gemm_with_epilogue_swizzle/50_hopper_gemm_with_epilogue_swizzle.cu
+csrc/flash_attn/cutlass/examples/51_hopper_gett/51_hopper_gett.cu
+csrc/flash_attn/cutlass/examples/51_hopper_gett/gett_kernel.cuh
 csrc/flash_attn/cutlass/examples/60_cutlass_import/main.cpp
 csrc/flash_attn/cutlass/examples/common/helper.h
 csrc/flash_attn/cutlass/examples/cute/tutorial/sgemm_nt_1.cu
 csrc/flash_attn/cutlass/include/cutlass/aligned_buffer.h
 csrc/flash_attn/cutlass/include/cutlass/array.h
 csrc/flash_attn/cutlass/include/cutlass/array_planar_complex.h
 csrc/flash_attn/cutlass/include/cutlass/array_subbyte.h
@@ -588,15 +596,52 @@
 csrc/flash_attn/cutlass/include/cutlass/transform/threadblock/regular_tile_iterator.h
 csrc/flash_attn/cutlass/include/cutlass/transform/threadblock/regular_tile_iterator_pitch_linear.h
 csrc/flash_attn/cutlass/include/cutlass/transform/threadblock/regular_tile_iterator_pitch_linear_2dthreadtile.h
 csrc/flash_attn/cutlass/include/cutlass/transform/threadblock/regular_tile_iterator_tensor_op.h
 csrc/flash_attn/cutlass/include/cutlass/transform/threadblock/regular_tile_iterator_tensor_op_sm70.h
 csrc/flash_attn/cutlass/include/cutlass/transform/threadblock/vector_iterator.h
 csrc/flash_attn/cutlass/include/cutlass/transform/warp/vector_fragment_iterator.h
+csrc/flash_attn/cutlass/python/cutlass/cpp/compiler.h
+csrc/flash_attn/cutlass/python/cutlass/cpp/cutlass_bindings.cpp
+csrc/flash_attn/cutlass/python/cutlass/cpp/library.h
+csrc/flash_attn/cutlass/python/cutlass/cpp/include/arch.h
+csrc/flash_attn/cutlass/python/cutlass/cpp/include/swizzling.h
+csrc/flash_attn/cutlass/python/cutlass/cpp/include/tensor_coord.h
+csrc/flash_attn/cutlass/python/cutlass/cpp/include/tensor_ref_view.h
+csrc/flash_attn/cutlass/python/cutlass/cpp/include/types.h
+csrc/flash_attn/cutlass/python/cutlass/cpp/include/conv/conv_problem_size.h
+csrc/flash_attn/cutlass/python/cutlass/cpp/include/conv/convolution.h
+csrc/flash_attn/cutlass/python/cutlass/cpp/include/conv/host.h
+csrc/flash_attn/cutlass/python/cutlass/cpp/include/epilogue/epilogue_visitor_generic.h
+csrc/flash_attn/cutlass/python/cutlass/cpp/include/epilogue/epilogue_visitor_with_layernorm.h
+csrc/flash_attn/cutlass/python/cutlass/cpp/include/epilogue/epilogue_visitor_op/binary_ops.h
+csrc/flash_attn/cutlass/python/cutlass/cpp/include/epilogue/epilogue_visitor_op/unary_ops.h
+csrc/flash_attn/cutlass/python/cutlass/cpp/include/epilogue/epilogue_visitor_op/visitor_op_accumulator.h
+csrc/flash_attn/cutlass/python/cutlass/cpp/include/epilogue/epilogue_visitor_op/visitor_op_binary.h
+csrc/flash_attn/cutlass/python/cutlass/cpp/include/epilogue/epilogue_visitor_op/visitor_op_column_broadcast.h
+csrc/flash_attn/cutlass/python/cutlass/cpp/include/epilogue/epilogue_visitor_op/visitor_op_column_reduction.h
+csrc/flash_attn/cutlass/python/cutlass/cpp/include/epilogue/epilogue_visitor_op/visitor_op_linear_combination.h
+csrc/flash_attn/cutlass/python/cutlass/cpp/include/epilogue/epilogue_visitor_op/visitor_op_row_broadcast.h
+csrc/flash_attn/cutlass/python/cutlass/cpp/include/epilogue/epilogue_visitor_op/visitor_op_row_reduction.h
+csrc/flash_attn/cutlass/python/cutlass/cpp/include/epilogue/epilogue_visitor_op/visitor_op_tensor_input.h
+csrc/flash_attn/cutlass/python/cutlass/cpp/include/epilogue/epilogue_visitor_op/visitor_op_tensor_output.h
+csrc/flash_attn/cutlass/python/cutlass/cpp/include/epilogue/epilogue_visitor_op/visitor_op_unary.h
+csrc/flash_attn/cutlass/python/cutlass/cpp/include/gemm/gemm.h
+csrc/flash_attn/cutlass/python/cutlass/cpp/include/gemm/gemm_universal_with_visitor.h
+csrc/flash_attn/cutlass/python/cutlass/cpp/include/gemm/host.h
+csrc/flash_attn/cutlass/python/cutlass/cpp/include/layout/layout.h
+csrc/flash_attn/cutlass/python/cutlass/cpp/include/layout/matrix.h
+csrc/flash_attn/cutlass/python/cutlass/cpp/include/layout/tensor.h
+csrc/flash_attn/cutlass/python/cutlass/cpp/test/conv/conv_problems.h
+csrc/flash_attn/cutlass/python/cutlass/cpp/test/conv/convolution.h
+csrc/flash_attn/cutlass/python/cutlass/cpp/test/conv/host.h
+csrc/flash_attn/cutlass/python/cutlass/cpp/test/gemm/gemm.h
+csrc/flash_attn/cutlass/python/cutlass/cpp/test/gemm/host.h
 csrc/flash_attn/cutlass/test/unit/test_unit.cpp
+csrc/flash_attn/cutlass/test/unit/cluster_launch/cluster_launch.cu
 csrc/flash_attn/cutlass/test/unit/common/cutlass_unit_test.h
 csrc/flash_attn/cutlass/test/unit/common/filter_architecture.cpp
 csrc/flash_attn/cutlass/test/unit/conv/device/cache_testbed_output.h
 csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_dgrad_implicit_gemm_cf32nhwc_cf32nhwc_cf32nhwc_simt_f32_sm50.cu
 csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_dgrad_implicit_gemm_cf32nhwc_cf32nhwc_cf32nhwc_simt_f32_sm80.cu
 csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_dgrad_implicit_gemm_f16nhwc_f16nhwc_f16nhwc_tensor_op_f16_sm80.cu
 csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_dgrad_implicit_gemm_f16nhwc_f16nhwc_f32nhwc_tensor_op_f32_sm70.cu
@@ -673,30 +718,35 @@
 csrc/flash_attn/cutlass/test/unit/core/quaternion.cu
 csrc/flash_attn/cutlass/test/unit/core/tensor_ref.cu
 csrc/flash_attn/cutlass/test/unit/core/tensor_view.cu
 csrc/flash_attn/cutlass/test/unit/core/test_unit_core.cpp
 csrc/flash_attn/cutlass/test/unit/core/tfloat32.cu
 csrc/flash_attn/cutlass/test/unit/cute/ampere/cp_async.cu
 csrc/flash_attn/cutlass/test/unit/cute/ampere/ldsm.cu
+csrc/flash_attn/cutlass/test/unit/cute/core/array_subbyte.cpp
 csrc/flash_attn/cutlass/test/unit/cute/core/bitfield.cpp
 csrc/flash_attn/cutlass/test/unit/cute/core/coalesce.cpp
+csrc/flash_attn/cutlass/test/unit/cute/core/compact_xmajor.cpp
 csrc/flash_attn/cutlass/test/unit/cute/core/compare.cpp
 csrc/flash_attn/cutlass/test/unit/cute/core/complement.cpp
 csrc/flash_attn/cutlass/test/unit/cute/core/composition.cpp
 csrc/flash_attn/cutlass/test/unit/cute/core/inverse_left.cpp
 csrc/flash_attn/cutlass/test/unit/cute/core/inverse_right.cpp
 csrc/flash_attn/cutlass/test/unit/cute/core/logical_divide.cpp
 csrc/flash_attn/cutlass/test/unit/cute/core/logical_product.cpp
 csrc/flash_attn/cutlass/test/unit/cute/core/mixedbits.cpp
 csrc/flash_attn/cutlass/test/unit/cute/core/transform.cpp
 csrc/flash_attn/cutlass/test/unit/cute/core/tuple.cpp
+csrc/flash_attn/cutlass/test/unit/cute/hopper/bulk_load.cu
+csrc/flash_attn/cutlass/test/unit/cute/hopper/bulk_store.cu
 csrc/flash_attn/cutlass/test/unit/cute/hopper/stsm.cu
 csrc/flash_attn/cutlass/test/unit/cute/hopper/tma_load.cu
 csrc/flash_attn/cutlass/test/unit/cute/hopper/tma_store.cu
 csrc/flash_attn/cutlass/test/unit/cute/layout/layout_operator.cu
+csrc/flash_attn/cutlass/test/unit/cute/msvc_compilation/tuple.cpp
 csrc/flash_attn/cutlass/test/unit/epilogue/thread/activation.cu
 csrc/flash_attn/cutlass/test/unit/epilogue/thread/linear_combination.cu
 csrc/flash_attn/cutlass/test/unit/epilogue/thread/linear_combination_planar_complex.cu
 csrc/flash_attn/cutlass/test/unit/epilogue/threadblock/epilogue_planar_complex.cu
 csrc/flash_attn/cutlass/test/unit/epilogue/threadblock/epilogue_simt.cu
 csrc/flash_attn/cutlass/test/unit/epilogue/threadblock/epilogue_simt_sm60.cu
 csrc/flash_attn/cutlass/test/unit/epilogue/threadblock/epilogue_simt_sm61.cu
@@ -822,14 +872,15 @@
 csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_s4t_s4n_s32t_wmma_tensor_op_s32_sm75.cu
 csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_s4t_s4n_s4n_tensor_op_s32_sm75.cu
 csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_s4t_s4n_s4n_tensor_op_s32_sm80.cu
 csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_s4t_s4n_s4t_tensor_op_s32_sm75.cu
 csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_s4t_s4n_s4t_tensor_op_s32_sm80.cu
 csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_s8n_s8t_s8n_tensor_op_s32_sm75.cu
 csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_s8n_s8t_s8n_tensor_op_s32_sm80.cu
+csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_s8t_s8n_f16t_tensor_op_s32_sm80.cu
 csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_s8t_s8n_s32n_tensor_op_s32_sm75.cu
 csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_s8t_s8n_s32n_tensor_op_s32_sm80.cu
 csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_s8t_s8n_s32n_wmma_tensor_op_s32_sm72.cu
 csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_s8t_s8n_s32t_tensor_op_s32_sm75.cu
 csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_s8t_s8n_s32t_tensor_op_s32_sm80.cu
 csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_s8t_s8n_s32t_tensor_op_s32_sparse_sm80.cu
 csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_s8t_s8n_s32t_wmma_tensor_op_s32_sm72.cu
@@ -926,20 +977,28 @@
 csrc/flash_attn/cutlass/test/unit/gemm/device/sm80_gemm_tf32_tf32_f32_tensor_op_f32.cu
 csrc/flash_attn/cutlass/test/unit/gemm/device/sm90_gemm_bf16_bf16_bf16_alignx_tensor_op_f32.cu
 csrc/flash_attn/cutlass/test/unit/gemm/device/sm90_gemm_bf16_bf16_bf16_tensor_op_f32.cu
 csrc/flash_attn/cutlass/test/unit/gemm/device/sm90_gemm_f16_f16_f16_alignx_tensor_op.cu
 csrc/flash_attn/cutlass/test/unit/gemm/device/sm90_gemm_f16_f16_f16_tensor_op.cu
 csrc/flash_attn/cutlass/test/unit/gemm/device/sm90_gemm_f16_f16_f16_tensor_op_f32_cluster_unspecialized.cu
 csrc/flash_attn/cutlass/test/unit/gemm/device/sm90_gemm_f16_f16_f16_tensor_op_f32_cluster_warpspecialized.cu
+csrc/flash_attn/cutlass/test/unit/gemm/device/sm90_gemm_f16_f16_f16_tensor_op_f32_cluster_warpspecialized_cooperative.cu
+csrc/flash_attn/cutlass/test/unit/gemm/device/sm90_gemm_f16_f16_f16_tensor_op_f32_cluster_warpspecialized_cooperative_bias_elementwise.cu
 csrc/flash_attn/cutlass/test/unit/gemm/device/sm90_gemm_f16_f16_f16_tensor_op_f32_cluster_warpspecialized_persistent.cu
+csrc/flash_attn/cutlass/test/unit/gemm/device/sm90_gemm_f16_f16_f16_tensor_op_f32_cluster_warpspecialized_pingpong.cu
+csrc/flash_attn/cutlass/test/unit/gemm/device/sm90_gemm_f16_f16_f16_tensor_op_f32_cluster_warpspecialized_pingpong_bias_elementwise.cu
+csrc/flash_attn/cutlass/test/unit/gemm/device/sm90_gemm_f16_f16_f16_tensor_op_f32_tensor_broadcast.cu
 csrc/flash_attn/cutlass/test/unit/gemm/device/sm90_gemm_f32_f32_f32_tensor_op_f32.cu
+csrc/flash_attn/cutlass/test/unit/gemm/device/sm90_gemm_f32_f32_f32_tensor_op_f32_tensor_broadcast.cu
 csrc/flash_attn/cutlass/test/unit/gemm/device/sm90_gemm_s8_s8_s8_alignx_tensor_op_s32.cu
 csrc/flash_attn/cutlass/test/unit/gemm/device/sm90_gemm_s8_s8_s8_tensor_op_s32.cu
+csrc/flash_attn/cutlass/test/unit/gemm/device/sm90_gemm_s8_s8_s8_tensor_op_s32_tensor_broadcast.cu
 csrc/flash_attn/cutlass/test/unit/gemm/device/sm90_gemm_tf32_tf32_f32_alignx_tensor_op_f32.cu
 csrc/flash_attn/cutlass/test/unit/gemm/device/sm90_gemm_tf32_tf32_f32_tensor_op_f32.cu
+csrc/flash_attn/cutlass/test/unit/gemm/device/sm90_gemm_tf32_tf32_f32_tensor_op_f32_gmma_rs_cluster_warpspecialized.cu
 csrc/flash_attn/cutlass/test/unit/gemm/device/symm_cf32n_cf32n_tensor_op_f32_ls_sm80.cu
 csrc/flash_attn/cutlass/test/unit/gemm/device/symm_cf32n_cf32n_tensor_op_f32_rs_sm80.cu
 csrc/flash_attn/cutlass/test/unit/gemm/device/symm_cf32n_cf32n_tensor_op_fast_f32_ls_sm80.cu
 csrc/flash_attn/cutlass/test/unit/gemm/device/symm_cf32n_cf32n_tensor_op_fast_f32_rs_sm80.cu
 csrc/flash_attn/cutlass/test/unit/gemm/device/symm_cf64_cf64_cf64_tensor_op_f64_sm90.cu
 csrc/flash_attn/cutlass/test/unit/gemm/device/symm_cf64n_cf64n_cf64n_tensor_op_ls_f64_gaussian_sm80.cu
 csrc/flash_attn/cutlass/test/unit/gemm/device/symm_cf64n_cf64n_cf64n_tensor_op_ls_f64_sm80.cu
@@ -1096,14 +1155,15 @@
 csrc/flash_attn/cutlass/test/unit/pipeline/testbed.h
 csrc/flash_attn/cutlass/test/unit/reduction/device/tensor_reduce_contiguous.cu
 csrc/flash_attn/cutlass/test/unit/reduction/device/tensor_reduce_strided.cu
 csrc/flash_attn/cutlass/test/unit/reduction/kernel/reduce_splitk.cu
 csrc/flash_attn/cutlass/test/unit/reduction/kernel/reduce_splitk_testbed.h
 csrc/flash_attn/cutlass/test/unit/reduction/thread/reduction_thread.cu
 csrc/flash_attn/cutlass/test/unit/reduction/thread/testbed.h
+csrc/flash_attn/cutlass/test/unit/substrate/dependent_false.cpp
 csrc/flash_attn/cutlass/test/unit/transform/threadblock/predicated_tile_iterator.cu
 csrc/flash_attn/cutlass/test/unit/transform/threadblock/regular_tile_iterator_tensor_op.cu
 csrc/flash_attn/cutlass/test/unit/util/cutlass_test_levels.cu
 csrc/flash_attn/cutlass/test/unit/util/tensor_reduce.cu
 csrc/flash_attn/cutlass/tools/library/include/cutlass/library/arch_mappings.h
 csrc/flash_attn/cutlass/tools/library/include/cutlass/library/handle.h
 csrc/flash_attn/cutlass/tools/library/include/cutlass/library/library.h
@@ -1481,14 +1541,15 @@
 flash_attn/models/vit.py
 flash_attn/modules/__init__.py
 flash_attn/modules/block.py
 flash_attn/modules/embedding.py
 flash_attn/modules/mha.py
 flash_attn/modules/mlp.py
 flash_attn/ops/__init__.py
+flash_attn/ops/activations.py
 flash_attn/ops/fused_dense.py
 flash_attn/ops/gelu_activation.py
 flash_attn/ops/layer_norm.py
 flash_attn/ops/rms_norm.py
 flash_attn/triton/__init__.py
 flash_attn/triton/fused_attention.py
 flash_attn/utils/__init__.py
```

### Comparing `flash_attn-1.0.1/setup.py` & `flash_attn-1.0.2/setup.py`

 * *Files 1% similar despite different names*

```diff
@@ -158,15 +158,15 @@
             Path(this_dir) / 'csrc' / 'flash_attn' / 'cutlass' / 'include',
         ],
     )
 )
 
 setup(
     name="flash_attn",
-    version="1.0.1",
+    version="1.0.2",
     packages=find_packages(
         exclude=("build", "csrc", "include", "tests", "dist", "docs", "benchmarks", "flash_attn.egg-info",)
     ),
     author="Tri Dao",
     author_email="trid@stanford.edu",
     description="Flash Attention: Fast and Memory-Efficient Exact Attention",
     long_description=long_description,
@@ -179,9 +179,10 @@
     ],
     ext_modules=ext_modules,
     cmdclass={"build_ext": BuildExtension} if ext_modules else {},
     python_requires=">=3.7",
     install_requires=[
         "torch",
         "einops",
+        "packaging",
     ],
 )
```

